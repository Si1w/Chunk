Chunking Method: FunctionLevel

------------------------- Chunk 1 (2 lines) -------------------------
def exists(val):
    return val is not None
Metadata:
  filepath: 
  function_name: exists
  type: function_definition
  chunk_size: 100
  line_count: 2
  start_line_no: 39
  end_line_no: 40

---------------------------------------------------------------------

------------------------- Chunk 2 (4 lines) -------------------------
def default(val, d):
    if exists(val):
        return val
    return d() if callable(d) else d
Metadata:
  filepath: 
  function_name: default
  type: function_definition
  chunk_size: 100
  line_count: 4
  start_line_no: 42
  end_line_no: 45

---------------------------------------------------------------------

------------------------- Chunk 3 (5 lines) -------------------------
def cast_tuple(val, length = 1):
    if isinstance(val, list):
        val = tuple(val)

    return val if isinstance(val, tuple) else ((val,) * length)
Metadata:
  filepath: 
  function_name: cast_tuple
  type: function_definition
  chunk_size: 100
  line_count: 5
  start_line_no: 47
  end_line_no: 51

---------------------------------------------------------------------

------------------------- Chunk 4 (5 lines) -------------------------
def find_first(fn, arr):
    for ind, el in enumerate(arr):
        if fn(el):
            return ind
    return -1
Metadata:
  filepath: 
  function_name: find_first
  type: function_definition
  chunk_size: 100
  line_count: 5
  start_line_no: 53
  end_line_no: 57

---------------------------------------------------------------------

------------------------- Chunk 5 (3 lines) -------------------------
def pick_and_pop(keys, d):
    values = list(map(lambda key: d.pop(key), keys))
    return dict(zip(keys, values))
Metadata:
  filepath: 
  function_name: pick_and_pop
  type: function_definition
  chunk_size: 100
  line_count: 3
  start_line_no: 59
  end_line_no: 61

---------------------------------------------------------------------

------------------------- Chunk 6 (7 lines) -------------------------
def group_dict_by_key(cond, d):
    return_val = [dict(),dict()]
    for key in d.keys():
        match = bool(cond(key))
        ind = int(not match)
        return_val[ind][key] = d[key]
    return (*return_val,)
Metadata:
  filepath: 
  function_name: group_dict_by_key
  type: function_definition
  chunk_size: 100
  line_count: 7
  start_line_no: 63
  end_line_no: 69

---------------------------------------------------------------------

------------------------- Chunk 7 (2 lines) -------------------------
def string_begins_with(prefix, str):
    return str.startswith(prefix)
Metadata:
  filepath: 
  function_name: string_begins_with
  type: function_definition
  chunk_size: 100
  line_count: 2
  start_line_no: 71
  end_line_no: 72

---------------------------------------------------------------------

------------------------- Chunk 8 (2 lines) -------------------------
def group_by_key_prefix(prefix, d):
    return group_dict_by_key(partial(string_begins_with, prefix), d)
Metadata:
  filepath: 
  function_name: group_by_key_prefix
  type: function_definition
  chunk_size: 100
  line_count: 2
  start_line_no: 74
  end_line_no: 75

---------------------------------------------------------------------

------------------------- Chunk 9 (4 lines) -------------------------
def groupby_prefix_and_trim(prefix, d):
    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)
    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))
    return kwargs_without_prefix, kwargs
Metadata:
  filepath: 
  function_name: groupby_prefix_and_trim
  type: function_definition
  chunk_size: 100
  line_count: 4
  start_line_no: 77
  end_line_no: 80

---------------------------------------------------------------------

------------------------- Chunk 10 (7 lines) -------------------------
def num_to_groups(num, divisor):
    groups = num // divisor
    remainder = num % divisor
    arr = [divisor] * groups
    if remainder > 0:
        arr.append(remainder)
    return arr
Metadata:
  filepath: 
  function_name: num_to_groups
  type: function_definition
  chunk_size: 100
  line_count: 7
  start_line_no: 82
  end_line_no: 88

----------------------------------------------------------------------

------------------------- Chunk 11 (10 lines) -------------------------
def url_to_bucket(url):
    if '://' not in url:
        return url

    _, suffix = url.split('://')

    if prefix in {'gs', 's3'}:
        return suffix.split('/')[0]
    else:
        raise ValueError(f'storage type prefix "{prefix}" is not supported yet')
Metadata:
  filepath: 
  function_name: url_to_bucket
  type: function_definition
  chunk_size: 100
  line_count: 10
  start_line_no: 92
  end_line_no: 101

-----------------------------------------------------------------------

------------------------- Chunk 12 (8 lines) -------------------------
def eval_decorator(fn):
    def inner(model, *args, **kwargs):
        was_training = model.training
        model.eval()
        out = fn(model, *args, **kwargs)
        model.train(was_training)
        return out
    return inner
Metadata:
  filepath: 
  function_name: eval_decorator
  type: function_definition
  chunk_size: 100
  line_count: 8
  start_line_no: 105
  end_line_no: 112

----------------------------------------------------------------------

------------------------- Chunk 13 (6 lines) -------------------------
def inner(model, *args, **kwargs):
        was_training = model.training
        model.eval()
        out = fn(model, *args, **kwargs)
        model.train(was_training)
        return out
Metadata:
  filepath: 
  function_name: inner
  type: function_definition
  chunk_size: 100
  line_count: 6
  start_line_no: 106
  end_line_no: 111

----------------------------------------------------------------------

------------------------- Chunk 14 (25 lines) -------------------------
def cast_torch_tensor(fn, cast_fp16 = False):
    @wraps(fn)
    def inner(model, *args, **kwargs):
        device = kwargs.pop('_device', model.device)
        cast_device = kwargs.pop('_cast_device', True)

        should_cast_fp16 = cast_fp16 and model.cast_half_at_training

        kwargs_keys = kwargs.keys()
        all_args = (*args, *kwargs.values())
        split_kwargs_index = len(all_args) - len(kwargs_keys)
        all_args = tuple(map(lambda t: torch.from_numpy(t) if exists(t) and isinstance(t, np.ndarray) else t, all_args))

        if cast_device:
            all_args = tuple(map(lambda t: t.to(device) if exists(t) and isinstance(t, torch.Tensor) else t, all_args))

        if should_cast_fp16:
            all_args = tuple(map(lambda t: t.half() if exists(t) and isinstance(t, torch.Tensor) and t.dtype != torch.bool else t, all_args))

        args, kwargs_values = all_args[:split_kwargs_index], all_args[split_kwargs_index:]
        kwargs = dict(tuple(zip(kwargs_keys, kwargs_values)))

        out = fn(model, *args, **kwargs)
        return out
    return inner
Metadata:
  filepath: 
  function_name: cast_torch_tensor
  type: function_definition
  chunk_size: 100
  line_count: 25
  start_line_no: 114
  end_line_no: 138

-----------------------------------------------------------------------

------------------------- Chunk 15 (22 lines) -------------------------
def inner(model, *args, **kwargs):
        device = kwargs.pop('_device', model.device)
        cast_device = kwargs.pop('_cast_device', True)

        should_cast_fp16 = cast_fp16 and model.cast_half_at_training

        kwargs_keys = kwargs.keys()
        all_args = (*args, *kwargs.values())
        split_kwargs_index = len(all_args) - len(kwargs_keys)
        all_args = tuple(map(lambda t: torch.from_numpy(t) if exists(t) and isinstance(t, np.ndarray) else t, all_args))

        if cast_device:
            all_args = tuple(map(lambda t: t.to(device) if exists(t) and isinstance(t, torch.Tensor) else t, all_args))

        if should_cast_fp16:
            all_args = tuple(map(lambda t: t.half() if exists(t) and isinstance(t, torch.Tensor) and t.dtype != torch.bool else t, all_args))

        args, kwargs_values = all_args[:split_kwargs_index], all_args[split_kwargs_index:]
        kwargs = dict(tuple(zip(kwargs_keys, kwargs_values)))

        out = fn(model, *args, **kwargs)
        return out
Metadata:
  filepath: 
  function_name: inner
  type: function_definition
  chunk_size: 100
  line_count: 22
  start_line_no: 116
  end_line_no: 137

-----------------------------------------------------------------------

------------------------- Chunk 16 (6 lines) -------------------------
def split_iterable(it, split_size):
    accum = []
    for ind in range(ceil(len(it) / split_size)):
        start_index = ind * split_size
        accum.append(it[start_index: (start_index + split_size)])
    return accum
Metadata:
  filepath: 
  function_name: split_iterable
  type: function_definition
  chunk_size: 100
  line_count: 6
  start_line_no: 142
  end_line_no: 147

----------------------------------------------------------------------

------------------------- Chunk 17 (11 lines) -------------------------
def split(t, split_size = None):
    if not exists(split_size):
        return t

    if isinstance(t, torch.Tensor):
        return t.split(split_size, dim = 0)

    if isinstance(t, Iterable):
        return split_iterable(t, split_size)

    return TypeError
Metadata:
  filepath: 
  function_name: split
  type: function_definition
  chunk_size: 100
  line_count: 11
  start_line_no: 149
  end_line_no: 159

-----------------------------------------------------------------------

------------------------- Chunk 18 (5 lines) -------------------------
def find_first(cond, arr):
    for el in arr:
        if cond(el):
            return el
    return None
Metadata:
  filepath: 
  function_name: find_first
  type: function_definition
  chunk_size: 100
  line_count: 5
  start_line_no: 161
  end_line_no: 165

----------------------------------------------------------------------

------------------------- Chunk 19 (22 lines) -------------------------
def split_args_and_kwargs(*args, split_size = None, **kwargs):
    all_args = (*args, *kwargs.values())
    len_all_args = len(all_args)
    first_tensor = find_first(lambda t: isinstance(t, torch.Tensor), all_args)
    assert exists(first_tensor)

    batch_size = len(first_tensor)
    split_size = default(split_size, batch_size)
    num_chunks = ceil(batch_size / split_size)

    dict_len = len(kwargs)
    dict_keys = kwargs.keys()
    split_kwargs_index = len_all_args - dict_len

    split_all_args = [split(arg, split_size = split_size) if exists(arg) and isinstance(arg, (torch.Tensor, Iterable)) else ((arg,) * num_chunks) for arg in all_args]
    chunk_sizes = num_to_groups(batch_size, split_size)

    for (chunk_size, *chunked_all_args) in tuple(zip(chunk_sizes, *split_all_args)):
        chunked_args, chunked_kwargs_values = chunked_all_args[:split_kwargs_index], chunked_all_args[split_kwargs_index:]
        chunked_kwargs = dict(tuple(zip(dict_keys, chunked_kwargs_values)))
        chunk_size_frac = chunk_size / batch_size
        yield chunk_size_frac, (chunked_args, chunked_kwargs)
Metadata:
  filepath: 
  function_name: split_args_and_kwargs
  type: function_definition
  chunk_size: 100
  line_count: 22
  start_line_no: 167
  end_line_no: 188

-----------------------------------------------------------------------

------------------------- Chunk 20 (19 lines) -------------------------
def imagen_sample_in_chunks(fn):
    @wraps(fn)
    def inner(self, *args, max_batch_size = None, **kwargs):
        if not exists(max_batch_size):
            return fn(self, *args, **kwargs)

        if self.imagen.unconditional:
            batch_size = kwargs.get('batch_size')
            batch_sizes = num_to_groups(batch_size, max_batch_size)
            outputs = [fn(self, *args, **{**kwargs, 'batch_size': sub_batch_size}) for sub_batch_size in batch_sizes]
        else:
            outputs = [fn(self, *chunked_args, **chunked_kwargs) for _, (chunked_args, chunked_kwargs) in split_args_and_kwargs(*args, split_size = max_batch_size, **kwargs)]

        if isinstance(outputs[0], torch.Tensor):
            return torch.cat(outputs, dim = 0)

        return list(map(lambda t: torch.cat(t, dim = 0), list(zip(*outputs))))

    return inner
Metadata:
  filepath: 
  function_name: imagen_sample_in_chunks
  type: function_definition
  chunk_size: 100
  line_count: 19
  start_line_no: 192
  end_line_no: 210

-----------------------------------------------------------------------

------------------------- Chunk 21 (15 lines) -------------------------
def inner(self, *args, max_batch_size = None, **kwargs):
        if not exists(max_batch_size):
            return fn(self, *args, **kwargs)

        if self.imagen.unconditional:
            batch_size = kwargs.get('batch_size')
            batch_sizes = num_to_groups(batch_size, max_batch_size)
            outputs = [fn(self, *args, **{**kwargs, 'batch_size': sub_batch_size}) for sub_batch_size in batch_sizes]
        else:
            outputs = [fn(self, *chunked_args, **chunked_kwargs) for _, (chunked_args, chunked_kwargs) in split_args_and_kwargs(*args, split_size = max_batch_size, **kwargs)]

        if isinstance(outputs[0], torch.Tensor):
            return torch.cat(outputs, dim = 0)

        return list(map(lambda t: torch.cat(t, dim = 0), list(zip(*outputs))))
Metadata:
  filepath: 
  function_name: inner
  type: function_definition
  chunk_size: 100
  line_count: 15
  start_line_no: 194
  end_line_no: 208

-----------------------------------------------------------------------

------------------------- Chunk 22 (12 lines) -------------------------
def restore_parts(state_dict_target, state_dict_from):
    for name, param in state_dict_from.items():

        if name not in state_dict_target:
            continue

        if param.size() == state_dict_target[name].size():
            state_dict_target[name].copy_(param)
        else:
            print(f"layer {name}({param.size()} different than target: {state_dict_target[name].size()}")

    return state_dict_target
Metadata:
  filepath: 
  function_name: restore_parts
  type: function_definition
  chunk_size: 100
  line_count: 12
  start_line_no: 213
  end_line_no: 224

-----------------------------------------------------------------------

------------------------- Chunk 23 (4 lines) -------------------------
def prepare(self):
        assert not self.prepared, f'The trainer is allready prepared'
        self.validate_and_set_unet_being_trained(self.only_train_unet_number)
        self.prepared = True
Metadata:
  filepath: 
  function_name: prepare
  type: function_definition
  chunk_size: 100
  line_count: 4
  start_line_no: 408
  end_line_no: 411

----------------------------------------------------------------------

------------------------- Chunk 24 (2 lines) -------------------------
def device(self):
        return self.accelerator.device
Metadata:
  filepath: 
  function_name: device
  type: function_definition
  chunk_size: 100
  line_count: 2
  start_line_no: 415
  end_line_no: 416

----------------------------------------------------------------------

------------------------- Chunk 25 (2 lines) -------------------------
def is_distributed(self):
        return not (self.accelerator.distributed_type == DistributedType.NO and self.accelerator.num_processes == 1)
Metadata:
  filepath: 
  function_name: is_distributed
  type: function_definition
  chunk_size: 100
  line_count: 2
  start_line_no: 419
  end_line_no: 420

----------------------------------------------------------------------

------------------------- Chunk 26 (2 lines) -------------------------
def is_main(self):
        return self.accelerator.is_main_process
Metadata:
  filepath: 
  function_name: is_main
  type: function_definition
  chunk_size: 100
  line_count: 2
  start_line_no: 423
  end_line_no: 424

----------------------------------------------------------------------

------------------------- Chunk 27 (2 lines) -------------------------
def is_local_main(self):
        return self.accelerator.is_local_main_process
Metadata:
  filepath: 
  function_name: is_local_main
  type: function_definition
  chunk_size: 100
  line_count: 2
  start_line_no: 427
  end_line_no: 428

----------------------------------------------------------------------

------------------------- Chunk 28 (2 lines) -------------------------
def unwrapped_unet(self):
        return self.accelerator.unwrap_model(self.unet_being_trained)
Metadata:
  filepath: 
  function_name: unwrapped_unet
  type: function_definition
  chunk_size: 100
  line_count: 2
  start_line_no: 431
  end_line_no: 432

----------------------------------------------------------------------

------------------------- Chunk 29 (7 lines) -------------------------
def get_lr(self, unet_number):
        self.validate_unet_number(unet_number)
        unet_index = unet_number - 1

        optim = getattr(self, f'optim{unet_index}')

        return optim.param_groups[0]['lr']
Metadata:
  filepath: 
  function_name: get_lr
  type: function_definition
  chunk_size: 100
  line_count: 7
  start_line_no: 436
  end_line_no: 442

----------------------------------------------------------------------

------------------------- Chunk 30 (13 lines) -------------------------
def validate_and_set_unet_being_trained(self, unet_number = None):
        if exists(unet_number):
            self.validate_unet_number(unet_number)

        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you cannot only train on one unet at a time. you will need to save the trainer into a checkpoint, and resume training on a new unet'

        self.only_train_unet_number = unet_number
        self.imagen.only_train_unet_number = unet_number

        if not exists(unet_number):
            return

        self.wrap_unet(unet_number)
Metadata:
  filepath: 
  function_name: validate_and_set_unet_being_trained
  type: function_definition
  chunk_size: 100
  line_count: 13
  start_line_no: 446
  end_line_no: 458

-----------------------------------------------------------------------

------------------------- Chunk 31 (22 lines) -------------------------
def wrap_unet(self, unet_number):
        if hasattr(self, 'one_unet_wrapped'):
            return

        unet = self.imagen.get_unet(unet_number)
        unet_index = unet_number - 1

        optimizer = getattr(self, f'optim{unet_index}')
        scheduler = getattr(self, f'scheduler{unet_index}')

        if self.train_dl:
            self.unet_being_trained, self.train_dl, optimizer = self.accelerator.prepare(unet, self.train_dl, optimizer)
        else:
            self.unet_being_trained, optimizer = self.accelerator.prepare(unet, optimizer)

        if exists(scheduler):
            scheduler = self.accelerator.prepare(scheduler)

        setattr(self, f'optim{unet_index}', optimizer)
        setattr(self, f'scheduler{unet_index}', scheduler)

        self.one_unet_wrapped = True
Metadata:
  filepath: 
  function_name: wrap_unet
  type: function_definition
  chunk_size: 100
  line_count: 22
  start_line_no: 460
  end_line_no: 481

-----------------------------------------------------------------------

------------------------- Chunk 32 (16 lines) -------------------------
def set_accelerator_scaler(self, unet_number):
        def patch_optimizer_step(accelerated_optimizer, method):
            def patched_step(*args, **kwargs):
                accelerated_optimizer._accelerate_step_called = True
                return method(*args, **kwargs)
            return patched_step

        unet_number = self.validate_unet_number(unet_number)
        scaler = getattr(self, f'scaler{unet_number - 1}')

        self.accelerator.scaler = scaler
        for optimizer in self.accelerator._optimizers:
            optimizer.scaler = scaler
            optimizer._accelerate_step_called = False
            optimizer._optimizer_original_step_method = optimizer.optimizer.step
            optimizer._optimizer_patched_step_method = patch_optimizer_step(optimizer, optimizer.optimizer.step)
Metadata:
  filepath: 
  function_name: set_accelerator_scaler
  type: function_definition
  chunk_size: 100
  line_count: 16
  start_line_no: 485
  end_line_no: 500

-----------------------------------------------------------------------

------------------------- Chunk 33 (5 lines) -------------------------
def patch_optimizer_step(accelerated_optimizer, method):
            def patched_step(*args, **kwargs):
                accelerated_optimizer._accelerate_step_called = True
                return method(*args, **kwargs)
            return patched_step
Metadata:
  filepath: 
  function_name: patch_optimizer_step
  type: function_definition
  chunk_size: 100
  line_count: 5
  start_line_no: 486
  end_line_no: 490

----------------------------------------------------------------------

------------------------- Chunk 34 (3 lines) -------------------------
def patched_step(*args, **kwargs):
                accelerated_optimizer._accelerate_step_called = True
                return method(*args, **kwargs)
Metadata:
  filepath: 
  function_name: patched_step
  type: function_definition
  chunk_size: 100
  line_count: 3
  start_line_no: 487
  end_line_no: 489

----------------------------------------------------------------------

------------------------- Chunk 35 (8 lines) -------------------------
def print(self, msg):
        if not self.is_main:
            return

        if not self.verbose:
            return

        return self.accelerator.print(msg)
Metadata:
  filepath: 
  function_name: print
  type: function_definition
  chunk_size: 100
  line_count: 8
  start_line_no: 504
  end_line_no: 511

----------------------------------------------------------------------

------------------------- Chunk 36 (6 lines) -------------------------
def validate_unet_number(self, unet_number = None):
        if self.num_unets == 1:
            unet_number = default(unet_number, 1)

        assert 0 < unet_number <= self.num_unets, f'unet number should be in between 1 and {self.num_unets}'
        return unet_number
Metadata:
  filepath: 
  function_name: validate_unet_number
  type: function_definition
  chunk_size: 100
  line_count: 6
  start_line_no: 515
  end_line_no: 520

----------------------------------------------------------------------

------------------------- Chunk 37 (5 lines) -------------------------
def num_steps_taken(self, unet_number = None):
        if self.num_unets == 1:
            unet_number = default(unet_number, 1)

        return self.steps[unet_number - 1].item()
Metadata:
  filepath: 
  function_name: num_steps_taken
  type: function_definition
  chunk_size: 100
  line_count: 5
  start_line_no: 524
  end_line_no: 528

----------------------------------------------------------------------

------------------------- Chunk 38 (12 lines) -------------------------
def print_untrained_unets(self):
        print_final_error = False

        for ind, (steps, unet) in enumerate(zip(self.steps.tolist(), self.imagen.unets)):
            if steps > 0 or isinstance(unet, NullUnet):
                continue

            self.print(f'unet {ind + 1} has not been trained')
            print_final_error = True

        if print_final_error:
            self.print('when sampling, you can pass stop_at_unet_number to stop early in the cascade, so it does not try to generate with untrained unets')
Metadata:
  filepath: 
  function_name: print_untrained_unets
  type: function_definition
  chunk_size: 100
  line_count: 12
  start_line_no: 530
  end_line_no: 541

-----------------------------------------------------------------------

------------------------- Chunk 39 (7 lines) -------------------------
def add_train_dataloader(self, dl = None):
        if not exists(dl):
            return

        assert not exists(self.train_dl), 'training dataloader was already added'
        assert not self.prepared, f'You need to add the dataset before preperation'
        self.train_dl = dl
Metadata:
  filepath: 
  function_name: add_train_dataloader
  type: function_definition
  chunk_size: 100
  line_count: 7
  start_line_no: 545
  end_line_no: 551

----------------------------------------------------------------------

------------------------- Chunk 40 (7 lines) -------------------------
def add_valid_dataloader(self, dl):
        if not exists(dl):
            return

        assert not exists(self.valid_dl), 'validation dataloader was already added'
        assert not self.prepared, f'You need to add the dataset before preperation'
        self.valid_dl = dl
Metadata:
  filepath: 
  function_name: add_valid_dataloader
  type: function_definition
  chunk_size: 100
  line_count: 7
  start_line_no: 553
  end_line_no: 559

----------------------------------------------------------------------

------------------------- Chunk 41 (21 lines) -------------------------
def add_train_dataset(self, ds = None, *, batch_size, **dl_kwargs):
        if not exists(ds):
            return

        assert not exists(self.train_dl), 'training dataloader was already added'

        valid_ds = None
        if self.split_valid_from_train:
            train_size = int((1 - self.split_valid_fraction) * len(ds))
            valid_size = len(ds) - train_size

            ds, valid_ds = random_split(ds, [train_size, valid_size], generator = torch.Generator().manual_seed(self.split_random_seed))
            self.print(f'training with dataset of {len(ds)} samples and validating with randomly splitted {len(valid_ds)} samples')

        dl = DataLoader(ds, batch_size = batch_size, **dl_kwargs)
        self.add_train_dataloader(dl)

        if not self.split_valid_from_train:
            return

        self.add_valid_dataset(valid_ds, batch_size = batch_size, **dl_kwargs)
Metadata:
  filepath: 
  function_name: add_train_dataset
  type: function_definition
  chunk_size: 100
  line_count: 21
  start_line_no: 561
  end_line_no: 581

-----------------------------------------------------------------------

------------------------- Chunk 42 (8 lines) -------------------------
def add_valid_dataset(self, ds, *, batch_size, **dl_kwargs):
        if not exists(ds):
            return

        assert not exists(self.valid_dl), 'validation dataloader was already added'

        dl = DataLoader(ds, batch_size = batch_size, **dl_kwargs)
        self.add_valid_dataloader(dl)
Metadata:
  filepath: 
  function_name: add_valid_dataset
  type: function_definition
  chunk_size: 100
  line_count: 8
  start_line_no: 583
  end_line_no: 590

----------------------------------------------------------------------

------------------------- Chunk 43 (7 lines) -------------------------
def create_train_iter(self):
        assert exists(self.train_dl), 'training dataloader has not been registered with the trainer yet'

        if exists(self.train_dl_iter):
            return

        self.train_dl_iter = cycle(self.train_dl)
Metadata:
  filepath: 
  function_name: create_train_iter
  type: function_definition
  chunk_size: 100
  line_count: 7
  start_line_no: 592
  end_line_no: 598

----------------------------------------------------------------------

------------------------- Chunk 44 (7 lines) -------------------------
def create_valid_iter(self):
        assert exists(self.valid_dl), 'validation dataloader has not been registered with the trainer yet'

        if exists(self.valid_dl_iter):
            return

        self.valid_dl_iter = cycle(self.valid_dl)
Metadata:
  filepath: 
  function_name: create_valid_iter
  type: function_definition
  chunk_size: 100
  line_count: 7
  start_line_no: 600
  end_line_no: 606

----------------------------------------------------------------------

------------------------- Chunk 45 (9 lines) -------------------------
def train_step(self, *, unet_number = None, **kwargs):
        if not self.prepared:
            self.prepare()
        self.create_train_iter()

        kwargs = {'unet_number': unet_number, **kwargs}
        loss = self.step_with_dl_iter(self.train_dl_iter, **kwargs)
        self.update(unet_number = unet_number)
        return loss
Metadata:
  filepath: 
  function_name: train_step
  type: function_definition
  chunk_size: 100
  line_count: 9
  start_line_no: 608
  end_line_no: 616

----------------------------------------------------------------------

------------------------- Chunk 46 (8 lines) -------------------------
def valid_step(self, **kwargs):
        if not self.prepared:
            self.prepare()
        self.create_valid_iter()
        context = self.use_ema_unets if kwargs.pop('use_ema_unets', False) else nullcontext
        with context():
            loss = self.step_with_dl_iter(self.valid_dl_iter, **kwargs)
        return loss
Metadata:
  filepath: 
  function_name: valid_step
  type: function_definition
  chunk_size: 100
  line_count: 8
  start_line_no: 620
  end_line_no: 627

----------------------------------------------------------------------

------------------------- Chunk 47 (5 lines) -------------------------
def step_with_dl_iter(self, dl_iter, **kwargs):
        dl_tuple_output = cast_tuple(next(dl_iter))
        model_input = dict(list(zip(self.dl_tuple_output_keywords_names, dl_tuple_output)))
        loss = self.forward(**{**kwargs, **model_input})
        return loss
Metadata:
  filepath: 
  function_name: step_with_dl_iter
  type: function_definition
  chunk_size: 100
  line_count: 5
  start_line_no: 629
  end_line_no: 633

----------------------------------------------------------------------

------------------------- Chunk 48 (5 lines) -------------------------
def all_checkpoints_sorted(self):
        glob_pattern = os.path.join(self.checkpoint_path, '*.pt')
        checkpoints = self.fs.glob(glob_pattern)
        sorted_checkpoints = sorted(checkpoints, key = lambda x: int(str(x).split('.')[-2]), reverse = True)
        return sorted_checkpoints
Metadata:
  filepath: 
  function_name: all_checkpoints_sorted
  type: function_definition
  chunk_size: 100
  line_count: 5
  start_line_no: 638
  end_line_no: 642

----------------------------------------------------------------------

------------------------- Chunk 49 (14 lines) -------------------------
def load_from_checkpoint_folder(self, last_total_steps = -1):
        if last_total_steps != -1:
            filepath = os.path.join(self.checkpoint_path, f'checkpoint.{last_total_steps}.pt')
            self.load(filepath)
            return

        sorted_checkpoints = self.all_checkpoints_sorted

        if len(sorted_checkpoints) == 0:
            self.print(f'no checkpoints found to load from at {self.checkpoint_path}')
            return

        last_checkpoint = sorted_checkpoints[0]
        self.load(last_checkpoint)
Metadata:
  filepath: 
  function_name: load_from_checkpoint_folder
  type: function_definition
  chunk_size: 100
  line_count: 14
  start_line_no: 644
  end_line_no: 657

-----------------------------------------------------------------------

------------------------- Chunk 50 (19 lines) -------------------------
def save_to_checkpoint_folder(self):
        self.accelerator.wait_for_everyone()

        if not self.can_checkpoint:
            return

        total_steps = int(self.steps.sum().item())
        filepath = os.path.join(self.checkpoint_path, f'checkpoint.{total_steps}.pt')

        self.save(filepath)

        if self.max_checkpoints_keep <= 0:
            return

        sorted_checkpoints = self.all_checkpoints_sorted
        checkpoints_to_discard = sorted_checkpoints[self.max_checkpoints_keep:]

        for checkpoint in checkpoints_to_discard:
            self.fs.rm(checkpoint)
Metadata:
  filepath: 
  function_name: save_to_checkpoint_folder
  type: function_definition
  chunk_size: 100
  line_count: 19
  start_line_no: 659
  end_line_no: 677

-----------------------------------------------------------------------

------------------------- Chunk 51 (66 lines) -------------------------
def save(
        self,
        path,
        overwrite = True,
        without_optim_and_sched = False,
        **kwargs
    ):
        self.accelerator.wait_for_everyone()

        if not self.can_checkpoint:
            return

        fs = self.fs

        assert not (fs.exists(path) and not overwrite)

        self.reset_ema_unets_all_one_device()

        save_obj = dict(
            model = self.imagen.state_dict(),
            version = __version__,
            steps = self.steps.cpu(),
            **kwargs
        )

        save_optim_and_sched_iter = range(0, self.num_unets) if not without_optim_and_sched else tuple()

        for ind in save_optim_and_sched_iter:
            scaler_key = f'scaler{ind}'
            optimizer_key = f'optim{ind}'
            scheduler_key = f'scheduler{ind}'
            warmup_scheduler_key = f'warmup{ind}'

            scaler = getattr(self, scaler_key)
            optimizer = getattr(self, optimizer_key)
            scheduler = getattr(self, scheduler_key)
            warmup_scheduler = getattr(self, warmup_scheduler_key)

            if exists(scheduler):
                save_obj = {**save_obj, scheduler_key: scheduler.state_dict()}

            if exists(warmup_scheduler):
                save_obj = {**save_obj, warmup_scheduler_key: warmup_scheduler.state_dict()}

            save_obj = {**save_obj, scaler_key: scaler.state_dict(), optimizer_key: optimizer.state_dict()}

        if self.use_ema:
            save_obj = {**save_obj, 'ema': self.ema_unets.state_dict()}

        # determine if imagen config is available

        if hasattr(self.imagen, '_config'):
            self.print(f'this checkpoint is commandable from the CLI - "imagen --model {str(path)} \"<prompt>\""')

            save_obj = {
                **save_obj,
                'imagen_type': 'elucidated' if self.is_elucidated else 'original',
                'imagen_params': self.imagen._config
            }

        #save to path

        with fs.open(path, 'wb') as f:
            torch.save(save_obj, f)

        self.print(f'checkpoint saved to {path}')
Metadata:
  filepath: 
  function_name: save
  type: function_definition
  chunk_size: 100
  line_count: 66
  start_line_no: 681
  end_line_no: 746

-----------------------------------------------------------------------

------------------------- Chunk 52 (66 lines) -------------------------
def load(self, path, only_model = False, strict = True, noop_if_not_exist = False):
        fs = self.fs

        if noop_if_not_exist and not fs.exists(path):
            self.print(f'trainer checkpoint not found at {str(path)}')
            return

        assert fs.exists(path), f'{path} does not exist'

        self.reset_ema_unets_all_one_device()

        # to avoid extra GPU memory usage in main process when using Accelerate

        with fs.open(path) as f:
            loaded_obj = torch.load(f, map_location='cpu')

        if version.parse(__version__) != version.parse(loaded_obj['version']):
            self.print(f'loading saved imagen at version {loaded_obj["version"]}, but current package version is {__version__}')

        try:
            self.imagen.load_state_dict(loaded_obj['model'], strict = strict)
        except RuntimeError:
            print("Failed loading state dict. Trying partial load")
            self.imagen.load_state_dict(restore_parts(self.imagen.state_dict(),
                                                      loaded_obj['model']))

        if only_model:
            return loaded_obj

        self.steps.copy_(loaded_obj['steps'])

        for ind in range(0, self.num_unets):
            scaler_key = f'scaler{ind}'
            optimizer_key = f'optim{ind}'
            scheduler_key = f'scheduler{ind}'
            warmup_scheduler_key = f'warmup{ind}'

            scaler = getattr(self, scaler_key)
            optimizer = getattr(self, optimizer_key)
            scheduler = getattr(self, scheduler_key)
            warmup_scheduler = getattr(self, warmup_scheduler_key)

            if exists(scheduler) and scheduler_key in loaded_obj:
                scheduler.load_state_dict(loaded_obj[scheduler_key])

            if exists(warmup_scheduler) and warmup_scheduler_key in loaded_obj:
                warmup_scheduler.load_state_dict(loaded_obj[warmup_scheduler_key])

            if exists(optimizer):
                try:
                    optimizer.load_state_dict(loaded_obj[optimizer_key])
                    scaler.load_state_dict(loaded_obj[scaler_key])
                except:
                    self.print('could not load optimizer and scaler, possibly because you have turned on mixed precision training since the last run. resuming with new optimizer and scalers')

        if self.use_ema:
            assert 'ema' in loaded_obj
            try:
                self.ema_unets.load_state_dict(loaded_obj['ema'], strict = strict)
            except RuntimeError:
                print("Failed loading state dict. Trying partial load")
                self.ema_unets.load_state_dict(restore_parts(self.ema_unets.state_dict(),
                                                             loaded_obj['ema']))

        self.print(f'checkpoint loaded from {path}')
        return loaded_obj
Metadata:
  filepath: 
  function_name: load
  type: function_definition
  chunk_size: 100
  line_count: 66
  start_line_no: 748
  end_line_no: 813

-----------------------------------------------------------------------

------------------------- Chunk 53 (2 lines) -------------------------
def unets(self):
        return nn.ModuleList([ema.ema_model for ema in self.ema_unets])
Metadata:
  filepath: 
  function_name: unets
  type: function_definition
  chunk_size: 100
  line_count: 2
  start_line_no: 818
  end_line_no: 819

----------------------------------------------------------------------

------------------------- Chunk 54 (18 lines) -------------------------
def get_ema_unet(self, unet_number = None):
        if not self.use_ema:
            return

        unet_number = self.validate_unet_number(unet_number)
        index = unet_number - 1

        if isinstance(self.unets, nn.ModuleList):
            unets_list = [unet for unet in self.ema_unets]
            delattr(self, 'ema_unets')
            self.ema_unets = unets_list

        if index != self.ema_unet_being_trained_index:
            for unet_index, unet in enumerate(self.ema_unets):
                unet.to(self.device if unet_index == index else 'cpu')

        self.ema_unet_being_trained_index = index
        return self.ema_unets[index]
Metadata:
  filepath: 
  function_name: get_ema_unet
  type: function_definition
  chunk_size: 100
  line_count: 18
  start_line_no: 821
  end_line_no: 838

-----------------------------------------------------------------------

------------------------- Chunk 55 (9 lines) -------------------------
def reset_ema_unets_all_one_device(self, device = None):
        if not self.use_ema:
            return

        device = default(device, self.device)
        self.ema_unets = nn.ModuleList([*self.ema_unets])
        self.ema_unets.to(device)

        self.ema_unet_being_trained_index = -1
Metadata:
  filepath: 
  function_name: reset_ema_unets_all_one_device
  type: function_definition
  chunk_size: 100
  line_count: 9
  start_line_no: 840
  end_line_no: 848

----------------------------------------------------------------------

------------------------- Chunk 56 (22 lines) -------------------------
def use_ema_unets(self):
        if not self.use_ema:
            output = yield
            return output

        self.reset_ema_unets_all_one_device()
        self.imagen.reset_unets_all_one_device()

        self.unets.eval()

        trainable_unets = self.imagen.unets
        self.imagen.unets = self.unets                  # swap in exponential moving averaged unets for sampling

        output = yield

        self.imagen.unets = trainable_unets             # restore original training unets

        # cast the ema_model unets back to original device
        for ema in self.ema_unets:
            ema.restore_ema_model_device()

        return output
Metadata:
  filepath: 
  function_name: use_ema_unets
  type: function_definition
  chunk_size: 100
  line_count: 22
  start_line_no: 852
  end_line_no: 873

-----------------------------------------------------------------------

------------------------- Chunk 57 (13 lines) -------------------------
def print_unet_devices(self):
        self.print('unet devices:')
        for i, unet in enumerate(self.imagen.unets):
            device = next(unet.parameters()).device
            self.print(f'\tunet {i}: {device}')

        if not self.use_ema:
            return

        self.print('\nema unet devices:')
        for i, ema_unet in enumerate(self.ema_unets):
            device = next(ema_unet.parameters()).device
            self.print(f'\tema unet {i}: {device}')
Metadata:
  filepath: 
  function_name: print_unet_devices
  type: function_definition
  chunk_size: 100
  line_count: 13
  start_line_no: 875
  end_line_no: 887

-----------------------------------------------------------------------

------------------------- Chunk 58 (3 lines) -------------------------
def state_dict(self, *args, **kwargs):
        self.reset_ema_unets_all_one_device()
        return super().state_dict(*args, **kwargs)
Metadata:
  filepath: 
  function_name: state_dict
  type: function_definition
  chunk_size: 100
  line_count: 3
  start_line_no: 891
  end_line_no: 893

----------------------------------------------------------------------

------------------------- Chunk 59 (3 lines) -------------------------
def load_state_dict(self, *args, **kwargs):
        self.reset_ema_unets_all_one_device()
        return super().load_state_dict(*args, **kwargs)
Metadata:
  filepath: 
  function_name: load_state_dict
  type: function_definition
  chunk_size: 100
  line_count: 3
  start_line_no: 895
  end_line_no: 897

----------------------------------------------------------------------

------------------------- Chunk 60 (2 lines) -------------------------
def encode_text(self, text, **kwargs):
        return self.imagen.encode_text(text, **kwargs)
Metadata:
  filepath: 
  function_name: encode_text
  type: function_definition
  chunk_size: 100
  line_count: 2
  start_line_no: 901
  end_line_no: 902

----------------------------------------------------------------------

------------------------- Chunk 61 (44 lines) -------------------------
def update(self, unet_number = None):
        unet_number = self.validate_unet_number(unet_number)
        self.validate_and_set_unet_being_trained(unet_number)
        self.set_accelerator_scaler(unet_number)

        index = unet_number - 1
        unet = self.unet_being_trained

        optimizer = getattr(self, f'optim{index}')
        scaler = getattr(self, f'scaler{index}')
        scheduler = getattr(self, f'scheduler{index}')
        warmup_scheduler = getattr(self, f'warmup{index}')

        # set the grad scaler on the accelerator, since we are managing one per u-net

        if exists(self.max_grad_norm):
            self.accelerator.clip_grad_norm_(unet.parameters(), self.max_grad_norm)

        optimizer.step()
        optimizer.zero_grad()

        if self.use_ema:
            ema_unet = self.get_ema_unet(unet_number)
            ema_unet.update()

        # scheduler, if needed

        maybe_warmup_context = nullcontext() if not exists(warmup_scheduler) else warmup_scheduler.dampening()

        with maybe_warmup_context:
            if exists(scheduler) and not self.accelerator.optimizer_step_was_skipped: # recommended in the docs
                scheduler.step()

        self.steps += F.one_hot(torch.tensor(unet_number - 1, device = self.steps.device), num_classes = len(self.steps))

        if not exists(self.checkpoint_path):
            return

        total_steps = int(self.steps.sum().item())

        if total_steps % self.checkpoint_every:
            return

        self.save_to_checkpoint_folder()
Metadata:
  filepath: 
  function_name: update
  type: function_definition
  chunk_size: 100
  line_count: 44
  start_line_no: 906
  end_line_no: 949

-----------------------------------------------------------------------

------------------------- Chunk 62 (12 lines) -------------------------
def sample(self, *args, **kwargs):
        context = nullcontext if  kwargs.pop('use_non_ema', False) else self.use_ema_unets

        self.print_untrained_unets()

        if not self.is_main:
            kwargs['use_tqdm'] = False

        with context():
            output = self.imagen.sample(*args, device = self.device, **kwargs)

        return output
Metadata:
  filepath: 
  function_name: sample
  type: function_definition
  chunk_size: 100
  line_count: 12
  start_line_no: 954
  end_line_no: 965

-----------------------------------------------------------------------

------------------------- Chunk 63 (26 lines) -------------------------
def forward(
        self,
        *args,
        unet_number = None,
        max_batch_size = None,
        **kwargs
    ):
        unet_number = self.validate_unet_number(unet_number)
        self.validate_and_set_unet_being_trained(unet_number)
        self.set_accelerator_scaler(unet_number)

        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, f'you can only train unet #{self.only_train_unet_number}'

        total_loss = 0.

        for chunk_size_frac, (chunked_args, chunked_kwargs) in split_args_and_kwargs(*args, split_size = max_batch_size, **kwargs):
            with self.accelerator.autocast():
                loss = self.imagen(*chunked_args, unet = self.unet_being_trained, unet_number = unet_number, **chunked_kwargs)
                loss = loss * chunk_size_frac

            total_loss += loss.item()

            if self.training:
                self.accelerator.backward(loss)

        return total_loss
Metadata:
  filepath: 
  function_name: forward
  type: function_definition
  chunk_size: 100
  line_count: 26
  start_line_no: 968
  end_line_no: 993

-----------------------------------------------------------------------

