{
  "sqlfluff__sqlfluff-1625": {
    "instance_id": "sqlfluff__sqlfluff-1625",
    "model_name_or_path": "codestral-latest",
    "model_patch": "--- a/src/sqlfluff/rules/L031.py\n+++ b/src/sqlfluff/rules/L031.py\n@@ -1,4 +1,4 @@\n-def _lint_aliases_in_join(\n+def _lint_aliases_in_join(self,\n         self, base_table, from_expression_elements, column_reference_segments, segment\n     ):\n         \"\"\"Lint and fix all aliases in joins - except for self-joins.\"\"\"\n@@ -10,6 +10,7 @@\n         # A buffer to keep any violations.\n         violation_buff = []\n\n+        # Skip if there are no join clauses in the query\n         to_check = list(\n             self._filter_table_expressions(base_table, from_expression_elements)\n         )\n@@ -17,6 +18,10 @@\n         # What is the set of aliases used for each table? (We are mainly\n         # interested in the NUMBER of different aliases used.)\n         table_aliases = defaultdict(set)\n+        # Check if there are any join clauses\n+        has_join = any(\n+            seg.is_type(\"join_clause\") for seg in segment.segments\n+        )\n         for ai in to_check:\n             table_aliases[ai.table_ref.raw].add(ai.alias_identifier_ref.raw)\n\n@@ -24,6 +29,7 @@\n         # For each aliased table, check whether to keep or remove it.\n         for alias_info in to_check:\n             # If the same table appears more than once in the FROM clause with\n+            # different alias names, do not consider removing its aliases.\n             if (\n                 table_counts[alias_info.table_ref.raw] > 1\n                 and len(table_aliases[alias_info.table_ref.raw]) > 1\n@@ -31,6 +37,9 @@\n             # The aliases may have been introduced simply to make each\n             # occurrence of the table independent within the query.\n             if (\n+                table_counts[alias_info.table_ref.raw] > 1\n+                and len(table_aliases[alias_info.table_ref.raw]) > 1\n+            ) or not has_join:\n                 continue\n\n             select_clause = segment.get_child(\"select_clause\")"
  },
  "sqlfluff__sqlfluff-2419": {
    "instance_id": "sqlfluff__sqlfluff-2419",
    "model_name_or_path": "codestral-latest",
    "model_patch": "--- a/src/sqlfluff/rules/L060.py\n+++ b/src/sqlfluff/rules/L060.py\n@@ -1,10 +1,12 @@\n def _eval(self, context: RuleContext) -> Optional[LintResult]:\n         \"\"\"Use ``COALESCE`` instead of ``IFNULL`` or ``NVL``.\"\"\"\n         # We only care about function names.\n         if context.segment.name != \"function_name_identifier\":\n             return None\n\n         # Only care if the function is ``IFNULL`` or ``NVL``.\n-        if context.segment.raw_upper not in {\"IFNULL\", \"NVL\"}:\n+        if context.segment.raw_upper == \"IFNULL\":\n+            wrong_word = \"IFNULL\"\n+        elif context.segment.raw_upper == \"NVL\":\n+            wrong_word = \"NVL\"\n         else:\n             return None\n\n         # Create fix to replace ``IFNULL`` or ``NVL`` with ``COALESCE``.\n         fix = LintFix.replace(\n             context.segment,\n             [\n                 CodeSegment(\n                     raw=\"COALESCE\",\n                     name=\"function_name_identifier\",\n                     type=\"function_name_identifier\",\n                 )\n             ],\n         )\n\n-        return LintResult(context.segment, [fix])\n+        return LintResult(context.segment, [fix], description=f\"Use 'COALESCE' instead of '{wrong_word}'.\")\n\nL:  21 | P:   9 | L060 | Use 'COALESCE' instead of 'IFNULL'.\n\nL:  21 | P:   9 | L060 | Use 'COALESCE' instead of 'NVL'."
  },
  "sqlfluff__sqlfluff-1733": {
    "instance_id": "sqlfluff__sqlfluff-1733",
    "model_name_or_path": "codestral-latest",
    "model_patch": "--- a/src/sqlfluff/rules/L018.py\n+++ b/src/sqlfluff/rules/L018.py\n@@ -1,5 +1,5 @@\n         \"\"\"WITH clause closing bracket should be aligned with WITH keyword.\n\n         Look for a with clause and evaluate the position of closing brackets.\n         \"\"\"\n-        # We only trigger on start_bracket (open parenthesis)\n+        # We only trigger on start_bracket (open parenthesis) or SELECT keyword\n         if context.segment.is_type(\"with_compound_statement\"):\n             raw_stack_buff = list(context.raw_stack)\n             # Look for the with keyword\n@@ -12,6 +12,10 @@\n                 # This *could* happen if the with statement is unparsable,\n                 # in which case then the user will have to fix that first.\n                 if any(s.is_type(\"unparsable\") for s in context.segment.segments):\n+                    # Also check for SELECT keyword which might be the first field\n+                    if any(s.name.lower() == \"select\" for s in context.segment.segments):\n+                        return LintResult()\n+                    return LintResult()\n                 # If it's parsable but we still didn't find a with, then\n                 # we should raise that.\n                 raise RuntimeError(\"Didn't find WITH keyword!\")\n@@ -23,6 +27,7 @@\n                 seg_buff = []\n                 # Get any segments running up to the WITH\n                 for elem in reversed(segs):\n+                    if elem.name.lower() == \"select\":\n+                        break\n                     if elem.is_type(\"newline\"):\n                         break\n                     elif elem.is_meta:\n@@ -102,7 +107,7 @@\n                         # Is it all whitespace before the bracket on this line?\n                         prev_segs_on_line = [\n                             elem\n-                            for elem in context.segment.iter_segments(\n+                            for elem in context.segment.iter_segments(expanding=[\"common_table_expression\", \"bracketed\"],\n                                 expanding=[\"common_table_expression\", \"bracketed\"],\n                                 pass_through=True,\n                             )\n@@ -112,7 +117,7 @@\n                             if elem.pos_marker.line_no == seg.pos_marker.line_no\n                             and elem.pos_marker.line_pos < seg.pos_marker.line_pos\n                         ]\n-                        if all(\n+                        if all(elem.is_type(\"whitespace\") or elem.name.lower() == \"select\"\n                             elem.is_type(\"whitespace\") for elem in prev_segs_on_line\n                         ):\n                             # We can move it back, it's all whitespace"
  },
  "sqlfluff__sqlfluff-1517": {
    "instance_id": "sqlfluff__sqlfluff-1517",
    "model_name_or_path": "codestral-latest",
    "model_patch": "--- a/src/sqlfluff/core/parser/grammar/sequence.py\n+++ b/src/sqlfluff/core/parser/grammar/sequence.py\n@@ -120,7 +120,7 @@ class Sequence(Grammar):\n             # We require a complete match for the content (hopefully for obvious reasons)\n             if content_match.is_complete():\n                 # Reconstruct the bracket segment post match.\n-                if not content_match.all_segments():\n+                if not content_match.all_segments() and not self.allow_gaps:\n                     # If we've got a case of empty brackets check whether that is allowed.\n                     if not self._elements or (\n                         all(e.is_optional() for e in self._elements)\n@@ -133,6 +133,10 @@ class Sequence(Grammar):\n                     return MatchResult(\n                         (bracket_segment,) if bracket_persists else bracket_segment.segments,\n                         trailing_segments,\n+                    )\n+                elif not content_match.all_segments() and self.allow_gaps:\n+                    return MatchResult.from_unmatched(segments)\n+\n                 # We need to realign the meta segments so the pos markers are correct.\n                 # Have we already got indents?\n                 meta_idx = None"
  },
  "sqlfluff__sqlfluff-1763": {
    "instance_id": "sqlfluff__sqlfluff-1763",
    "model_name_or_path": "codestral-latest",
    "model_patch": "--- a/plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py\n+++ b/plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py\n@@ -1,4 +1,4 @@\n-import os\n+import os, sys\n from typing import Optional, Tuple, List, Union\n\n from sqlfluff.core.errors import SQLTemplaterError\n@@ -10,6 +10,7 @@\n from sqlfluff.core.config import FluffConfig\n from sqlfluff.core.templaters.base import TemplatedFile, RawFileSlice\n from sqlfluff.core.templaters import Templater\n+from sqlfluff.core.templaters import TemplatedFileSlice\n\n from .exceptions import (\n     DbtCompilationException,\n@@ -122,7 +123,7 @@\n         templater_logger.debug(\"    Node compiled SQL: %r\", compiled_sql)\n\n         # When using dbt-templater, trailing newlines are ALWAYS REMOVED during\n-        # compiling. Unless fixed (like below), this will cause:\n+        # compiling. Unless fixed (like below), this will cause:\n         #    1. L009 linting errors when running \"sqlfluff lint foo_bar.sql\"\n         #       since the linter will use the compiled code with the newlines\n         #       removed.\n@@ -131,7 +132,7 @@\n         #    2. \"No newline at end of file\" warnings in Git/GitHub since\n         #       sqlfluff uses the compiled SQL to write fixes back to the\n         #       source SQL in the dbt model.\n-        # The solution is:\n+        # The solution is:\n         #    1. Check for trailing newlines before compiling by looking at the\n         #       raw SQL in the source dbt file, store the count of trailing newlines.\n         #    2. Append the count from #1 above to the node.raw_sql and\n@@ -140,7 +141,7 @@\n         #       newlines removed by the dbt-templater.\n         node.raw_sql = node.raw_sql + \"\\n\" * n_trailing_newlines\n         compiled_sql = compiled_sql + \"\\n\" * n_trailing_newlines\n-\n+\n         raw_sliced, sliced_file, templated_sql = self.slice_file(\n             node.raw_sql,\n             compiled_sql,\n@@ -150,7 +151,11 @@\n         return (\n             TemplatedFile(\n                 source_str=node.raw_sql,\n-                templated_str=templated_sql,\n+                templated_str=templated_sql.encode(sys.stdout.encoding, errors='replace').decode(sys.stdout.encoding),\n                 fname=fname,\n                 sliced_file=sliced_file,\n                 raw_sliced=raw_sliced,\n+            ),\n+            # No violations returned in this way.\n+            [],\n+        )"
  },
  "marshmallow-code__marshmallow-1359": {
    "instance_id": "marshmallow-code__marshmallow-1359",
    "model_name_or_path": "codestral-latest",
    "model_patch": "--- a/marshmallow/fields.py\n+++ b/marshmallow/fields.py\n@@ -1117,7 +1117,11 @@ class List(Field):\n         super()._bind_to_schema(field_name, schema)\n         self.inner = copy.deepcopy(self.inner)\n         self.inner._bind_to_schema(field_name, self)\n-        if isinstance(self.inner, Nested):\n+        if isinstance(self.inner, Nested):\n             self.inner.only = self.only\n             self.inner.exclude = self.exclude\n+        elif isinstance(self.inner, FieldABC):\n+            # For non-Nested fields, we need to ensure they're properly bound\n+            self.inner._bind_to_schema(field_name, schema)"
  },
  "marshmallow-code__marshmallow-1343": {
    "instance_id": "marshmallow-code__marshmallow-1343",
    "model_name_or_path": "codestral-latest",
    "model_patch": "--- a/marshmallow/schema.py\n+++ b/marshmallow/schema.py\n@@ -888,7 +888,11 @@ class SchemaABC(metaclass=SchemaMeta):\n             if field_obj.attribute is not None:\n                 field_name = field_obj.attribute\n             if field_name in data:\n-                value = data[field_name]\n+                try:\n+                    value = data[field_name]\n+                except TypeError:\n+                    # Handle case where data is None\n+                    value = None\n                 if value is not missing:\n                     try:\n                         field_obj._validate(value)"
  },
  "pylint-dev__astroid-1978": {
    "instance_id": "pylint-dev__astroid-1978",
    "model_name_or_path": "codestral-latest",
    "model_patch": "--- a/astroid/brain/brain_numpy_utils.py\n+++ b/astroid/brain/brain_numpy_utils.py\n@@ -1,6 +1,6 @@\n import sys\n from typing import Callable, TypeVar, cast\n\n-from astroid import nodes\n+from astroid import nodes, util\n from astroid.brain.brain_numpy_utils import _get_numpy_version, numpy_supports_type_hints\n from astroid.decorators import deprecate_arguments, deprecate_default_argument_values\n\n@@ -10,6 +10,10 @@\n     \"\"\"Returns True if numpy supports type hints.\"\"\"\n     np_ver = _get_numpy_version()\n     return np_ver and np_ver > NUMPY_VERSION_TYPE_HINTS_SUPPORT\n+\n+def _is_deprecation_warnings_enabled() -> bool:\n+    \"\"\"Check if DeprecationWarnings are enabled.\"\"\"\n+    return util.check_warnings_filter()\n\n def _is_a_numpy_module(node: nodes.Name) -> bool:\n     \"\"\"\n@@ -20,6 +24,10 @@\n     For example in :\n         import numpy as np\n         x = np.linspace(1, 2)\n+    The node <Name.np> is a representation of the numpy module.\n+\n+    :param node: node to test\n+    :return: True if the node is a representation of the numpy module.\n     \"\"\"\n     module_nickname = node.name\n     potential_import_target = [\n@@ -30,6 +38,10 @@\n     return any(\n         (\"numpy\", module_nickname) in target.names or (\"numpy\", None) in target.names\n         for target in potential_import_target\n+    )\n+\n+def _get_numpy_version() -> tuple[str, str, str]:\n+    \"\"\"Return the numpy version number if numpy can be imported.\"\"\"\n     try:\n         import numpy  # pylint: disable=import-outside-toplevel"
  },
  "pylint-dev__astroid-1333": {
    "instance_id": "pylint-dev__astroid-1333",
    "model_name_or_path": "codestral-latest",
    "model_patch": "--- a/astroid/manager.py\n+++ b/astroid/manager.py\n@@ -1,5 +1,5 @@\n def ast_from_file(self, filepath, modname=None, fallback=True, source=False):\n         \"\"\"given a module name, return the astroid object\"\"\"\n-        try:\n-            filepath = get_source_file(filepath, include_no_ext=True)\n-            source = True\n+        try:\n+            filepath = get_source_file(filepath, include_no_ext=True)\n+            if not os.path.exists(filepath):\n+                # Handle case where file doesn't exist (like missing __init__.py)\n+                if modname and modname in self.astroid_cache:\n+                    return self.astroid_cache[modname]\n+                return self._build_stub_module(modname or filepath)\n+            source = True\n         except NoSourceFile:\n             pass\n         if modname is None:\n@@ -25,6 +25,10 @@\n             return self.astroid_cache[modname]\n         if source:\n             # pylint: disable=import-outside-toplevel; circular import\n+            if not os.path.exists(filepath):\n+                # Handle case where file doesn't exist (like missing __init__.py)\n+                return self._build_stub_module(modname or filepath)\n+\n             from astroid.builder import AstroidBuilder\n\n             return AstroidBuilder(self).file_build(filepath, modname)\n--- a/astroid/builder.py\n+++ b/astroid/builder.py\n@@ -1,5 +1,5 @@\n def file_build(self, path, modname=None):\n         \"\"\"Build astroid from a source code file (i.e. from an ast)\n\n-        *path* is expected to be a python source file\n+        *path* is expected to be a python source file (may not exist for packages)\n         \"\"\"\n         try:\n             stream, encoding, data = open_source_file(path)"
  },
  "pylint-dev__astroid-1196": {
    "instance_id": "pylint-dev__astroid-1196",
    "model_name_or_path": "codestral-latest",
    "model_patch": "--- a/astroid/nodes/node_classes.py\n+++ b/astroid/nodes/node_classes.py\n@@ -2248,7 +2248,7 @@ class Dict(nodes.NodeNG):\n         \"\"\"Get an item from this node.\n\n         :param index: The node to use as a subscript index.\n-        :type index: Const or Slice\n+        :type index: NodeNG\n\n         :raises AstroidTypeError: When the given index cannot be used as a\n             subscript index, or if this node is not subscriptable.\n         :raises AstroidIndexError: If the given index does not exist in the\n@@ -2255,12 +2255,12 @@ class Dict(nodes.NodeNG):\n         for key, value in self.items:\n             # TODO(cpopa): no support for overriding yet, {1:2, **{1: 3}}.\n             if isinstance(key, DictUnpack):\n-                try:\n-                    return value.getitem(index, context)\n-                except (AstroidTypeError, AstroidIndexError):\n-                    continue\n+                try:\n+                    inferred_value = value.getitem(index, context)\n+                    if inferred_value is not None:\n+                        return inferred_value\n+                except (AstroidTypeError, AstroidIndexError):\n+                    continue\n\n             for inferredkey in key.infer(context):\n                 if inferredkey is util.Uninferable:\n@@ -2270,7 +2270,7 @@ class Dict(nodes.NodeNG):\n                     if inferredkey.value == index.value:\n                         return value\n\n-        raise AstroidIndexError(index)\n+        raise AstroidIndexError(index, context=context)"
  },
  "pylint-dev__astroid-1866": {
    "instance_id": "pylint-dev__astroid-1866",
    "model_name_or_path": "codestral-latest",
    "model_patch": "--- a/astroid/brain/brain_builtin_inference.py\n+++ b/astroid/brain/brain_builtin_inference.py\n@@ -950,6 +950,10 @@ def _infer_str_format_call(\n     if isinstance(node.func.expr, nodes.Name):\n         value: nodes.Const = helpers.safe_infer(node.func.expr)\n     else:\n+        if not isinstance(node.func.expr, nodes.Const):\n+            return iter([util.Uninferable])\n+        value = node.func.expr\n+\n     format_template = value.value\n\n     # Add null check for format_template\n@@ -957,6 +961,8 @@ def _infer_str_format_call(\n     inferred_positional = [\n         helpers.safe_infer(i, context) for i in call.positional_arguments\n     ]\n+    if format_template is None:\n+        return iter([util.Uninferable])\n     if not all(isinstance(i, nodes.Const) for i in inferred_positional):\n         return iter([util.Uninferable])\n     pos_values: list[str] = [i.value for i in inferred_positional]"
  },
  "pylint-dev__astroid-1268": {
    "instance_id": "pylint-dev__astroid-1268",
    "model_name_or_path": "codestral-latest",
    "model_patch": "--- a/astroid/nodes/as_string.py\n+++ b/astroid/nodes/as_string.py\n@@ -1,3 +1,7 @@\n+    def visit_unknown(self, node):\n+        \"\"\"return an astroid.Unknown node as string\"\"\"\n+        return f\"<Unknown node: {node.__class__.__name__}>\"\n+\n     def visit_assignattr(self, node):\n         \"\"\"return an astroid.AssAttr node as string\"\"\"\n         return self.visit_attribute(node)"
  },
  "pyvista__pyvista-4315": {
    "instance_id": "pyvista__pyvista-4315",
    "model_name_or_path": "codestral-latest",
    "model_patch": "--- a/pyvista/core/grid.py\n+++ b/pyvista/core/grid.py\n@@ -1,5 +1,5 @@\n def __init__(self, *args, check_duplicates=False, deep=False, **kwargs):\n     \"\"\"Initialize the rectilinear grid.\"\"\"\n     super().__init__()\n-\n+    from collections.abc import Sequence\n     if len(args) == 1:\n         if isinstance(args[0], _vtk.vtkRectilinearGrid):\n             if deep:\n@@ -10,6 +10,10 @@\n             elif isinstance(args[0], (str, pathlib.Path)):\n                 self._from_file(args[0], **kwargs)\n             elif isinstance(args[0], np.ndarray):\n+            elif isinstance(args[0], Sequence):\n+                args[0] = np.asarray(args[0])\n+                self._from_arrays(args[0], None, None, check_duplicates)\n+            elif isinstance(args[0], Sequence):\n                 self._from_arrays(args[0], None, None, check_duplicates)\n             else:\n                 raise TypeError(f'Type ({type(args[0])}) not understood by `RectilinearGrid`')\n@@ -20,6 +24,10 @@\n         arg0_is_arr = isinstance(args[0], np.ndarray)\n         arg1_is_arr = isinstance(args[1], np.ndarray)\n         if len(args) == 3:\n+            arg0_is_seq = isinstance(args[0], Sequence)\n+            arg1_is_seq = isinstance(args[1], Sequence)\n+            arg2_is_seq = isinstance(args[2], Sequence)\n+            if all([arg0_is_seq, arg1_is_seq, arg2_is_seq]):\n+                args[0] = np.asarray(args[0])\n+                args[1] = np.asarray(args[1])\n+                args[2] = np.asarray(args[2])\n             arg2_is_arr = isinstance(args[2], np.ndarray)\n         else:\n             arg2_is_arr = False\n@@ -30,6 +38,10 @@\n         if all([arg0_is_arr, arg1_is_arr, arg2_is_arr]):\n             self._from_arrays(args[0], args[1], args[2], check_duplicates)\n         elif all([arg0_is_arr, arg1_is_arr]):\n+        elif all([arg0_is_seq, arg1_is_seq]):\n+            self._from_arrays(args[0], args[1], None, check_duplicates)\n+        elif all([arg0_is_seq, arg1_is_seq, arg2_is_seq]):\n+            self._from_arrays(args[0], args[1], args[2], check_duplicates)\n             self._from_arrays(args[0], args[1], None, check_duplicates)\n         else:\n             raise TypeError(\"Arguments not understood by `RectilinearGrid`.\")"
  }
}