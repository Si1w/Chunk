{
  "sqlfluff__sqlfluff-1625": {
    "query": "TSQL - L031 incorrectly triggers \"Avoid using aliases in join condition\" when no join present\n## Expected Behaviour\r\n\r\nBoth of these queries should pass, the only difference is the addition of a table alias 'a':\r\n\r\n1/ no alias\r\n\r\n```\r\nSELECT [hello]\r\nFROM\r\n    mytable\r\n```\r\n\r\n2/ same query with alias\r\n\r\n```\r\nSELECT a.[hello]\r\nFROM\r\n    mytable AS a\r\n```\r\n\r\n## Observed Behaviour\r\n\r\n1/ passes\r\n2/ fails with: L031: Avoid using aliases in join condition.\r\n\r\nBut there is no join condition :-)\r\n\r\n## Steps to Reproduce\r\n\r\nLint queries above\r\n\r\n## Dialect\r\n\r\nTSQL\r\n\r\n## Version\r\n\r\nsqlfluff 0.6.9\r\nPython 3.6.9\r\n\r\n## Configuration\r\n\r\nN/A\n",
    "method": "hierarchical",
    "top_k": 10,
    "results": [
      {
        "rank": 1,
        "score": 0.7734122276306152,
        "content": "class Rule_L031(BaseRule):\n    \"\"\"Avoid table aliases in from clauses and join conditions.\n\n    | **Anti-pattern**\n    | In this example, alias 'o' is used for the orders table, and 'c' is used for 'customers' table.\n\n    .. code-block:: sql\n\n        SELECT\n            COUNT(o.customer_id) as order_amount,\n            c.name\n        FROM orders as o\n        JOIN customers as c on o.id = c.user_id\n\n\n    | **Best practice**\n    |  Avoid aliases.\n\n    .. code-block:: sql\n\n        SELECT\n            COUNT(orders.customer_id) as order_amount,\n            customers.name\n        FROM orders\n        JOIN customers on orders.id = customers.user_id\n\n        -- Self-join will not raise issue\n\n        SELECT\n            table.a,\n            table_alias.b,\n        FROM\n            table\n            LEFT JOIN table AS table_alias ON table.foreign_key = table_alias.foreign_key\n\n    \"\"\"\n\n    def _eval(self, segment, **kwargs):\n        \"\"\"Identify aliases in from clause and join conditions.\n\n        Find base table, table expressions in join, and other expressions in select clause\n        and decide if it's needed to report them.\n        \"\"\"\n        if segment.is_type(\"select_statement\"):\n            # A buffer for all table expressions in join conditions\n            from_expression_elements = []\n            column_reference_segments = []\n\n            from_clause_segment = segment.get_child(\"from_clause\")\n\n            if not from_clause_segment:\n                return None\n\n            from_expression = from_clause_segment.get_child(\"from_expression\")\n            from_expression_element = None\n            if from_expression:\n                from_expression_element = from_expression.get_child(\n                    \"from_expression_element\"\n                )\n\n            if not from_expression_element:\n                return None\n            from_expression_element = from_expression_element.get_child(\n                \"table_expression\"\n            )\n\n            # Find base table\n            base_table = None\n            if from_expression_element:\n                base_table = from_expression_element.get_child(\"object_reference\")\n\n            from_clause_index = segment.segments.index(from_clause_segment)\n            from_clause_and_after = segment.segments[from_clause_index:]\n\n            for clause in from_clause_and_after:\n                for from_expression_element in clause.recursive_crawl(\n                    \"from_expression_element\"\n                ):\n                    from_expression_elements.append(from_expression_element)\n                for column_reference in clause.recursive_crawl(\"column_reference\"):\n                    column_reference_segments.append(column_reference)\n\n            return (\n                self._lint_aliases_in_join(\n                    base_table,\n                    from_expression_elements,\n                    column_reference_segments,\n                    segment,\n                )\n                or None\n            )\n        return None\n\n    class TableAliasInfo(NamedTuple):\n        \"\"\"Structure yielded by_filter_table_expressions().\"\"\"\n\n        table_ref: BaseSegment\n        whitespace_ref: BaseSegment\n        alias_exp_ref: BaseSegment\n        alias_identifier_ref: BaseSegment\n\n    @classmethod\n    def _filter_table_expressions(\n        cls, base_table, from_expression_elements\n    ) -> Generator[TableAliasInfo, None, None]:\n        for from_expression in from_expression_elements:\n            table_expression = from_expression.get_child(\"table_expression\")\n            if not table_expression:\n                continue\n            table_ref = table_expression.get_child(\"object_reference\")\n\n            # If the from_expression_element has no object_references - skip it\n            # An example case is a lateral flatten, where we have a function segment\n            # instead of a table_reference segment.\n            if not table_ref:\n                continue\n\n            # If this is self-join - skip it\n            if (\n                base_table\n                and base_table.raw == table_ref.raw\n                and base_table != table_ref\n            ):\n                continue\n\n            whitespace_ref = from_expression.get_child(\"whitespace\")\n\n            # If there's no alias expression - skip it\n            alias_exp_ref = from_expression.get_child(\"alias_expression\")\n            if alias_exp_ref is None:\n                continue\n\n            alias_identifier_ref = alias_exp_ref.get_child(\"identifier\")\n            yield cls.TableAliasInfo(\n                table_ref, whitespace_ref, alias_exp_ref, alias_identifier_ref\n            )\n\n    def _lint_aliases_in_join(\n        self, base_table, from_expression_elements, column_reference_segments, segment\n    ):\n        \"\"\"Lint and fix all aliases in joins - except for self-joins.\"\"\"\n        # A buffer to keep any violations.\n        violation_buff = []\n\n        to_check = list(\n            self._filter_table_expressions(base_table, from_expression_elements)\n        )\n\n        # How many times does each table appear in the FROM clause?\n        table_counts = Counter(ai.table_ref.raw for ai in to_check)\n\n        # What is the set of aliases used for each table? (We are mainly\n        # interested in the NUMBER of different aliases used.)\n        table_aliases = defaultdict(set)\n        for ai in to_check:\n            table_aliases[ai.table_ref.raw].add(ai.alias_identifier_ref.raw)\n\n        # For each aliased table, check whether to keep or remove it.\n        for alias_info in to_check:\n            # If the same table appears more than once in the FROM clause with\n            # different alias names, do not consider removing its aliases.\n            # The aliases may have been introduced simply to make each\n            # occurrence of the table independent within the query.\n            if (\n                table_counts[alias_info.table_ref.raw] > 1\n                and len(table_aliases[alias_info.table_ref.raw]) > 1\n            ):\n                continue\n\n            select_clause = segment.get_child(\"select_clause\")\n\n            ids_refs = []\n\n            # Find all references to alias in select clause\n            alias_name = alias_info.alias_identifier_ref.raw\n            for alias_with_column in select_clause.recursive_crawl(\"object_reference\"):\n                used_alias_ref = alias_with_column.get_child(\"identifier\")\n                if used_alias_ref and used_alias_ref.raw == alias_name:\n                    ids_refs.append(used_alias_ref)\n\n            # Find all references to alias in column references\n            for exp_ref in column_reference_segments:\n                used_alias_ref = exp_ref.get_child(\"identifier\")\n                # exp_ref.get_child('dot') ensures that the column reference includes a table reference\n                if used_alias_ref.raw == alias_name and exp_ref.get_child(\"dot\"):\n                    ids_refs.append(used_alias_ref)\n\n            # Fixes for deleting ` as sth` and for editing references to aliased tables\n            fixes = [\n                *[\n                    LintFix(\"delete\", d)\n                    for d in [alias_info.alias_exp_ref, alias_info.whitespace_ref]\n                ],\n                *[\n                    LintFix(\"edit\", alias, alias.edit(alias_info.table_ref.raw))\n                    for alias in [alias_info.alias_identifier_ref, *ids_refs]\n                ],\n            ]\n\n            violation_buff.append(\n                LintResult(\n                    anchor=alias_info.alias_identifier_ref,\n                    description=\"Avoid using aliases in join condition\",\n                    fixes=fixes,\n                )\n            )\n\n        return violation_buff or None",
        "file_path": "src/sqlfluff/rules/L031.py",
        "chunk_index": 0,
        "metadata": {
          "class_name": "Rule_L031",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "BaseRule"
            ]
          },
          "parent_name": null,
          "private_methods_count": 2,
          "public_methods_count": 0,
          "node_count": 998,
          "chunk_size": 7742
        }
      },
      {
        "rank": 2,
        "score": 0.6935659646987915,
        "content": "class Rule_L025(Rule_L020):\n    \"\"\"Tables should not be aliased if that alias is not used.\n\n    | **Anti-pattern**\n\n    .. code-block:: sql\n\n        SELECT\n            a\n        FROM foo AS zoo\n\n    | **Best practice**\n    | Use the alias or remove it. An unused alias makes code\n    | harder to read without changing any functionality.\n\n    .. code-block:: sql\n\n        SELECT\n            zoo.a\n        FROM foo AS zoo\n\n        -- Alternatively...\n\n        SELECT\n            a\n        FROM foo\n\n    \"\"\"\n\n    def _lint_references_and_aliases(\n        self,\n        table_aliases,\n        standalone_aliases,\n        references,\n        col_aliases,\n        using_cols,\n        parent_select,\n    ):\n        \"\"\"Check all aliased references against tables referenced in the query.\"\"\"\n        # A buffer to keep any violations.\n        violation_buff = []\n        # Check all the references that we have, keep track of which aliases we refer to.\n        tbl_refs = set()\n        for r in references:\n            tbl_refs.update(\n                tr.part\n                for tr in r.extract_possible_references(\n                    level=r.ObjectReferenceLevel.TABLE\n                )\n            )\n\n        alias: AliasInfo\n        for alias in table_aliases:\n            if alias.aliased and alias.ref_str not in tbl_refs:\n                fixes = [LintFix(\"delete\", alias.alias_expression)]\n                found_alias_segment = False\n                # Walk back to remove indents/whitespaces\n                for segment in reversed(alias.from_expression_element.segments):\n                    if not found_alias_segment:\n                        if segment is alias.alias_expression:\n                            found_alias_segment = True\n                    else:\n                        if (\n                            segment.name == \"whitespace\"\n                            or segment.name == \"newline\"\n                            or segment.is_meta\n                        ):\n                            fixes.append(LintFix(\"delete\", segment))\n                        else:\n                            # Stop once we reach an other, \"regular\" segment.\n                            break\n                violation_buff.append(\n                    LintResult(\n                        anchor=alias.segment,\n                        description=\"Alias {!r} is never used in SELECT statement.\".format(\n                            alias.ref_str\n                        ),\n                        fixes=fixes,\n                    )\n                )\n        return violation_buff or None",
        "file_path": "src/sqlfluff/rules/L025.py",
        "chunk_index": 0,
        "metadata": {
          "class_name": "Rule_L025",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "Rule_L020"
            ]
          },
          "parent_name": null,
          "private_methods_count": 1,
          "public_methods_count": 0,
          "node_count": 319,
          "chunk_size": 2583
        }
      },
      {
        "rank": 3,
        "score": 0.6692994236946106,
        "content": "class Rule_L013(BaseRule):\n    \"\"\"Column expression without alias. Use explicit `AS` clause.\n\n    | **Anti-pattern**\n    | In this example, there is no alias for both sums.\n\n    .. code-block:: sql\n\n        SELECT\n            sum(a),\n            sum(b)\n        FROM foo\n\n    | **Best practice**\n    | Add aliases.\n\n    .. code-block:: sql\n\n        SELECT\n            sum(a) AS a_sum,\n            sum(b) AS b_sum\n        FROM foo\n\n    \"\"\"\n\n    config_keywords = [\"allow_scalar\"]\n\n    def _eval(self, segment, parent_stack, **kwargs):\n        \"\"\"Column expression without alias. Use explicit `AS` clause.\n\n        We look for the select_clause_element segment, and then evaluate\n        whether it has an alias segment or not and whether the expression\n        is complicated enough. `parent_stack` is to assess how many other\n        elements there are.\n\n        \"\"\"\n        if segment.is_type(\"select_clause_element\"):\n            if not any(e.is_type(\"alias_expression\") for e in segment.segments):\n                types = {e.get_type() for e in segment.segments if e.name != \"star\"}\n                unallowed_types = types - {\n                    \"whitespace\",\n                    \"newline\",\n                    \"column_reference\",\n                    \"wildcard_expression\",\n                }\n                if len(unallowed_types) > 0:\n                    # No fixes, because we don't know what the alias should be,\n                    # the user should document it themselves.\n                    if self.allow_scalar:\n                        # Check *how many* elements there are in the select\n                        # statement. If this is the only one, then we won't\n                        # report an error.\n                        num_elements = sum(\n                            e.is_type(\"select_clause_element\")\n                            for e in parent_stack[-1].segments\n                        )\n                        if num_elements > 1:\n                            return LintResult(anchor=segment)\n                        else:\n                            return None\n                    else:\n                        # Just error if we don't care.\n                        return LintResult(anchor=segment)\n        return None",
        "file_path": "src/sqlfluff/rules/L013.py",
        "chunk_index": 0,
        "metadata": {
          "class_name": "Rule_L013",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "BaseRule"
            ]
          },
          "parent_name": null,
          "private_methods_count": 1,
          "public_methods_count": 0,
          "node_count": 265,
          "chunk_size": 2250
        }
      },
      {
        "rank": 4,
        "score": 0.6687676906585693,
        "content": "class Rule_L011(BaseRule):\n    \"\"\"Implicit/explicit aliasing of table.\n\n    Aliasing of table to follow preference\n    (explicit using an `AS` clause is default).\n\n    | **Anti-pattern**\n    | In this example, the alias 'voo' is implicit.\n\n    .. code-block:: sql\n\n        SELECT\n            voo.a\n        FROM foo voo\n\n    | **Best practice**\n    | Add `AS` to make it explicit.\n\n    .. code-block:: sql\n\n        SELECT\n            voo.a\n        FROM foo AS voo\n\n    \"\"\"\n\n    config_keywords = [\"aliasing\"]\n\n    _target_elems = (\"from_expression_element\",)\n\n    def _eval(self, segment, parent_stack, raw_stack, **kwargs):\n        \"\"\"Implicit aliasing of table/column not allowed. Use explicit `AS` clause.\n\n        We look for the alias segment, and then evaluate its parent and whether\n        it contains an AS keyword. This is the _eval function for both L011 and L012.\n\n        The use of `raw_stack` is just for working out how much whitespace to add.\n\n        \"\"\"\n        fixes = []\n\n        if segment.is_type(\"alias_expression\"):\n            if parent_stack[-1].is_type(*self._target_elems):\n                if any(e.name.lower() == \"as\" for e in segment.segments):\n                    if self.aliasing == \"implicit\":\n                        if segment.segments[0].name.lower() == \"as\":\n\n                            # Remove the AS as we're using implict aliasing\n                            fixes.append(LintFix(\"delete\", segment.segments[0]))\n                            anchor = raw_stack[-1]\n\n                            # Remove whitespace before (if exists) or after (if not)\n                            if (\n                                len(raw_stack) > 0\n                                and raw_stack[-1].type == \"whitespace\"\n                            ):\n                                fixes.append(LintFix(\"delete\", raw_stack[-1]))\n                            elif (\n                                len(segment.segments) > 0\n                                and segment.segments[1].type == \"whitespace\"\n                            ):\n                                fixes.append(LintFix(\"delete\", segment.segments[1]))\n\n                            return LintResult(anchor=anchor, fixes=fixes)\n\n                else:\n                    insert_buff = []\n\n                    # Add initial whitespace if we need to...\n                    if raw_stack[-1].name not in [\"whitespace\", \"newline\"]:\n                        insert_buff.append(WhitespaceSegment())\n\n                    # Add an AS (Uppercase for now, but could be corrected later)\n                    insert_buff.append(KeywordSegment(\"AS\"))\n\n                    # Add a trailing whitespace if we need to\n                    if segment.segments[0].name not in [\"whitespace\", \"newline\"]:\n                        insert_buff.append(WhitespaceSegment())\n\n                    return LintResult(\n                        anchor=segment,\n                        fixes=[LintFix(\"create\", segment.segments[0], insert_buff)],\n                    )\n        return None",
        "file_path": "src/sqlfluff/rules/L011.py",
        "chunk_index": 0,
        "metadata": {
          "class_name": "Rule_L011",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "BaseRule"
            ]
          },
          "parent_name": null,
          "private_methods_count": 1,
          "public_methods_count": 0,
          "node_count": 528,
          "chunk_size": 3040
        }
      },
      {
        "rank": 5,
        "score": 0.6540638208389282,
        "content": "class Rule_L012(Rule_L011):\n    \"\"\"Implicit/explicit aliasing of columns.\n\n    Aliasing of columns to follow preference\n    (explicit using an `AS` clause is default).\n\n    NB: This rule inherits its functionality from obj:`Rule_L011` but is\n    separate so that they can be enabled and disabled separately.\n\n    | **Anti-pattern**\n    | In this example, the alias for column 'a' is implicit.\n\n    .. code-block:: sql\n\n        SELECT\n            a\n        FROM foo\n\n    | **Best practice**\n    | Add `AS` to make it explicit.\n\n    .. code-block:: sql\n\n        SELECT\n            a AS alias_col\n        FROM foo\n\n    \"\"\"\n\n    config_keywords = [\"aliasing\"]\n\n    _target_elems = (\"select_clause_element\",)",
        "file_path": "src/sqlfluff/rules/L012.py",
        "chunk_index": 0,
        "metadata": {
          "class_name": "Rule_L012",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "Rule_L011"
            ]
          },
          "parent_name": null,
          "private_methods_count": 0,
          "public_methods_count": 0,
          "node_count": 37,
          "chunk_size": 703
        }
      },
      {
        "rank": 6,
        "score": 0.6406741738319397,
        "content": "class Rule_L020(BaseRule):\n    \"\"\"Table aliases should be unique within each clause.\n\n    | **Anti-pattern**\n    | In this example, the alias 't' is reused for two different ables:\n\n    .. code-block:: sql\n\n        SELECT\n            t.a,\n            t.b\n        FROM foo AS t, bar AS t\n\n        -- this can also happen when using schemas where the implicit alias is the table name:\n\n        SELECT\n            a,\n            b\n        FROM\n            2020.foo,\n            2021.foo\n\n    | **Best practice**\n    | Make all tables have a unique alias\n\n    .. code-block:: sql\n\n        SELECT\n            f.a,\n            b.b\n        FROM foo AS f, bar AS b\n\n        -- Also use explicit alias's when referencing two tables with same name from two different schemas\n\n        SELECT\n            f1.a,\n            f2.b\n        FROM\n            2020.foo AS f1,\n            2021.foo AS f2\n\n    \"\"\"\n\n    def _lint_references_and_aliases(\n        self,\n        table_aliases,\n        standalone_aliases,\n        references,\n        col_aliases,\n        using_cols,\n        parent_select,\n    ):\n        \"\"\"Check whether any aliases are duplicates.\n\n        NB: Subclasses of this error should override this function.\n\n        \"\"\"\n        # Are any of the aliases the same?\n        duplicate = set()\n        for a1, a2 in itertools.combinations(table_aliases, 2):\n            # Compare the strings\n            if a1.ref_str == a2.ref_str and a1.ref_str:\n                duplicate.add(a2)\n        if duplicate:\n            return [\n                LintResult(\n                    # Reference the element, not the string.\n                    anchor=aliases.segment,\n                    description=(\n                        \"Duplicate table alias {!r}. Table \" \"aliases should be unique.\"\n                    ).format(aliases.ref_str),\n                )\n                for aliases in duplicate\n            ]\n        else:\n            return None\n\n    def _eval(self, segment, parent_stack, dialect, **kwargs):\n        \"\"\"Get References and Aliases and allow linting.\n\n        This rule covers a lot of potential cases of odd usages of\n        references, see the code for each of the potential cases.\n\n        Subclasses of this rule should override the\n        `_lint_references_and_aliases` method.\n        \"\"\"\n        if segment.is_type(\"select_statement\"):\n            select_info = get_select_statement_info(segment, dialect)\n            if not select_info:\n                return None\n\n            # Work out if we have a parent select function\n            parent_select = None\n            for seg in reversed(parent_stack):\n                if seg.is_type(\"select_statement\"):\n                    parent_select = seg\n                    break\n\n            # Pass them all to the function that does all the work.\n            # NB: Subclasses of this rules should override the function below\n            return self._lint_references_and_aliases(\n                select_info.table_aliases,\n                select_info.standalone_aliases,\n                select_info.reference_buffer,\n                select_info.col_aliases,\n                select_info.using_cols,\n                parent_select,\n            )\n        return None",
        "file_path": "src/sqlfluff/rules/L020.py",
        "chunk_index": 0,
        "metadata": {
          "class_name": "Rule_L020",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "BaseRule"
            ]
          },
          "parent_name": null,
          "private_methods_count": 2,
          "public_methods_count": 0,
          "node_count": 310,
          "chunk_size": 3223
        }
      },
      {
        "rank": 7,
        "score": 0.6358122825622559,
        "content": "class Rule_L032(BaseRule):\n    \"\"\"Prefer specifying join keys instead of using \"USING\".\n\n    | **Anti-pattern**\n\n    .. code-block:: sql\n\n        SELECT\n            table_a.field_1,\n            table_b.field_2\n        FROM\n            table_a\n        INNER JOIN table_b USING (id)\n\n    | **Best practice**\n    |  Specify the keys directly\n\n    .. code-block:: sql\n\n        SELECT\n            table_a.field_1,\n            table_b.field_2\n        FROM\n            table_a\n        INNER JOIN table_b\n            ON table_a.id = table_b.id\n\n    \"\"\"\n\n    def _eval(self, segment, **kwargs):\n        \"\"\"Look for USING in a join clause.\"\"\"\n        if segment.is_type(\"join_clause\"):\n            for seg in segment.segments:\n                if seg.is_type(\"keyword\") and seg.name == \"using\":\n                    return [\n                        LintResult(\n                            # Reference the element, not the string.\n                            anchor=seg,\n                            description=(\n                                \"Found USING statement. Expected only ON statements.\"\n                            ),\n                        )\n                    ]\n        return None",
        "file_path": "src/sqlfluff/rules/L032.py",
        "chunk_index": 0,
        "metadata": {
          "class_name": "Rule_L032",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "BaseRule"
            ]
          },
          "parent_name": null,
          "private_methods_count": 1,
          "public_methods_count": 0,
          "node_count": 118,
          "chunk_size": 1184
        }
      },
      {
        "rank": 8,
        "score": 0.6223047971725464,
        "content": "class Rule_L028(Rule_L025):\n    \"\"\"References should be consistent in statements with a single table.\n\n    NB: This rule is disabled by default for BigQuery due to its use of\n    structs which trigger false positives. It can be enabled with the\n    `force_enable = True` flag.\n\n    | **Anti-pattern**\n    | In this example, only the field `b` is referenced.\n\n    .. code-block:: sql\n\n        SELECT\n            a,\n            foo.b\n        FROM foo\n\n    | **Best practice**\n    |  Remove all the reference or reference all the fields.\n\n    .. code-block:: sql\n\n        SELECT\n            a,\n            b\n        FROM foo\n\n        -- Also good\n\n        SELECT\n            foo.a,\n            foo.b\n        FROM foo\n\n    \"\"\"\n\n    config_keywords = [\"single_table_references\", \"force_enable\"]\n\n    def _lint_references_and_aliases(\n        self,\n        table_aliases,\n        standalone_aliases,\n        references,\n        col_aliases,\n        using_cols,\n        parent_select,\n    ):\n        \"\"\"Iterate through references and check consistency.\"\"\"\n        # How many aliases are there? If more than one then abort.\n        if len(table_aliases) > 1:\n            return None\n        # A buffer to keep any violations.\n        violation_buff = []\n        # Check all the references that we have.\n        seen_ref_types = set()\n        for ref in references:\n            # We skip any unqualified wildcard references (i.e. *). They shouldn't count.\n            if not ref.is_qualified() and ref.is_type(\"wildcard_identifier\"):\n                continue\n            # Oddball case: Column aliases provided via function calls in by\n            # FROM or JOIN. References to these don't need to be qualified.\n            # Note there could be a table with a column by the same name as\n            # this alias, so avoid bogus warnings by just skipping them\n            # entirely rather than trying to enforce anything.\n            if ref.raw in standalone_aliases:\n                continue\n            this_ref_type = ref.qualification()\n            if self.single_table_references == \"consistent\":\n                if seen_ref_types and this_ref_type not in seen_ref_types:\n                    violation_buff.append(\n                        LintResult(\n                            anchor=ref,\n                            description=f\"{this_ref_type.capitalize()} reference \"\n                            f\"{ref.raw!r} found in single table select which is \"\n                            \"inconsistent with previous references.\",\n                        )\n                    )\n            elif self.single_table_references != this_ref_type:\n                violation_buff.append(\n                    LintResult(\n                        anchor=ref,\n                        description=\"{} reference {!r} found in single table select.\".format(\n                            this_ref_type.capitalize(), ref.raw\n                        ),\n                    )\n                )\n            seen_ref_types.add(this_ref_type)\n\n        return violation_buff or None\n\n    def _eval(self, segment, parent_stack, dialect, **kwargs):\n        \"\"\"Override Rule L025 for dialects that use structs.\n\n        Some dialects use structs (e.g. column.field) which look like\n        table references and so incorrectly trigger this rule.\n        \"\"\"\n        if dialect.name in [\"bigquery\"] and not self.force_enable:\n            return LintResult()\n\n        return super()._eval(segment, parent_stack, dialect, **kwargs)",
        "file_path": "src/sqlfluff/rules/L028.py",
        "chunk_index": 0,
        "metadata": {
          "class_name": "Rule_L028",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "Rule_L025"
            ]
          },
          "parent_name": null,
          "private_methods_count": 2,
          "public_methods_count": 0,
          "node_count": 395,
          "chunk_size": 3493
        }
      },
      {
        "rank": 9,
        "score": 0.621888279914856,
        "content": "class Rule_L026(Rule_L020):\n    \"\"\"References cannot reference objects not present in FROM clause.\n\n    NB: This rule is disabled by default for BigQuery due to its use of\n    structs which trigger false positives. It can be enabled with the\n    `force_enable = True` flag.\n\n    | **Anti-pattern**\n    | In this example, the reference 'vee' has not been declared.\n\n    .. code-block:: sql\n\n        SELECT\n            vee.a\n        FROM foo\n\n    | **Best practice**\n    |  Remove the reference.\n\n    .. code-block:: sql\n\n        SELECT\n            a\n        FROM foo\n\n    \"\"\"\n\n    config_keywords = [\"force_enable\"]\n\n    @staticmethod\n    def _is_bad_tbl_ref(table_aliases, parent_select, tbl_ref):\n        \"\"\"Given a table reference, try to find what it's referring to.\"\"\"\n        # Is it referring to one of the table aliases?\n        if tbl_ref[0] in [a.ref_str for a in table_aliases]:\n            # Yes. Therefore okay.\n            return False\n\n        # Not a table alias. It it referring to a correlated subquery?\n        if parent_select:\n            parent_aliases, _ = get_aliases_from_select(parent_select)\n            if parent_aliases and tbl_ref[0] in [a[0] for a in parent_aliases]:\n                # Yes. Therefore okay.\n                return False\n\n        # It's not referring to an alias or a correlated subquery. Looks like a\n        # bad reference (i.e. referring to something unknown.)\n        return True\n\n    def _lint_references_and_aliases(\n        self,\n        table_aliases,\n        standalone_aliases,\n        references,\n        col_aliases,\n        using_cols,\n        parent_select,\n    ):\n        # A buffer to keep any violations.\n        violation_buff = []\n\n        # Check all the references that we have, do they reference present aliases?\n        for r in references:\n            tbl_refs = r.extract_possible_references(level=r.ObjectReferenceLevel.TABLE)\n            if tbl_refs and all(\n                self._is_bad_tbl_ref(table_aliases, parent_select, tbl_ref)\n                for tbl_ref in tbl_refs\n            ):\n                violation_buff.append(\n                    LintResult(\n                        # Return the first segment rather than the string\n                        anchor=tbl_refs[0].segments[0],\n                        description=f\"Reference {r.raw!r} refers to table/view \"\n                        \"not found in the FROM clause or found in parent \"\n                        \"subquery.\",\n                    )\n                )\n        return violation_buff or None\n\n    def _eval(self, segment, parent_stack, dialect, **kwargs):\n        \"\"\"Override Rule L020 for dialects that use structs.\n\n        Some dialects use structs (e.g. column.field) which look like\n        table references and so incorrectly trigger this rule.\n        \"\"\"\n        if dialect.name in [\"bigquery\"] and not self.force_enable:\n            return LintResult()\n\n        return super()._eval(segment, parent_stack, dialect, **kwargs)",
        "file_path": "src/sqlfluff/rules/L026.py",
        "chunk_index": 0,
        "metadata": {
          "class_name": "Rule_L026",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "Rule_L020"
            ]
          },
          "parent_name": null,
          "private_methods_count": 2,
          "public_methods_count": 0,
          "node_count": 362,
          "chunk_size": 2976
        }
      },
      {
        "rank": 10,
        "score": 0.6172903776168823,
        "content": "class Rule_L042(BaseRule):\n    \"\"\"Join/From clauses should not contain subqueries. Use CTEs instead.\n\n    By default this rule is configured to allow subqueries within `FROM`\n    clauses but not within `JOIN` clauses. If you prefer a stricter lint\n    then this is configurable.\n\n    NB: Some dialects don't allow CTEs, and for those dialects\n    this rule makes no sense and should be disabled.\n\n    | **Anti-pattern**\n\n    .. code-block:: sql\n\n        select\n            a.x, a.y, b.z\n        from a\n        join (\n            select x, z from b\n        ) using(x)\n\n\n    | **Best practice**\n\n    .. code-block:: sql\n\n        with c as (\n            select x, z from b\n        )\n        select\n            a.x, a.y, c.z\n        from a\n        join c using(x)\n\n    \"\"\"\n\n    config_keywords = [\"forbid_subquery_in\"]\n\n    _config_mapping = {\n        \"join\": [\"join_clause\"],\n        \"from\": [\"from_expression\"],\n        \"both\": [\"join_clause\", \"from_expression\"],\n    }\n\n    def _eval(self, segment, **kwargs):\n        \"\"\"Join/From clauses should not contain subqueries. Use CTEs instead.\n\n        NB: No fix for this routine because it would be very complex to\n        implement reliably.\n        \"\"\"\n        parent_types = self._config_mapping[self.forbid_subquery_in]\n        for parent_type in parent_types:\n            if segment.is_type(parent_type):\n                # Get the referenced table segment\n                from_expression_element = segment.get_child(\"from_expression_element\")\n                if not from_expression_element:  # pragma: no cover\n                    return None  # There isn't one. We're done.\n                # Get the main bit\n                from_expression_element = from_expression_element.get_child(\n                    \"table_expression\"\n                )\n                if not from_expression_element:  # pragma: no cover\n                    return None  # There isn't one. We're done.\n                # Is it bracketed?\n                bracketed_expression = from_expression_element.get_child(\"bracketed\")\n                # If it is, lint that instead\n                if bracketed_expression:\n                    from_expression_element = bracketed_expression\n                # If any of the following are found, raise an issue.\n                # If not, we're fine.\n                problem_children = [\n                    \"with_compound_statement\",\n                    \"set_expression\",\n                    \"select_statement\",\n                ]\n                for seg_type in problem_children:\n                    seg = from_expression_element.get_child(seg_type)\n                    if seg:\n                        return LintResult(\n                            anchor=seg,\n                            description=f\"{parent_type} clauses should not contain subqueries. Use CTEs instead\",\n                        )",
        "file_path": "src/sqlfluff/rules/L042.py",
        "chunk_index": 0,
        "metadata": {
          "class_name": "Rule_L042",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "BaseRule"
            ]
          },
          "parent_name": null,
          "private_methods_count": 1,
          "public_methods_count": 0,
          "node_count": 293,
          "chunk_size": 2858
        }
      }
    ]
  },
  "sqlfluff__sqlfluff-2419": {
    "query": "Rule L060 could give a specific error message\nAt the moment rule L060 flags something like this:\r\n\r\n```\r\nL:  21 | P:   9 | L060 | Use 'COALESCE' instead of 'IFNULL' or 'NVL'.\r\n```\r\n\r\nSince we likely know the wrong word, it might be nice to actually flag that instead of both `IFNULL` and `NVL` - like most of the other rules do.\r\n\r\nThat is it should flag this:\r\n\r\n```\r\nL:  21 | P:   9 | L060 | Use 'COALESCE' instead of 'IFNULL'.\r\n```\r\n Or this:\r\n\r\n```\r\nL:  21 | P:   9 | L060 | Use 'COALESCE' instead of 'NVL'.\r\n```\r\n\r\nAs appropriate.\r\n\r\nWhat do you think @jpy-git ?\r\n\n",
    "method": "hierarchical",
    "top_k": 10,
    "results": [
      {
        "rank": 1,
        "score": 0.7729009389877319,
        "content": "class Rule_L060(BaseRule):\n    \"\"\"Use ``COALESCE`` instead of ``IFNULL`` or ``NVL``.\n\n    | **Anti-pattern**\n    | ``IFNULL`` or ``NVL`` are used to fill ``NULL`` values.\n\n    .. code-block:: sql\n\n        SELECT ifnull(foo, 0) AS bar,\n        FROM baz;\n\n        SELECT nvl(foo, 0) AS bar,\n        FROM baz;\n\n    | **Best practice**\n    | Use ``COALESCE`` instead.\n    | ``COALESCE`` is universally supported,\n    | whereas Redshift doesn't support ``IFNULL``\n    | and BigQuery doesn't support ``NVL``.\n    | Additionally ``COALESCE`` is more flexible\n    | and accepts an arbitrary number of arguments.\n\n    .. code-block:: sql\n\n        SELECT coalesce(foo, 0) AS bar,\n        FROM baz;\n\n    \"\"\"\n\n    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Use ``COALESCE`` instead of ``IFNULL`` or ``NVL``.\"\"\"\n        # We only care about function names.\n        if context.segment.name != \"function_name_identifier\":\n            return None\n\n        # Only care if the function is ``IFNULL`` or ``NVL``.\n        if context.segment.raw_upper not in {\"IFNULL\", \"NVL\"}:\n            return None\n\n        # Create fix to replace ``IFNULL`` or ``NVL`` with ``COALESCE``.\n        fix = LintFix.replace(\n            context.segment,\n            [\n                CodeSegment(\n                    raw=\"COALESCE\",\n                    name=\"function_name_identifier\",\n                    type=\"function_name_identifier\",\n                )\n            ],\n        )\n\n        return LintResult(context.segment, [fix])",
        "file_path": "src/sqlfluff/rules/L060.py",
        "chunk_index": 0,
        "metadata": {
          "class_name": "Rule_L060",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "BaseRule"
            ]
          },
          "parent_name": null,
          "private_methods_count": 1,
          "public_methods_count": 0,
          "node_count": 162,
          "chunk_size": 1524
        }
      },
      {
        "rank": 2,
        "score": 0.6435484886169434,
        "content": "class Rule_L043(BaseRule):\n    \"\"\"Unnecessary ``CASE`` statement.\n\n    | **Anti-pattern**\n    | ``CASE`` statement returns booleans.\n\n    .. code-block:: sql\n        :force:\n\n        select\n            case\n                when fab > 0 then true\n                else false\n            end as is_fab\n        from fancy_table\n\n        -- This rule can also simplify CASE statements\n        -- that aim to fill NULL values.\n\n        select\n            case\n                when fab is null then 0\n                else fab\n            end as fab_clean\n        from fancy_table\n\n        -- This also covers where the case statement\n        -- replaces NULL values with NULL values.\n\n        select\n            case\n                when fab is null then null\n                else fab\n            end as fab_clean\n        from fancy_table\n\n    | **Best practice**\n    | Reduce to ``WHEN`` condition within ``COALESCE`` function.\n\n    .. code-block:: sql\n        :force:\n\n        select\n            coalesce(fab > 0, false) as is_fab\n        from fancy_table\n\n        -- To fill NULL values.\n\n        select\n            coalesce(fab, 0) as fab_clean\n        from fancy_table\n\n        -- NULL filling NULL.\n\n        select fab as fab_clean\n        from fancy_table\n\n\n    \"\"\"\n\n    @staticmethod\n    def _coalesce_fix_list(\n        context: RuleContext,\n        coalesce_arg_1: BaseSegment,\n        coalesce_arg_2: BaseSegment,\n        preceding_not: bool = False,\n    ) -> List[LintFix]:\n        \"\"\"Generate list of fixes to convert CASE statement to COALESCE function.\"\"\"\n        # Add coalesce and opening parenthesis.\n        edits = [\n            KeywordSegment(\"coalesce\"),\n            SymbolSegment(\"(\", name=\"start_bracket\", type=\"start_bracket\"),\n            coalesce_arg_1,\n            SymbolSegment(\",\", name=\"comma\", type=\"comma\"),\n            WhitespaceSegment(),\n            coalesce_arg_2,\n            SymbolSegment(\")\", name=\"end_bracket\", type=\"end_bracket\"),\n        ]\n\n        if preceding_not:\n            not_edits: List[BaseSegment] = [\n                KeywordSegment(\"not\"),\n                WhitespaceSegment(),\n            ]\n            edits = not_edits + edits\n\n        fixes = [\n            LintFix.replace(\n                context.segment,\n                edits,\n            )\n        ]\n        return fixes\n\n    @staticmethod\n    def _column_only_fix_list(\n        context: RuleContext,\n        column_reference_segment: BaseSegment,\n    ) -> List[LintFix]:\n        \"\"\"Generate list of fixes to reduce CASE statement to a single column.\"\"\"\n        fixes = [\n            LintFix.replace(\n                context.segment,\n                [column_reference_segment],\n            )\n        ]\n        return fixes\n\n    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Unnecessary CASE statement.\"\"\"\n        # Look for CASE expression.\n        if (\n            context.segment.is_type(\"case_expression\")\n            and context.segment.segments[0].name == \"case\"\n        ):\n            # Find all 'WHEN' clauses and the optional 'ELSE' clause.\n            children = context.functional.segment.children()\n            when_clauses = children.select(sp.is_type(\"when_clause\"))\n            else_clauses = children.select(sp.is_type(\"else_clause\"))\n\n            # Can't fix if multiple WHEN clauses.\n            if len(when_clauses) > 1:\n                return None\n\n            # Find condition and then expressions.\n            condition_expression = when_clauses.children(sp.is_type(\"expression\"))[0]\n            then_expression = when_clauses.children(sp.is_type(\"expression\"))[1]\n\n            # Method 1: Check if THEN/ELSE expressions are both Boolean and can\n            # therefore be reduced.\n            if else_clauses:\n                else_expression = else_clauses.children(sp.is_type(\"expression\"))[0]\n                upper_bools = [\"TRUE\", \"FALSE\"]\n                if (\n                    (then_expression.raw_upper in upper_bools)\n                    and (else_expression.raw_upper in upper_bools)\n                    and (then_expression.raw_upper != else_expression.raw_upper)\n                ):\n                    coalesce_arg_1 = condition_expression\n                    coalesce_arg_2 = KeywordSegment(\"false\")\n                    preceding_not = then_expression.raw_upper == \"FALSE\"\n\n                    fixes = self._coalesce_fix_list(\n                        context,\n                        coalesce_arg_1,\n                        coalesce_arg_2,\n                        preceding_not,\n                    )\n\n                    return LintResult(\n                        anchor=condition_expression,\n                        fixes=fixes,\n                        description=\"Unnecessary CASE statement. \"\n                        \"Use COALESCE function instead.\",\n                    )\n\n            # Method 2: Check if the condition expression is comparing a column\n            # reference to NULL and whether that column reference is also in either the\n            # THEN/ELSE expression. We can only apply this method when there is only\n            # one condition in the condition expression.\n            condition_expression_segments_raw = {\n                segment.raw_upper for segment in condition_expression.segments\n            }\n            if {\"IS\", \"NULL\"}.issubset(condition_expression_segments_raw) and (\n                not condition_expression_segments_raw.intersection({\"AND\", \"OR\"})\n            ):\n                # Check if the comparison is to NULL or NOT NULL.\n                is_not_prefix = \"NOT\" in condition_expression_segments_raw\n\n                # Locate column reference in condition expression.\n                column_reference_segment = (\n                    Segments(condition_expression)\n                    .children(sp.is_type(\"column_reference\"))\n                    .get()\n                )\n\n                # Return None if none found (this condition does not apply to functions)\n                if not column_reference_segment:\n                    return None\n\n                if else_clauses:\n                    else_expression = else_clauses.children(sp.is_type(\"expression\"))[0]\n                    # Check if we can reduce the CASE expression to a single coalesce\n                    # function.\n                    if (\n                        not is_not_prefix\n                        and column_reference_segment.raw_upper\n                        == else_expression.raw_upper\n                    ):\n                        coalesce_arg_1 = else_expression\n                        coalesce_arg_2 = then_expression\n                    elif (\n                        is_not_prefix\n                        and column_reference_segment.raw_upper\n                        == then_expression.raw_upper\n                    ):\n                        coalesce_arg_1 = then_expression\n                        coalesce_arg_2 = else_expression\n                    else:\n                        return None\n\n                    if coalesce_arg_2.raw_upper == \"NULL\":\n                        # Can just specify the column on it's own\n                        # rather than using a COALESCE function.\n                        return LintResult(\n                            anchor=condition_expression,\n                            fixes=self._column_only_fix_list(\n                                context,\n                                column_reference_segment,\n                            ),\n                            description=\"Unnecessary CASE statement. \"\n                            f\"Just use column '{column_reference_segment.raw}'.\",\n                        )\n\n                    return LintResult(\n                        anchor=condition_expression,\n                        fixes=self._coalesce_fix_list(\n                            context,\n                            coalesce_arg_1,\n                            coalesce_arg_2,\n                        ),\n                        description=\"Unnecessary CASE statement. \"\n                        \"Use COALESCE function instead.\",\n                    )\n                elif (\n                    column_reference_segment.raw_segments_upper\n                    == then_expression.raw_segments_upper\n                ):\n                    # Can just specify the column on it's own\n                    # rather than using a COALESCE function.\n                    # In this case no ELSE statement is equivalent to ELSE NULL.\n                    return LintResult(\n                        anchor=condition_expression,\n                        fixes=self._column_only_fix_list(\n                            context,\n                            column_reference_segment,\n                        ),\n                        description=\"Unnecessary CASE statement. \"\n                        f\"Just use column '{column_reference_segment.raw}'.\",\n                    )\n\n        return None",
        "file_path": "src/sqlfluff/rules/L043.py",
        "chunk_index": 0,
        "metadata": {
          "class_name": "Rule_L043",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "BaseRule"
            ]
          },
          "parent_name": null,
          "private_methods_count": 1,
          "public_methods_count": 0,
          "node_count": 1121,
          "chunk_size": 8932
        }
      },
      {
        "rank": 3,
        "score": 0.6098366975784302,
        "content": "class Rule_L049(Rule_L006):\n    \"\"\"Comparisons with NULL should use \"IS\" or \"IS NOT\".\n\n    | **Anti-pattern**\n    | In this example, the ``=`` operator is used to check for ``NULL`` values.\n\n    .. code-block:: sql\n\n        SELECT\n            a\n        FROM foo\n        WHERE a = NULL\n\n\n    | **Best practice**\n    | Use ``IS`` or ``IS NOT`` to check for ``NULL`` values.\n\n    .. code-block:: sql\n\n        SELECT\n            a\n        FROM foo\n        WHERE a IS NULL\n    \"\"\"\n\n    def _eval(self, context: RuleContext) -> LintResult:\n        \"\"\"Relational operators should not be used to check for NULL values.\"\"\"\n        # Context/motivation for this rule:\n        # https://news.ycombinator.com/item?id=28772289\n        # https://stackoverflow.com/questions/9581745/sql-is-null-and-null\n        if len(context.segment.segments) <= 2:\n            return LintResult()\n\n        # Allow assignments in SET clauses\n        if context.parent_stack and context.parent_stack[-1].is_type(\n            \"set_clause_list\", \"execute_script_statement\"\n        ):\n            return LintResult()\n\n        # Allow assignments in EXEC clauses\n        if context.segment.is_type(\"set_clause_list\", \"execute_script_statement\"):\n            return LintResult()\n\n        # Iterate through children of this segment looking for equals or \"not\n        # equals\". Once found, check if the next code segment is a NULL literal.\n        idx_operator = None\n        operator = None\n        for idx, sub_seg in enumerate(context.segment.segments):\n            # Skip anything which is whitespace or non-code.\n            if sub_seg.is_whitespace or not sub_seg.is_code:\n                continue\n\n            # Look for \"=\" or \"<>\".\n            if not operator and sub_seg.name in (\"equals\", \"not_equal_to\"):\n                self.logger.debug(\n                    \"Found equals/not equals @%s: %r\", sub_seg.pos_marker, sub_seg.raw\n                )\n                idx_operator = idx\n                operator = sub_seg\n            elif operator:\n                # Look for a \"NULL\" literal.\n                if sub_seg.name == \"null_literal\":\n                    self.logger.debug(\n                        \"Found NULL literal following equals/not equals @%s: %r\",\n                        sub_seg.pos_marker,\n                        sub_seg.raw,\n                    )\n                    if sub_seg.raw[0] == \"N\":\n                        is_seg = KeywordSegment(\"IS\")\n                        not_seg = KeywordSegment(\"NOT\")\n                    else:\n                        is_seg = KeywordSegment(\"is\")\n                        not_seg = KeywordSegment(\"not\")\n\n                    edit: List[Union[WhitespaceSegment, KeywordSegment]] = (\n                        [is_seg]\n                        if operator.name == \"equals\"\n                        else [\n                            is_seg,\n                            WhitespaceSegment(),\n                            not_seg,\n                        ]\n                    )\n                    prev_seg = self._find_segment(\n                        idx_operator, context.segment.segments, before=True\n                    )\n                    next_seg = self._find_segment(\n                        idx_operator, context.segment.segments, before=False\n                    )\n                    if self._missing_whitespace(prev_seg, before=True):\n                        whitespace_segment: List[\n                            Union[WhitespaceSegment, KeywordSegment]\n                        ] = [WhitespaceSegment()]\n                        edit = whitespace_segment + edit\n                    if self._missing_whitespace(next_seg, before=False):\n                        edit = edit + [WhitespaceSegment()]\n                    return LintResult(\n                        anchor=operator,\n                        fixes=[\n                            LintFix.replace(\n                                operator,\n                                edit,\n                            )\n                        ],\n                    )\n        # If we get to here, it's not a violation\n        return LintResult()",
        "file_path": "src/sqlfluff/rules/L049.py",
        "chunk_index": 0,
        "metadata": {
          "class_name": "Rule_L049",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "Rule_L006"
            ]
          },
          "parent_name": null,
          "private_methods_count": 1,
          "public_methods_count": 0,
          "node_count": 614,
          "chunk_size": 4117
        }
      },
      {
        "rank": 4,
        "score": 0.6074455380439758,
        "content": "class Rule_L035(BaseRule):\n    \"\"\"Do not specify ``else null`` in a case when statement (redundant).\n\n    | **Anti-pattern**\n\n    .. code-block:: sql\n\n        select\n            case\n                when name like '%cat%' then 'meow'\n                when name like '%dog%' then 'woof'\n                else null\n            end\n        from x\n\n    | **Best practice**\n    |  Omit ``else null``\n\n    .. code-block:: sql\n\n        select\n            case\n                when name like '%cat%' then 'meow'\n                when name like '%dog%' then 'woof'\n            end\n        from x\n    \"\"\"\n\n    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Find rule violations and provide fixes.\n\n        0. Look for a case expression\n        1. Look for \"ELSE\"\n        2. Mark \"ELSE\" for deletion (populate \"fixes\")\n        3. Backtrack and mark all newlines/whitespaces for deletion\n        4. Look for a raw \"NULL\" segment\n        5.a. The raw \"NULL\" segment is found, we mark it for deletion and return\n        5.b. We reach the end of case when without matching \"NULL\": the rule passes\n        \"\"\"\n        if context.segment.is_type(\"case_expression\"):\n            children = context.functional.segment.children()\n            else_clause = children.first(sp.is_type(\"else_clause\"))\n\n            # Does the \"ELSE\" have a \"NULL\"? NOTE: Here, it's safe to look for\n            # \"NULL\", as an expression would *contain* NULL but not be == NULL.\n            if else_clause and else_clause.children(\n                lambda child: child.raw_upper == \"NULL\"\n            ):\n                # Found ELSE with NULL. Delete the whole else clause as well as\n                # indents/whitespaces/meta preceding the ELSE. :TRICKY: Note\n                # the use of reversed() to make select() effectively search in\n                # reverse.\n                before_else = children.reversed().select(\n                    start_seg=else_clause[0],\n                    loop_while=sp.or_(\n                        sp.is_name(\"whitespace\", \"newline\"), sp.is_meta()\n                    ),\n                )\n                return LintResult(\n                    anchor=context.segment,\n                    fixes=[LintFix.delete(else_clause[0])]\n                    + [LintFix.delete(seg) for seg in before_else],\n                )\n        return None",
        "file_path": "src/sqlfluff/rules/L035.py",
        "chunk_index": 0,
        "metadata": {
          "class_name": "Rule_L035",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "BaseRule"
            ]
          },
          "parent_name": null,
          "private_methods_count": 1,
          "public_methods_count": 0,
          "node_count": 263,
          "chunk_size": 2350
        }
      },
      {
        "rank": 5,
        "score": 0.554082453250885,
        "content": "class Rule_L040(Rule_L010):\n    \"\"\"Inconsistent capitalisation of boolean/null literal.\n\n    The functionality for this rule is inherited from :obj:`Rule_L010`.\n\n    | **Anti-pattern**\n    | In this example, 'null' and 'false' are in lower-case whereas 'TRUE' is in\n    | upper-case.\n\n    .. code-block:: sql\n\n        select\n            a,\n            null,\n            TRUE,\n            false\n        from foo\n\n    | **Best practice**\n    | Ensure all literal null/true/false literals cases are used consistently\n\n    .. code-block:: sql\n\n        select\n            a,\n            NULL,\n            TRUE,\n            FALSE\n        from foo\n\n        -- Also good\n\n        select\n            a,\n            null,\n            true,\n            false\n        from foo\n\n    \"\"\"\n\n    _target_elems: List[Tuple[str, str]] = [\n        (\"name\", \"null_literal\"),\n        (\"name\", \"boolean_literal\"),\n    ]\n    _description_elem = \"Boolean/null literals\"",
        "file_path": "src/sqlfluff/rules/L040.py",
        "chunk_index": 0,
        "metadata": {
          "class_name": "Rule_L040",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "Rule_L010"
            ]
          },
          "parent_name": null,
          "private_methods_count": 0,
          "public_methods_count": 0,
          "node_count": 73,
          "chunk_size": 944
        }
      },
      {
        "rank": 6,
        "score": 0.5277564525604248,
        "content": "class Rule_L011(BaseRule):\n    \"\"\"Implicit/explicit aliasing of table.\n\n    Aliasing of table to follow preference\n    (explicit using an `AS` clause is default).\n\n    | **Anti-pattern**\n    | In this example, the alias 'voo' is implicit.\n\n    .. code-block:: sql\n\n        SELECT\n            voo.a\n        FROM foo voo\n\n    | **Best practice**\n    | Add `AS` to make it explicit.\n\n    .. code-block:: sql\n\n        SELECT\n            voo.a\n        FROM foo AS voo\n\n    \"\"\"\n\n    config_keywords = [\"aliasing\"]\n\n    _target_elems = (\"from_expression_element\",)\n\n    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Implicit aliasing of table/column not allowed. Use explicit `AS` clause.\n\n        We look for the alias segment, and then evaluate its parent and whether\n        it contains an AS keyword. This is the _eval function for both L011 and L012.\n\n        The use of `raw_stack` is just for working out how much whitespace to add.\n\n        \"\"\"\n        fixes = []\n\n        if context.segment.is_type(\"alias_expression\"):\n            if context.parent_stack[-1].is_type(*self._target_elems):\n                if any(e.name.lower() == \"as\" for e in context.segment.segments):\n                    if self.aliasing == \"implicit\":  # type: ignore\n                        if context.segment.segments[0].name.lower() == \"as\":\n\n                            # Remove the AS as we're using implict aliasing\n                            fixes.append(LintFix.delete(context.segment.segments[0]))\n                            anchor = context.raw_stack[-1]\n\n                            # Remove whitespace before (if exists) or after (if not)\n                            if (\n                                len(context.raw_stack) > 0\n                                and context.raw_stack[-1].type == \"whitespace\"\n                            ):\n                                fixes.append(LintFix.delete(context.raw_stack[-1]))\n                            elif (\n                                len(context.segment.segments) > 0\n                                and context.segment.segments[1].type == \"whitespace\"\n                            ):\n                                fixes.append(\n                                    LintFix.delete(context.segment.segments[1])\n                                )\n\n                            return LintResult(anchor=anchor, fixes=fixes)\n\n                else:\n                    insert_buff: List[Union[WhitespaceSegment, KeywordSegment]] = []\n\n                    # Add initial whitespace if we need to...\n                    if context.raw_stack[-1].name not in [\"whitespace\", \"newline\"]:\n                        insert_buff.append(WhitespaceSegment())\n\n                    # Add an AS (Uppercase for now, but could be corrected later)\n                    insert_buff.append(KeywordSegment(\"AS\"))\n\n                    # Add a trailing whitespace if we need to\n                    if context.segment.segments[0].name not in [\n                        \"whitespace\",\n                        \"newline\",\n                    ]:\n                        insert_buff.append(WhitespaceSegment())\n\n                    return LintResult(\n                        anchor=context.segment,\n                        fixes=[\n                            LintFix.create_before(\n                                context.segment.segments[0],\n                                insert_buff,\n                            )\n                        ],\n                    )\n        return None",
        "file_path": "src/sqlfluff/rules/L011.py",
        "chunk_index": 0,
        "metadata": {
          "class_name": "Rule_L011",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "BaseRule"
            ]
          },
          "parent_name": null,
          "private_methods_count": 1,
          "public_methods_count": 0,
          "node_count": 594,
          "chunk_size": 3519
        }
      },
      {
        "rank": 7,
        "score": 0.520809531211853,
        "content": "class Rule_L013(BaseRule):\n    \"\"\"Column expression without alias. Use explicit `AS` clause.\n\n    | **Anti-pattern**\n    | In this example, there is no alias for both sums.\n\n    .. code-block:: sql\n\n        SELECT\n            sum(a),\n            sum(b)\n        FROM foo\n\n    | **Best practice**\n    | Add aliases.\n\n    .. code-block:: sql\n\n        SELECT\n            sum(a) AS a_sum,\n            sum(b) AS b_sum\n        FROM foo\n\n    \"\"\"\n\n    config_keywords = [\"allow_scalar\"]\n\n    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Column expression without alias. Use explicit `AS` clause.\n\n        We look for the select_clause_element segment, and then evaluate\n        whether it has an alias segment or not and whether the expression\n        is complicated enough. `parent_stack` is to assess how many other\n        elements there are.\n\n        \"\"\"\n        segment = context.functional.segment\n        children = segment.children()\n        if segment.all(sp.is_type(\"select_clause_element\")) and not children.any(\n            sp.is_type(\"alias_expression\")\n        ):\n            # Ignore if it's a function with EMITS clause as EMITS is equivalent to AS\n            if (\n                children.select(sp.is_type(\"function\"))\n                .children()\n                .select(sp.is_type(\"emits_segment\"))\n            ):\n                return None\n\n            types = set(\n                children.select(sp.not_(sp.is_name(\"star\"))).apply(sp.get_type())\n            )\n            unallowed_types = types - {\n                \"whitespace\",\n                \"newline\",\n                \"column_reference\",\n                \"wildcard_expression\",\n            }\n            if unallowed_types:\n                # No fixes, because we don't know what the alias should be,\n                # the user should document it themselves.\n                if self.allow_scalar:  # type: ignore\n                    # Check *how many* elements there are in the select\n                    # statement. If this is the only one, then we won't\n                    # report an error.\n                    immediate_parent = context.functional.parent_stack.last()\n                    num_elements = len(\n                        immediate_parent.children(sp.is_type(\"select_clause_element\"))\n                    )\n                    if num_elements > 1:\n                        return LintResult(anchor=context.segment)\n                    else:\n                        return None\n                else:\n                    # Just error if we don't care.\n                    return LintResult(anchor=context.segment)\n        return None",
        "file_path": "src/sqlfluff/rules/L013.py",
        "chunk_index": 0,
        "metadata": {
          "class_name": "Rule_L013",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "BaseRule"
            ]
          },
          "parent_name": null,
          "private_methods_count": 1,
          "public_methods_count": 0,
          "node_count": 381,
          "chunk_size": 2643
        }
      },
      {
        "rank": 8,
        "score": 0.5106078386306763,
        "content": "class Rule_L025(BaseRule):\n    \"\"\"Tables should not be aliased if that alias is not used.\n\n    | **Anti-pattern**\n\n    .. code-block:: sql\n\n        SELECT\n            a\n        FROM foo AS zoo\n\n    | **Best practice**\n    | Use the alias or remove it. An unused alias makes code\n    | harder to read without changing any functionality.\n\n    .. code-block:: sql\n\n        SELECT\n            zoo.a\n        FROM foo AS zoo\n\n        -- Alternatively...\n\n        SELECT\n            a\n        FROM foo\n\n    \"\"\"\n\n    def _eval(self, context: RuleContext) -> EvalResultType:\n        violations: List[LintResult] = []\n        if context.segment.is_type(\"select_statement\"):\n            # Exit early if the SELECT does not define any aliases.\n            select_info = get_select_statement_info(context.segment, context.dialect)\n            if not select_info or not select_info.table_aliases:\n                return None\n\n            # Analyze the SELECT.\n            crawler = SelectCrawler(\n                context.segment, context.dialect, query_class=L025Query\n            )\n            query: L025Query = cast(L025Query, crawler.query_tree)\n            self._analyze_table_aliases(query, context.dialect)\n\n            alias: AliasInfo\n            for alias in query.aliases:\n                if alias.aliased and alias.ref_str not in query.tbl_refs:\n                    # Unused alias. Report and fix.\n                    violations.append(self._report_unused_alias(alias))\n        return violations or None\n\n    @classmethod\n    def _analyze_table_aliases(cls, query: L025Query, dialect: Dialect):\n        # Get table aliases defined in query.\n        for selectable in query.selectables:\n            select_info = selectable.select_info\n            if select_info:\n                # Record the aliases.\n                query.aliases += select_info.table_aliases\n\n                # Look at each table reference; if it's an alias reference,\n                # resolve the alias: could be an alias defined in \"query\"\n                # itself or an \"ancestor\" query.\n                for r in select_info.reference_buffer:\n                    for tr in r.extract_possible_references(\n                        level=r.ObjectReferenceLevel.TABLE\n                    ):\n                        # This function walks up the query's parent stack if necessary.\n                        cls._resolve_and_mark_reference(query, tr.part)\n\n        # Visit children.\n        for child in query.children:\n            cls._analyze_table_aliases(cast(L025Query, child), dialect)\n\n    @classmethod\n    def _resolve_and_mark_reference(cls, query: L025Query, ref: str):\n        # Does this query define the referenced alias?\n        if any(ref == a.ref_str for a in query.aliases):\n            # Yes. Record the reference.\n            query.tbl_refs.add(ref)\n        elif query.parent:\n            # No. Recursively check the query's parent hierarchy.\n            cls._resolve_and_mark_reference(cast(L025Query, query.parent), ref)\n\n    @classmethod\n    def _report_unused_alias(cls, alias: AliasInfo) -> LintResult:\n        fixes = [LintFix.delete(alias.alias_expression)]  # type: ignore\n        # Walk back to remove indents/whitespaces\n        to_delete = (\n            Segments(*alias.from_expression_element.segments)\n            .reversed()\n            .select(\n                start_seg=alias.alias_expression,\n                # Stop once we reach an other, \"regular\" segment.\n                loop_while=sp.or_(sp.is_whitespace(), sp.is_meta()),\n            )\n        )\n        fixes += [LintFix.delete(seg) for seg in to_delete]\n        return LintResult(\n            anchor=alias.segment,\n            description=\"Alias {!r} is never used in SELECT statement.\".format(\n                alias.ref_str\n            ),\n            fixes=fixes,\n        )",
        "file_path": "src/sqlfluff/rules/L025.py",
        "chunk_index": 1,
        "metadata": {
          "class_name": "Rule_L025",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "BaseRule"
            ]
          },
          "parent_name": null,
          "private_methods_count": 1,
          "public_methods_count": 0,
          "node_count": 639,
          "chunk_size": 3828
        }
      },
      {
        "rank": 9,
        "score": 0.505753755569458,
        "content": "class Rule_L012(Rule_L011):\n    \"\"\"Implicit/explicit aliasing of columns.\n\n    Aliasing of columns to follow preference\n    (explicit using an `AS` clause is default).\n\n    NB: This rule inherits its functionality from :obj:`Rule_L011` but is\n    separate so that they can be enabled and disabled separately.\n\n    | **Anti-pattern**\n    | In this example, the alias for column 'a' is implicit.\n\n    .. code-block:: sql\n\n        SELECT\n            a alias_col\n        FROM foo\n\n    | **Best practice**\n    | Add `AS` to make it explicit.\n\n    .. code-block:: sql\n\n        SELECT\n            a AS alias_col\n        FROM foo\n\n    \"\"\"\n\n    config_keywords = [\"aliasing\"]\n\n    _target_elems = (\"select_clause_element\",)",
        "file_path": "src/sqlfluff/rules/L012.py",
        "chunk_index": 0,
        "metadata": {
          "class_name": "Rule_L012",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "Rule_L011"
            ]
          },
          "parent_name": null,
          "private_methods_count": 0,
          "public_methods_count": 0,
          "node_count": 37,
          "chunk_size": 714
        }
      },
      {
        "rank": 10,
        "score": 0.5046294927597046,
        "content": "class Rule_L054(BaseRule):\n    \"\"\"Inconsistent column references in ``GROUP BY/ORDER BY`` clauses.\n\n    | **Anti-pattern**\n    | A mix of implicit and explicit column references are used in a ``GROUP BY``\n    | clause.\n\n    .. code-block:: sql\n       :force:\n\n        SELECT\n            foo,\n            bar,\n            sum(baz) AS sum_value\n        FROM fake_table\n        GROUP BY\n            foo, 2;\n\n        -- The same also applies to column\n        -- references in ORDER BY clauses.\n\n        SELECT\n            foo,\n            bar\n        FROM fake_table\n        ORDER BY\n            1, bar;\n\n    | **Best practice**\n    | Reference all ``GROUP BY/ORDER BY`` columns either by name or by position.\n\n    .. code-block:: sql\n       :force:\n\n        -- GROUP BY: Explicit\n        SELECT\n            foo,\n            bar,\n            sum(baz) AS sum_value\n        FROM fake_table\n        GROUP BY\n            foo, bar;\n\n        -- ORDER BY: Explicit\n        SELECT\n            foo,\n            bar\n        FROM fake_table\n        ORDER BY\n            foo, bar;\n\n        -- GROUP BY: Implicit\n        SELECT\n            foo,\n            bar,\n            sum(baz) AS sum_value\n        FROM fake_table\n        GROUP BY\n            1, 2;\n\n        -- ORDER BY: Implicit\n        SELECT\n            foo,\n            bar\n        FROM fake_table\n        ORDER BY\n            1, 2;\n    \"\"\"\n\n    config_keywords = [\"group_by_and_order_by_style\"]\n\n    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Inconsistent column references in GROUP BY/ORDER BY clauses.\"\"\"\n        # Config type hints\n        self.group_by_and_order_by_style: str\n\n        # We only care about GROUP BY/ORDER BY clauses.\n        if not context.segment.is_type(\"groupby_clause\", \"orderby_clause\"):\n            return None\n\n        # Look at child segments and map column references to either the implict or\n        # explicit category.\n        # N.B. segment names are used as the numeric literal type is 'raw', so best to\n        # be specific with the name.\n        column_reference_category_map = {\n            \"ColumnReferenceSegment\": \"explicit\",\n            \"ExpressionSegment\": \"explicit\",\n            \"numeric_literal\": \"implicit\",\n        }\n        column_reference_category_set = {\n            column_reference_category_map[segment.name]\n            for segment in context.segment.segments\n            if segment.name in column_reference_category_map\n        }\n\n        # If there are no column references then just return\n        if not column_reference_category_set:\n            return LintResult(memory=context.memory)\n\n        if self.group_by_and_order_by_style == \"consistent\":\n            # If consistent naming then raise lint error if either:\n\n            if len(column_reference_category_set) > 1:\n                # 1. Both implicit and explicit column references are found in the same\n                # clause.\n                return LintResult(\n                    anchor=context.segment,\n                    memory=context.memory,\n                )\n            else:\n                # 2. A clause is found to contain column name references that\n                #    contradict the precedent set in earlier clauses.\n                current_group_by_order_by_convention = (\n                    column_reference_category_set.pop()\n                )\n                prior_group_by_order_by_convention = context.memory.get(\n                    \"prior_group_by_order_by_convention\"\n                )\n\n                if prior_group_by_order_by_convention and (\n                    prior_group_by_order_by_convention\n                    != current_group_by_order_by_convention\n                ):\n                    return LintResult(\n                        anchor=context.segment,\n                        memory=context.memory,\n                    )\n\n                context.memory[\n                    \"prior_group_by_order_by_convention\"\n                ] = current_group_by_order_by_convention\n        else:\n            # If explicit or implicit naming then raise lint error\n            # if the opposite reference type is detected.\n            if any(\n                category != self.group_by_and_order_by_style\n                for category in column_reference_category_set\n            ):\n                return LintResult(\n                    anchor=context.segment,\n                    memory=context.memory,\n                )\n\n        # Return memory for later clauses.\n        return LintResult(memory=context.memory)",
        "file_path": "src/sqlfluff/rules/L054.py",
        "chunk_index": 0,
        "metadata": {
          "class_name": "Rule_L054",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "BaseRule"
            ]
          },
          "parent_name": null,
          "private_methods_count": 1,
          "public_methods_count": 0,
          "node_count": 405,
          "chunk_size": 4536
        }
      }
    ]
  },
  "sqlfluff__sqlfluff-1733": {
    "query": "Extra space when first field moved to new line in a WITH statement\nNote, the query below uses a `WITH` statement. If I just try to fix the SQL within the CTE, this works fine.\r\n\r\nGiven the following SQL:\r\n\r\n```sql\r\nWITH example AS (\r\n    SELECT my_id,\r\n        other_thing,\r\n        one_more\r\n    FROM\r\n        my_table\r\n)\r\n\r\nSELECT *\r\nFROM example\r\n```\r\n\r\n## Expected Behaviour\r\n\r\nafter running `sqlfluff fix` I'd expect (`my_id` gets moved down and indented properly):\r\n\r\n```sql\r\nWITH example AS (\r\n    SELECT\r\n        my_id,\r\n        other_thing,\r\n        one_more\r\n    FROM\r\n        my_table\r\n)\r\n\r\nSELECT *\r\nFROM example\r\n```\r\n\r\n## Observed Behaviour\r\n\r\nafter running `sqlfluff fix` we get (notice that `my_id` is indented one extra space)\r\n\r\n```sql\r\nWITH example AS (\r\n    SELECT\r\n         my_id,\r\n        other_thing,\r\n        one_more\r\n    FROM\r\n        my_table\r\n)\r\n\r\nSELECT *\r\nFROM example\r\n```\r\n\r\n## Steps to Reproduce\r\n\r\nNoted above. Create a file with the initial SQL and fun `sqfluff fix` on it.\r\n\r\n## Dialect\r\n\r\nRunning with default config.\r\n\r\n## Version\r\nInclude the output of `sqlfluff --version` along with your Python version\r\n\r\nsqlfluff, version 0.7.0\r\nPython 3.7.5\r\n\r\n## Configuration\r\n\r\nDefault config.\r\n\n",
    "method": "hierarchical",
    "top_k": 10,
    "results": [
      {
        "rank": 1,
        "score": 0.695878267288208,
        "content": "class Rule_L004(BaseRule):\n    \"\"\"Incorrect indentation type.\n\n    Note 1: spaces are only fixed to tabs if the number of spaces in the\n    indent is an integer multiple of the tab_space_size config.\n    Note 2: fixes are only applied to indents at the start of a line. Indents\n    after other text on the same line are not fixed.\n\n    | **Anti-pattern**\n    | Using tabs instead of spaces when indent_unit config set to spaces (default).\n\n    .. code-block:: sql\n       :force:\n\n        select\n        a,\n           b\n        from foo\n\n    | **Best practice**\n    | Change the line to use spaces only.\n\n    .. code-block:: sql\n       :force:\n\n        select\n        a,\n        b\n        from foo\n    \"\"\"\n\n    config_keywords = [\"indent_unit\", \"tab_space_size\"]\n\n    # TODO fix indents after text: https://github.com/sqlfluff/sqlfluff/pull/590#issuecomment-739484190\n    def _eval(self, context: RuleContext) -> LintResult:\n        \"\"\"Incorrect indentation found in file.\"\"\"\n        # Config type hints\n        self.tab_space_size: int\n        self.indent_unit: str\n\n        tab = \"\\t\"\n        space = \" \"\n        correct_indent = (\n            space * self.tab_space_size if self.indent_unit == \"space\" else tab\n        )\n        wrong_indent = (\n            tab if self.indent_unit == \"space\" else space * self.tab_space_size\n        )\n        if (\n            context.segment.is_type(\"whitespace\")\n            and wrong_indent in context.segment.raw\n        ):\n            fixes = []\n            description = \"Incorrect indentation type found in file.\"\n            edit_indent = context.segment.raw.replace(wrong_indent, correct_indent)\n            # Ensure that the number of space indents is a multiple of tab_space_size\n            # before attempting to convert spaces to tabs to avoid mixed indents\n            # unless we are converted tabs to spaces (indent_unit = space)\n            if (\n                (\n                    self.indent_unit == \"space\"\n                    or context.segment.raw.count(space) % self.tab_space_size == 0\n                )\n                # Only attempt a fix at the start of a newline for now\n                and (\n                    len(context.raw_stack) == 0\n                    or context.raw_stack[-1].is_type(\"newline\")\n                )\n            ):\n                fixes = [\n                    LintFix(\n                        \"edit\",\n                        context.segment,\n                        WhitespaceSegment(raw=edit_indent),\n                    )\n                ]\n            elif not (\n                len(context.raw_stack) == 0 or context.raw_stack[-1].is_type(\"newline\")\n            ):\n                # give a helpful message if the wrong indent has been found and is not at the start of a newline\n                description += (\n                    \" The indent occurs after other text, so a manual fix is needed.\"\n                )\n            else:\n                # If we get here, the indent_unit is tabs, and the number of spaces is not a multiple of tab_space_size\n                description += \" The number of spaces is not a multiple of tab_space_size, so a manual fix is needed.\"\n            return LintResult(\n                anchor=context.segment, fixes=fixes, description=description\n            )\n        return LintResult()",
        "file_path": "src/sqlfluff/rules/L004.py",
        "chunk_index": 0,
        "metadata": {
          "class_name": "Rule_L004",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "BaseRule"
            ]
          },
          "parent_name": null,
          "private_methods_count": 1,
          "public_methods_count": 0,
          "node_count": 438,
          "chunk_size": 3354
        }
      },
      {
        "rank": 2,
        "score": 0.6867011785507202,
        "content": "class Rule_L018(BaseRule):\n    \"\"\"WITH clause closing bracket should be aligned with WITH keyword.\n\n    | **Anti-pattern**\n    | The  character represents a space.\n    | In this example, the closing bracket is not aligned with WITH keyword.\n\n    .. code-block:: sql\n       :force:\n\n        WITH zoo AS (\n            SELECT a FROM foo\n        )\n\n        SELECT * FROM zoo\n\n    | **Best practice**\n    | Remove the spaces to align the WITH keyword with the closing bracket.\n\n    .. code-block:: sql\n\n        WITH zoo AS (\n            SELECT a FROM foo\n        )\n\n        SELECT * FROM zoo\n\n    \"\"\"\n\n    _works_on_unparsable = False\n    config_keywords = [\"tab_space_size\"]\n\n    def _eval(self, context: RuleContext) -> LintResult:\n        \"\"\"WITH clause closing bracket should be aligned with WITH keyword.\n\n        Look for a with clause and evaluate the position of closing brackets.\n        \"\"\"\n        # We only trigger on start_bracket (open parenthesis)\n        if context.segment.is_type(\"with_compound_statement\"):\n            raw_stack_buff = list(context.raw_stack)\n            # Look for the with keyword\n            for seg in context.segment.segments:\n                if seg.name.lower() == \"with\":\n                    seg_line_no = seg.pos_marker.line_no\n                    break\n            else:  # pragma: no cover\n                # This *could* happen if the with statement is unparsable,\n                # in which case then the user will have to fix that first.\n                if any(s.is_type(\"unparsable\") for s in context.segment.segments):\n                    return LintResult()\n                # If it's parsable but we still didn't find a with, then\n                # we should raise that.\n                raise RuntimeError(\"Didn't find WITH keyword!\")\n\n            def indent_size_up_to(segs):\n                seg_buff = []\n                # Get any segments running up to the WITH\n                for elem in reversed(segs):\n                    if elem.is_type(\"newline\"):\n                        break\n                    elif elem.is_meta:\n                        continue\n                    else:\n                        seg_buff.append(elem)\n                # reverse the indent if we have one\n                if seg_buff:\n                    seg_buff = list(reversed(seg_buff))\n                indent_str = \"\".join(seg.raw for seg in seg_buff).replace(\n                    \"\\t\", \" \" * self.tab_space_size\n                )\n                indent_size = len(indent_str)\n                return indent_size, indent_str\n\n            balance = 0\n            with_indent, with_indent_str = indent_size_up_to(raw_stack_buff)\n            for seg in context.segment.iter_segments(\n                expanding=[\"common_table_expression\", \"bracketed\"], pass_through=True\n            ):\n                if seg.name == \"start_bracket\":\n                    balance += 1\n                elif seg.name == \"end_bracket\":\n                    balance -= 1\n                    if balance == 0:\n                        closing_bracket_indent, _ = indent_size_up_to(raw_stack_buff)\n                        indent_diff = closing_bracket_indent - with_indent\n                        # Is indent of closing bracket not the same as\n                        # indent of WITH keyword.\n                        if seg.pos_marker.line_no == seg_line_no:\n                            # Skip if it's the one-line version. That's ok\n                            pass\n                        elif indent_diff < 0:\n                            return LintResult(\n                                anchor=seg,\n                                fixes=[\n                                    LintFix(\n                                        \"create\",\n                                        seg,\n                                        WhitespaceSegment(\" \" * (-indent_diff)),\n                                    )\n                                ],\n                            )\n                        elif indent_diff > 0:\n                            # Is it all whitespace before the bracket on this line?\n                            prev_segs_on_line = [\n                                elem\n                                for elem in context.segment.iter_segments(\n                                    expanding=[\"common_table_expression\", \"bracketed\"],\n                                    pass_through=True,\n                                )\n                                if elem.pos_marker.line_no == seg.pos_marker.line_no\n                                and elem.pos_marker.line_pos < seg.pos_marker.line_pos\n                            ]\n                            if all(\n                                elem.is_type(\"whitespace\") for elem in prev_segs_on_line\n                            ):\n                                # We can move it back, it's all whitespace\n                                fixes = [\n                                    LintFix(\n                                        \"create\",\n                                        seg,\n                                        [WhitespaceSegment(with_indent_str)],\n                                    )\n                                ] + [\n                                    LintFix(\"delete\", elem)\n                                    for elem in prev_segs_on_line\n                                ]\n                            else:\n                                # We have to move it to a newline\n                                fixes = [\n                                    LintFix(\n                                        \"create\",\n                                        seg,\n                                        [\n                                            NewlineSegment(),\n                                            WhitespaceSegment(with_indent_str),\n                                        ],\n                                    )\n                                ]\n                            return LintResult(anchor=seg, fixes=fixes)\n                else:\n                    raw_stack_buff.append(seg)\n        return LintResult()",
        "file_path": "src/sqlfluff/rules/L018.py",
        "chunk_index": 0,
        "metadata": {
          "class_name": "Rule_L018",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "BaseRule"
            ]
          },
          "parent_name": null,
          "private_methods_count": 1,
          "public_methods_count": 0,
          "node_count": 789,
          "chunk_size": 6126
        }
      },
      {
        "rank": 3,
        "score": 0.6472625732421875,
        "content": "class Rule_L003(BaseRule):\n    \"\"\"Indentation not consistent with previous lines.\n\n    Note:\n        This rule used to be _\"Indentation length is not a multiple\n        of `tab_space_size`\"_, but was changed to be much smarter.\n\n    | **Anti-pattern**\n    | The  character represents a space.\n    | In this example, the third line contains five spaces instead of four.\n\n    .. code-block:: sql\n       :force:\n\n        SELECT\n        a,\n        b\n        FROM foo\n\n\n    | **Best practice**\n    | Change the indentation to use a multiple of four spaces.\n\n    .. code-block:: sql\n       :force:\n\n        SELECT\n        a,\n        b\n        FROM foo\n\n    \"\"\"\n\n    _works_on_unparsable = False\n    _ignore_types: List[str] = [\"script_content\"]\n    config_keywords = [\"tab_space_size\", \"indent_unit\"]\n\n    @staticmethod\n    def _make_indent(\n        num: int = 1, tab_space_size: int = 4, indent_unit: str = \"space\"\n    ) -> str:\n        if indent_unit == \"tab\":\n            base_unit = \"\\t\"\n        elif indent_unit == \"space\":\n            base_unit = \" \" * tab_space_size\n        else:\n            raise ValueError(\n                f\"Parameter indent_unit has unexpected value: `{indent_unit}`. Expected `tab` or `space`.\"\n            )\n        return base_unit * num\n\n    @staticmethod\n    def _indent_size(segments: Sequence[RawSegment], tab_space_size: int = 4) -> int:\n        indent_size = 0\n        for elem in segments:\n            raw = elem.raw\n            # convert to spaces for convenience (and hanging indents)\n            raw = raw.replace(\"\\t\", \" \" * tab_space_size)\n            indent_size += len(raw)\n        return indent_size\n\n    @classmethod\n    def _reorder_raw_stack(\n        cls,\n        raw_stack: Tuple[RawSegment, ...],\n        templated_file: Optional[TemplatedFile],\n    ) -> Tuple[RawSegment, ...]:\n        \"\"\"Reorder raw_stack to simplify indentation logic.\n\n        Context: The indentation logic was mostly designed to work with normal\n        segment types. Templating introduces additional segments into the parse\n        tree, often in the \"wrong\" place with respect to the indentation logic,\n        for example, where do indent/dedent segments appear with respect to the\n        segments that trigger indent/dedent behavior? This function reorders\n        nodes locally (i.e. only within L003) to get the desired behavior.\n        \"\"\"\n\n        def segment_info(idx: int) -> Tuple[str, Optional[str]]:\n            \"\"\"Helper function for sort_current_line().\"\"\"\n            seg = current_line[idx]\n            return seg.type, cls._get_element_template_info(seg, templated_file)\n\n        def move_indent_before_templated() -> None:\n            \"\"\"Swap position of template and indent segment if code follows.\n\n            This allows for correct indentation of templated table names in\n            \"FROM\", for example:\n\n            SELECT brand\n            FROM\n                {{ product }}\n\n            \"\"\"\n            for idx in range(2, len(current_line)):\n                if (\n                    segment_info(idx - 2)\n                    == (\n                        \"placeholder\",\n                        \"templated\",\n                    )\n                    and segment_info(idx - 1) == (\"indent\", None)\n                    and segment_info(idx) == (\"raw\", None)\n                ):\n                    current_line[idx - 2], current_line[idx - 1] = (\n                        current_line[idx - 1],\n                        current_line[idx - 2],\n                    )\n\n        # Break raw_stack into lines.\n        lines = []\n        current_line = []\n        for elem in raw_stack:\n            if not elem.is_type(\"newline\"):\n                current_line.append(elem)\n            else:\n                move_indent_before_templated()\n                current_line.append(elem)\n                lines.append(current_line)\n                current_line = []\n        if current_line:\n            move_indent_before_templated()\n            lines.append(current_line)\n        new_raw_stack = [s for line in lines for s in line]\n        return tuple(new_raw_stack)\n\n    @classmethod\n    def _process_raw_stack(\n        cls,\n        raw_stack: Tuple[RawSegment, ...],\n        memory: dict = None,\n        tab_space_size: int = 4,\n        templated_file: Optional[TemplatedFile] = None,\n    ) -> dict:\n        \"\"\"Take the raw stack, split into lines and evaluate some stats.\"\"\"\n        raw_stack = cls._reorder_raw_stack(raw_stack, templated_file)\n        indent_balance = 0\n        line_no = 1\n        in_indent = True\n        indent_buffer: List[RawSegment] = []\n        line_buffer: List[RawSegment] = []\n        result_buffer = {}\n        indent_size = 0\n        line_indent_stack: List[int] = []\n        this_indent_balance = 0\n        clean_indent = False\n        hanger_pos = None\n\n        for elem in raw_stack:\n            line_buffer.append(elem)\n            # Pin indent_balance to above zero\n            if indent_balance < 0:\n                indent_balance = 0\n\n            if elem.is_type(\"newline\"):\n                result_buffer[line_no] = {\n                    \"line_no\": line_no,\n                    # Using slicing to copy line_buffer here to be py2 compliant\n                    \"line_buffer\": line_buffer[:],\n                    \"indent_buffer\": indent_buffer,\n                    \"indent_size\": indent_size,\n                    # Indent balance is the indent at the start of the first content\n                    \"indent_balance\": this_indent_balance,\n                    \"hanging_indent\": hanger_pos if line_indent_stack else None,\n                    # Clean indent is true if the line *ends* with an indent\n                    # or has an indent in the initial whitespace.\n                    \"clean_indent\": clean_indent,\n                }\n                line_no += 1\n                indent_buffer = []\n                line_buffer = []\n                indent_size = 0\n                in_indent = True\n                line_indent_stack = []\n                hanger_pos = None\n                # Assume an unclean indent, but if the last line\n                # ended with an indent then we might be ok.\n                clean_indent = False\n                # Was there an indent after the last code element of the previous line?\n                for search_elem in reversed(result_buffer[line_no - 1][\"line_buffer\"]):  # type: ignore\n                    if not search_elem.is_code and not search_elem.is_meta:\n                        continue\n                    elif search_elem.is_meta and search_elem.indent_val > 0:\n                        clean_indent = True\n                    break\n            elif in_indent:\n                if elem.is_type(\"whitespace\"):\n                    indent_buffer.append(elem)\n                elif elem.is_meta and elem.indent_val != 0:  # type: ignore\n                    indent_balance += elem.indent_val  # type: ignore\n                    if elem.indent_val > 0:  # type: ignore\n                        # a \"clean\" indent is one where it contains\n                        # an increase in indentation? Can't quite\n                        # remember the logic here. Let's go with that.\n                        clean_indent = True\n                else:\n                    in_indent = False\n                    this_indent_balance = indent_balance\n                    indent_size = cls._indent_size(\n                        indent_buffer, tab_space_size=tab_space_size\n                    )\n            elif elem.is_meta and elem.indent_val != 0:  # type: ignore\n                indent_balance += elem.indent_val  # type: ignore\n                if elem.indent_val > 0:  # type: ignore\n                    # Keep track of the indent at the last ... indent\n                    line_indent_stack.append(\n                        cls._indent_size(line_buffer, tab_space_size=tab_space_size)\n                    )\n                    hanger_pos = None\n                else:\n                    # this is a dedent, we could still have a hanging indent,\n                    # but only if there's enough on the stack\n                    if line_indent_stack:\n                        line_indent_stack.pop()\n            elif elem.is_code:\n                if hanger_pos is None:\n                    hanger_pos = cls._indent_size(\n                        line_buffer[:-1], tab_space_size=tab_space_size\n                    )\n\n            # If we hit the trigger element, stop processing.\n            if memory and elem is memory[\"trigger\"]:\n                break\n\n        # If we get to the end, and still have a buffer, add it on\n        if line_buffer:\n            result_buffer[line_no] = {\n                \"line_no\": line_no,\n                \"line_buffer\": line_buffer,\n                \"indent_buffer\": indent_buffer,\n                \"indent_size\": indent_size,\n                \"indent_balance\": this_indent_balance,\n                \"hanging_indent\": line_indent_stack.pop()\n                if line_indent_stack\n                else None,\n                \"clean_indent\": clean_indent,\n            }\n        return result_buffer\n\n    def _coerce_indent_to(\n        self,\n        desired_indent: str,\n        current_indent_buffer: Tuple[RawSegment, ...],\n        current_anchor: BaseSegment,\n    ) -> List[LintFix]:\n        \"\"\"Generate fixes to make an indent a certain size.\"\"\"\n        # If there shouldn't be an indent at all, just delete.\n        if len(desired_indent) == 0:\n            fixes = [LintFix(\"delete\", elem) for elem in current_indent_buffer]\n        # If we don't have any indent and we should, then add a single\n        elif len(\"\".join(elem.raw for elem in current_indent_buffer)) == 0:\n            fixes = [\n                LintFix(\n                    \"create\",\n                    current_anchor,\n                    WhitespaceSegment(\n                        raw=desired_indent,\n                    ),\n                )\n            ]\n        # Otherwise edit the first element to be the right size\n        else:\n            # Edit the first element of this line's indent.\n            fixes = [\n                LintFix(\n                    \"edit\",\n                    current_indent_buffer[0],\n                    WhitespaceSegment(\n                        raw=desired_indent,\n                    ),\n                )\n            ]\n        return fixes\n\n    @staticmethod\n    def _strip_buffers(line_dict: dict) -> dict:\n        \"\"\"Strip a line dict of buffers for logging.\"\"\"\n        return {\n            key: line_dict[key]\n            for key in line_dict\n            if key not in (\"line_buffer\", \"indent_buffer\")\n        }\n\n    @classmethod\n    def _is_last_segment(\n        cls,\n        segment: BaseSegment,\n        memory: dict,\n        parent_stack: Tuple[BaseSegment, ...],\n        siblings_post: Tuple[BaseSegment, ...],\n    ) -> bool:\n        \"\"\"Returns True if 'segment' is the very last node in the parse tree.\"\"\"\n        if siblings_post:\n            # We have subsequent siblings. Not finished.\n            return False\n        elif parent_stack:\n            # No subsequent siblings. Our parent is finished.\n            memory[\"finished\"].add(parent_stack[-1])\n        if segment.segments:\n            # We have children. Not finished.\n            return False\n\n        # We have no subsequent siblings or children. If all our parents are\n        # finished, the whole parse tree is finished.\n        for parent in parent_stack:\n            if parent not in memory[\"finished\"]:\n                return False\n        return True\n\n    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Indentation not consistent with previous lines.\n\n        To set the default tab size, set the `tab_space_size` value\n        in the appropriate configuration.\n\n        We compare each line (first non-whitespace element of the\n        line), with the indentation of previous lines. The presence\n        (or lack) of indent or dedent meta-characters indicate whether\n        the indent is appropriate.\n\n        - Any line is assessed by the indent level at the first non\n          whitespace element.\n        - Any increase in indentation may be _up to_ the number of\n          indent characters.\n        - Any line must be in line with the previous line which had\n          the same indent balance at its start.\n        - Apart from \"whole\" indents, a \"hanging\" indent is possible\n          if the line starts in line with either the indent of the\n          previous line or if it starts at the same indent as the *last*\n          indent meta segment in the previous line.\n\n        \"\"\"\n        # Config type hints\n        self.tab_space_size: int\n        self.indent_unit: str\n\n        raw_stack = context.raw_stack\n\n        # We ignore certain types (e.g. non-SQL scripts in functions)\n        # so check if on ignore list\n        if context.segment.type in self._ignore_types:\n            return LintResult()\n        for parent in context.parent_stack:\n            if parent.type in self._ignore_types:\n                return LintResult()\n\n        # Memory keeps track of what we've seen\n        if not context.memory:\n            memory: dict = {\n                # in_indent keeps track of whether we're in an indent right now\n                \"in_indent\": True,\n                # problem_lines keeps track of lines with problems so that we\n                # don't compare to them.\n                \"problem_lines\": [],\n                # hanging_lines keeps track of hanging lines so that we don't\n                # compare to them when assessing indent.\n                \"hanging_lines\": [],\n                # comment_lines keeps track of lines which are all comment.\n                \"comment_lines\": [],\n                # segments we've seen the last child of\n                \"finished\": set(),\n                # First non-whitespace node on a line.\n                \"trigger\": None,\n            }\n        else:\n            memory = context.memory\n\n        if context.segment.is_type(\"newline\"):\n            memory[\"in_indent\"] = True\n        elif memory[\"in_indent\"]:\n            if context.segment.is_type(\"whitespace\"):\n                # it's whitespace, carry on\n                pass\n            elif context.segment.segments or (context.segment.is_meta and context.segment.indent_val != 0):  # type: ignore\n                # it's not a raw segment or placeholder. Carry on.\n                pass\n            else:\n                memory[\"in_indent\"] = False\n                # we're found a non-whitespace element. This is our trigger,\n                # which we'll handle after this if-statement\n                memory[\"trigger\"] = context.segment\n        else:\n            # Not in indent and not a newline, don't trigger here.\n            pass\n\n        # Is this the last segment? If so, need to \"flush\" any leftovers.\n        is_last = self._is_last_segment(\n            context.segment, memory, context.parent_stack, context.siblings_post\n        )\n\n        if not context.segment.is_type(\"newline\") and not is_last:\n            # We only process complete lines or on the very last segment\n            # (since there may not be a newline on the very last line)..\n            return LintResult(memory=memory)\n\n        if raw_stack and raw_stack[-1] is not context.segment:\n            raw_stack = raw_stack + (context.segment,)\n        res = self._process_raw_stack(\n            raw_stack,\n            memory,\n            tab_space_size=self.tab_space_size,\n            templated_file=context.templated_file,\n        )\n\n        if res:\n            # Saw a newline or end of parse tree. Is the current line empty?\n            trigger_segment = memory[\"trigger\"]\n            if trigger_segment:\n                # Not empty. Process it.\n                result = self._process_current_line(res, memory)\n                if context.segment.is_type(\"newline\"):\n                    memory[\"trigger\"] = None\n                return result\n        return LintResult(memory=memory)\n\n    def _process_current_line(self, res: dict, memory: dict) -> LintResult:\n        \"\"\"Checks indentation of one line of code, returning a LintResult.\n\n        The _eval() function calls it for the current line of code:\n        - When passed a newline segment (thus ending a line)\n        - When passed the *final* segment in the entire parse tree (which may\n          not be a newline)\n        \"\"\"\n        this_line_no = max(res.keys())\n        this_line = res.pop(this_line_no)\n        self.logger.debug(\n            \"Evaluating line #%s. %s\",\n            this_line_no,\n            # Don't log the line or indent buffer, it's too noisy.\n            self._strip_buffers(this_line),\n        )\n        trigger_segment = memory[\"trigger\"]\n\n        # Is this line just comments? (Disregard trailing newline if present.)\n        check_comment_line = this_line[\"line_buffer\"]\n        if check_comment_line and all(\n            seg.is_type(\n                \"whitespace\", \"comment\", \"indent\"  # dedent is a subtype of indent\n            )\n            for seg in check_comment_line\n        ):\n            # Comment line, deal with it later.\n            memory[\"comment_lines\"].append(this_line_no)\n            self.logger.debug(\"    Comment Line. #%s\", this_line_no)\n            return LintResult(memory=memory)\n\n        # Is it a hanging indent?\n        # Find last meaningful line indent.\n        last_code_line = None\n        for k in sorted(res.keys(), reverse=True):\n            if any(seg.is_code for seg in res[k][\"line_buffer\"]):\n                last_code_line = k\n                break\n\n        if len(res) > 0 and last_code_line:\n            last_line_hanger_indent = res[last_code_line][\"hanging_indent\"]\n            # Let's just deal with hanging indents here.\n            if (\n                # NB: Hangers are only allowed if there was content after the last\n                # indent on the previous line. Otherwise it's just an indent.\n                this_line[\"indent_size\"] == last_line_hanger_indent\n                # Or they're if the indent balance is the same and the indent is the\n                # same AND the previous line was a hanger\n                or (\n                    this_line[\"indent_size\"] == res[last_code_line][\"indent_size\"]\n                    and this_line[\"indent_balance\"]\n                    == res[last_code_line][\"indent_balance\"]\n                    and last_code_line in memory[\"hanging_lines\"]\n                )\n            ) and (\n                # There MUST also be a non-zero indent. Otherwise we're just on the baseline.\n                this_line[\"indent_size\"]\n                > 0\n            ):\n                # This is a HANGER\n                memory[\"hanging_lines\"].append(this_line_no)\n                self.logger.debug(\"    Hanger Line. #%s\", this_line_no)\n                self.logger.debug(\n                    \"    Last Line: %s\", self._strip_buffers(res[last_code_line])\n                )\n                return LintResult(memory=memory)\n\n        # Is this an indented first line?\n        elif len(res) == 0:\n            if this_line[\"indent_size\"] > 0:\n                self.logger.debug(\"    Indented First Line. #%s\", this_line_no)\n                return LintResult(\n                    anchor=trigger_segment,\n                    memory=memory,\n                    description=\"First line has unexpected indent\",\n                    fixes=[\n                        LintFix(\"delete\", elem) for elem in this_line[\"indent_buffer\"]\n                    ],\n                )\n\n        # Assuming it's not a hanger, let's compare it to the other previous\n        # lines. We do it in reverse so that closer lines are more relevant.\n        for k in sorted(res.keys(), reverse=True):\n\n            # Is this a problem line?\n            if k in memory[\"problem_lines\"] + memory[\"hanging_lines\"]:\n                # Skip it if it is\n                continue\n\n            # Is this an empty line?\n            if not any(elem.is_code for elem in res[k][\"line_buffer\"]):\n                # Skip if it is\n                continue\n\n            # Work out the difference in indent\n            indent_diff = this_line[\"indent_balance\"] - res[k][\"indent_balance\"]\n            # If we're comparing to a previous, more deeply indented line, then skip and keep looking.\n            if indent_diff < 0:\n                continue\n            # Is the indent balance the same?\n            elif indent_diff == 0:\n                self.logger.debug(\"    [same indent balance] Comparing to #%s\", k)\n                if this_line[\"indent_size\"] != res[k][\"indent_size\"]:\n                    # Indents don't match even though balance is the same...\n                    memory[\"problem_lines\"].append(this_line_no)\n\n                    # Work out desired indent\n                    if res[k][\"indent_size\"] == 0:\n                        desired_indent = \"\"\n                    elif this_line[\"indent_size\"] == 0:\n                        desired_indent = self._make_indent(\n                            indent_unit=self.indent_unit,\n                            tab_space_size=self.tab_space_size,\n                        )\n                    else:\n                        # The previous indent.\n                        desired_indent = \"\".join(\n                            elem.raw for elem in res[k][\"indent_buffer\"]\n                        )\n\n                    # Make fixes\n                    fixes = self._coerce_indent_to(\n                        desired_indent=desired_indent,\n                        current_indent_buffer=this_line[\"indent_buffer\"],\n                        current_anchor=trigger_segment,\n                    )\n                    self.logger.debug(\n                        \"    !! Indentation does not match #%s. Fixes: %s\", k, fixes\n                    )\n                    return LintResult(\n                        anchor=trigger_segment,\n                        memory=memory,\n                        description=\"Indentation not consistent with line #{}\".format(\n                            k\n                        ),\n                        # See above for logic\n                        fixes=fixes,\n                    )\n            # Are we at a deeper indent?\n            elif indent_diff > 0:\n                self.logger.debug(\"    [deeper indent balance] Comparing to #%s\", k)\n                # NB: We shouldn't need to deal with correct hanging indents\n                # here, they should already have been dealt with before. We\n                # may still need to deal with *creating* hanging indents if\n                # appropriate.\n                self.logger.debug(\n                    \"    Comparison Line: %s\", self._strip_buffers(res[k])\n                )\n\n                # Check to see if we've got a whole number of multiples. If\n                # we do then record the number for later, otherwise raise\n                # an error. We do the comparison here so we have a reference\n                # point to do the repairs. We need a sensible previous line\n                # to base the repairs off. If there's no indent at all, then\n                # we should also take this route because there SHOULD be one.\n                if this_line[\"indent_size\"] % self.tab_space_size != 0:\n                    memory[\"problem_lines\"].append(this_line_no)\n\n                    # The default indent is the one just reconstructs it from\n                    # the indent size.\n                    default_indent = \"\".join(\n                        elem.raw for elem in res[k][\"indent_buffer\"]\n                    ) + self._make_indent(\n                        indent_unit=self.indent_unit,\n                        tab_space_size=self.tab_space_size,\n                        num=indent_diff,\n                    )\n                    # If we have a clean indent, we can just add steps in line\n                    # with the difference in the indent buffers. simples.\n                    if this_line[\"clean_indent\"]:\n                        self.logger.debug(\"        Use clean indent.\")\n                        desired_indent = default_indent\n                    # If we have the option of a hanging indent then use it.\n                    elif res[k][\"hanging_indent\"]:\n                        self.logger.debug(\"        Use hanging indent.\")\n                        desired_indent = \" \" * res[k][\"hanging_indent\"]\n                    else:  # pragma: no cover\n                        self.logger.debug(\"        Use default indent.\")\n                        desired_indent = default_indent\n\n                    # Make fixes\n                    fixes = self._coerce_indent_to(\n                        desired_indent=desired_indent,\n                        current_indent_buffer=this_line[\"indent_buffer\"],\n                        current_anchor=trigger_segment,\n                    )\n\n                    return LintResult(\n                        anchor=trigger_segment,\n                        memory=memory,\n                        description=(\n                            \"Indentation not hanging or a multiple of {} spaces\"\n                        ).format(self.tab_space_size),\n                        fixes=fixes,\n                    )\n                else:\n                    # We'll need this value later.\n                    this_indent_num = this_line[\"indent_size\"] // self.tab_space_size\n\n                # We know that the indent balance is higher, what actually is\n                # the difference in indent counts? It should be a whole number\n                # if we're still here.\n                comp_indent_num = res[k][\"indent_size\"] // self.tab_space_size\n\n                # The indent number should be at least 1, and can be UP TO\n                # and including the difference in the indent balance.\n                if comp_indent_num == this_indent_num:\n                    # We have two lines indented the same, but with a different starting\n                    # indent balance. This is either a problem OR a sign that one of the\n                    # opening indents wasn't used. We account for the latter and then\n                    # have a violation if that wasn't the case.\n\n                    # Does the comparison line have enough unused indent to get us back\n                    # to where we need to be? NB: This should only be applied if this is\n                    # a CLOSING bracket.\n\n                    # First work out if we have some closing brackets, and if so, how many.\n                    b_idx = 0\n                    b_num = 0\n                    while True:\n                        if len(this_line[\"line_buffer\"][b_idx:]) == 0:\n                            break\n\n                        elem = this_line[\"line_buffer\"][b_idx]\n                        if not elem.is_code:\n                            b_idx += 1\n                            continue\n                        else:\n                            if elem.is_type(\"end_bracket\", \"end_square_bracket\"):\n                                b_idx += 1\n                                b_num += 1\n                                continue\n                            break  # pragma: no cover\n\n                    if b_num >= indent_diff:\n                        # It does. This line is fine.\n                        pass\n                    else:\n                        # It doesn't. That means we *should* have an indent when compared to\n                        # this line and we DON'T.\n                        memory[\"problem_lines\"].append(this_line_no)\n                        return LintResult(\n                            anchor=trigger_segment,\n                            memory=memory,\n                            description=\"Indent expected and not found compared to line #{}\".format(\n                                k\n                            ),\n                            # Add in an extra bit of whitespace for the indent\n                            fixes=[\n                                LintFix(\n                                    \"create\",\n                                    trigger_segment,\n                                    WhitespaceSegment(\n                                        raw=self._make_indent(\n                                            indent_unit=self.indent_unit,\n                                            tab_space_size=self.tab_space_size,\n                                        ),\n                                    ),\n                                )\n                            ],\n                        )\n                elif this_indent_num < comp_indent_num:\n                    memory[\"problem_lines\"].append(this_line_no)\n                    return LintResult(\n                        anchor=trigger_segment,\n                        memory=memory,\n                        description=\"Line under-indented compared to line #{}\".format(\n                            k\n                        ),\n                        fixes=[\n                            LintFix(\n                                \"create\",\n                                trigger_segment,\n                                WhitespaceSegment(\n                                    # Make the minimum indent for it to be ok.\n                                    raw=self._make_indent(\n                                        num=comp_indent_num - this_indent_num,\n                                        indent_unit=self.indent_unit,\n                                        tab_space_size=self.tab_space_size,\n                                    ),\n                                ),\n                            )\n                        ],\n                    )\n                elif this_indent_num > comp_indent_num + indent_diff:\n                    # Calculate the lowest ok indent:\n                    desired_indent = self._make_indent(\n                        num=comp_indent_num - this_indent_num,\n                        indent_unit=self.indent_unit,\n                        tab_space_size=self.tab_space_size,\n                    )\n\n                    # Make fixes\n                    fixes = self._coerce_indent_to(\n                        desired_indent=desired_indent,\n                        current_indent_buffer=this_line[\"indent_buffer\"],\n                        current_anchor=trigger_segment,\n                    )\n\n                    memory[\"problem_lines\"].append(this_line_no)\n                    return LintResult(\n                        anchor=trigger_segment,\n                        memory=memory,\n                        description=\"Line over-indented compared to line #{}\".format(k),\n                        fixes=fixes,\n                    )\n\n            # This was a valid comparison, so if it doesn't flag then\n            # we can assume that we're ok.\n            self.logger.debug(\"    Indent deemed ok comparing to #%s\", k)\n\n            # Given that this line is ok, consider if the preceding lines are\n            # comments. If they are, lint the indentation of the comment(s).\n            fixes = []\n            for n in range(this_line_no - 1, -1, -1):\n                if n in memory[\"comment_lines\"]:\n                    # The previous line WAS a comment.\n                    prev_line = res[n]\n                    if this_line[\"indent_size\"] != prev_line[\"indent_size\"]:\n                        # It's not aligned.\n                        # Find the anchor first.\n                        anchor: BaseSegment = None  # type: ignore\n                        for seg in prev_line[\"line_buffer\"]:\n                            if seg.is_type(\"comment\"):\n                                anchor = seg\n                                break\n                        # Make fixes.\n                        fixes += self._coerce_indent_to(\n                            desired_indent=\"\".join(\n                                elem.raw for elem in this_line[\"indent_buffer\"]\n                            ),\n                            current_indent_buffer=prev_line[\"indent_buffer\"],\n                            current_anchor=anchor,\n                        )\n\n                        memory[\"problem_lines\"].append(n)\n                else:\n                    break\n\n            if fixes:\n                return LintResult(\n                    anchor=anchor,\n                    memory=memory,\n                    description=\"Comment not aligned with following line.\",\n                    fixes=fixes,\n                )\n\n            # Otherwise all good.\n            return LintResult(memory=memory)\n\n            # NB: At shallower indents, we don't check, we just check the\n            # previous lines with the same balance. Deeper indents can check\n            # themselves.\n\n        # If we get to here, then we're all good for now.\n        return LintResult(memory=memory)\n\n    @classmethod\n    def _get_element_template_info(\n        cls, elem: BaseSegment, templated_file: Optional[TemplatedFile]\n    ) -> Optional[str]:\n        if elem.is_type(\"placeholder\"):\n            if templated_file is None:\n                raise ValueError(\"Parameter templated_file cannot be: None.\")\n            slices = templated_file.raw_slices_spanning_source_slice(\n                elem.pos_marker.source_slice\n            )\n            if slices:\n                return slices[0].slice_type\n        return None",
        "file_path": "src/sqlfluff/rules/L003.py",
        "chunk_index": 0,
        "metadata": {
          "class_name": "Rule_L003",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "BaseRule"
            ]
          },
          "parent_name": null,
          "private_methods_count": 3,
          "public_methods_count": 0,
          "node_count": 4726,
          "chunk_size": 33371
        }
      },
      {
        "rank": 4,
        "score": 0.6365892887115479,
        "content": "class Rule_L023(BaseRule):\n    \"\"\"Single whitespace expected after AS in WITH clause.\n\n    | **Anti-pattern**\n\n    .. code-block:: sql\n\n        WITH plop AS(\n            SELECT * FROM foo\n        )\n\n        SELECT a FROM plop\n\n\n    | **Best practice**\n    | The  character represents a space.\n    | Add a space after AS, to avoid confusing\n    | it for a function.\n\n    .. code-block:: sql\n       :force:\n\n        WITH plop AS(\n            SELECT * FROM foo\n        )\n\n        SELECT a FROM plop\n    \"\"\"\n\n    expected_mother_segment_type = \"with_compound_statement\"\n    pre_segment_identifier = (\"name\", \"as\")\n    post_segment_identifier = (\"type\", \"bracketed\")\n    allow_newline = False\n    expand_children: Optional[List[str]] = [\"common_table_expression\"]\n\n    def _eval(self, context: RuleContext) -> Optional[List[LintResult]]:\n        \"\"\"Single whitespace expected in mother segment between pre and post segments.\"\"\"\n        error_buffer: List[LintResult] = []\n        if context.segment.is_type(self.expected_mother_segment_type):\n            last_code = None\n            mid_segs: List[BaseSegment] = []\n            for seg in context.segment.iter_segments(expanding=self.expand_children):\n                if seg.is_code:\n                    if (\n                        last_code\n                        and self.matches_target_tuples(\n                            last_code, [self.pre_segment_identifier]\n                        )\n                        and self.matches_target_tuples(\n                            seg, [self.post_segment_identifier]\n                        )\n                    ):\n                        # Do we actually have the right amount of whitespace?\n                        raw_inner = \"\".join(s.raw for s in mid_segs)\n                        if raw_inner != \" \" and not (\n                            self.allow_newline\n                            and any(s.name == \"newline\" for s in mid_segs)\n                        ):\n                            if not raw_inner:\n                                # There's nothing between. Just add a whitespace\n                                fixes = [\n                                    LintFix(\n                                        \"create\",\n                                        seg,\n                                        [WhitespaceSegment()],\n                                    )\n                                ]\n                            else:\n                                # Don't otherwise suggest a fix for now.\n                                # TODO: Enable more complex fixing here.\n                                fixes = None  # pragma: no cover\n                            error_buffer.append(\n                                LintResult(anchor=last_code, fixes=fixes)\n                            )\n                    mid_segs = []\n                    if not seg.is_meta:\n                        last_code = seg\n                else:\n                    mid_segs.append(seg)\n        return error_buffer or None",
        "file_path": "src/sqlfluff/rules/L023.py",
        "chunk_index": 0,
        "metadata": {
          "class_name": "Rule_L023",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "BaseRule"
            ]
          },
          "parent_name": null,
          "private_methods_count": 1,
          "public_methods_count": 0,
          "node_count": 435,
          "chunk_size": 3019
        }
      },
      {
        "rank": 5,
        "score": 0.6276134252548218,
        "content": "class Rule_L039(BaseRule):\n    \"\"\"Unnecessary whitespace found.\n\n    | **Anti-pattern**\n\n    .. code-block:: sql\n\n        SELECT\n            a,        b\n        FROM foo\n\n    | **Best practice**\n    | Unless an indent or preceding a comment, whitespace should\n    | be a single space.\n\n    .. code-block:: sql\n\n        SELECT\n            a, b\n        FROM foo\n    \"\"\"\n\n    def _eval(self, context: RuleContext) -> Optional[List[LintResult]]:\n        \"\"\"Unnecessary whitespace.\"\"\"\n        # For the given segment, lint whitespace directly within it.\n        prev_newline = True\n        prev_whitespace = None\n        violations = []\n        for seg in context.segment.segments:\n            if seg.is_type(\"newline\"):\n                prev_newline = True\n                prev_whitespace = None\n            elif seg.is_type(\"whitespace\"):\n                # This is to avoid indents\n                if not prev_newline:\n                    prev_whitespace = seg\n                prev_newline = False\n            elif seg.is_type(\"comment\"):\n                prev_newline = False\n                prev_whitespace = None\n            else:\n                if prev_whitespace:\n                    if prev_whitespace.raw != \" \":\n                        violations.append(\n                            LintResult(\n                                anchor=prev_whitespace,\n                                fixes=[\n                                    LintFix(\n                                        \"edit\",\n                                        prev_whitespace,\n                                        WhitespaceSegment(),\n                                    )\n                                ],\n                            )\n                        )\n                prev_newline = False\n                prev_whitespace = None\n        return violations or None",
        "file_path": "src/sqlfluff/rules/L039.py",
        "chunk_index": 0,
        "metadata": {
          "class_name": "Rule_L039",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "BaseRule"
            ]
          },
          "parent_name": null,
          "private_methods_count": 1,
          "public_methods_count": 0,
          "node_count": 249,
          "chunk_size": 1844
        }
      },
      {
        "rank": 6,
        "score": 0.6238242387771606,
        "content": "class Rule_L001(BaseRule):\n    \"\"\"Unnecessary trailing whitespace.\n\n    | **Anti-pattern**\n    | The  character represents a space.\n\n    .. code-block:: sql\n       :force:\n\n        SELECT\n            a\n        FROM foo\n\n    | **Best practice**\n    | Remove trailing spaces.\n\n    .. code-block:: sql\n\n        SELECT\n            a\n        FROM foo\n    \"\"\"\n\n    def _eval(self, context: RuleContext) -> LintResult:\n        \"\"\"Unnecessary trailing whitespace.\n\n        Look for newline segments, and then evaluate what\n        it was preceded by.\n        \"\"\"\n        # We only trigger on newlines\n        if (\n            context.segment.is_type(\"newline\")\n            and len(context.raw_stack) > 0\n            and context.raw_stack[-1].is_type(\"whitespace\")\n        ):\n            # If we find a newline, which is preceded by whitespace, then bad\n            deletions = []\n            idx = -1\n            while abs(idx) <= len(context.raw_stack) and context.raw_stack[idx].is_type(\n                \"whitespace\"\n            ):\n                deletions.append(context.raw_stack[idx])\n                idx -= 1\n            last_deletion_slice = deletions[-1].pos_marker.source_slice\n\n            # Check the raw source (before template expansion) immediately\n            # following the whitespace we want to delete. Often, what looks\n            # like trailing whitespace in rendered SQL is actually a line like:\n            # \"    {% for elem in elements %}\\n\", in which case the code is\n            # fine -- it's not trailing whitespace from a source code\n            # perspective.\n            if context.templated_file:\n                next_raw_slice = (\n                    context.templated_file.raw_slices_spanning_source_slice(\n                        slice(last_deletion_slice.stop, last_deletion_slice.stop)\n                    )\n                )\n                # If the next slice is literal, that means it's regular code, so\n                # it's safe to delete the trailing whitespace. If it's anything\n                # else, it's template code, so don't delete the whitespace because\n                # it's not REALLY trailing whitespace in terms of the raw source\n                # code.\n                if next_raw_slice[0].slice_type != \"literal\":\n                    return LintResult()\n            return LintResult(\n                anchor=deletions[-1],\n                fixes=[LintFix(\"delete\", d) for d in deletions],\n            )\n        return LintResult()",
        "file_path": "src/sqlfluff/rules/L001.py",
        "chunk_index": 0,
        "metadata": {
          "class_name": "Rule_L001",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "BaseRule"
            ]
          },
          "parent_name": null,
          "private_methods_count": 1,
          "public_methods_count": 0,
          "node_count": 317,
          "chunk_size": 2493
        }
      },
      {
        "rank": 7,
        "score": 0.6053709983825684,
        "content": "class Rule_L002(BaseRule):\n    \"\"\"Mixed Tabs and Spaces in single whitespace.\n\n    This rule will fail if a single section of whitespace\n    contains both tabs and spaces.\n\n    | **Anti-pattern**\n    | The  character represents a space and the  character represents a tab.\n    | In this example, the second line contains two spaces and one tab.\n\n    .. code-block:: sql\n       :force:\n\n        SELECT\n        a\n        FROM foo\n\n    | **Best practice**\n    | Change the line to use spaces only.\n\n    .. code-block:: sql\n       :force:\n\n        SELECT\n        a\n        FROM foo\n\n    \"\"\"\n\n    config_keywords = [\"tab_space_size\"]\n\n    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Mixed Tabs and Spaces in single whitespace.\n\n        Only trigger from whitespace segments if they contain\n        multiple kinds of whitespace.\n        \"\"\"\n        # Config type hints\n        self.tab_space_size: int\n\n        if context.segment.is_type(\"whitespace\"):\n            if \" \" in context.segment.raw and \"\\t\" in context.segment.raw:\n                if len(context.raw_stack) == 0 or context.raw_stack[-1].is_type(\n                    \"newline\"\n                ):\n                    # We've got a single whitespace at the beginning of a line.\n                    # It's got a mix of spaces and tabs. Replace each tab with\n                    # a multiple of spaces\n                    return LintResult(\n                        anchor=context.segment,\n                        fixes=[\n                            LintFix(\n                                \"edit\",\n                                context.segment,\n                                context.segment.edit(\n                                    context.segment.raw.replace(\n                                        \"\\t\", \" \" * self.tab_space_size\n                                    )\n                                ),\n                            )\n                        ],\n                    )\n        return None",
        "file_path": "src/sqlfluff/rules/L002.py",
        "chunk_index": 0,
        "metadata": {
          "class_name": "Rule_L002",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "BaseRule"
            ]
          },
          "parent_name": null,
          "private_methods_count": 1,
          "public_methods_count": 0,
          "node_count": 240,
          "chunk_size": 2018
        }
      },
      {
        "rank": 8,
        "score": 0.6026754379272461,
        "content": "def fix(sql, dialect=\"ansi\", rules=None):\n    \"\"\"Fix a sql string or file.\n\n    Args:\n        sql (:obj:`str` or file-like object): The sql to be linted\n            either as a string or a subclass of :obj:`TextIOBase`.\n        dialect (:obj:`str`, optional): A reference to the dialect of the sql\n            to be linted. Defaults to `ansi`.\n        rules (:obj:`str` or iterable of :obj:`str`, optional): A subset of rule\n            reference to lint for.\n\n    Returns:\n        :obj:`str` for the fixed sql if possible.\n    \"\"\"\n    sql = _unify_str_or_file(sql)\n    linter = Linter(dialect=dialect, rules=rules)\n\n    result = linter.lint_string_wrapped(sql, fix=True)\n    fixed_string = result.paths[0].files[0].fix_string()[0]\n    return fixed_string",
        "file_path": "src/sqlfluff/api/simple.py",
        "chunk_index": 3,
        "metadata": {
          "func_name": "fix",
          "chunk_type": "function",
          "relationship": {
            "call": [
              "_unify_str_or_file",
              "Linter",
              "linter.lint_string_wrapped",
              "result.paths[0].files[0].fix_string"
            ]
          },
          "class_name": null
        }
      },
      {
        "rank": 9,
        "score": 0.5999051332473755,
        "content": "class Rule_L048(Rule_L006):\n    \"\"\"Quoted literals should be surrounded by a single whitespace.\n\n    | **Anti-pattern**\n    | In this example, there is a space missing space between the string 'foo'\n    | and the keyword AS.\n\n    .. code-block:: sql\n\n        SELECT\n            'foo'AS bar\n        FROM foo\n\n\n    | **Best practice**\n    | Keep a single space.\n\n    .. code-block:: sql\n\n        SELECT\n            'foo' AS bar\n        FROM foo\n    \"\"\"\n\n    _target_elems: List[Tuple[str, str]] = [\n        (\"name\", \"quoted_literal\"),\n    ]\n\n    @staticmethod\n    def _missing_whitespace(seg: BaseSegment, before=True) -> bool:\n        \"\"\"Check whether we're missing whitespace given an adjoining segment.\n\n        This avoids flagging for commas after quoted strings.\n        https://github.com/sqlfluff/sqlfluff/issues/943\n        \"\"\"\n        simple_res = Rule_L006._missing_whitespace(seg, before=before)\n        if not before and seg and seg.is_type(\"comma\", \"statement_terminator\"):\n            return False\n        return simple_res",
        "file_path": "src/sqlfluff/rules/L048.py",
        "chunk_index": 0,
        "metadata": {
          "class_name": "Rule_L048",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "Rule_L006"
            ]
          },
          "parent_name": null,
          "private_methods_count": 0,
          "public_methods_count": 0,
          "node_count": 135,
          "chunk_size": 1036
        }
      },
      {
        "rank": 10,
        "score": 0.5991525650024414,
        "content": "class Rule_L022(BaseRule):\n    \"\"\"Blank line expected but not found after CTE closing bracket.\n\n    | **Anti-pattern**\n    | There is no blank line after the CTE closing bracket. In queries with many\n    | CTEs this hinders readability.\n\n    .. code-block:: sql\n\n        WITH plop AS (\n            SELECT * FROM foo\n        )\n        SELECT a FROM plop\n\n    | **Best practice**\n    | Add a blank line.\n\n    .. code-block:: sql\n\n        WITH plop AS (\n            SELECT * FROM foo\n        )\n\n        SELECT a FROM plop\n\n    \"\"\"\n\n    config_keywords = [\"comma_style\"]\n\n    def _eval(self, context: RuleContext) -> Optional[List[LintResult]]:\n        \"\"\"Blank line expected but not found after CTE definition.\"\"\"\n        # Config type hints\n        self.comma_style: str\n\n        error_buffer = []\n        if context.segment.is_type(\"with_compound_statement\"):\n            # First we need to find all the commas, the end brackets, the\n            # things that come after that and the blank lines in between.\n\n            # Find all the closing brackets. They are our anchor points.\n            bracket_indices = []\n            expanded_segments = list(\n                context.segment.iter_segments(expanding=[\"common_table_expression\"])\n            )\n            for idx, seg in enumerate(expanded_segments):\n                if seg.is_type(\"bracketed\"):\n                    bracket_indices.append(idx)\n\n            # Work through each point and deal with it individually\n            for bracket_idx in bracket_indices:\n                forward_slice = expanded_segments[bracket_idx:]\n                seg_idx = 1\n                line_idx = 0\n                comma_seg_idx = 0\n                blank_lines = 0\n                comma_line_idx = None\n                line_blank = False\n                comma_style = None\n                line_starts = {}\n                comment_lines = []\n\n                self.logger.info(\n                    \"## CTE closing bracket found at %s, idx: %s. Forward slice: %.20r\",\n                    forward_slice[0].pos_marker,\n                    bracket_idx,\n                    \"\".join(elem.raw for elem in forward_slice),\n                )\n\n                # Work forward to map out the following segments.\n                while (\n                    forward_slice[seg_idx].is_type(\"comma\")\n                    or not forward_slice[seg_idx].is_code\n                ):\n                    if forward_slice[seg_idx].is_type(\"newline\"):\n                        if line_blank:\n                            # It's a blank line!\n                            blank_lines += 1\n                        line_blank = True\n                        line_idx += 1\n                        line_starts[line_idx] = seg_idx + 1\n                    elif forward_slice[seg_idx].is_type(\"comment\"):\n                        # Lines with comments aren't blank\n                        line_blank = False\n                        comment_lines.append(line_idx)\n                    elif forward_slice[seg_idx].is_type(\"comma\"):\n                        # Keep track of where the comma is.\n                        # We'll evaluate it later.\n                        comma_line_idx = line_idx\n                        comma_seg_idx = seg_idx\n                    seg_idx += 1\n\n                # Infer the comma style (NB this could be different for each case!)\n                if comma_line_idx is None:\n                    comma_style = \"final\"\n                elif line_idx == 0:\n                    comma_style = \"oneline\"\n                elif comma_line_idx == 0:\n                    comma_style = \"trailing\"\n                elif comma_line_idx == line_idx:\n                    comma_style = \"leading\"\n                else:\n                    comma_style = \"floating\"\n\n                # Readout of findings\n                self.logger.info(\n                    \"blank_lines: %s, comma_line_idx: %s. final_line_idx: %s, final_seg_idx: %s\",\n                    blank_lines,\n                    comma_line_idx,\n                    line_idx,\n                    seg_idx,\n                )\n                self.logger.info(\n                    \"comma_style: %r, line_starts: %r, comment_lines: %r\",\n                    comma_style,\n                    line_starts,\n                    comment_lines,\n                )\n\n                if blank_lines < 1:\n                    # We've got an issue\n                    self.logger.info(\"!! Found CTE without enough blank lines.\")\n\n                    # Based on the current location of the comma we insert newlines\n                    # to correct the issue.\n                    fix_type = \"create\"  # In most cases we just insert newlines.\n                    if comma_style == \"oneline\":\n                        # Here we respect the target comma style to insert at the relevant point.\n                        if self.comma_style == \"trailing\":\n                            # Add a blank line after the comma\n                            fix_point = forward_slice[comma_seg_idx + 1]\n                            # Optionally here, if the segment we've landed on is\n                            # whitespace then we REPLACE it rather than inserting.\n                            if forward_slice[comma_seg_idx + 1].is_type(\"whitespace\"):\n                                fix_type = \"edit\"\n                        elif self.comma_style == \"leading\":\n                            # Add a blank line before the comma\n                            fix_point = forward_slice[comma_seg_idx]\n                        # In both cases it's a double newline.\n                        num_newlines = 2\n                    else:\n                        # In the following cases we only care which one we're in\n                        # when comments don't get in the way. If they *do*, then\n                        # we just work around them.\n                        if not comment_lines or line_idx - 1 not in comment_lines:\n                            self.logger.info(\"Comment routines not applicable\")\n                            if comma_style in (\"trailing\", \"final\", \"floating\"):\n                                # Detected an existing trailing comma or it's a final CTE,\n                                # OR the comma isn't leading or trailing.\n                                # If the preceding segment is whitespace, replace it\n                                if forward_slice[seg_idx - 1].is_type(\"whitespace\"):\n                                    fix_point = forward_slice[seg_idx - 1]\n                                    fix_type = \"edit\"\n                                else:\n                                    # Otherwise add a single newline before the end content.\n                                    fix_point = forward_slice[seg_idx]\n                            elif comma_style == \"leading\":\n                                # Detected an existing leading comma.\n                                fix_point = forward_slice[comma_seg_idx]\n                        else:\n                            self.logger.info(\"Handling preceding comments\")\n                            offset = 1\n                            while line_idx - offset in comment_lines:\n                                offset += 1\n                            fix_point = forward_slice[\n                                line_starts[line_idx - (offset - 1)]\n                            ]\n                        # Note: There is an edge case where this isn't enough, if\n                        # comments are in strange places, but we'll catch them on\n                        # the next iteration.\n                        num_newlines = 1\n\n                    fixes = [\n                        LintFix(\n                            fix_type,\n                            fix_point,\n                            [NewlineSegment()] * num_newlines,\n                        )\n                    ]\n                    # Create a result, anchored on the start of the next content.\n                    error_buffer.append(\n                        LintResult(anchor=forward_slice[seg_idx], fixes=fixes)\n                    )\n        # Return the buffer if we have one.\n        return error_buffer or None",
        "file_path": "src/sqlfluff/rules/L022.py",
        "chunk_index": 0,
        "metadata": {
          "class_name": "Rule_L022",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "BaseRule"
            ]
          },
          "parent_name": null,
          "private_methods_count": 1,
          "public_methods_count": 0,
          "node_count": 999,
          "chunk_size": 8194
        }
      }
    ]
  },
  "sqlfluff__sqlfluff-1517": {
    "query": "\"Dropped elements in sequence matching\" when doubled semicolon\n## Expected Behaviour\r\nFrankly, I'm not sure whether it (doubled `;`) should be just ignored or rather some specific rule should be triggered.\r\n## Observed Behaviour\r\n```console\r\n(.venv) ?master ~/prod/_inne/sqlfluff> echo \"select id from tbl;;\" | sqlfluff lint -\r\nTraceback (most recent call last):\r\n  File \"/home/adam/prod/_inne/sqlfluff/.venv/bin/sqlfluff\", line 11, in <module>\r\n    load_entry_point('sqlfluff', 'console_scripts', 'sqlfluff')()\r\n  File \"/home/adam/prod/_inne/sqlfluff/.venv/lib/python3.9/site-packages/click/core.py\", line 1137, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/home/adam/prod/_inne/sqlfluff/.venv/lib/python3.9/site-packages/click/core.py\", line 1062, in main\r\n    rv = self.invoke(ctx)\r\n  File \"/home/adam/prod/_inne/sqlfluff/.venv/lib/python3.9/site-packages/click/core.py\", line 1668, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"/home/adam/prod/_inne/sqlfluff/.venv/lib/python3.9/site-packages/click/core.py\", line 1404, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"/home/adam/prod/_inne/sqlfluff/.venv/lib/python3.9/site-packages/click/core.py\", line 763, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/cli/commands.py\", line 347, in lint\r\n    result = lnt.lint_string_wrapped(sys.stdin.read(), fname=\"stdin\")\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/linter/linter.py\", line 789, in lint_string_wrapped\r\n    linted_path.add(self.lint_string(string, fname=fname, fix=fix))\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/linter/linter.py\", line 668, in lint_string\r\n    parsed = self.parse_string(in_str=in_str, fname=fname, config=config)\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/linter/linter.py\", line 607, in parse_string\r\n    return self.parse_rendered(rendered, recurse=recurse)\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/linter/linter.py\", line 313, in parse_rendered\r\n    parsed, pvs = cls._parse_tokens(\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/linter/linter.py\", line 190, in _parse_tokens\r\n    parsed: Optional[BaseSegment] = parser.parse(\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/parser/parser.py\", line 32, in parse\r\n    parsed = root_segment.parse(parse_context=ctx)\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/parser/segments/base.py\", line 821, in parse\r\n    check_still_complete(segments, m.matched_segments, m.unmatched_segments)\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/parser/helpers.py\", line 30, in check_still_complete\r\n    raise RuntimeError(\r\nRuntimeError: Dropped elements in sequence matching! 'select id from tbl;;' != ';'\r\n\r\n```\r\n## Steps to Reproduce\r\nRun \r\n```console\r\necho \"select id from tbl;;\" | sqlfluff lint -\r\n```\r\n## Dialect\r\ndefault (ansi)\r\n## Version\r\n```\r\nsqlfluff, version 0.6.6\r\nPython 3.9.5\r\n```\r\n## Configuration\r\nNone\r\n\n",
    "method": "hierarchical",
    "top_k": 10,
    "results": [
      {
        "rank": 1,
        "score": 0.637961208820343,
        "content": "class Rule_L038(BaseRule):\n    \"\"\"Trailing commas within select clause.\n\n    For some database backends this is allowed. For some users\n    this may be something they wish to enforce (in line with\n    python best practice). Many database backends regard this\n    as a syntax error, and as such the sqlfluff default is to\n    forbid trailing commas in the select clause.\n\n    | **Anti-pattern**\n\n    .. code-block:: sql\n\n        SELECT\n            a, b,\n        FROM foo\n\n    | **Best practice**\n\n    .. code-block:: sql\n\n        SELECT\n            a, b\n        FROM foo\n    \"\"\"\n\n    config_keywords = [\"select_clause_trailing_comma\"]\n\n    def _eval(self, segment, parent_stack, **kwargs):\n        \"\"\"Trailing commas within select clause.\"\"\"\n        if segment.is_type(\"select_clause\"):\n            # Iterate content to find last element\n            last_content = None\n            for seg in segment.segments:\n                if seg.is_code:\n                    last_content = seg\n\n            # What mode are we in?\n            if self.select_clause_trailing_comma == \"forbid\":\n                # Is it a comma?\n                if last_content.is_type(\"comma\"):\n                    return LintResult(\n                        anchor=last_content,\n                        fixes=[LintFix(\"delete\", last_content)],\n                        description=\"Trailing comma in select statement forbidden\",\n                    )\n            elif self.select_clause_trailing_comma == \"require\":\n                if not last_content.is_type(\"comma\"):\n                    new_comma = SymbolSegment(\",\", name=\"comma\", type=\"comma\")\n                    return LintResult(\n                        anchor=last_content,\n                        fixes=[\n                            LintFix(\"edit\", last_content, [last_content, new_comma])\n                        ],\n                        description=\"Trailing comma in select statement required\",\n                    )\n        return None",
        "file_path": "src/sqlfluff/rules/L038.py",
        "chunk_index": 0,
        "metadata": {
          "class_name": "Rule_L038",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "BaseRule"
            ]
          },
          "parent_name": null,
          "private_methods_count": 1,
          "public_methods_count": 0,
          "node_count": 271,
          "chunk_size": 1967
        }
      },
      {
        "rank": 2,
        "score": 0.6166772842407227,
        "content": "class DropSequenceStatementSegment(BaseSegment):\n    \"\"\"Drop Sequence Statement.\n\n    As specified in https://docs.oracle.com/cd/E11882_01/server.112/e41084/statements_9001.htm\n    \"\"\"\n\n    type = \"drop_sequence_statement\"\n\n    match_grammar = Sequence(\"DROP\", \"SEQUENCE\", Ref(\"SequenceReferenceSegment\"))",
        "file_path": "src/sqlfluff/dialects/dialect_ansi.py",
        "chunk_index": 108,
        "metadata": {
          "class_name": "DropSequenceStatementSegment",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "BaseSegment"
            ]
          },
          "parent_name": null,
          "private_methods_count": 0,
          "public_methods_count": 0,
          "node_count": 50,
          "chunk_size": 305
        }
      },
      {
        "rank": 3,
        "score": 0.6004999876022339,
        "content": "class Rule_L041(BaseRule):\n    \"\"\"SELECT clause modifiers such as DISTINCT must be on the same line as SELECT.\n\n    | **Anti-pattern**\n\n    .. code-block:: sql\n\n        select\n            distinct a,\n            b\n        from x\n\n\n    | **Best practice**\n\n    .. code-block:: sql\n\n        select distinct\n            a,\n            b\n        from x\n\n    \"\"\"\n\n    def _eval(self, segment, **kwargs):\n        \"\"\"Select clause modifiers must appear on same line as SELECT.\"\"\"\n        if segment.is_type(\"select_clause\"):\n            # Does the select clause have modifiers?\n            select_modifier = segment.get_child(\"select_clause_modifier\")\n            if not select_modifier:\n                return None  # No. We're done.\n            select_modifier_idx = segment.segments.index(select_modifier)\n\n            # Does the select clause contain a newline?\n            newline = segment.get_child(\"newline\")\n            if not newline:\n                return None  # No. We're done.\n            newline_idx = segment.segments.index(newline)\n\n            # Is there a newline before the select modifier?\n            if newline_idx > select_modifier_idx:\n                return None  # No, we're done.\n\n            # Yes to all the above. We found an issue.\n\n            # E.g.: \" DISTINCT\\n\"\n            replace_newline_with = [\n                WhitespaceSegment(),\n                select_modifier,\n                NewlineSegment(),\n            ]\n            fixes = [\n                # E.g. \"\\n\" -> \" DISTINCT\\n.\n                LintFix(\"edit\", newline, replace_newline_with),\n                # E.g. \"DISTINCT\" -> X\n                LintFix(\"delete\", select_modifier),\n            ]\n\n            # E.g. \" \" after \"DISTINCT\"\n            ws_to_delete = segment.select_children(\n                start_seg=select_modifier,\n                select_if=lambda s: s.is_type(\"whitespace\"),\n                loop_while=lambda s: s.is_type(\"whitespace\") or s.is_meta,\n            )\n\n            # E.g. \" \" -> X\n            fixes += [LintFix(\"delete\", ws) for ws in ws_to_delete]\n            return LintResult(\n                anchor=segment,\n                fixes=fixes,\n            )",
        "file_path": "src/sqlfluff/rules/L041.py",
        "chunk_index": 0,
        "metadata": {
          "class_name": "Rule_L041",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "BaseRule"
            ]
          },
          "parent_name": null,
          "private_methods_count": 1,
          "public_methods_count": 0,
          "node_count": 316,
          "chunk_size": 2172
        }
      },
      {
        "rank": 4,
        "score": 0.5977278351783752,
        "content": "class DropSequenceStatementSegment(BaseSegment):\n    \"\"\"Drop Sequence Statement.\n\n    As specified in https://www.postgresql.org/docs/13/sql-dropsequence.html\n    \"\"\"\n\n    type = \"drop_sequence_statement\"\n\n    match_grammar = Sequence(\n        \"DROP\",\n        \"SEQUENCE\",\n        Ref(\"IfExistsGrammar\", optional=True),\n        Delimited(Ref(\"SequenceReferenceSegment\")),\n        OneOf(\"CASCADE\", \"RESTRICT\", optional=True),\n    )",
        "file_path": "src/sqlfluff/dialects/dialect_postgres.py",
        "chunk_index": 35,
        "metadata": {
          "class_name": "DropSequenceStatementSegment",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "BaseSegment"
            ]
          },
          "parent_name": null,
          "private_methods_count": 0,
          "public_methods_count": 0,
          "node_count": 91,
          "chunk_size": 429
        }
      },
      {
        "rank": 5,
        "score": 0.5933413505554199,
        "content": "class Rule_L019(BaseRule):\n    \"\"\"Leading/Trailing comma enforcement.\n\n    | **Anti-pattern**\n    | There is a mixture of leading and trailing commas.\n\n    .. code-block:: sql\n\n        SELECT\n            a\n            , b,\n            c\n        FROM foo\n\n    | **Best practice**\n    | By default sqlfluff prefers trailing commas, however it\n    | is configurable for leading commas. Whichever option you chose\n    | it does expect you to be consistent.\n\n    .. code-block:: sql\n\n        SELECT\n            a,\n            b,\n            c\n        FROM foo\n\n        -- Alternatively, set the configuration file to 'leading'\n        -- and then the following would be acceptable:\n\n        SELECT\n            a\n            , b\n            , c\n        FROM foo\n\n\n    \"\"\"\n\n    _works_on_unparsable = False\n    config_keywords = [\"comma_style\"]\n\n    @staticmethod\n    def _last_comment_seg(raw_stack):\n        \"\"\"Trace the raw stack back to the most recent comment segment.\n\n        A return value of `None` indicates no code segments preceding the current position.\n        \"\"\"\n        for segment in raw_stack[::-1]:\n            if segment.is_comment:\n                return segment\n        return None\n\n    @staticmethod\n    def _last_code_seg(raw_stack):\n        \"\"\"Trace the raw stack back to the most recent code segment.\n\n        A return value of `None` indicates no code segments preceding the current position.\n        \"\"\"\n        for segment in raw_stack[::-1]:\n            if segment.is_code or segment.is_type(\"newline\"):\n                return segment\n        return None\n\n    def _eval(self, segment, raw_stack, memory, **kwargs):\n        \"\"\"Enforce comma placement.\n\n        For leading commas we're looking for trailing commas, so\n        we look for newline segments. For trailing commas we're\n        looking for leading commas, so we look for the comma itself.\n\n        We also want to handle proper whitespace removal/addition. We remove\n        any trailing whitespace after the leading comma, when converting a\n        leading comma to a trailing comma. We add whitespace after the leading\n        comma when converting a trailing comma to a leading comma.\n        \"\"\"\n        if not memory:\n            memory: Dict[str, Any] = {\n                # Trailing comma keys\n                #\n                # Do we have a fix in place for removing a leading\n                # comma violation, and inserting a new trailing comma?\n                \"insert_trailing_comma\": False,\n                # A list of whitespace segments that come after a\n                # leading comma violation, to be removed during fixing.\n                \"whitespace_deletions\": None,\n                # The leading comma violation segment to be removed during fixing\n                \"last_leading_comma_seg\": None,\n                # The newline segment where we're going to insert our new trailing\n                # comma during fixing\n                \"anchor_for_new_trailing_comma_seg\": None,\n                #\n                # Leading comma keys\n                #\n                # Do we have a fix in place for removing a trailing\n                # comma violation, and inserting a new leading comma?\n                \"insert_leading_comma\": False,\n                # The trailing comma violation segment to be removed during fixing\n                \"last_trailing_comma_segment\": None,\n            }\n\n        if self.comma_style == \"trailing\":\n            # A comma preceded by a new line == a leading comma\n            if segment.is_type(\"comma\"):\n                last_seg = self._last_code_seg(raw_stack)\n                if last_seg.is_type(\"newline\"):\n                    # Recorded where the fix should be applied\n                    memory[\"last_leading_comma_seg\"] = segment\n                    last_comment_seg = self._last_comment_seg(raw_stack)\n                    inline_comment = (\n                        last_comment_seg.pos_marker.line_no\n                        == last_seg.pos_marker.line_no\n                        if last_comment_seg\n                        else False\n                    )\n                    # If we have a comment right before the newline, then anchor\n                    # the fix at the comment instead\n                    memory[\"anchor_for_new_trailing_comma_seg\"] = (\n                        last_seg if not inline_comment else last_comment_seg\n                    )\n                    # Trigger fix routine\n                    memory[\"insert_trailing_comma\"] = True\n                    memory[\"whitespace_deletions\"] = []\n                    return LintResult(memory=memory)\n            # Have we found a leading comma violation?\n            if memory[\"insert_trailing_comma\"]:\n                # Search for trailing whitespace to delete after the leading\n                # comma violation\n                if segment.is_type(\"whitespace\"):\n                    memory[\"whitespace_deletions\"] += [segment]\n                    return LintResult(memory=memory)\n                else:\n                    # We've run out of whitespace to delete, time to fix\n                    last_leading_comma_seg = memory[\"last_leading_comma_seg\"]\n                    # Scan backwards to find the last code segment, skipping\n                    # over lines that are either entirely blank or just a\n                    # comment. We want to place the comma immediately after it.\n                    last_code_seg = None\n                    while last_code_seg is None or last_code_seg.is_type(\"newline\"):\n                        last_code_seg = self._last_code_seg(\n                            raw_stack[\n                                : raw_stack.index(\n                                    last_code_seg\n                                    if last_code_seg\n                                    else memory[\"last_leading_comma_seg\"]\n                                )\n                            ]\n                        )\n                    return LintResult(\n                        anchor=last_leading_comma_seg,\n                        description=\"Found leading comma. Expected only trailing.\",\n                        fixes=[\n                            LintFix(\"delete\", last_leading_comma_seg),\n                            *[\n                                LintFix(\"delete\", d)\n                                for d in memory[\"whitespace_deletions\"]\n                            ],\n                            LintFix(\n                                \"edit\",\n                                last_code_seg,\n                                # Reuse the previous leading comma violation to\n                                # create a new trailing comma\n                                [last_code_seg, last_leading_comma_seg],\n                            ),\n                        ],\n                    )\n\n        elif self.comma_style == \"leading\":\n            # A new line preceded by a comma == a trailing comma\n            if segment.is_type(\"newline\"):\n                last_seg = self._last_code_seg(raw_stack)\n                # no code precedes the current position: no issue\n                if last_seg is None:\n                    return None\n                if last_seg.is_type(\"comma\"):\n                    # Trigger fix routine\n                    memory[\"insert_leading_comma\"] = True\n                    # Record where the fix should be applied\n                    memory[\"last_trailing_comma_segment\"] = last_seg\n                    return LintResult(memory=memory)\n            # Have we found a trailing comma violation?\n            if memory[\"insert_leading_comma\"]:\n                # Only insert the comma here if this isn't a comment/whitespace segment\n                if segment.is_code:\n                    last_comma_seg = memory[\"last_trailing_comma_segment\"]\n                    # Create whitespace to insert after the new leading comma\n                    new_whitespace_seg = WhitespaceSegment()\n                    return LintResult(\n                        anchor=last_comma_seg,\n                        description=\"Found trailing comma. Expected only leading.\",\n                        fixes=[\n                            LintFix(\"delete\", anchor=last_comma_seg),\n                            LintFix(\n                                \"create\",\n                                anchor=segment,\n                                edit=[last_comma_seg, new_whitespace_seg],\n                            ),\n                        ],\n                    )\n        # Otherwise, no issue\n        return None",
        "file_path": "src/sqlfluff/rules/L019.py",
        "chunk_index": 0,
        "metadata": {
          "class_name": "Rule_L019",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "BaseRule"
            ]
          },
          "parent_name": null,
          "private_methods_count": 1,
          "public_methods_count": 0,
          "node_count": 887,
          "chunk_size": 8546
        }
      },
      {
        "rank": 6,
        "score": 0.590981125831604,
        "content": "class Rule_L048(Rule_L006):\n    \"\"\"Quoted literals should be surrounded by a single whitespace.\n\n    | **Anti-pattern**\n    | In this example, there is a space missing space between the string 'foo'\n    | and the keyword AS.\n\n    .. code-block:: sql\n\n        SELECT\n            'foo'AS bar\n        FROM foo\n\n\n    | **Best practice**\n    | Keep a single space.\n\n    .. code-block:: sql\n\n        SELECT\n            'foo' AS bar\n        FROM foo\n    \"\"\"\n\n    _target_elems: List[Tuple[str, str]] = [\n        (\"name\", \"quoted_literal\"),\n    ]\n\n    @staticmethod\n    def _missing_whitespace(seg, before=True):\n        \"\"\"Check whether we're missing whitespace given an adjoining segment.\n\n        This avoids flagging for commas after quoted strings.\n        https://github.com/sqlfluff/sqlfluff/issues/943\n        \"\"\"\n        simple_res = Rule_L006._missing_whitespace(seg, before=before)\n        if not before and seg and seg.is_type(\"comma\"):\n            return False\n        return simple_res",
        "file_path": "src/sqlfluff/rules/L048.py",
        "chunk_index": 0,
        "metadata": {
          "class_name": "Rule_L048",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "Rule_L006"
            ]
          },
          "parent_name": null,
          "private_methods_count": 0,
          "public_methods_count": 0,
          "node_count": 123,
          "chunk_size": 991
        }
      },
      {
        "rank": 7,
        "score": 0.5870593786239624,
        "content": "class Rule_L036(BaseRule):\n    \"\"\"Select targets should be on a new line unless there is only one select target.\n\n    | **Anti-pattern**\n\n    .. code-block:: sql\n\n        select\n            *\n        from x\n\n\n    | **Best practice**\n\n    .. code-block:: sql\n\n        select\n            a,\n            b,\n            c\n        from x\n\n    \"\"\"\n\n    def _eval(self, segment, raw_stack, **kwargs):\n        if segment.is_type(\"select_clause\"):\n            select_targets_info = self._get_indexes(segment)\n            if len(select_targets_info.select_targets) == 1:\n                parent_stack = kwargs.get(\"parent_stack\")\n                return self._eval_single_select_target_element(\n                    select_targets_info, segment, parent_stack\n                )\n            elif len(select_targets_info.select_targets) > 1:\n                return self._eval_multiple_select_target_elements(\n                    select_targets_info, segment\n                )\n\n    @staticmethod\n    def _get_indexes(segment):\n        select_idx = -1\n        first_new_line_idx = -1\n        first_select_target_idx = -1\n        first_whitespace_idx = -1\n        select_targets = []\n        for fname_idx, seg in enumerate(segment.segments):\n            if seg.is_type(\"select_clause_element\"):\n                select_targets.append(seg)\n                if first_select_target_idx == -1:\n                    first_select_target_idx = fname_idx\n            if seg.is_type(\"keyword\") and seg.name == \"select\" and select_idx == -1:\n                select_idx = fname_idx\n            if seg.is_type(\"newline\") and first_new_line_idx == -1:\n                first_new_line_idx = fname_idx\n            # TRICKY: Ignore whitespace prior to the first newline, e.g. if\n            # the line with \"SELECT\" (before any select targets) has trailing\n            # whitespace.\n            if (\n                seg.is_type(\"whitespace\")\n                and first_new_line_idx != -1\n                and first_whitespace_idx == -1\n            ):\n                first_whitespace_idx = fname_idx\n\n        return SelectTargetsInfo(\n            select_idx,\n            first_new_line_idx,\n            first_select_target_idx,\n            first_whitespace_idx,\n            select_targets,\n        )\n\n    def _eval_multiple_select_target_elements(self, select_targets_info, segment):\n        \"\"\"Multiple select targets. Ensure each is on a separate line.\"\"\"\n        # Insert newline before every select target.\n        fixes = []\n        for i, select_target in enumerate(select_targets_info.select_targets):\n            base_segment = (\n                segment if not i else select_targets_info.select_targets[i - 1]\n            )\n            if (\n                base_segment.pos_marker.working_line_no\n                == select_target.pos_marker.working_line_no\n            ):\n                # Find and delete any whitespace before the select target.\n                start_seg = select_targets_info.select_idx\n                # If any select modifier (e.g. distinct ) is present, start\n                # there rather than at the beginning.\n                modifier = segment.get_child(\"select_clause_modifier\")\n                if modifier:\n                    start_seg = segment.segments.index(modifier)\n\n                ws_to_delete = segment.select_children(\n                    start_seg=segment.segments[start_seg]\n                    if not i\n                    else select_targets_info.select_targets[i - 1],\n                    select_if=lambda s: s.is_type(\"whitespace\"),\n                    loop_while=lambda s: s.is_type(\"whitespace\", \"comma\") or s.is_meta,\n                )\n                fixes += [LintFix(\"delete\", ws) for ws in ws_to_delete]\n                fixes.append(LintFix(\"create\", select_target, NewlineSegment()))\n        if fixes:\n            return LintResult(anchor=segment, fixes=fixes)\n\n    def _eval_single_select_target_element(\n        self, select_targets_info, select_clause, parent_stack\n    ):\n        is_wildcard = False\n        for segment in select_clause.segments:\n            if segment.is_type(\"select_clause_element\"):\n                for sub_segment in segment.segments:\n                    if sub_segment.is_type(\"wildcard_expression\"):\n                        is_wildcard = True\n\n        if is_wildcard:\n            return None\n        elif (\n            select_targets_info.select_idx\n            < select_targets_info.first_new_line_idx\n            < select_targets_info.first_select_target_idx\n        ):\n            # Do we have a modifier?\n            modifier = select_clause.get_child(\"select_clause_modifier\")\n\n            # Prepare the select clause which will be inserted\n            # In most (but not all) case we'll want to replace the newline with\n            # the statement and a newline, but in some cases however (see #1424)\n            # we don't need the final newline.\n            copy_with_newline = True\n            insert_buff = [\n                WhitespaceSegment(),\n                select_clause.segments[select_targets_info.first_select_target_idx],\n            ]\n\n            # Check if the modifier is one we care about\n            if modifier:\n                # If it's already on the first line, ignore it.\n                if (\n                    select_clause.segments.index(modifier)\n                    < select_targets_info.first_new_line_idx\n                ):\n                    modifier = None\n            fixes = [\n                # Delete the first select target from its original location.\n                # We'll add it to the right section at the end, once we know\n                # what to add.\n                LintFix(\n                    \"delete\",\n                    select_clause.segments[select_targets_info.first_select_target_idx],\n                ),\n            ]\n\n            start_idx = 0\n\n            # If we have a modifier to move:\n            if modifier:\n\n                # Add it to the insert\n                insert_buff = [WhitespaceSegment(), modifier] + insert_buff\n\n                modifier_idx = select_clause.segments.index(modifier)\n                # Delete the whitespace after it (which is two after, thanks to indent)\n                if (\n                    len(select_clause.segments) > modifier_idx + 1\n                    and select_clause.segments[modifier_idx + 2].is_whitespace\n                ):\n                    fixes += [\n                        LintFix(\n                            \"delete\",\n                            select_clause.segments[modifier_idx + 2],\n                        ),\n                    ]\n\n                # Delete the modifier itself\n                fixes += [\n                    LintFix(\n                        \"delete\",\n                        modifier,\n                    ),\n                ]\n\n                # Set the position marker for removing the preceding\n                # whitespace and newline, which we'll use below.\n                start_idx = modifier_idx\n            else:\n                # Set the position marker for removing the preceding\n                # whitespace and newline, which we'll use below.\n                start_idx = select_targets_info.first_select_target_idx\n\n            if parent_stack and parent_stack[-1].is_type(\"select_statement\"):\n                select_stmt = parent_stack[-1]\n                select_clause_idx = select_stmt.segments.index(select_clause)\n                after_select_clause_idx = select_clause_idx + 1\n                if len(select_stmt.segments) > after_select_clause_idx:\n                    if select_stmt.segments[after_select_clause_idx].is_type(\"newline\"):\n                        # The select_clause is immediately followed by a\n                        # newline. Delete the newline in order to avoid leaving\n                        # behind an empty line after fix.\n                        delete_last_newline = True\n\n                        # Since, we're deleting the newline, we should also delete all\n                        # whitespace before it or it will add random whitespace to\n                        # following statements. So walk back through the segment\n                        # deleting whitespace until you get the previous newline, or\n                        # something else.\n                        idx = 1\n                        while start_idx - idx < len(select_clause.segments):\n                            # Delete any whitespace\n                            if select_clause.segments[start_idx - idx].is_type(\n                                \"whitespace\"\n                            ):\n                                fixes += [\n                                    LintFix(\n                                        \"delete\",\n                                        select_clause.segments[start_idx - idx],\n                                    ),\n                                ]\n\n                            # Once we see a newline, then we're done\n                            if select_clause.segments[start_idx - idx].is_type(\n                                \"newline\",\n                            ):\n                                break\n\n                            # If we see anything other than whitespace,\n                            # then we're done, but in this case we want to\n                            # keep the final newline.\n                            if not select_clause.segments[start_idx - idx].is_type(\n                                \"whitespace\", \"newline\"\n                            ):\n                                delete_last_newline = False\n                                break\n\n                            idx += 1\n\n                        # Finally delete the newline, unless we've decided not to\n                        if delete_last_newline:\n                            fixes.append(\n                                LintFix(\n                                    \"delete\",\n                                    select_stmt.segments[after_select_clause_idx],\n                                )\n                            )\n\n                    elif select_stmt.segments[after_select_clause_idx].is_type(\n                        \"whitespace\"\n                    ):\n                        # The select_clause has stuff after (most likely a comment)\n                        # Delete the whitespace immeadiately after the select clause\n                        # so the other stuff aligns nicely based on where the select\n                        # clause started\n                        fixes += [\n                            LintFix(\n                                \"delete\",\n                                select_stmt.segments[after_select_clause_idx],\n                            ),\n                        ]\n                    elif select_stmt.segments[after_select_clause_idx].is_type(\n                        \"dedent\"\n                    ):\n                        # The end of the select statement, so this is the one\n                        # case we don't want the newline added to end of\n                        # select_clause (see #1424)\n                        copy_with_newline = False\n\n                        # Again let's strip back the whitespace, bnut simpler\n                        # as don't need to worry about new line so just break\n                        # if see non-whitespace\n                        idx = 1\n                        start_idx = select_clause_idx - 1\n                        while start_idx - idx < len(select_clause.segments):\n                            # Delete any whitespace\n                            if select_clause.segments[start_idx - idx].is_type(\n                                \"whitespace\"\n                            ):\n                                fixes += [\n                                    LintFix(\n                                        \"delete\",\n                                        select_clause.segments[start_idx - idx],\n                                    ),\n                                ]\n\n                            # Once we see a newline, then we're done\n                            if select_clause.segments[start_idx - idx].is_type(\n                                \"newline\"\n                            ):\n                                break\n\n                            # If we see anything other than whitespace,\n                            # then we're done, but in this case we want to\n                            # keep the final newline.\n                            if not select_clause.segments[start_idx - idx].is_type(\n                                \"whitespace\", \"newline\"\n                            ):\n                                copy_with_newline = True\n                                break\n                            idx += 1\n\n            if copy_with_newline:\n                insert_buff = insert_buff + [NewlineSegment()]\n\n            fixes += [\n                # Insert the select_clause in place of the first newlin in the\n                # Select statement\n                LintFix(\n                    \"edit\",\n                    select_clause.segments[select_targets_info.first_new_line_idx],\n                    insert_buff,\n                ),\n            ]\n\n            return LintResult(\n                anchor=select_clause,\n                fixes=fixes,\n            )\n        else:\n            return None",
        "file_path": "src/sqlfluff/rules/L036.py",
        "chunk_index": 1,
        "metadata": {
          "class_name": "Rule_L036",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "BaseRule"
            ]
          },
          "parent_name": null,
          "private_methods_count": 3,
          "public_methods_count": 0,
          "node_count": 1698,
          "chunk_size": 13381
        }
      },
      {
        "rank": 8,
        "score": 0.5813389420509338,
        "content": "class SQLLintError(SQLBaseError):\n    \"\"\"An error which occurred during linting.\n\n    In particular we reference the rule here to do extended logging based on\n    the rule in question which caused the fail.\n\n    Args:\n        segment (:obj:`BaseSegment`, optional): The segment which is relevant\n            for the failure in parsing. This is likely to be a subclass of\n            `BaseSegment` rather than the parent class itself. This is mostly\n            used for logging and for referencing position.\n\n    \"\"\"\n\n    _identifier = \"linting\"\n\n    def __init__(\n        self, *args, segment=None, rule=None, fixes=None, description=None, **kwargs\n    ):\n        # Something about position, message and fix?\n        self.segment = segment\n        if self.segment:\n            kwargs[\"pos\"] = self.segment.pos_marker\n        self.rule = rule\n        self.fixes = fixes or []\n        self.description = description\n        super().__init__(*args, **kwargs)\n\n    @property\n    def fixable(self):\n        \"\"\"Should this error be considered fixable?\"\"\"\n        if self.fixes:\n            return True\n        return False\n\n\n    def __repr__(self):\n        return \"<SQLLintError: rule {} pos:{!r}, #fixes: {}, description: {}>\".format(\n            self.rule_code(),\n            (self.line_no, self.line_pos),\n            len(self.fixes),\n            self.description,\n        )",
        "file_path": "src/sqlfluff/core/errors.py",
        "chunk_index": 9,
        "metadata": {
          "class_name": "SQLLintError",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "SQLBaseError"
            ]
          },
          "parent_name": null,
          "private_methods_count": 2,
          "public_methods_count": 1,
          "node_count": 274,
          "chunk_size": 1372
        }
      },
      {
        "rank": 9,
        "score": 0.5758199691772461,
        "content": "class Rule_L022(BaseRule):\n    \"\"\"Blank line expected but not found after CTE closing bracket.\n\n    | **Anti-pattern**\n    | There is no blank line after the CTE closing bracket. In queries with many\n    | CTEs this hinders readability.\n\n    .. code-block:: sql\n\n        WITH plop AS (\n            SELECT * FROM foo\n        )\n        SELECT a FROM plop\n\n    | **Best practice**\n    | Add a blank line.\n\n    .. code-block:: sql\n\n        WITH plop AS (\n            SELECT * FROM foo\n        )\n\n        SELECT a FROM plop\n\n    \"\"\"\n\n    config_keywords = [\"comma_style\"]\n\n    def _eval(self, segment, **kwargs):\n        \"\"\"Blank line expected but not found after CTE definition.\"\"\"\n        error_buffer = []\n        if segment.is_type(\"with_compound_statement\"):\n            # First we need to find all the commas, the end brackets, the\n            # things that come after that and the blank lines in between.\n\n            # Find all the closing brackets. They are our anchor points.\n            bracket_indices = []\n            expanded_segments = list(\n                segment.iter_segments(expanding=[\"common_table_expression\"])\n            )\n            for idx, seg in enumerate(expanded_segments):\n                if seg.is_type(\"bracketed\"):\n                    bracket_indices.append(idx)\n\n            # Work through each point and deal with it individually\n            for bracket_idx in bracket_indices:\n                forward_slice = expanded_segments[bracket_idx:]\n                seg_idx = 1\n                line_idx = 0\n                comma_seg_idx = None\n                blank_lines = 0\n                comma_line_idx = None\n                line_blank = False\n                comma_style = None\n                line_starts = {}\n                comment_lines = []\n\n                self.logger.info(\n                    \"## CTE closing bracket found at %s, idx: %s. Forward slice: %.20r\",\n                    forward_slice[0].pos_marker,\n                    bracket_idx,\n                    \"\".join(elem.raw for elem in forward_slice),\n                )\n\n                # Work forward to map out the following segments.\n                while (\n                    forward_slice[seg_idx].is_type(\"comma\")\n                    or not forward_slice[seg_idx].is_code\n                ):\n                    if forward_slice[seg_idx].is_type(\"newline\"):\n                        if line_blank:\n                            # It's a blank line!\n                            blank_lines += 1\n                        line_blank = True\n                        line_idx += 1\n                        line_starts[line_idx] = seg_idx + 1\n                    elif forward_slice[seg_idx].is_type(\"comment\"):\n                        # Lines with comments aren't blank\n                        line_blank = False\n                        comment_lines.append(line_idx)\n                    elif forward_slice[seg_idx].is_type(\"comma\"):\n                        # Keep track of where the comma is.\n                        # We'll evaluate it later.\n                        comma_line_idx = line_idx\n                        comma_seg_idx = seg_idx\n                    seg_idx += 1\n\n                # Infer the comma style (NB this could be different for each case!)\n                if comma_line_idx is None:\n                    comma_style = \"final\"\n                elif line_idx == 0:\n                    comma_style = \"oneline\"\n                elif comma_line_idx == 0:\n                    comma_style = \"trailing\"\n                elif comma_line_idx == line_idx:\n                    comma_style = \"leading\"\n                else:\n                    comma_style = \"floating\"\n\n                # Readout of findings\n                self.logger.info(\n                    \"blank_lines: %s, comma_line_idx: %s. final_line_idx: %s, final_seg_idx: %s\",\n                    blank_lines,\n                    comma_line_idx,\n                    line_idx,\n                    seg_idx,\n                )\n                self.logger.info(\n                    \"comma_style: %r, line_starts: %r, comment_lines: %r\",\n                    comma_style,\n                    line_starts,\n                    comment_lines,\n                )\n\n                if blank_lines < 1:\n                    # We've got an issue\n                    self.logger.info(\"!! Found CTE without enough blank lines.\")\n\n                    # Based on the current location of the comma we insert newlines\n                    # to correct the issue.\n                    fix_type = \"create\"  # In most cases we just insert newlines.\n                    if comma_style == \"oneline\":\n                        # Here we respect the target comma style to insert at the relevant point.\n                        if self.comma_style == \"trailing\":\n                            # Add a blank line after the comma\n                            fix_point = forward_slice[comma_seg_idx + 1]\n                            # Optionally here, if the segment we've landed on is\n                            # whitespace then we REPLACE it rather than inserting.\n                            if forward_slice[comma_seg_idx + 1].is_type(\"whitespace\"):\n                                fix_type = \"edit\"\n                        elif self.comma_style == \"leading\":\n                            # Add a blank line before the comma\n                            fix_point = forward_slice[comma_seg_idx]\n                        # In both cases it's a double newline.\n                        num_newlines = 2\n                    else:\n                        # In the following cases we only care which one we're in\n                        # when comments don't get in the way. If they *do*, then\n                        # we just work around them.\n                        if not comment_lines or line_idx - 1 not in comment_lines:\n                            self.logger.info(\"Comment routines not applicable\")\n                            if comma_style in (\"trailing\", \"final\", \"floating\"):\n                                # Detected an existing trailing comma or it's a final CTE,\n                                # OR the comma isn't leading or trailing.\n                                # If the preceding segment is whitespace, replace it\n                                if forward_slice[seg_idx - 1].is_type(\"whitespace\"):\n                                    fix_point = forward_slice[seg_idx - 1]\n                                    fix_type = \"edit\"\n                                else:\n                                    # Otherwise add a single newline before the end content.\n                                    fix_point = forward_slice[seg_idx]\n                            elif comma_style == \"leading\":\n                                # Detected an existing leading comma.\n                                fix_point = forward_slice[comma_seg_idx]\n                        else:\n                            self.logger.info(\"Handling preceding comments\")\n                            offset = 1\n                            while line_idx - offset in comment_lines:\n                                offset += 1\n                            fix_point = forward_slice[\n                                line_starts[line_idx - (offset - 1)]\n                            ]\n                        # Note: There is an edge case where this isn't enough, if\n                        # comments are in strange places, but we'll catch them on\n                        # the next iteration.\n                        num_newlines = 1\n\n                    fixes = [\n                        LintFix(\n                            fix_type,\n                            fix_point,\n                            [NewlineSegment()] * num_newlines,\n                        )\n                    ]\n                    # Create a result, anchored on the start of the next content.\n                    error_buffer.append(\n                        LintResult(anchor=forward_slice[seg_idx], fixes=fixes)\n                    )\n        # Return the buffer if we have one.\n        return error_buffer or None",
        "file_path": "src/sqlfluff/rules/L022.py",
        "chunk_index": 0,
        "metadata": {
          "class_name": "Rule_L022",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "BaseRule"
            ]
          },
          "parent_name": null,
          "private_methods_count": 1,
          "public_methods_count": 0,
          "node_count": 968,
          "chunk_size": 8089
        }
      },
      {
        "rank": 10,
        "score": 0.5715382099151611,
        "content": "def lint(sql, dialect=\"ansi\", rules=None):\n    \"\"\"Lint a sql string or file.\n\n    Args:\n        sql (:obj:`str` or file-like object): The sql to be linted\n            either as a string or a subclass of :obj:`TextIOBase`.\n        dialect (:obj:`str`, optional): A reference to the dialect of the sql\n            to be linted. Defaults to `ansi`.\n        rules (:obj:`str` or iterable of :obj:`str`, optional): A subset of rule\n            reference to lint for.\n\n    Returns:\n        :obj:`list` of :obj:`dict` for each violation found.\n    \"\"\"\n    sql = _unify_str_or_file(sql)\n    linter = Linter(dialect=dialect, rules=rules)\n\n    result = linter.lint_string_wrapped(sql)\n    result_records = result.as_records()\n    # Return just the violations for this file\n    return [] if not result_records else result_records[0][\"violations\"]",
        "file_path": "src/sqlfluff/api/simple.py",
        "chunk_index": 2,
        "metadata": {
          "func_name": "lint",
          "chunk_type": "function",
          "relationship": {
            "call": [
              "_unify_str_or_file",
              "Linter",
              "linter.lint_string_wrapped",
              "result.as_records"
            ]
          },
          "class_name": null
        }
      }
    ]
  },
  "sqlfluff__sqlfluff-1763": {
    "query": "dbt postgres fix command errors with UnicodeEncodeError and also wipes the .sql file\n_If this is a parsing or linting issue, please include a minimal SQL example which reproduces the issue, along with the `sqlfluff parse` output, `sqlfluff lint` output and `sqlfluff fix` output when relevant._\r\n\r\n## Expected Behaviour\r\nViolation failure notice at a minimum, without wiping the file. Would like a way to ignore the known error at a minimum as --noqa is not getting past this. Actually would expect --noqa to totally ignore this.\r\n\r\n## Observed Behaviour\r\nReported error: `UnicodeEncodeError: 'charmap' codec can't encode character '\\u2192' in position 120: character maps to <undefined>`\r\n\r\n## Steps to Reproduce\r\nSQL file:\r\n```sql\r\nSELECT\r\n    reacted_table_name_right.descendant_id AS category_id,\r\n    string_agg(redacted_table_name_left.name, '  ' ORDER BY reacted_table_name_right.generations DESC) AS breadcrumbs -- noqa\r\nFROM {{ ref2('redacted_schema_name', 'redacted_table_name_left') }} AS redacted_table_name_left\r\nINNER JOIN {{ ref2('redacted_schema_name', 'reacted_table_name_right') }} AS reacted_table_name_right\r\n    ON redacted_table_name_left.id = order_issue_category_hierarchies.ancestor_id\r\nGROUP BY reacted_table_name_right.descendant_id\r\n```\r\nRunning `sqlfluff fix --ignore templating,parsing,lexing -vvvv` and accepting proposed fixes for linting violations.\r\n\r\n## Dialect\r\n`postgres`, with `dbt` templater\r\n\r\n## Version\r\n`python 3.7.12`\r\n`sqlfluff 0.7.0`\r\n`sqlfluff-templater-dbt 0.7.0`\r\n\r\n## Configuration\r\nI've tried a few, here's one:\r\n```\r\n[sqlfluff]\r\nverbose = 2\r\ndialect = postgres\r\ntemplater = dbt\r\nexclude_rules = None\r\noutput_line_length = 80\r\nrunaway_limit = 10\r\nignore_templated_areas = True\r\nprocesses = 3\r\n# Comma separated list of file extensions to lint.\r\n\r\n# NB: This config will only apply in the root folder.\r\nsql_file_exts = .sql\r\n\r\n[sqlfluff:indentation]\r\nindented_joins = False\r\nindented_using_on = True\r\ntemplate_blocks_indent = True\r\n\r\n[sqlfluff:templater]\r\nunwrap_wrapped_queries = True\r\n\r\n[sqlfluff:templater:jinja]\r\napply_dbt_builtins = True\r\n\r\n[sqlfluff:templater:jinja:macros]\r\n# Macros provided as builtins for dbt projects\r\ndbt_ref = {% macro ref(model_ref) %}{{model_ref}}{% endmacro %}\r\ndbt_source = {% macro source(source_name, table) %}{{source_name}}_{{table}}{% endmacro %}\r\ndbt_config = {% macro config() %}{% for k in kwargs %}{% endfor %}{% endmacro %}\r\ndbt_var = {% macro var(variable, default='') %}item{% endmacro %}\r\ndbt_is_incremental = {% macro is_incremental() %}True{% endmacro %}\r\n\r\n# Common config across rules\r\n[sqlfluff:rules]\r\ntab_space_size = 4\r\nindent_unit = space\r\nsingle_table_references = consistent\r\nunquoted_identifiers_policy = all\r\n\r\n# L001 - Remove trailing whitespace (fix)\r\n# L002 - Single section of whitespace should not contain both tabs and spaces (fix)\r\n# L003 - Keep consistent indentation (fix)\r\n# L004 - We use 4 spaces for indentation just for completeness (fix)\r\n# L005 - Remove space before commas (fix)\r\n# L006 - Operators (+, -, *, /) will be wrapped by a single space each side (fix)\r\n\r\n# L007 - Operators should not be at the end of a line\r\n[sqlfluff:rules:L007]  # Keywords\r\noperator_new_lines = after\r\n\r\n# L008 - Always use a single whitespace after a comma (fix)\r\n# L009 - Files will always end with a trailing newline\r\n\r\n# L010 - All keywords will use full upper case (fix)\r\n[sqlfluff:rules:L010]  # Keywords\r\ncapitalisation_policy = upper\r\n\r\n# L011 - Always explicitly alias tables (fix)\r\n[sqlfluff:rules:L011]  # Aliasing\r\naliasing = explicit\r\n\r\n# L012 - Do not have to explicitly alias all columns\r\n[sqlfluff:rules:L012]  # Aliasing\r\naliasing = explicit\r\n\r\n# L013 - Always explicitly alias a column with an expression in it (fix)\r\n[sqlfluff:rules:L013]  # Aliasing\r\nallow_scalar = False\r\n\r\n# L014 - Always user full lower case for 'quoted identifiers' -> column refs. without an alias (fix)\r\n[sqlfluff:rules:L014]  # Unquoted identifiers\r\nextended_capitalisation_policy = lower\r\n\r\n# L015 - Always remove parenthesis when using DISTINCT to be clear that DISTINCT applies to all columns (fix)\r\n\r\n# L016 - Lines should be 120 characters of less. Comment lines should not be ignored (fix)\r\n[sqlfluff:rules:L016]\r\nignore_comment_lines = False\r\nmax_line_length = 120\r\n\r\n# L017 - There should not be whitespace between function name and brackets (fix)\r\n# L018 - Always align closing bracket of WITH to the WITH keyword (fix)\r\n\r\n# L019 - Always use trailing commas / commas at the end of the line (fix)\r\n[sqlfluff:rules:L019]\r\ncomma_style = trailing\r\n\r\n# L020 - Table aliases will always be unique per statement\r\n# L021 - Remove any use of ambiguous DISTINCT and GROUP BY combinations. Lean on removing the GROUP BY.\r\n# L022 - Add blank lines after common table expressions (CTE) / WITH.\r\n# L023 - Always add a single whitespace after AS in a WITH clause (fix)\r\n\r\n[sqlfluff:rules:L026]\r\nforce_enable = False\r\n\r\n# L027 - Always add references if more than one referenced table or view is used\r\n\r\n[sqlfluff:rules:L028]\r\nforce_enable = False\r\n\r\n[sqlfluff:rules:L029]  # Keyword identifiers\r\nunquoted_identifiers_policy = aliases\r\n\r\n[sqlfluff:rules:L030]  # Function names\r\ncapitalisation_policy = upper\r\n\r\n# L032 - We prefer use of join keys rather than USING\r\n# L034 - We prefer ordering of columns in select statements as (fix):\r\n# 1. wildcards\r\n# 2. single identifiers\r\n# 3. calculations and aggregates\r\n\r\n# L035 - Omit 'else NULL'; it is redundant (fix)\r\n# L036 - Move select targets / identifiers onto new lines each (fix)\r\n# L037 - When using ORDER BY, make the direction explicit (fix)\r\n\r\n# L038 - Never use trailing commas at the end of the SELECT clause\r\n[sqlfluff:rules:L038]\r\nselect_clause_trailing_comma = forbid\r\n\r\n# L039 - Remove unnecessary whitespace (fix)\r\n\r\n[sqlfluff:rules:L040]  # Null & Boolean Literals\r\ncapitalisation_policy = upper\r\n\r\n# L042 - Join clauses should not contain subqueries. Use common tables expressions (CTE) instead.\r\n[sqlfluff:rules:L042]\r\n# By default, allow subqueries in from clauses, but not join clauses.\r\nforbid_subquery_in = join\r\n\r\n# L043 - Reduce CASE WHEN conditions to COALESCE (fix)\r\n# L044 - Prefer a known number of columns along the path to the source data\r\n# L045 - Remove unused common tables expressions (CTE) / WITH statements (fix)\r\n# L046 - Jinja tags should have a single whitespace on both sides\r\n\r\n# L047 - Use COUNT(*) instead of COUNT(0) or COUNT(1) alternatives (fix)\r\n[sqlfluff:rules:L047]  # Consistent syntax to count all rows\r\nprefer_count_1 = False\r\nprefer_count_0 = False\r\n\r\n# L048 - Quoted literals should be surrounded by a single whitespace (fix)\r\n# L049 - Always use IS or IS NOT for comparisons with NULL (fix)\r\n```\r\n\n",
    "method": "hierarchical",
    "top_k": 10,
    "results": [
      {
        "rank": 1,
        "score": 0.6809530258178711,
        "content": "def fix(sql, dialect=\"ansi\", rules=None):\n    \"\"\"Fix a sql string or file.\n\n    Args:\n        sql (:obj:`str` or file-like object): The sql to be linted\n            either as a string or a subclass of :obj:`TextIOBase`.\n        dialect (:obj:`str`, optional): A reference to the dialect of the sql\n            to be linted. Defaults to `ansi`.\n        rules (:obj:`str` or iterable of :obj:`str`, optional): A subset of rule\n            reference to lint for.\n\n    Returns:\n        :obj:`str` for the fixed sql if possible.\n    \"\"\"\n    sql = _unify_str_or_file(sql)\n    linter = Linter(dialect=dialect, rules=rules)\n\n    result = linter.lint_string_wrapped(sql, fix=True)\n    fixed_string = result.paths[0].files[0].fix_string()[0]\n    return fixed_string",
        "file_path": "src/sqlfluff/api/simple.py",
        "chunk_index": 3,
        "metadata": {
          "func_name": "fix",
          "chunk_type": "function",
          "relationship": {
            "call": [
              "_unify_str_or_file",
              "Linter",
              "linter.lint_string_wrapped",
              "result.paths[0].files[0].fix_string"
            ]
          },
          "class_name": null
        }
      },
      {
        "rank": 2,
        "score": 0.6608344316482544,
        "content": "def fix(\n    force: bool,\n    paths: Tuple[str],\n    processes: int,\n    bench: bool = False,\n    fixed_suffix: str = \"\",\n    logger: Optional[logging.Logger] = None,\n    **kwargs,\n) -> NoReturn:\n    \"\"\"Fix SQL files.\n\n    PATH is the path to a sql file or directory to lint. This can be either a\n    file ('path/to/file.sql'), a path ('directory/of/sql/files'), a single ('-')\n    character to indicate reading from *stdin* or a dot/blank ('.'/' ') which will\n    be interpreted like passing the current working directory as a path argument.\n    \"\"\"\n    # some quick checks\n    fixing_stdin = (\"-\",) == paths\n\n    config = get_config(**kwargs)\n    lnt, formatter = get_linter_and_formatter(config, silent=fixing_stdin)\n    verbose = config.get(\"verbose\")\n    exit_code = 0\n\n    formatter.dispatch_config(lnt)\n\n    # Set up logging.\n    set_logging_level(verbosity=verbose, logger=logger, stderr_output=fixing_stdin)\n\n    # handle stdin case. should output formatted sql to stdout and nothing else.\n    if fixing_stdin:\n        stdin = sys.stdin.read()\n\n        result = lnt.lint_string_wrapped(stdin, fname=\"stdin\", fix=True)\n        templater_error = result.num_violations(types=SQLTemplaterError) > 0\n        unfixable_error = result.num_violations(types=SQLLintError, fixable=False) > 0\n\n        if result.num_violations(types=SQLLintError, fixable=True) > 0:\n            stdout = result.paths[0].files[0].fix_string()[0]\n        else:\n            stdout = stdin\n\n        if templater_error:\n            click.echo(\n                colorize(\n                    \"Fix aborted due to unparseable template variables.\",\n                    Color.red,\n                ),\n                err=True,\n            )\n            click.echo(\n                colorize(\n                    \"Use '--ignore templating' to attempt to fix anyway.\",\n                    Color.red,\n                ),\n                err=True,\n            )\n        if unfixable_error:\n            click.echo(colorize(\"Unfixable violations detected.\", Color.red), err=True)\n\n        click.echo(stdout, nl=False)\n        sys.exit(1 if templater_error or unfixable_error else 0)\n\n    # Lint the paths (not with the fix argument at this stage), outputting as we go.\n    click.echo(\"==== finding fixable violations ====\")\n    try:\n        result = lnt.lint_paths(\n            paths, fix=True, ignore_non_existent_files=False, processes=processes\n        )\n    except OSError:\n        click.echo(\n            colorize(\n                f\"The path(s) '{paths}' could not be accessed. Check it/they exist(s).\",\n                Color.red,\n            ),\n            err=True,\n        )\n        sys.exit(1)\n\n    # NB: We filter to linting violations here, because they're\n    # the only ones which can be potentially fixed.\n    if result.num_violations(types=SQLLintError, fixable=True) > 0:\n        click.echo(\"==== fixing violations ====\")\n        click.echo(\n            f\"{result.num_violations(types=SQLLintError, fixable=True)} fixable linting violations found\"\n        )\n        if force:\n            click.echo(f\"{colorize('FORCE MODE', Color.red)}: Attempting fixes...\")\n            success = do_fixes(\n                lnt,\n                result,\n                formatter,\n                types=SQLLintError,\n                fixed_file_suffix=fixed_suffix,\n            )\n            if not success:\n                sys.exit(1)  # pragma: no cover\n        else:\n            click.echo(\n                \"Are you sure you wish to attempt to fix these? [Y/n] \", nl=False\n            )\n            c = click.getchar().lower()\n            click.echo(\"...\")\n            if c in (\"y\", \"\\r\", \"\\n\"):\n                click.echo(\"Attempting fixes...\")\n                success = do_fixes(\n                    lnt,\n                    result,\n                    formatter,\n                    types=SQLLintError,\n                    fixed_file_suffix=fixed_suffix,\n                )\n                if not success:\n                    sys.exit(1)  # pragma: no cover\n                else:\n                    _completion_message(config)\n            elif c == \"n\":\n                click.echo(\"Aborting...\")\n                exit_code = 1\n            else:  # pragma: no cover\n                click.echo(\"Invalid input, please enter 'Y' or 'N'\")\n                click.echo(\"Aborting...\")\n                exit_code = 1\n    else:\n        click.echo(\"==== no fixable linting violations found ====\")\n        _completion_message(config)\n\n    if result.num_violations(types=SQLLintError, fixable=False) > 0:\n        click.echo(\n            f\"  [{result.num_violations(types=SQLLintError, fixable=False)} unfixable linting violations found]\"\n        )\n        exit_code = 1\n\n    if result.num_violations(types=SQLTemplaterError) > 0:\n        click.echo(\n            f\"  [{result.num_violations(types=SQLTemplaterError)} templating errors found]\"\n        )\n        exit_code = 1\n\n    if bench:\n        click.echo(\"==== overall timings ====\")\n        click.echo(cli_table([(\"Clock time\", result.total_time)]))\n        timing_summary = result.timing_summary()\n        for step in timing_summary:\n            click.echo(f\"=== {step} ===\")\n            click.echo(cli_table(timing_summary[step].items()))\n\n    sys.exit(exit_code)",
        "file_path": "src/sqlfluff/cli/commands.py",
        "chunk_index": 13,
        "metadata": {
          "func_name": "fix",
          "chunk_type": "function",
          "relationship": {
            "call": [
              "get_config",
              "get_linter_and_formatter",
              "config.get",
              "formatter.dispatch_config",
              "set_logging_level",
              "sys.stdin.read",
              "lnt.lint_string_wrapped",
              "result.num_violations",
              "result.paths[0].files[0].fix_string",
              "click.echo",
              "colorize",
              "sys.exit",
              "lnt.lint_paths",
              "do_fixes",
              "click.getchar().lower",
              "click.getchar",
              "_completion_message",
              "cli_table",
              "result.timing_summary",
              "timing_summary[step].items"
            ]
          },
          "class_name": null
        }
      },
      {
        "rank": 3,
        "score": 0.6396315693855286,
        "content": "class SQLFluffViolationReporter(BaseViolationReporter):\n    \"\"\"Class that implements diff-quality integration.\"\"\"\n\n    supported_extensions = [\"sql\"]\n\n    def __init__(self):\n        \"\"\"Calls the base class constructor to set the object's name.\"\"\"\n        super().__init__(\"sqlfluff\")\n\n    @staticmethod\n    def violations(src_path: str) -> List[Violation]:\n        \"\"\"Return list of violations.\n\n        Given the path to a .sql file, analyze it and return a list of\n        violations (i.e. formatting or style issues).\n        \"\"\"\n        linter = Linter(config=FluffConfig.from_root())\n        linted_path = linter.lint_path(src_path, ignore_non_existent_files=True)\n        result = []\n        for violation in linted_path.get_violations():\n            try:\n                # Normal SQLFluff warnings\n                message = f\"{violation.rule_code()}: {violation.description}\"\n            except AttributeError:\n                # Parse errors\n                message = str(violation)\n            result.append(Violation(violation.line_no, message))\n        return result\n",
        "file_path": "src/sqlfluff/diff_quality_plugin.py",
        "chunk_index": 0,
        "metadata": {
          "class_name": "SQLFluffViolationReporter",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "BaseViolationReporter"
            ]
          },
          "parent_name": null,
          "private_methods_count": 1,
          "public_methods_count": 1,
          "node_count": 240,
          "chunk_size": 1078
        }
      },
      {
        "rank": 4,
        "score": 0.6223207712173462,
        "content": "class LintedFile(NamedTuple):\n    \"\"\"A class to store the idea of a linted file.\"\"\"\n\n    path: str\n    violations: List[SQLBaseError]\n    time_dict: dict\n    tree: Optional[BaseSegment]\n    ignore_mask: List[NoQaDirective]\n    templated_file: TemplatedFile\n    encoding: str\n\n    def check_tuples(self, raise_on_non_linting_violations=True) -> List[CheckTuple]:\n        \"\"\"Make a list of check_tuples.\n\n        This assumes that all the violations found are\n        linting violations (and therefore implement `check_tuple()`).\n        If they don't then this function raises that error.\n        \"\"\"\n        vs: List[CheckTuple] = []\n        v: SQLLintError\n        for v in self.get_violations():\n            if hasattr(v, \"check_tuple\"):\n                vs.append(v.check_tuple())\n            elif raise_on_non_linting_violations:\n                raise v\n        return vs\n\n    def get_violations(\n        self,\n        rules: Optional[Union[str, Tuple[str, ...]]] = None,\n        types: Optional[Union[Type[SQLBaseError], Iterable[Type[SQLBaseError]]]] = None,\n        filter_ignore: bool = True,\n        fixable: bool = None,\n    ) -> list:\n        \"\"\"Get a list of violations, respecting filters and ignore options.\n\n        Optionally now with filters.\n        \"\"\"\n        violations = self.violations\n        # Filter types\n        if types:\n            # If it's a singular type, make it a single item in a tuple\n            # otherwise coerce to tuple normally so that we can use it with\n            # isinstance.\n            if isinstance(types, type) and issubclass(types, SQLBaseError):\n                types = (types,)\n            else:\n                types = tuple(types)  # pragma: no cover TODO?\n            violations = [v for v in violations if isinstance(v, types)]\n        # Filter rules\n        if rules:\n            if isinstance(rules, str):\n                rules = (rules,)\n            else:\n                rules = tuple(rules)\n            violations = [v for v in violations if v.rule_code() in rules]\n        # Filter fixable\n        if fixable is not None:\n            # Assume that fixable is true or false if not None\n            violations = [v for v in violations if v.fixable is fixable]\n        # Filter ignorable violations\n        if filter_ignore:\n            violations = [v for v in violations if not v.ignore]\n            # Ignore any rules in the ignore mask\n            if self.ignore_mask:\n                violations = self.ignore_masked_violations(violations, self.ignore_mask)\n        return violations\n\n    @staticmethod\n    def _ignore_masked_violations_single_line(\n        violations: List[SQLBaseError], ignore_mask: List[NoQaDirective]\n    ):\n        \"\"\"Returns whether to ignore error for line-specific directives.\n\n        The \"ignore\" list is assumed to ONLY contain NoQaDirectives with\n        action=None.\n        \"\"\"\n        for ignore in ignore_mask:\n            violations = [\n                v\n                for v in violations\n                if not (\n                    v.line_no == ignore.line_no\n                    and (ignore.rules is None or v.rule_code() in ignore.rules)\n                )\n            ]\n        return violations\n\n    @staticmethod\n    def _should_ignore_violation_line_range(\n        line_no: int, ignore_rule: List[NoQaDirective]\n    ):\n        \"\"\"Returns whether to ignore a violation at line_no.\"\"\"\n        # Loop through the NoQaDirectives to find the state of things at\n        # line_no. Assumptions about \"ignore_rule\":\n        # - Contains directives for only ONE RULE, i.e. the rule that was\n        #   violated at line_no\n        # - Sorted in ascending order by line number\n        disable = False\n        for ignore in ignore_rule:\n            if ignore.line_no > line_no:\n                break\n            disable = ignore.action == \"disable\"\n        return disable\n\n    @classmethod\n    def _ignore_masked_violations_line_range(\n        cls, violations: List[SQLBaseError], ignore_mask: List[NoQaDirective]\n    ):\n        \"\"\"Returns whether to ignore error for line-range directives.\n\n        The \"ignore\" list is assumed to ONLY contain NoQaDirectives where\n        action is \"enable\" or \"disable\".\n        \"\"\"\n        result = []\n        for v in violations:\n            # Find the directives that affect the violated rule \"v\", either\n            # because they specifically reference it or because they don't\n            # specify a list of rules, thus affecting ALL rules.\n            ignore_rule = sorted(\n                (\n                    ignore\n                    for ignore in ignore_mask\n                    if not ignore.rules\n                    or (v.rule_code() in cast(Tuple[str, ...], ignore.rules))\n                ),\n                key=lambda ignore: ignore.line_no,\n            )\n            # Determine whether to ignore the violation, based on the relevant\n            # enable/disable directives.\n            if not cls._should_ignore_violation_line_range(v.line_no, ignore_rule):\n                result.append(v)\n        return result\n\n    @classmethod\n    def ignore_masked_violations(\n        cls, violations: List[SQLBaseError], ignore_mask: List[NoQaDirective]\n    ) -> List[SQLBaseError]:\n        \"\"\"Remove any violations specified by ignore_mask.\n\n        This involves two steps:\n        1. Filter out violations affected by single-line \"noqa\" directives.\n        2. Filter out violations affected by disable/enable \"noqa\" directives.\n        \"\"\"\n        ignore_specific = [ignore for ignore in ignore_mask if not ignore.action]\n        ignore_range = [ignore for ignore in ignore_mask if ignore.action]\n        violations = cls._ignore_masked_violations_single_line(\n            violations, ignore_specific\n        )\n        violations = cls._ignore_masked_violations_line_range(violations, ignore_range)\n        return violations\n\n    def num_violations(self, **kwargs) -> int:\n        \"\"\"Count the number of violations.\n\n        Optionally now with filters.\n        \"\"\"\n        violations = self.get_violations(**kwargs)\n        return len(violations)\n\n    def is_clean(self) -> bool:\n        \"\"\"Return True if there are no ignorable violations.\"\"\"\n        return not any(self.get_violations(filter_ignore=True))\n\n    @staticmethod\n    def _log_hints(\n        patch: Union[EnrichedFixPatch, FixPatch], templated_file: TemplatedFile\n    ):\n        \"\"\"Log hints for debugging during patch generation.\"\"\"\n        # This next bit is ALL FOR LOGGING AND DEBUGGING\n        max_log_length = 10\n        if patch.templated_slice.start >= max_log_length:\n            pre_hint = templated_file.templated_str[\n                patch.templated_slice.start\n                - max_log_length : patch.templated_slice.start\n            ]\n        else:\n            pre_hint = templated_file.templated_str[: patch.templated_slice.start]\n        if patch.templated_slice.stop + max_log_length < len(\n            templated_file.templated_str\n        ):\n            post_hint = templated_file.templated_str[\n                patch.templated_slice.stop : patch.templated_slice.stop + max_log_length\n            ]\n        else:\n            post_hint = templated_file.templated_str[patch.templated_slice.stop :]\n        linter_logger.debug(\n            \"        Templated Hint: ...%r <> %r...\", pre_hint, post_hint\n        )\n\n    def fix_string(self) -> Tuple[Any, bool]:\n        \"\"\"Obtain the changes to a path as a string.\n\n        We use the source mapping features of TemplatedFile\n        to generate a list of \"patches\" which cover the non\n        templated parts of the file and refer back to the locations\n        in the original file.\n\n        NB: This is MUCH FASTER than the original approach\n        using difflib in pre 0.4.0.\n\n        There is an important distinction here between Slices and\n        Segments. A Slice is a portion of a file which is determined\n        by the templater based on which portions of the source file\n        are templated or not, and therefore before Lexing and so is\n        completely dialect agnostic. A Segment is determined by the\n        Lexer from portions of strings after templating.\n        \"\"\"\n        linter_logger.debug(\"Original Tree: %r\", self.templated_file.templated_str)\n        assert self.tree\n        linter_logger.debug(\"Fixed Tree: %r\", self.tree.raw)\n\n        # The sliced file is contiguous in the TEMPLATED space.\n        # NB: It has gaps and repeats in the source space.\n        # It's also not the FIXED file either.\n        linter_logger.debug(\"### Templated File.\")\n        for idx, file_slice in enumerate(self.templated_file.sliced_file):\n            t_str = self.templated_file.templated_str[file_slice.templated_slice]\n            s_str = self.templated_file.source_str[file_slice.source_slice]\n            if t_str == s_str:\n                linter_logger.debug(\n                    \"    File slice: %s %r [invariant]\", idx, file_slice\n                )\n            else:\n                linter_logger.debug(\"    File slice: %s %r\", idx, file_slice)\n                linter_logger.debug(\"    \\t\\t\\ttemplated: %r\\tsource: %r\", t_str, s_str)\n\n        original_source = self.templated_file.source_str\n\n        # Make sure no patches overlap and divide up the source file into slices.\n        # Any Template tags in the source file are off limits.\n        source_only_slices = self.templated_file.source_only_slices()\n\n        linter_logger.debug(\"Source-only slices: %s\", source_only_slices)\n\n        # Iterate patches, filtering and translating as we go:\n        linter_logger.debug(\"### Beginning Patch Iteration.\")\n        filtered_source_patches = []\n        dedupe_buffer = []\n        # We use enumerate so that we get an index for each patch. This is entirely\n        # so when debugging logs we can find a given patch again!\n        patch: Union[EnrichedFixPatch, FixPatch]\n        for idx, patch in enumerate(\n            self.tree.iter_patches(templated_str=self.templated_file.templated_str)\n        ):\n            linter_logger.debug(\"  %s Yielded patch: %s\", idx, patch)\n            self._log_hints(patch, self.templated_file)\n\n            # Attempt to convert to source space.\n            try:\n                source_slice = self.templated_file.templated_slice_to_source_slice(\n                    patch.templated_slice,\n                )\n            except ValueError:\n                linter_logger.info(\n                    \"      - Skipping. Source space Value Error. i.e. attempted insertion within templated section.\"\n                )\n                # If we try and slice within a templated section, then we may fail\n                # in which case, we should skip this patch.\n                continue\n\n            # Check for duplicates\n            dedupe_tuple = (source_slice, patch.fixed_raw)\n            if dedupe_tuple in dedupe_buffer:\n                linter_logger.info(\n                    \"      - Skipping. Source space Duplicate: %s\", dedupe_tuple\n                )\n                continue\n\n            # We now evaluate patches in the source-space for whether they overlap\n            # or disrupt any templated sections.\n            # The intent here is that unless explicitly stated, a fix should never\n            # disrupt a templated section.\n            # NOTE: We rely here on the patches being sorted.\n            # TODO: Implement a mechanism for doing templated section fixes. For\n            # now it's just not allowed.\n\n            # Get the affected raw slices.\n            local_raw_slices = self.templated_file.raw_slices_spanning_source_slice(\n                source_slice\n            )\n            local_type_list = [slc.slice_type for slc in local_raw_slices]\n\n            enriched_patch = EnrichedFixPatch(\n                source_slice=source_slice,\n                templated_slice=patch.templated_slice,\n                patch_category=patch.patch_category,\n                fixed_raw=patch.fixed_raw,\n                templated_str=self.templated_file.templated_str[patch.templated_slice],\n                source_str=self.templated_file.source_str[source_slice],\n            )\n\n            # Deal with the easy case of only literals\n            if set(local_type_list) == {\"literal\"}:\n                linter_logger.info(\n                    \"      * Keeping patch on literal-only section: %s\", enriched_patch\n                )\n                filtered_source_patches.append(enriched_patch)\n                dedupe_buffer.append(enriched_patch.dedupe_tuple())\n            # Is it a zero length patch.\n            elif (\n                enriched_patch.source_slice.start == enriched_patch.source_slice.stop\n                and enriched_patch.source_slice.start == local_raw_slices[0].source_idx\n            ):\n                linter_logger.info(\n                    \"      * Keeping insertion patch on slice boundary: %s\",\n                    enriched_patch,\n                )\n                filtered_source_patches.append(enriched_patch)\n                dedupe_buffer.append(enriched_patch.dedupe_tuple())\n            # If it's ONLY templated then we should skip it.\n            elif \"literal\" not in local_type_list:\n                linter_logger.info(\n                    \"      - Skipping patch over templated section: %s\", enriched_patch\n                )\n            # If we span more than two slices then we should just skip it. Too Hard.\n            elif len(local_raw_slices) > 2:\n                linter_logger.info(\n                    \"      - Skipping patch over more than two raw slices: %s\",\n                    enriched_patch,\n                )\n            # If it's an insertion (i.e. the string in the pre-fix template is '') then we\n            # won't be able to place it, so skip.\n            elif not enriched_patch.templated_str:  # pragma: no cover TODO?\n                linter_logger.info(\n                    \"      - Skipping insertion patch in templated section: %s\",\n                    enriched_patch,\n                )\n            # If the string from the templated version isn't in the source, then we can't fix it.\n            elif (\n                enriched_patch.templated_str not in enriched_patch.source_str\n            ):  # pragma: no cover TODO?\n                linter_logger.info(\n                    \"      - Skipping edit patch on templated content: %s\",\n                    enriched_patch,\n                )\n            else:\n                # Identify all the places the string appears in the source content.\n                positions = list(\n                    findall(enriched_patch.templated_str, enriched_patch.source_str)\n                )\n                if len(positions) != 1:\n                    linter_logger.debug(\n                        \"        - Skipping edit patch on non-unique templated content: %s\",\n                        enriched_patch,\n                    )\n                    continue\n                # We have a single occurrence of the thing we want to patch. This\n                # means we can use its position to place our patch.\n                new_source_slice = slice(  # pragma: no cover\n                    enriched_patch.source_slice.start + positions[0],\n                    enriched_patch.source_slice.start\n                    + positions[0]\n                    + len(enriched_patch.templated_str),\n                )\n                enriched_patch = EnrichedFixPatch(  # pragma: no cover\n                    source_slice=new_source_slice,\n                    templated_slice=enriched_patch.templated_slice,\n                    patch_category=enriched_patch.patch_category,\n                    fixed_raw=enriched_patch.fixed_raw,\n                    templated_str=enriched_patch.templated_str,\n                    source_str=enriched_patch.source_str,\n                )\n                linter_logger.debug(  # pragma: no cover\n                    \"      * Keeping Tricky Case. Positions: %s, New Slice: %s, Patch: %s\",\n                    positions,\n                    new_source_slice,\n                    enriched_patch,\n                )\n                filtered_source_patches.append(enriched_patch)  # pragma: no cover\n                dedupe_buffer.append(enriched_patch.dedupe_tuple())  # pragma: no cover\n                continue  # pragma: no cover\n\n        # Sort the patches before building up the file.\n        filtered_source_patches = sorted(\n            filtered_source_patches, key=lambda x: x.source_slice.start\n        )\n        # We now slice up the file using the patches and any source only slices.\n        # This gives us regions to apply changes to.\n        slice_buff = []\n        source_idx = 0\n        for patch in filtered_source_patches:\n            # Are there templated slices at or before the start of this patch?\n            while (\n                source_only_slices\n                and source_only_slices[0].source_idx < patch.source_slice.start\n            ):\n                next_so_slice = source_only_slices.pop(0).source_slice()\n                # Add a pre-slice before the next templated slices if needed.\n                if next_so_slice.start > source_idx:\n                    slice_buff.append(slice(source_idx, next_so_slice.start))\n                # Add the templated slice.\n                slice_buff.append(next_so_slice)\n                source_idx = next_so_slice.stop\n\n            # Is there a gap between current position and this patch?\n            if patch.source_slice.start > source_idx:\n                # Add a slice up to this patch.\n                slice_buff.append(slice(source_idx, patch.source_slice.start))\n\n            # Is this patch covering an area we've already covered?\n            if patch.source_slice.start < source_idx:\n                linter_logger.info(\n                    \"Skipping overlapping patch at Index %s, Patch: %s\",\n                    source_idx,\n                    patch,\n                )\n                # Ignore the patch for now...\n                continue\n\n            # Add this patch.\n            slice_buff.append(patch.source_slice)\n            source_idx = patch.source_slice.stop\n        # Add a tail slice.\n        if source_idx < len(self.templated_file.source_str):\n            slice_buff.append(slice(source_idx, len(self.templated_file.source_str)))\n\n        linter_logger.debug(\"Final slice buffer: %s\", slice_buff)\n\n        # Iterate through the patches, building up the new string.\n        str_buff = \"\"\n        for source_slice in slice_buff:\n            # Is it one in the patch buffer:\n            for patch in filtered_source_patches:\n                if patch.source_slice == source_slice:\n                    # Use the patched version\n                    linter_logger.debug(\n                        \"%-30s    %s    %r > %r\",\n                        f\"Appending {patch.patch_category} Patch:\",\n                        patch.source_slice,\n                        patch.source_str,\n                        patch.fixed_raw,\n                    )\n                    str_buff += patch.fixed_raw\n                    break\n            else:\n                # Use the raw string\n                linter_logger.debug(\n                    \"Appending Raw:                    %s     %r\",\n                    source_slice,\n                    self.templated_file.source_str[source_slice],\n                )\n                str_buff += self.templated_file.source_str[source_slice]\n\n        # The success metric here is whether anything ACTUALLY changed.\n        return str_buff, str_buff != original_source\n\n    def persist_tree(self, suffix: str = \"\") -> bool:\n        \"\"\"Persist changes to the given path.\"\"\"\n        write_buff, success = self.fix_string()\n\n        if success:\n            fname = self.path\n            # If there is a suffix specified, then use it.s\n            if suffix:\n                root, ext = os.path.splitext(fname)\n                fname = root + suffix + ext\n            # Actually write the file.\n            with open(fname, \"w\", encoding=self.encoding) as f:\n                f.write(write_buff)\n        return success",
        "file_path": "src/sqlfluff/core/linter/linted_file.py",
        "chunk_index": 0,
        "metadata": {
          "class_name": "LintedFile",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "NamedTuple"
            ]
          },
          "parent_name": null,
          "private_methods_count": 0,
          "public_methods_count": 6,
          "node_count": 2948,
          "chunk_size": 20072
        }
      },
      {
        "rank": 5,
        "score": 0.622316300868988,
        "content": "def lint(sql, dialect=\"ansi\", rules=None):\n    \"\"\"Lint a sql string or file.\n\n    Args:\n        sql (:obj:`str` or file-like object): The sql to be linted\n            either as a string or a subclass of :obj:`TextIOBase`.\n        dialect (:obj:`str`, optional): A reference to the dialect of the sql\n            to be linted. Defaults to `ansi`.\n        rules (:obj:`str` or iterable of :obj:`str`, optional): A subset of rule\n            reference to lint for.\n\n    Returns:\n        :obj:`list` of :obj:`dict` for each violation found.\n    \"\"\"\n    sql = _unify_str_or_file(sql)\n    linter = Linter(dialect=dialect, rules=rules)\n\n    result = linter.lint_string_wrapped(sql)\n    result_records = result.as_records()\n    # Return just the violations for this file\n    return [] if not result_records else result_records[0][\"violations\"]",
        "file_path": "src/sqlfluff/api/simple.py",
        "chunk_index": 2,
        "metadata": {
          "func_name": "lint",
          "chunk_type": "function",
          "relationship": {
            "call": [
              "_unify_str_or_file",
              "Linter",
              "linter.lint_string_wrapped",
              "result.as_records"
            ]
          },
          "class_name": null
        }
      },
      {
        "rank": 6,
        "score": 0.6076276898384094,
        "content": "def lint(\n    paths: Tuple[str],\n    processes: int,\n    format: str,\n    annotation_level: str,\n    nofail: bool,\n    disregard_sqlfluffignores: bool,\n    logger: Optional[logging.Logger] = None,\n    bench: bool = False,\n    **kwargs,\n) -> NoReturn:\n    \"\"\"Lint SQL files via passing a list of files or using stdin.\n\n    PATH is the path to a sql file or directory to lint. This can be either a\n    file ('path/to/file.sql'), a path ('directory/of/sql/files'), a single ('-')\n    character to indicate reading from *stdin* or a dot/blank ('.'/' ') which will\n    be interpreted like passing the current working directory as a path argument.\n\n    Linting SQL files:\n\n        sqlfluff lint path/to/file.sql\n        sqlfluff lint directory/of/sql/files\n\n    Linting a file via stdin (note the lone '-' character):\n\n        cat path/to/file.sql | sqlfluff lint -\n        echo 'select col from tbl' | sqlfluff lint -\n\n    \"\"\"\n    config = get_config(**kwargs)\n    non_human_output = format != FormatType.human.value\n    lnt, formatter = get_linter_and_formatter(config, silent=non_human_output)\n    verbose = config.get(\"verbose\")\n\n    formatter.dispatch_config(lnt)\n\n    # Set up logging.\n    set_logging_level(verbosity=verbose, logger=logger, stderr_output=non_human_output)\n    # add stdin if specified via lone '-'\n    if (\"-\",) == paths:\n        result = lnt.lint_string_wrapped(sys.stdin.read(), fname=\"stdin\")\n    else:\n        # Output the results as we go\n        if verbose >= 1:\n            click.echo(format_linting_result_header())\n        try:\n            result = lnt.lint_paths(\n                paths,\n                ignore_non_existent_files=False,\n                ignore_files=not disregard_sqlfluffignores,\n                processes=processes,\n            )\n        except OSError:\n            click.echo(\n                colorize(\n                    f\"The path(s) '{paths}' could not be accessed. Check it/they exist(s).\",\n                    Color.red,\n                )\n            )\n            sys.exit(1)\n        # Output the final stats\n        if verbose >= 1:\n            click.echo(format_linting_stats(result, verbose=verbose))\n\n    if format == FormatType.json.value:\n        click.echo(json.dumps(result.as_records()))\n    elif format == FormatType.yaml.value:\n        click.echo(yaml.dump(result.as_records()))\n    elif format == FormatType.github_annotation.value:\n        github_result = []\n        for record in result.as_records():\n            filepath = record[\"filepath\"]\n            for violation in record[\"violations\"]:\n                # NOTE: The output format is designed for this GitHub action:\n                # https://github.com/yuzutech/annotations-action\n                # It is similar, but not identical, to the native GitHub format:\n                # https://docs.github.com/en/rest/reference/checks#annotations-items\n                github_result.append(\n                    {\n                        \"file\": filepath,\n                        \"line\": violation[\"line_no\"],\n                        \"start_column\": violation[\"line_pos\"],\n                        \"end_column\": violation[\"line_pos\"],\n                        \"title\": \"SQLFluff\",\n                        \"message\": f\"{violation['code']}: {violation['description']}\",\n                        \"annotation_level\": annotation_level,\n                    }\n                )\n        click.echo(json.dumps(github_result))\n\n    if bench:\n        click.echo(\"==== overall timings ====\")\n        click.echo(cli_table([(\"Clock time\", result.total_time)]))\n        timing_summary = result.timing_summary()\n        for step in timing_summary:\n            click.echo(f\"=== {step} ===\")\n            click.echo(cli_table(timing_summary[step].items()))\n\n    if not nofail:\n        if not non_human_output:\n            _completion_message(config)\n        sys.exit(result.stats()[\"exit code\"])\n    else:\n        sys.exit(0)",
        "file_path": "src/sqlfluff/cli/commands.py",
        "chunk_index": 11,
        "metadata": {
          "func_name": "lint",
          "chunk_type": "function",
          "relationship": {
            "call": [
              "get_config",
              "get_linter_and_formatter",
              "config.get",
              "formatter.dispatch_config",
              "set_logging_level",
              "lnt.lint_string_wrapped",
              "sys.stdin.read",
              "click.echo",
              "format_linting_result_header",
              "lnt.lint_paths",
              "colorize",
              "sys.exit",
              "format_linting_stats",
              "json.dumps",
              "result.as_records",
              "yaml.dump",
              "github_result.append",
              "cli_table",
              "result.timing_summary",
              "timing_summary[step].items",
              "_completion_message",
              "result.stats"
            ]
          },
          "class_name": null
        }
      },
      {
        "rank": 7,
        "score": 0.6073077917098999,
        "content": "class SQLLintError(SQLBaseError):\n    \"\"\"An error which occurred during linting.\n\n    In particular we reference the rule here to do extended logging based on\n    the rule in question which caused the fail.\n\n    Args:\n        segment (:obj:`BaseSegment`, optional): The segment which is relevant\n            for the failure in parsing. This is likely to be a subclass of\n            `BaseSegment` rather than the parent class itself. This is mostly\n            used for logging and for referencing position.\n\n    \"\"\"\n\n    _identifier = \"linting\"\n\n    def __init__(\n        self, *args, segment=None, rule=None, fixes=None, description=None, **kwargs\n    ):\n        # Something about position, message and fix?\n        self.segment = segment\n        if self.segment:\n            kwargs[\"pos\"] = self.segment.pos_marker\n        self.rule = rule\n        self.fixes = fixes or []\n        self.description = description\n        super().__init__(*args, **kwargs)\n\n    @property\n    def fixable(self):\n        \"\"\"Should this error be considered fixable?\"\"\"\n        if self.fixes:\n            return True\n        return False\n\n\n    def __repr__(self):\n        return \"<SQLLintError: rule {} pos:{!r}, #fixes: {}, description: {}>\".format(\n            self.rule_code(),\n            (self.line_no, self.line_pos),\n            len(self.fixes),\n            self.description,\n        )",
        "file_path": "src/sqlfluff/core/errors.py",
        "chunk_index": 9,
        "metadata": {
          "class_name": "SQLLintError",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "SQLBaseError"
            ]
          },
          "parent_name": null,
          "private_methods_count": 2,
          "public_methods_count": 1,
          "node_count": 274,
          "chunk_size": 1372
        }
      },
      {
        "rank": 8,
        "score": 0.605713963508606,
        "content": "def process(self, *, fname, in_str=None, config=None, formatter=None):\n        \"\"\"Compile a dbt model and return the compiled SQL.\n\n        Args:\n            fname (:obj:`str`): Path to dbt model(s)\n            in_str (:obj:`str`, optional): This is ignored for dbt\n            config (:obj:`FluffConfig`, optional): A specific config to use for this\n                templating operation. Only necessary for some templaters.\n            formatter (:obj:`CallbackFormatter`): Optional object for output.\n        \"\"\"\n        # Stash the formatter if provided to use in cached methods.\n        self.formatter = formatter\n        self.sqlfluff_config = config\n        self.project_dir = self._get_project_dir()\n        self.profiles_dir = self._get_profiles_dir()\n        fname_absolute_path = os.path.abspath(fname)\n\n        try:\n            os.chdir(self.project_dir)\n            processed_result = self._unsafe_process(fname_absolute_path, in_str, config)\n            # Reset the fail counter\n            self._sequential_fails = 0\n            return processed_result\n        except DbtCompilationException as e:\n            # Increment the counter\n            self._sequential_fails += 1\n            if e.node:\n                return None, [\n                    SQLTemplaterError(\n                        f\"dbt compilation error on file '{e.node.original_file_path}', {e.msg}\",\n                        # It's fatal if we're over the limit\n                        fatal=self._sequential_fails > self.sequential_fail_limit,\n                    )\n                ]\n            else:\n                raise  # pragma: no cover\n        except DbtFailedToConnectException as e:\n            return None, [\n                SQLTemplaterError(\n                    \"dbt tried to connect to the database and failed: \"\n                    \"you could use 'execute' https://docs.getdbt.com/reference/dbt-jinja-functions/execute/ \"\n                    f\"to skip the database calls. Error: {e.msg}\",\n                    fatal=True,\n                )\n            ]\n        # If a SQLFluff error is raised, just pass it through\n        except SQLTemplaterError as e:  # pragma: no cover\n            return None, [e]\n        finally:\n            os.chdir(self.working_dir)",
        "file_path": "plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py",
        "chunk_index": 4,
        "metadata": {
          "func_name": "process",
          "chunk_type": "function",
          "relationship": {
            "call": [
              "self._get_project_dir",
              "self._get_profiles_dir",
              "os.path.abspath",
              "os.chdir",
              "self._unsafe_process",
              "SQLTemplaterError"
            ]
          },
          "class_name": "DbtTemplater"
        }
      },
      {
        "rank": 9,
        "score": 0.6053187847137451,
        "content": "class DbtTemplater(JinjaTemplater):\n    \"\"\"A templater using dbt.\"\"\"\n\n    name = \"dbt\"\n    sequential_fail_limit = 3\n\n    def __init__(self, **kwargs):\n        self.sqlfluff_config = None\n        self.formatter = None\n        self.project_dir = None\n        self.profiles_dir = None\n        self.working_dir = os.getcwd()\n        self._sequential_fails = 0\n        super().__init__(**kwargs)\n\n\n    @property\n    def dbt_version(self):\n        \"\"\"Gets the dbt version.\"\"\"\n        return DBT_VERSION_STRING\n\n    @property\n    def dbt_version_tuple(self):\n        \"\"\"Gets the dbt version as a tuple on (major, minor).\"\"\"\n        return DBT_VERSION_TUPLE\n\n    @cached_property\n    def dbt_config(self):\n        \"\"\"Loads the dbt config.\"\"\"\n        self.dbt_config = DbtRuntimeConfig.from_args(\n            DbtConfigArgs(\n                project_dir=self.project_dir,\n                profiles_dir=self.profiles_dir,\n                profile=self._get_profile(),\n            )\n        )\n        register_adapter(self.dbt_config)\n        return self.dbt_config\n\n    @cached_property\n    def dbt_compiler(self):\n        \"\"\"Loads the dbt compiler.\"\"\"\n        self.dbt_compiler = DbtCompiler(self.dbt_config)\n        return self.dbt_compiler\n\n    @cached_property\n    def dbt_manifest(self):\n        \"\"\"Loads the dbt manifest.\"\"\"\n        # Identity function used for macro hooks\n        def identity(x):\n            return x\n\n        # Set dbt not to run tracking. We don't load\n        # a dull project and so some tracking routines\n        # may fail.\n        from dbt.tracking import do_not_track\n\n        do_not_track()\n\n        if self.dbt_version_tuple <= (0, 19):\n\n            if self.dbt_version_tuple == (0, 17):  # pragma: no cover TODO?\n                # dbt version 0.17.*\n                from dbt.parser.manifest import (\n                    load_internal_manifest as load_macro_manifest,\n                )\n            else:\n                # dbt version 0.18.* & # 0.19.*\n                from dbt.parser.manifest import load_macro_manifest\n\n                load_macro_manifest = partial(load_macro_manifest, macro_hook=identity)\n\n            from dbt.parser.manifest import load_manifest\n\n            dbt_macros_manifest = load_macro_manifest(self.dbt_config)\n            self.dbt_manifest = load_manifest(\n                self.dbt_config, dbt_macros_manifest, macro_hook=identity\n            )\n        else:\n            # dbt 0.20.* and onward\n            from dbt.parser.manifest import ManifestLoader\n\n            projects = self.dbt_config.load_dependencies()\n            loader = ManifestLoader(self.dbt_config, projects, macro_hook=identity)\n            self.dbt_manifest = loader.load()\n\n        return self.dbt_manifest\n\n    @cached_property\n    def dbt_selector_method(self):\n        \"\"\"Loads the dbt selector method.\"\"\"\n        if self.formatter:  # pragma: no cover TODO?\n            self.formatter.dispatch_compilation_header(\n                \"dbt templater\", \"Compiling dbt project...\"\n            )\n\n        if self.dbt_version_tuple == (0, 17):  # pragma: no cover TODO?\n            from dbt.graph.selector import PathSelector\n\n            self.dbt_selector_method = PathSelector(self.dbt_manifest)\n        else:\n            from dbt.graph.selector_methods import (\n                MethodManager as DbtSelectorMethodManager,\n                MethodName as DbtMethodName,\n            )\n\n            selector_methods_manager = DbtSelectorMethodManager(\n                self.dbt_manifest, previous_state=None\n            )\n            self.dbt_selector_method = selector_methods_manager.get_method(\n                DbtMethodName.Path, method_arguments=[]\n            )\n\n        if self.formatter:  # pragma: no cover TODO?\n            self.formatter.dispatch_compilation_header(\n                \"dbt templater\", \"Project Compiled.\"\n            )\n\n        return self.dbt_selector_method\n\n    def _get_profiles_dir(self):\n        \"\"\"Get the dbt profiles directory from the configuration.\n\n        The default is `~/.dbt` in 0.17 but we use the\n        PROFILES_DIR variable from the dbt library to\n        support a change of default in the future, as well\n        as to support the same overwriting mechanism as\n        dbt (currently an environment variable).\n        \"\"\"\n        dbt_profiles_dir = os.path.abspath(\n            os.path.expanduser(\n                self.sqlfluff_config.get_section(\n                    (self.templater_selector, self.name, \"profiles_dir\")\n                )\n                or PROFILES_DIR\n            )\n        )\n\n        if not os.path.exists(dbt_profiles_dir):\n            templater_logger.error(\n                f\"dbt_profiles_dir: {dbt_profiles_dir} could not be accessed. Check it exists.\"\n            )\n\n        return dbt_profiles_dir\n\n    def _get_project_dir(self):\n        \"\"\"Get the dbt project directory from the configuration.\n\n        Defaults to the working directory.\n        \"\"\"\n        dbt_project_dir = os.path.abspath(\n            os.path.expanduser(\n                self.sqlfluff_config.get_section(\n                    (self.templater_selector, self.name, \"project_dir\")\n                )\n                or os.getcwd()\n            )\n        )\n        if not os.path.exists(dbt_project_dir):\n            templater_logger.error(\n                f\"dbt_project_dir: {dbt_project_dir} could not be accessed. Check it exists.\"\n            )\n\n        return dbt_project_dir\n\n    def _get_profile(self):\n        \"\"\"Get a dbt profile name from the configuration.\"\"\"\n        return self.sqlfluff_config.get_section(\n            (self.templater_selector, self.name, \"profile\")\n        )\n\n\n\n    def _find_node(self, fname, config=None):\n        if not config:  # pragma: no cover\n            raise ValueError(\n                \"For the dbt templater, the `process()` method requires a config object.\"\n            )\n        if not fname:  # pragma: no cover\n            raise ValueError(\n                \"For the dbt templater, the `process()` method requires a file name\"\n            )\n        elif fname == \"stdin\":  # pragma: no cover\n            raise ValueError(\n                \"The dbt templater does not support stdin input, provide a path instead\"\n            )\n        selected = self.dbt_selector_method.search(\n            included_nodes=self.dbt_manifest.nodes,\n            # Selector needs to be a relative path\n            selector=os.path.relpath(fname, start=os.getcwd()),\n        )\n        results = [self.dbt_manifest.expect(uid) for uid in selected]\n\n        if not results:\n            model_name = os.path.splitext(os.path.basename(fname))[0]\n            disabled_model = self.dbt_manifest.find_disabled_by_name(name=model_name)\n            if disabled_model and os.path.abspath(\n                disabled_model.original_file_path\n            ) == os.path.abspath(fname):\n                raise SQLTemplaterSkipFile(\n                    f\"Skipped file {fname} because the model was disabled\"\n                )\n            raise RuntimeError(\n                \"File %s was not found in dbt project\" % fname\n            )  # pragma: no cover\n        return results[0]\n\n    def _unsafe_process(self, fname, in_str=None, config=None):\n        node = self._find_node(fname, config)\n\n        node = self.dbt_compiler.compile_node(\n            node=node,\n            manifest=self.dbt_manifest,\n        )\n\n        if hasattr(node, \"injected_sql\"):\n            # If injected SQL is present, it contains a better picture\n            # of what will actually hit the database (e.g. with tests).\n            # However it's not always present.\n            compiled_sql = node.injected_sql\n        else:\n            compiled_sql = node.compiled_sql\n\n        if not compiled_sql:  # pragma: no cover\n            raise SQLTemplaterError(\n                \"dbt templater compilation failed silently, check your configuration \"\n                \"by running `dbt compile` directly.\"\n            )\n\n        with open(fname) as source_dbt_model:\n            source_dbt_sql = source_dbt_model.read()\n\n        n_trailing_newlines = len(source_dbt_sql) - len(source_dbt_sql.rstrip(\"\\n\"))\n\n        templater_logger.debug(\n            \"    Trailing newline count in source dbt model: %r\", n_trailing_newlines\n        )\n        templater_logger.debug(\"    Raw SQL before compile: %r\", source_dbt_sql)\n        templater_logger.debug(\"    Node raw SQL: %r\", node.raw_sql)\n        templater_logger.debug(\"    Node compiled SQL: %r\", compiled_sql)\n\n        # When using dbt-templater, trailing newlines are ALWAYS REMOVED during\n        # compiling. Unless fixed (like below), this will cause:\n        #    1. L009 linting errors when running \"sqlfluff lint foo_bar.sql\"\n        #       since the linter will use the compiled code with the newlines\n        #       removed.\n        #    2. \"No newline at end of file\" warnings in Git/GitHub since\n        #       sqlfluff uses the compiled SQL to write fixes back to the\n        #       source SQL in the dbt model.\n        # The solution is:\n        #    1. Check for trailing newlines before compiling by looking at the\n        #       raw SQL in the source dbt file, store the count of trailing newlines.\n        #    2. Append the count from #1 above to the node.raw_sql and\n        #       compiled_sql objects, both of which have had the trailing\n        #       newlines removed by the dbt-templater.\n        node.raw_sql = node.raw_sql + \"\\n\" * n_trailing_newlines\n        compiled_sql = compiled_sql + \"\\n\" * n_trailing_newlines\n\n        raw_sliced, sliced_file, templated_sql = self.slice_file(\n            node.raw_sql,\n            compiled_sql,\n            config=config,\n        )\n\n        return (\n            TemplatedFile(\n                source_str=node.raw_sql,\n                templated_str=templated_sql,\n                fname=fname,\n                sliced_file=sliced_file,\n                raw_sliced=raw_sliced,\n            ),\n            # No violations returned in this way.\n            [],\n        )",
        "file_path": "plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py",
        "chunk_index": 1,
        "metadata": {
          "class_name": "DbtTemplater",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "JinjaTemplater"
            ]
          },
          "parent_name": null,
          "private_methods_count": 6,
          "public_methods_count": 3,
          "node_count": 2527,
          "chunk_size": 10015
        }
      },
      {
        "rank": 10,
        "score": 0.6039659976959229,
        "content": "class SQLBaseError(ValueError):\n    \"\"\"Base Error Class for all violations.\"\"\"\n\n    _code: Optional[str] = None\n    _identifier = \"base\"\n\n    def __init__(\n        self,\n        *args,\n        pos=None,\n        line_no=0,\n        line_pos=0,\n        ignore=False,\n        fatal=False,\n        **kwargs\n    ):\n        self.fatal = fatal\n        self.ignore = ignore\n        if pos:\n            self.line_no, self.line_pos = pos.source_position()\n        else:\n            self.line_no = line_no\n            self.line_pos = line_pos\n        super().__init__(*args, **kwargs)\n\n    @property\n    def fixable(self):\n        \"\"\"Should this error be considered fixable?\"\"\"\n        return False\n\n\n\n",
        "file_path": "src/sqlfluff/core/errors.py",
        "chunk_index": 0,
        "metadata": {
          "class_name": "SQLBaseError",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "ValueError"
            ]
          },
          "parent_name": null,
          "private_methods_count": 1,
          "public_methods_count": 4,
          "node_count": 498,
          "chunk_size": 690
        }
      }
    ]
  },
  "marshmallow-code__marshmallow-1359": {
    "query": "3.0: DateTime fields cannot be used as inner field for List or Tuple fields\nBetween releases 3.0.0rc8 and 3.0.0rc9, `DateTime` fields have started throwing an error when being instantiated as inner fields of container fields like `List` or `Tuple`. The snippet below works in <=3.0.0rc8 and throws the error below in >=3.0.0rc9 (and, worryingly, 3.0.0):\r\n\r\n```python\r\nfrom marshmallow import fields, Schema\r\n\r\nclass MySchema(Schema):\r\n    times = fields.List(fields.DateTime())\r\n\r\ns = MySchema()\r\n```\r\n\r\nTraceback:\r\n```\r\nTraceback (most recent call last):\r\n  File \"test-mm.py\", line 8, in <module>\r\n    s = MySchema()\r\n  File \"/Users/victor/.pyenv/versions/marshmallow/lib/python3.6/site-packages/marshmallow/schema.py\", line 383, in __init__\r\n    self.fields = self._init_fields()\r\n  File \"/Users/victor/.pyenv/versions/marshmallow/lib/python3.6/site-packages/marshmallow/schema.py\", line 913, in _init_fields\r\n    self._bind_field(field_name, field_obj)\r\n  File \"/Users/victor/.pyenv/versions/marshmallow/lib/python3.6/site-packages/marshmallow/schema.py\", line 969, in _bind_field\r\n    field_obj._bind_to_schema(field_name, self)\r\n  File \"/Users/victor/.pyenv/versions/marshmallow/lib/python3.6/site-packages/marshmallow/fields.py\", line 636, in _bind_to_schema\r\n    self.inner._bind_to_schema(field_name, self)\r\n  File \"/Users/victor/.pyenv/versions/marshmallow/lib/python3.6/site-packages/marshmallow/fields.py\", line 1117, in _bind_to_schema\r\n    or getattr(schema.opts, self.SCHEMA_OPTS_VAR_NAME)\r\nAttributeError: 'List' object has no attribute 'opts'\r\n```\r\n\r\nIt seems like it's treating the parent field as a Schema without checking that it is indeed a schema, so the `schema.opts` statement fails as fields don't have an `opts` attribute.\n",
    "method": "hierarchical",
    "top_k": 10,
    "results": [
      {
        "rank": 1,
        "score": 0.6318709850311279,
        "content": "class FieldInstanceResolutionError(MarshmallowError, TypeError):\n    \"\"\"Raised when schema to instantiate is neither a Schema class nor an instance.\"\"\"",
        "file_path": "src/marshmallow/exceptions.py",
        "chunk_index": 5,
        "metadata": {
          "class_name": "FieldInstanceResolutionError",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "MarshmallowError",
              "TypeError"
            ]
          },
          "parent_name": null,
          "private_methods_count": 0,
          "public_methods_count": 0,
          "node_count": 16,
          "chunk_size": 151
        }
      },
      {
        "rank": 2,
        "score": 0.6296285390853882,
        "content": "class List(Field):\n    \"\"\"A list field, composed with another `Field` class or\n    instance.\n\n    Example: ::\n\n        numbers = fields.List(fields.Float())\n\n    :param Field cls_or_instance: A field class or instance.\n    :param bool default: Default value for serialization.\n    :param kwargs: The same keyword arguments that :class:`Field` receives.\n\n    .. versionchanged:: 2.0.0\n        The ``allow_none`` parameter now applies to deserialization and\n        has the same semantics as the other fields.\n\n    .. versionchanged:: 3.0.0rc9\n        Does not serialize scalar values to single-item lists.\n    \"\"\"\n\n    default_error_messages = {\"invalid\": \"Not a valid list.\"}\n\n    def __init__(self, cls_or_instance, **kwargs):\n        super().__init__(**kwargs)\n        try:\n            self.inner = resolve_field_instance(cls_or_instance)\n        except FieldInstanceResolutionError as error:\n            raise ValueError(\n                \"The list elements must be a subclass or instance of \"\n                \"marshmallow.base.FieldABC.\"\n            ) from error\n        if isinstance(self.inner, Nested):\n            self.only = self.inner.only\n            self.exclude = self.inner.exclude\n\n    def _bind_to_schema(self, field_name, schema):\n        super()._bind_to_schema(field_name, schema)\n        self.inner = copy.deepcopy(self.inner)\n        self.inner._bind_to_schema(field_name, self)\n        if isinstance(self.inner, Nested):\n            self.inner.only = self.only\n            self.inner.exclude = self.exclude\n\n    def _serialize(self, value, attr, obj, **kwargs):\n        if value is None:\n            return None\n        # Optimize dumping a list of Nested objects by calling dump(many=True)\n        if isinstance(self.inner, Nested) and not self.inner.many:\n            return self.inner._serialize(value, attr, obj, many=True, **kwargs)\n        return [self.inner._serialize(each, attr, obj, **kwargs) for each in value]\n\n    def _deserialize(self, value, attr, data, **kwargs):\n        if not utils.is_collection(value):\n            raise self.make_error(\"invalid\")\n        # Optimize loading a list of Nested objects by calling load(many=True)\n        if isinstance(self.inner, Nested) and not self.inner.many:\n            return self.inner.deserialize(value, many=True, **kwargs)\n\n        result = []\n        errors = {}\n        for idx, each in enumerate(value):\n            try:\n                result.append(self.inner.deserialize(each, **kwargs))\n            except ValidationError as error:\n                if error.valid_data is not None:\n                    result.append(error.valid_data)\n                errors.update({idx: error.messages})\n        if errors:\n            raise ValidationError(errors, valid_data=result)\n        return result",
        "file_path": "src/marshmallow/fields.py",
        "chunk_index": 9,
        "metadata": {
          "class_name": "List",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "Field"
            ]
          },
          "parent_name": null,
          "private_methods_count": 4,
          "public_methods_count": 0,
          "node_count": 600,
          "chunk_size": 2776
        }
      },
      {
        "rank": 3,
        "score": 0.5866649150848389,
        "content": "class DateTime(Field):\n    \"\"\"A formatted datetime string.\n\n    Example: ``'2014-12-22T03:12:58.019077+00:00'``\n\n    :param str format: Either ``\"rfc\"`` (for RFC822), ``\"iso\"`` (for ISO8601),\n        or a date format string. If `None`, defaults to \"iso\".\n    :param kwargs: The same keyword arguments that :class:`Field` receives.\n\n    .. versionchanged:: 3.0.0rc9\n        Does not modify timezone information on (de)serialization.\n    \"\"\"\n\n    SERIALIZATION_FUNCS = {\n        \"iso\": utils.isoformat,\n        \"iso8601\": utils.isoformat,\n        \"rfc\": utils.rfcformat,\n        \"rfc822\": utils.rfcformat,\n    }\n\n    DESERIALIZATION_FUNCS = {\n        \"iso\": utils.from_iso_datetime,\n        \"iso8601\": utils.from_iso_datetime,\n        \"rfc\": utils.from_rfc,\n        \"rfc822\": utils.from_rfc,\n    }\n\n    DEFAULT_FORMAT = \"iso\"\n\n    OBJ_TYPE = \"datetime\"\n\n    SCHEMA_OPTS_VAR_NAME = \"datetimeformat\"\n\n    default_error_messages = {\n        \"invalid\": \"Not a valid {obj_type}.\",\n        \"invalid_awareness\": \"Not a valid {awareness} {obj_type}.\",\n        \"format\": '\"{input}\" cannot be formatted as a {obj_type}.',\n    }\n\n    def __init__(self, format=None, **kwargs):\n        super().__init__(**kwargs)\n        # Allow this to be None. It may be set later in the ``_serialize``\n        # or ``_deserialize`` methods. This allows a Schema to dynamically set the\n        # format, e.g. from a Meta option\n        self.format = format\n\n    def _bind_to_schema(self, field_name, schema):\n        super()._bind_to_schema(field_name, schema)\n        self.format = (\n            self.format\n            or getattr(schema.opts, self.SCHEMA_OPTS_VAR_NAME)\n            or self.DEFAULT_FORMAT\n        )\n\n    def _serialize(self, value, attr, obj, **kwargs):\n        if value is None:\n            return None\n        data_format = self.format or self.DEFAULT_FORMAT\n        format_func = self.SERIALIZATION_FUNCS.get(data_format)\n        if format_func:\n            return format_func(value)\n        else:\n            return value.strftime(data_format)\n\n    def _deserialize(self, value, attr, data, **kwargs):\n        if not value:  # Falsy values, e.g. '', None, [] are not valid\n            raise self.make_error(\"invalid\", input=value, obj_type=self.OBJ_TYPE)\n        data_format = self.format or self.DEFAULT_FORMAT\n        func = self.DESERIALIZATION_FUNCS.get(data_format)\n        if func:\n            try:\n                return func(value)\n            except (TypeError, AttributeError, ValueError) as error:\n                raise self.make_error(\n                    \"invalid\", input=value, obj_type=self.OBJ_TYPE\n                ) from error\n        else:\n            try:\n                return self._make_object_from_format(value, data_format)\n            except (TypeError, AttributeError, ValueError) as error:\n                raise self.make_error(\n                    \"invalid\", input=value, obj_type=self.OBJ_TYPE\n                ) from error\n\n    @staticmethod\n    def _make_object_from_format(value, data_format):\n        return dt.datetime.strptime(value, data_format)",
        "file_path": "src/marshmallow/fields.py",
        "chunk_index": 18,
        "metadata": {
          "class_name": "DateTime",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "Field"
            ]
          },
          "parent_name": null,
          "private_methods_count": 4,
          "public_methods_count": 0,
          "node_count": 621,
          "chunk_size": 3073
        }
      },
      {
        "rank": 4,
        "score": 0.5817797183990479,
        "content": "class Nested(Field):\n    \"\"\"Allows you to nest a :class:`Schema <marshmallow.Schema>`\n    inside a field.\n\n    Examples: ::\n\n        user = fields.Nested(UserSchema)\n        user2 = fields.Nested('UserSchema')  # Equivalent to above\n        collaborators = fields.Nested(UserSchema, many=True, only=('id',))\n        parent = fields.Nested('self')\n\n    When passing a `Schema <marshmallow.Schema>` instance as the first argument,\n    the instance's ``exclude``, ``only``, and ``many`` attributes will be respected.\n\n    Therefore, when passing the ``exclude``, ``only``, or ``many`` arguments to `fields.Nested`,\n    you should pass a `Schema <marshmallow.Schema>` class (not an instance) as the first argument.\n\n    ::\n\n        # Yes\n        author = fields.Nested(UserSchema, only=('id', 'name'))\n\n        # No\n        author = fields.Nested(UserSchema(), only=('id', 'name'))\n\n    :param Schema nested: The Schema class or class name (string)\n        to nest, or ``\"self\"`` to nest the :class:`Schema` within itself.\n    :param tuple exclude: A list or tuple of fields to exclude.\n    :param only: A list or tuple of fields to marshal. If `None`, all fields are marshalled.\n        This parameter takes precedence over ``exclude``.\n    :param bool many: Whether the field is a collection of objects.\n    :param unknown: Whether to exclude, include, or raise an error for unknown\n        fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`.\n    :param kwargs: The same keyword arguments that :class:`Field` receives.\n    \"\"\"\n\n    default_error_messages = {\"type\": \"Invalid type.\"}\n\n    def __init__(\n        self, nested, *, default=missing_, exclude=tuple(), only=None, **kwargs\n    ):\n        # Raise error if only or exclude is passed as string, not list of strings\n        if only is not None and not is_collection(only):\n            raise StringNotCollectionError('\"only\" should be a collection of strings.')\n        if exclude is not None and not is_collection(exclude):\n            raise StringNotCollectionError(\n                '\"exclude\" should be a collection of strings.'\n            )\n        self.nested = nested\n        self.only = only\n        self.exclude = exclude\n        self.many = kwargs.get(\"many\", False)\n        self.unknown = kwargs.get(\"unknown\")\n        self._schema = None  # Cached Schema instance\n        super().__init__(default=default, **kwargs)\n\n    @property\n    def schema(self):\n        \"\"\"The nested Schema object.\n\n        .. versionchanged:: 1.0.0\n            Renamed from `serializer` to `schema`.\n        \"\"\"\n        if not self._schema:\n            # Inherit context from parent.\n            context = getattr(self.parent, \"context\", {})\n            if isinstance(self.nested, SchemaABC):\n                self._schema = self.nested\n                self._schema.context.update(context)\n            else:\n                if isinstance(self.nested, type) and issubclass(self.nested, SchemaABC):\n                    schema_class = self.nested\n                elif not isinstance(self.nested, (str, bytes)):\n                    raise ValueError(\n                        \"Nested fields must be passed a \"\n                        \"Schema, not {}.\".format(self.nested.__class__)\n                    )\n                elif self.nested == \"self\":\n                    ret = self\n                    while not isinstance(ret, SchemaABC):\n                        ret = ret.parent\n                    schema_class = ret.__class__\n                else:\n                    schema_class = class_registry.get_class(self.nested)\n                self._schema = schema_class(\n                    many=self.many,\n                    only=self.only,\n                    exclude=self.exclude,\n                    context=context,\n                    load_only=self._nested_normalized_option(\"load_only\"),\n                    dump_only=self._nested_normalized_option(\"dump_only\"),\n                )\n        return self._schema\n\n    def _nested_normalized_option(self, option_name):\n        nested_field = \"%s.\" % self.name\n        return [\n            field.split(nested_field, 1)[1]\n            for field in getattr(self.root, option_name, set())\n            if field.startswith(nested_field)\n        ]\n\n    def _serialize(self, nested_obj, attr, obj, many=False, **kwargs):\n        # Load up the schema first. This allows a RegistryError to be raised\n        # if an invalid schema name was passed\n        schema = self.schema\n        if nested_obj is None:\n            return None\n        return schema.dump(nested_obj, many=self.many or many)\n\n    def _test_collection(self, value, many=False):\n        many = self.many or many\n        if many and not utils.is_collection(value):\n            raise self.make_error(\"type\", input=value, type=value.__class__.__name__)\n\n    def _load(self, value, data, partial=None, many=False):\n        try:\n            valid_data = self.schema.load(\n                value, unknown=self.unknown, partial=partial, many=self.many or many\n            )\n        except ValidationError as error:\n            raise ValidationError(\n                error.messages, valid_data=error.valid_data\n            ) from error\n        return valid_data\n\n    def _deserialize(self, value, attr, data, partial=None, many=False, **kwargs):\n        \"\"\"Same as :meth:`Field._deserialize` with additional ``partial`` argument.\n\n        :param bool|tuple partial: For nested schemas, the ``partial``\n            parameter passed to `Schema.load`.\n\n        .. versionchanged:: 3.0.0\n            Add ``partial`` parameter.\n        \"\"\"\n        self._test_collection(value, many=many)\n        return self._load(value, data, partial=partial, many=many)",
        "file_path": "src/marshmallow/fields.py",
        "chunk_index": 7,
        "metadata": {
          "class_name": "Nested",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "Field"
            ]
          },
          "parent_name": null,
          "private_methods_count": 6,
          "public_methods_count": 0,
          "node_count": 949,
          "chunk_size": 5685
        }
      },
      {
        "rank": 5,
        "score": 0.5796363353729248,
        "content": "def resolve_field_instance(cls_or_instance):\n    \"\"\"Return a Schema instance from a Schema class or instance.\n\n    :param type|Schema cls_or_instance: Marshmallow Schema class or instance.\n    \"\"\"\n    if isinstance(cls_or_instance, type):\n        if not issubclass(cls_or_instance, FieldABC):\n            raise FieldInstanceResolutionError\n        return cls_or_instance()\n    else:\n        if not isinstance(cls_or_instance, FieldABC):\n            raise FieldInstanceResolutionError\n        return cls_or_instance",
        "file_path": "src/marshmallow/utils.py",
        "chunk_index": 25,
        "metadata": {
          "func_name": "resolve_field_instance",
          "chunk_type": "function",
          "relationship": {
            "call": [
              "cls_or_instance"
            ]
          },
          "class_name": null
        }
      },
      {
        "rank": 6,
        "score": 0.5689556002616882,
        "content": "class Tuple(Field):\n    \"\"\"A tuple field, composed of a fixed number of other `Field` classes or\n    instances\n\n    Example: ::\n\n        row = Tuple((fields.String(), fields.Integer(), fields.Float()))\n\n    .. note::\n        Because of the structured nature of `collections.namedtuple` and\n        `typing.NamedTuple`, using a Schema within a Nested field for them is\n        more appropriate than using a `Tuple` field.\n\n    :param Iterable[Field] tuple_fields: An iterable of field classes or\n        instances.\n    :param kwargs: The same keyword arguments that :class:`Field` receives.\n\n    .. versionadded:: 3.0.0rc4\n    \"\"\"\n\n    default_error_messages = {\"invalid\": \"Not a valid tuple.\"}\n\n    def __init__(self, tuple_fields, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        if not utils.is_collection(tuple_fields):\n            raise ValueError(\n                \"tuple_fields must be an iterable of Field classes or \" \"instances.\"\n            )\n\n        try:\n            self.tuple_fields = [\n                resolve_field_instance(cls_or_instance)\n                for cls_or_instance in tuple_fields\n            ]\n        except FieldInstanceResolutionError as error:\n            raise ValueError(\n                'Elements of \"tuple_fields\" must be subclasses or '\n                \"instances of marshmallow.base.FieldABC.\"\n            ) from error\n\n        self.validate_length = Length(equal=len(self.tuple_fields))\n\n    def _bind_to_schema(self, field_name, schema):\n        super()._bind_to_schema(field_name, schema)\n        new_tuple_fields = []\n        for field in self.tuple_fields:\n            field = copy.deepcopy(field)\n            field._bind_to_schema(field_name, self)\n            new_tuple_fields.append(field)\n\n        self.tuple_fields = new_tuple_fields\n\n    def _serialize(self, value, attr, obj, **kwargs):\n        if value is None:\n            return None\n\n        return tuple(\n            field._serialize(each, attr, obj, **kwargs)\n            for field, each in zip(self.tuple_fields, value)\n        )\n\n    def _deserialize(self, value, attr, data, **kwargs):\n        if not utils.is_collection(value):\n            raise self.make_error(\"invalid\")\n\n        self.validate_length(value)\n\n        result = []\n        errors = {}\n\n        for idx, (field, each) in enumerate(zip(self.tuple_fields, value)):\n            try:\n                result.append(field.deserialize(each, **kwargs))\n            except ValidationError as error:\n                if error.valid_data is not None:\n                    result.append(error.valid_data)\n                errors.update({idx: error.messages})\n        if errors:\n            raise ValidationError(errors, valid_data=result)\n\n        return tuple(result)",
        "file_path": "src/marshmallow/fields.py",
        "chunk_index": 10,
        "metadata": {
          "class_name": "Tuple",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "Field"
            ]
          },
          "parent_name": null,
          "private_methods_count": 4,
          "public_methods_count": 0,
          "node_count": 546,
          "chunk_size": 2747
        }
      },
      {
        "rank": 7,
        "score": 0.568890392780304,
        "content": "class SchemaOpts:\n    \"\"\"class Meta options for the :class:`Schema`. Defines defaults.\"\"\"\n\n    def __init__(self, meta, ordered=False):\n        self.fields = getattr(meta, \"fields\", ())\n        if not isinstance(self.fields, (list, tuple)):\n            raise ValueError(\"`fields` option must be a list or tuple.\")\n        self.additional = getattr(meta, \"additional\", ())\n        if not isinstance(self.additional, (list, tuple)):\n            raise ValueError(\"`additional` option must be a list or tuple.\")\n        if self.fields and self.additional:\n            raise ValueError(\n                \"Cannot set both `fields` and `additional` options\"\n                \" for the same Schema.\"\n            )\n        self.exclude = getattr(meta, \"exclude\", ())\n        if not isinstance(self.exclude, (list, tuple)):\n            raise ValueError(\"`exclude` must be a list or tuple.\")\n        self.dateformat = getattr(meta, \"dateformat\", None)\n        self.datetimeformat = getattr(meta, \"datetimeformat\", None)\n        if hasattr(meta, \"json_module\"):\n            warnings.warn(\n                \"The json_module class Meta option is deprecated. Use render_module instead.\",\n                DeprecationWarning,\n            )\n            render_module = getattr(meta, \"json_module\", json)\n        else:\n            render_module = json\n        self.render_module = getattr(meta, \"render_module\", render_module)\n        self.ordered = getattr(meta, \"ordered\", ordered)\n        self.index_errors = getattr(meta, \"index_errors\", True)\n        self.include = getattr(meta, \"include\", {})\n        self.load_only = getattr(meta, \"load_only\", ())\n        self.dump_only = getattr(meta, \"dump_only\", ())\n        self.unknown = getattr(meta, \"unknown\", RAISE)\n        self.register = getattr(meta, \"register\", True)",
        "file_path": "src/marshmallow/schema.py",
        "chunk_index": 4,
        "metadata": {
          "class_name": "SchemaOpts",
          "chunk_type": "class",
          "relationship": null,
          "parent_name": null,
          "private_methods_count": 1,
          "public_methods_count": 0,
          "node_count": 484,
          "chunk_size": 1800
        }
      },
      {
        "rank": 8,
        "score": 0.5490224361419678,
        "content": "class BaseSchema(base.SchemaABC):\n    \"\"\"Base schema class with which to define custom schemas.\n\n    Example usage:\n\n    .. code-block:: python\n\n        import datetime as dt\n        from marshmallow import Schema, fields\n\n        class Album:\n            def __init__(self, title, release_date):\n                self.title = title\n                self.release_date = release_date\n\n        class AlbumSchema(Schema):\n            title = fields.Str()\n            release_date = fields.Date()\n\n        # Or, equivalently\n        class AlbumSchema2(Schema):\n            class Meta:\n                fields = (\"title\", \"release_date\")\n\n        album = Album(\"Beggars Banquet\", dt.date(1968, 12, 6))\n        schema = AlbumSchema()\n        data = schema.dump(album)\n        data  # {'release_date': '1968-12-06', 'title': 'Beggars Banquet'}\n\n    :param tuple|list only: Whitelist of the declared fields to select when\n        instantiating the Schema. If None, all fields are used. Nested fields\n        can be represented with dot delimiters.\n    :param tuple|list exclude: Blacklist of the declared fields to exclude\n        when instantiating the Schema. If a field appears in both `only` and\n        `exclude`, it is not used. Nested fields can be represented with dot\n        delimiters.\n    :param bool many: Should be set to `True` if ``obj`` is a collection\n        so that the object will be serialized to a list.\n    :param dict context: Optional context passed to :class:`fields.Method` and\n        :class:`fields.Function` fields.\n    :param tuple|list load_only: Fields to skip during serialization (write-only fields)\n    :param tuple|list dump_only: Fields to skip during deserialization (read-only fields)\n    :param bool|tuple partial: Whether to ignore missing fields and not require\n        any fields declared. Propagates down to ``Nested`` fields as well. If\n        its value is an iterable, only missing fields listed in that iterable\n        will be ignored. Use dot delimiters to specify nested fields.\n    :param unknown: Whether to exclude, include, or raise an error for unknown\n        fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`.\n\n    .. versionchanged:: 3.0.0\n        `prefix` parameter removed.\n\n    .. versionchanged:: 2.0.0\n        `__validators__`, `__preprocessors__`, and `__data_handlers__` are removed in favor of\n        `marshmallow.decorators.validates_schema`,\n        `marshmallow.decorators.pre_load` and `marshmallow.decorators.post_dump`.\n        `__accessor__` and `__error_handler__` are deprecated. Implement the\n        `handle_error` and `get_attribute` methods instead.\n    \"\"\"\n\n    TYPE_MAPPING = {\n        str: ma_fields.String,\n        bytes: ma_fields.String,\n        dt.datetime: ma_fields.DateTime,\n        float: ma_fields.Float,\n        bool: ma_fields.Boolean,\n        tuple: ma_fields.Raw,\n        list: ma_fields.Raw,\n        set: ma_fields.Raw,\n        int: ma_fields.Integer,\n        uuid.UUID: ma_fields.UUID,\n        dt.time: ma_fields.Time,\n        dt.date: ma_fields.Date,\n        dt.timedelta: ma_fields.TimeDelta,\n        decimal.Decimal: ma_fields.Decimal,\n    }\n    #: Overrides for default schema-level error messages\n    error_messages = {}\n\n    _default_error_messages = {\n        \"type\": \"Invalid input type.\",\n        \"unknown\": \"Unknown field.\",\n    }\n\n    OPTIONS_CLASS = SchemaOpts\n\n    class Meta:\n        \"\"\"Options object for a Schema.\n\n        Example usage: ::\n\n            class Meta:\n                fields = (\"id\", \"email\", \"date_created\")\n                exclude = (\"password\", \"secret_attribute\")\n\n        Available options:\n\n        - ``fields``: Tuple or list of fields to include in the serialized result.\n        - ``additional``: Tuple or list of fields to include *in addition* to the\n            explicitly declared fields. ``additional`` and ``fields`` are\n            mutually-exclusive options.\n        - ``include``: Dictionary of additional fields to include in the schema. It is\n            usually better to define fields as class variables, but you may need to\n            use this option, e.g., if your fields are Python keywords. May be an\n            `OrderedDict`.\n        - ``exclude``: Tuple or list of fields to exclude in the serialized result.\n            Nested fields can be represented with dot delimiters.\n        - ``dateformat``: Default format for `Date <fields.Date>` fields.\n        - ``datetimeformat``: Default format for `DateTime <fields.DateTime>` fields.\n        - ``render_module``: Module to use for `loads <Schema.loads>` and `dumps <Schema.dumps>`.\n            Defaults to `json` from the standard library.\n        - ``ordered``: If `True`, order serialization output according to the\n            order in which fields were declared. Output of `Schema.dump` will be a\n            `collections.OrderedDict`.\n        - ``index_errors``: If `True`, errors dictionaries will include the index\n            of invalid items in a collection.\n        - ``load_only``: Tuple or list of fields to exclude from serialized results.\n        - ``dump_only``: Tuple or list of fields to exclude from deserialization\n        - ``unknown``: Whether to exclude, include, or raise an error for unknown\n            fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`.\n        - ``register``: Whether to register the `Schema` with marshmallow's internal\n            class registry. Must be `True` if you intend to refer to this `Schema`\n            by class name in `Nested` fields. Only set this to `False` when memory\n            usage is critical. Defaults to `True`.\n        \"\"\"\n\n        pass\n\n    def __init__(\n        self,\n        *,\n        only=None,\n        exclude=(),\n        many=False,\n        context=None,\n        load_only=(),\n        dump_only=(),\n        partial=False,\n        unknown=None\n    ):\n        # Raise error if only or exclude is passed as string, not list of strings\n        if only is not None and not is_collection(only):\n            raise StringNotCollectionError('\"only\" should be a list of strings')\n        if exclude is not None and not is_collection(exclude):\n            raise StringNotCollectionError('\"exclude\" should be a list of strings')\n        # copy declared fields from metaclass\n        self.declared_fields = copy.deepcopy(self._declared_fields)\n        self.many = many\n        self.only = only\n        self.exclude = set(self.opts.exclude) | set(exclude)\n        self.ordered = self.opts.ordered\n        self.load_only = set(load_only) or set(self.opts.load_only)\n        self.dump_only = set(dump_only) or set(self.opts.dump_only)\n        self.partial = partial\n        self.unknown = unknown or self.opts.unknown\n        self.context = context or {}\n        self._normalize_nested_options()\n        #: Dictionary mapping field_names -> :class:`Field` objects\n        self.fields = self._init_fields()\n        self.dump_fields, self.load_fields = self.dict_class(), self.dict_class()\n        for field_name, field_obj in self.fields.items():\n            if field_obj.load_only:\n                self.load_fields[field_name] = field_obj\n            elif field_obj.dump_only:\n                self.dump_fields[field_name] = field_obj\n            else:\n                self.load_fields[field_name] = field_obj\n                self.dump_fields[field_name] = field_obj\n        messages = {}\n        messages.update(self._default_error_messages)\n        for cls in reversed(self.__class__.__mro__):\n            messages.update(getattr(cls, \"error_messages\", {}))\n        messages.update(self.error_messages or {})\n        self.error_messages = messages\n\n    def __repr__(self):\n        return \"<{ClassName}(many={self.many})>\".format(\n            ClassName=self.__class__.__name__, self=self\n        )\n\n    @property\n    def dict_class(self):\n        return OrderedDict if self.ordered else dict\n\n    @property\n    def set_class(self):\n        return OrderedSet if self.ordered else set\n\n    @classmethod\n    def from_dict(\n        cls, fields: typing.Dict[str, ma_fields.Field], *, name: str = \"GeneratedSchema\"\n    ) -> typing.Type[\"Schema\"]:\n        \"\"\"Generate a `Schema` class given a dictionary of fields.\n\n        .. code-block:: python\n\n            from marshmallow import Schema, fields\n\n            PersonSchema = Schema.from_dict({\"name\": fields.Str()})\n            print(PersonSchema().load({\"name\": \"David\"}))  # => {'name': 'David'}\n\n        Generated schemas are not added to the class registry and therefore cannot\n        be referred to by name in `Nested` fields.\n\n        :param dict fields: Dictionary mapping field names to field instances.\n        :param str name: Optional name for the class, which will appear in\n            the ``repr`` for the class.\n\n        .. versionadded:: 3.0.0\n        \"\"\"\n        attrs = fields.copy()\n        attrs[\"Meta\"] = type(\n            \"GeneratedMeta\", (getattr(cls, \"Meta\", object),), {\"register\": False}\n        )\n        schema_cls = type(name, (cls,), attrs)\n        return schema_cls\n\n    ##### Override-able methods #####\n\n\n\n    ##### Serialization/Deserialization API #####\n\n    @staticmethod\n    def _call_and_store(getter_func, data, *, field_name, error_store, index=None):\n        \"\"\"Call ``getter_func`` with ``data`` as its argument, and store any `ValidationErrors`.\n\n        :param callable getter_func: Function for getting the serialized/deserialized\n            value from ``data``.\n        :param data: The data passed to ``getter_func``.\n        :param str field_name: Field name.\n        :param int index: Index of the item being validated, if validating a collection,\n            otherwise `None`.\n        \"\"\"\n        try:\n            value = getter_func(data)\n        except ValidationError as error:\n            error_store.store_error(error.messages, field_name, index=index)\n            # When a Nested field fails validation, the marshalled data is stored\n            # on the ValidationError's valid_data attribute\n            return error.valid_data or missing\n        return value\n\n    def _serialize(self, obj, *, many=False):\n        \"\"\"Serialize ``obj``.\n\n        :param obj: The object(s) to serialize.\n        :param bool many: `True` if ``data`` should be serialized as a collection.\n        :return: A dictionary of the serialized data\n\n        .. versionchanged:: 1.0.0\n            Renamed from ``marshal``.\n        \"\"\"\n        if many and obj is not None:\n            return [self._serialize(d, many=False) for d in obj]\n        ret = self.dict_class()\n        for attr_name, field_obj in self.dump_fields.items():\n            value = field_obj.serialize(attr_name, obj, accessor=self.get_attribute)\n            if value is missing:\n                continue\n            key = field_obj.data_key or attr_name\n            ret[key] = value\n        return ret\n\n\n\n    def _deserialize(\n        self, data, *, error_store, many=False, partial=False, unknown=RAISE, index=None\n    ):\n        \"\"\"Deserialize ``data``.\n\n        :param dict data: The data to deserialize.\n        :param ErrorStore error_store: Structure to store errors.\n        :param bool many: `True` if ``data`` should be deserialized as a collection.\n        :param bool|tuple partial: Whether to ignore missing fields and not require\n            any fields declared. Propagates down to ``Nested`` fields as well. If\n            its value is an iterable, only missing fields listed in that iterable\n            will be ignored. Use dot delimiters to specify nested fields.\n        :param unknown: Whether to exclude, include, or raise an error for unknown\n            fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`.\n        :param int index: Index of the item being serialized (for storing errors) if\n            serializing a collection, otherwise `None`.\n        :return: A dictionary of the deserialized data.\n        \"\"\"\n        index_errors = self.opts.index_errors\n        index = index if index_errors else None\n        if many:\n            if not is_collection(data):\n                error_store.store_error([self.error_messages[\"type\"]], index=index)\n                ret = []\n            else:\n                ret = [\n                    self._deserialize(\n                        d,\n                        error_store=error_store,\n                        many=False,\n                        partial=partial,\n                        unknown=unknown,\n                        index=idx,\n                    )\n                    for idx, d in enumerate(data)\n                ]\n            return ret\n        ret = self.dict_class()\n        # Check data is a dict\n        if not isinstance(data, Mapping):\n            error_store.store_error([self.error_messages[\"type\"]], index=index)\n        else:\n            partial_is_collection = is_collection(partial)\n            for attr_name, field_obj in self.load_fields.items():\n                field_name = field_obj.data_key or attr_name\n                raw_value = data.get(field_name, missing)\n                if raw_value is missing:\n                    # Ignore missing field if we're allowed to.\n                    if partial is True or (\n                        partial_is_collection and attr_name in partial\n                    ):\n                        continue\n                d_kwargs = {}\n                # Allow partial loading of nested schemas.\n                if partial_is_collection:\n                    prefix = field_name + \".\"\n                    len_prefix = len(prefix)\n                    sub_partial = [\n                        f[len_prefix:] for f in partial if f.startswith(prefix)\n                    ]\n                    d_kwargs[\"partial\"] = sub_partial\n                else:\n                    d_kwargs[\"partial\"] = partial\n                getter = lambda val: field_obj.deserialize(\n                    val, field_name, data, **d_kwargs\n                )\n                value = self._call_and_store(\n                    getter_func=getter,\n                    data=raw_value,\n                    field_name=field_name,\n                    error_store=error_store,\n                    index=index,\n                )\n                if value is not missing:\n                    key = field_obj.attribute or attr_name\n                    set_value(ret, key, value)\n            if unknown != EXCLUDE:\n                fields = {\n                    field_obj.data_key or field_name\n                    for field_name, field_obj in self.load_fields.items()\n                }\n                for key in set(data) - fields:\n                    value = data[key]\n                    if unknown == INCLUDE:\n                        set_value(ret, key, value)\n                    elif unknown == RAISE:\n                        error_store.store_error(\n                            [self.error_messages[\"unknown\"]],\n                            key,\n                            (index if index_errors else None),\n                        )\n        return ret\n\n\n\n    def _run_validator(\n        self,\n        validator_func,\n        output,\n        *,\n        original_data,\n        error_store,\n        many,\n        partial,\n        pass_original,\n        index=None\n    ):\n        try:\n            if pass_original:  # Pass original, raw data (before unmarshalling)\n                validator_func(output, original_data, partial=partial, many=many)\n            else:\n                validator_func(output, partial=partial, many=many)\n        except ValidationError as err:\n            error_store.store_error(err.messages, err.field_name, index=index)\n\n\n    ##### Private Helpers #####\n\n    def _do_load(\n        self, data, *, many=None, partial=None, unknown=None, postprocess=True\n    ):\n        \"\"\"Deserialize `data`, returning the deserialized result.\n\n        :param data: The data to deserialize.\n        :param bool many: Whether to deserialize `data` as a collection. If `None`, the\n            value for `self.many` is used.\n        :param bool|tuple partial: Whether to validate required fields. If its\n            value is an iterable, only fields listed in that iterable will be\n            ignored will be allowed missing. If `True`, all fields will be allowed missing.\n            If `None`, the value for `self.partial` is used.\n        :param unknown: Whether to exclude, include, or raise an error for unknown\n            fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`.\n            If `None`, the value for `self.unknown` is used.\n        :param bool postprocess: Whether to run post_load methods..\n        :return: A dict of deserialized data\n        :rtype: dict\n        \"\"\"\n        error_store = ErrorStore()\n        errors = {}\n        many = self.many if many is None else bool(many)\n        unknown = unknown or self.unknown\n        if partial is None:\n            partial = self.partial\n        # Run preprocessors\n        if self._has_processors(PRE_LOAD):\n            try:\n                processed_data = self._invoke_load_processors(\n                    PRE_LOAD, data, many=many, original_data=data, partial=partial\n                )\n            except ValidationError as err:\n                errors = err.normalized_messages()\n                result = None\n        else:\n            processed_data = data\n        if not errors:\n            # Deserialize data\n            result = self._deserialize(\n                processed_data,\n                error_store=error_store,\n                many=many,\n                partial=partial,\n                unknown=unknown,\n            )\n            # Run field-level validation\n            self._invoke_field_validators(\n                error_store=error_store, data=result, many=many\n            )\n            # Run schema-level validation\n            if self._has_processors(VALIDATES_SCHEMA):\n                field_errors = bool(error_store.errors)\n                self._invoke_schema_validators(\n                    error_store=error_store,\n                    pass_many=True,\n                    data=result,\n                    original_data=data,\n                    many=many,\n                    partial=partial,\n                    field_errors=field_errors,\n                )\n                self._invoke_schema_validators(\n                    error_store=error_store,\n                    pass_many=False,\n                    data=result,\n                    original_data=data,\n                    many=many,\n                    partial=partial,\n                    field_errors=field_errors,\n                )\n            errors = error_store.errors\n            # Run post processors\n            if not errors and postprocess and self._has_processors(POST_LOAD):\n                try:\n                    result = self._invoke_load_processors(\n                        POST_LOAD,\n                        result,\n                        many=many,\n                        original_data=data,\n                        partial=partial,\n                    )\n                except ValidationError as err:\n                    errors = err.normalized_messages()\n        if errors:\n            exc = ValidationError(errors, data=data, valid_data=result)\n            self.handle_error(exc, data, many=many, partial=partial)\n            raise exc\n\n        return result\n\n    def _normalize_nested_options(self):\n        \"\"\"Apply then flatten nested schema options\"\"\"\n        if self.only is not None:\n            # Apply the only option to nested fields.\n            self.__apply_nested_option(\"only\", self.only, \"intersection\")\n            # Remove the child field names from the only option.\n            self.only = self.set_class([field.split(\".\", 1)[0] for field in self.only])\n        if self.exclude:\n            # Apply the exclude option to nested fields.\n            self.__apply_nested_option(\"exclude\", self.exclude, \"union\")\n            # Remove the parent field names from the exclude option.\n            self.exclude = self.set_class(\n                [field for field in self.exclude if \".\" not in field]\n            )\n\n    def __apply_nested_option(self, option_name, field_names, set_operation):\n        \"\"\"Apply nested options to nested fields\"\"\"\n        # Split nested field names on the first dot.\n        nested_fields = [name.split(\".\", 1) for name in field_names if \".\" in name]\n        # Partition the nested field names by parent field.\n        nested_options = defaultdict(list)\n        for parent, nested_names in nested_fields:\n            nested_options[parent].append(nested_names)\n        # Apply the nested field options.\n        for key, options in iter(nested_options.items()):\n            new_options = self.set_class(options)\n            original_options = getattr(self.declared_fields[key], option_name, ())\n            if original_options:\n                if set_operation == \"union\":\n                    new_options |= self.set_class(original_options)\n                if set_operation == \"intersection\":\n                    new_options &= self.set_class(original_options)\n            setattr(self.declared_fields[key], option_name, new_options)\n\n    def _init_fields(self):\n        \"\"\"Update fields based on schema options.\"\"\"\n        if self.opts.fields:\n            available_field_names = self.set_class(self.opts.fields)\n        else:\n            available_field_names = self.set_class(self.declared_fields.keys())\n            if self.opts.additional:\n                available_field_names |= self.set_class(self.opts.additional)\n\n        invalid_fields = self.set_class()\n\n        if self.only is not None:\n            # Return only fields specified in only option\n            field_names = self.set_class(self.only)\n\n            invalid_fields |= field_names - available_field_names\n        else:\n            field_names = available_field_names\n\n        # If \"exclude\" option or param is specified, remove those fields.\n        if self.exclude:\n            # Note that this isn't available_field_names, since we want to\n            # apply \"only\" for the actual calculation.\n            field_names = field_names - self.exclude\n            invalid_fields |= self.exclude - available_field_names\n\n        if invalid_fields:\n            message = \"Invalid fields for {}: {}.\".format(self, invalid_fields)\n            raise ValueError(message)\n\n        fields_dict = self.dict_class()\n        for field_name in field_names:\n            field_obj = self.declared_fields.get(field_name, ma_fields.Inferred())\n            self._bind_field(field_name, field_obj)\n            fields_dict[field_name] = field_obj\n\n        dump_data_keys = [\n            obj.data_key or name\n            for name, obj in fields_dict.items()\n            if not obj.load_only\n        ]\n        if len(dump_data_keys) != len(set(dump_data_keys)):\n            data_keys_duplicates = {\n                x for x in dump_data_keys if dump_data_keys.count(x) > 1\n            }\n            raise ValueError(\n                \"The data_key argument for one or more fields collides \"\n                \"with another field's name or data_key argument. \"\n                \"Check the following field names and \"\n                \"data_key arguments: {}\".format(list(data_keys_duplicates))\n            )\n\n        load_attributes = [\n            obj.attribute or name\n            for name, obj in fields_dict.items()\n            if not obj.dump_only\n        ]\n        if len(load_attributes) != len(set(load_attributes)):\n            attributes_duplicates = {\n                x for x in load_attributes if load_attributes.count(x) > 1\n            }\n            raise ValueError(\n                \"The attribute argument for one or more fields collides \"\n                \"with another field's name or attribute argument. \"\n                \"Check the following field names and \"\n                \"attribute arguments: {}\".format(list(attributes_duplicates))\n            )\n\n        return fields_dict\n\n\n    def _bind_field(self, field_name, field_obj):\n        \"\"\"Bind field to the schema, setting any necessary attributes on the\n        field (e.g. parent and name).\n\n        Also set field load_only and dump_only values if field_name was\n        specified in ``class Meta``.\n        \"\"\"\n        try:\n            if field_name in self.load_only:\n                field_obj.load_only = True\n            if field_name in self.dump_only:\n                field_obj.dump_only = True\n            field_obj._bind_to_schema(field_name, self)\n            self.on_bind_field(field_name, field_obj)\n        except TypeError as error:\n            # field declared as a class, not an instance\n            if isinstance(field_obj, type) and issubclass(field_obj, base.FieldABC):\n                msg = (\n                    'Field for \"{}\" must be declared as a '\n                    \"Field instance, not a class. \"\n                    'Did you mean \"fields.{}()\"?'.format(field_name, field_obj.__name__)\n                )\n                raise TypeError(msg) from error\n\n    @lru_cache(maxsize=8)\n    def _has_processors(self, tag):\n        return self._hooks[(tag, True)] or self._hooks[(tag, False)]\n\n    def _invoke_dump_processors(self, tag, data, *, many, original_data=None):\n        # The pass_many post-dump processors may do things like add an envelope, so\n        # invoke those after invoking the non-pass_many processors which will expect\n        # to get a list of items.\n        data = self._invoke_processors(\n            tag, pass_many=False, data=data, many=many, original_data=original_data\n        )\n        data = self._invoke_processors(\n            tag, pass_many=True, data=data, many=many, original_data=original_data\n        )\n        return data\n\n    def _invoke_load_processors(self, tag, data, *, many, original_data, partial):\n        # This has to invert the order of the dump processors, so run the pass_many\n        # processors first.\n        data = self._invoke_processors(\n            tag,\n            pass_many=True,\n            data=data,\n            many=many,\n            original_data=original_data,\n            partial=partial,\n        )\n        data = self._invoke_processors(\n            tag,\n            pass_many=False,\n            data=data,\n            many=many,\n            original_data=original_data,\n            partial=partial,\n        )\n        return data\n\n    def _invoke_field_validators(self, *, error_store, data, many):\n        for attr_name in self._hooks[VALIDATES]:\n            validator = getattr(self, attr_name)\n            validator_kwargs = validator.__marshmallow_hook__[VALIDATES]\n            field_name = validator_kwargs[\"field_name\"]\n\n            try:\n                field_obj = self.fields[field_name]\n            except KeyError as error:\n                if field_name in self.declared_fields:\n                    continue\n                raise ValueError(\n                    '\"{}\" field does not exist.'.format(field_name)\n                ) from error\n\n            if many:\n                for idx, item in enumerate(data):\n                    try:\n                        value = item[field_obj.attribute or field_name]\n                    except KeyError:\n                        pass\n                    else:\n                        validated_value = self._call_and_store(\n                            getter_func=validator,\n                            data=value,\n                            field_name=field_obj.data_key or field_name,\n                            error_store=error_store,\n                            index=(idx if self.opts.index_errors else None),\n                        )\n                        if validated_value is missing:\n                            data[idx].pop(field_name, None)\n            else:\n                try:\n                    value = data[field_obj.attribute or field_name]\n                except KeyError:\n                    pass\n                else:\n                    validated_value = self._call_and_store(\n                        getter_func=validator,\n                        data=value,\n                        field_name=field_obj.data_key or field_name,\n                        error_store=error_store,\n                    )\n                    if validated_value is missing:\n                        data.pop(field_name, None)\n\n    def _invoke_schema_validators(\n        self,\n        *,\n        error_store,\n        pass_many,\n        data,\n        original_data,\n        many,\n        partial,\n        field_errors=False\n    ):\n        for attr_name in self._hooks[(VALIDATES_SCHEMA, pass_many)]:\n            validator = getattr(self, attr_name)\n            validator_kwargs = validator.__marshmallow_hook__[\n                (VALIDATES_SCHEMA, pass_many)\n            ]\n            if field_errors and validator_kwargs[\"skip_on_field_errors\"]:\n                continue\n            pass_original = validator_kwargs.get(\"pass_original\", False)\n\n            if many and not pass_many:\n                for idx, (item, orig) in enumerate(zip(data, original_data)):\n                    self._run_validator(\n                        validator,\n                        item,\n                        original_data=orig,\n                        error_store=error_store,\n                        many=many,\n                        partial=partial,\n                        index=idx,\n                        pass_original=pass_original,\n                    )\n            else:\n                self._run_validator(\n                    validator,\n                    data,\n                    original_data=original_data,\n                    error_store=error_store,\n                    many=many,\n                    pass_original=pass_original,\n                    partial=partial,\n                )\n\n    def _invoke_processors(\n        self, tag, *, pass_many, data, many, original_data=None, **kwargs\n    ):\n        key = (tag, pass_many)\n        for attr_name in self._hooks[key]:\n            # This will be a bound method.\n            processor = getattr(self, attr_name)\n\n            processor_kwargs = processor.__marshmallow_hook__[key]\n            pass_original = processor_kwargs.get(\"pass_original\", False)\n\n            if pass_many:\n                if pass_original:\n                    data = processor(data, original_data, many=many, **kwargs)\n                else:\n                    data = processor(data, many=many, **kwargs)\n            elif many:\n                if pass_original:\n                    data = [\n                        processor(item, original, many=many, **kwargs)\n                        for item, original in zip(data, original_data)\n                    ]\n                else:\n                    data = [processor(item, many=many, **kwargs) for item in data]\n            else:\n                if pass_original:\n                    data = processor(data, original_data, many=many, **kwargs)\n                else:\n                    data = processor(data, many=many, **kwargs)\n        return data",
        "file_path": "src/marshmallow/schema.py",
        "chunk_index": 5,
        "metadata": {
          "class_name": "BaseSchema",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "base.SchemaABC"
            ]
          },
          "parent_name": null,
          "private_methods_count": 15,
          "public_methods_count": 8,
          "node_count": 5230,
          "chunk_size": 30931
        }
      },
      {
        "rank": 9,
        "score": 0.5430379509925842,
        "content": "class SchemaMeta(type):\n    \"\"\"Metaclass for the Schema class. Binds the declared fields to\n    a ``_declared_fields`` attribute, which is a dictionary mapping attribute\n    names to field objects. Also sets the ``opts`` class attribute, which is\n    the Schema class's ``class Meta`` options.\n    \"\"\"\n\n    def __new__(mcs, name, bases, attrs):\n        meta = attrs.get(\"Meta\")\n        ordered = getattr(meta, \"ordered\", False)\n        if not ordered:\n            # Inherit 'ordered' option\n            # Warning: We loop through bases instead of MRO because we don't\n            # yet have access to the class object\n            # (i.e. can't call super before we have fields)\n            for base_ in bases:\n                if hasattr(base_, \"Meta\") and hasattr(base_.Meta, \"ordered\"):\n                    ordered = base_.Meta.ordered\n                    break\n            else:\n                ordered = False\n        cls_fields = _get_fields(attrs, base.FieldABC, pop=True, ordered=ordered)\n        klass = super().__new__(mcs, name, bases, attrs)\n        inherited_fields = _get_fields_by_mro(klass, base.FieldABC, ordered=ordered)\n\n        meta = klass.Meta\n        # Set klass.opts in __new__ rather than __init__ so that it is accessible in\n        # get_declared_fields\n        klass.opts = klass.OPTIONS_CLASS(meta, ordered=ordered)\n        # Add fields specified in the `include` class Meta option\n        cls_fields += list(klass.opts.include.items())\n\n        dict_cls = OrderedDict if ordered else dict\n        # Assign _declared_fields on class\n        klass._declared_fields = mcs.get_declared_fields(\n            klass=klass,\n            cls_fields=cls_fields,\n            inherited_fields=inherited_fields,\n            dict_cls=dict_cls,\n        )\n        return klass\n\n    @classmethod\n    def get_declared_fields(mcs, klass, cls_fields, inherited_fields, dict_cls):\n        \"\"\"Returns a dictionary of field_name => `Field` pairs declard on the class.\n        This is exposed mainly so that plugins can add additional fields, e.g. fields\n        computed from class Meta options.\n\n        :param type klass: The class object.\n        :param list cls_fields: The fields declared on the class, including those added\n            by the ``include`` class Meta option.\n        :param list inherited_fields: Inherited fields.\n        :param type dict_class: Either `dict` or `OrderedDict`, depending on the whether\n            the user specified `ordered=True`.\n        \"\"\"\n        return dict_cls(inherited_fields + cls_fields)\n\n    def __init__(cls, name, bases, attrs):\n        super().__init__(cls, bases, attrs)\n        if name and cls.opts.register:\n            class_registry.register(name, cls)\n        cls._hooks = cls.resolve_hooks()\n",
        "file_path": "src/marshmallow/schema.py",
        "chunk_index": 2,
        "metadata": {
          "class_name": "SchemaMeta",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "type"
            ]
          },
          "parent_name": null,
          "private_methods_count": 2,
          "public_methods_count": 1,
          "node_count": 569,
          "chunk_size": 2757
        }
      },
      {
        "rank": 10,
        "score": 0.5334171056747437,
        "content": "class Time(Field):\n    \"\"\"ISO8601-formatted time string.\n\n    :param kwargs: The same keyword arguments that :class:`Field` receives.\n    \"\"\"\n\n    default_error_messages = {\n        \"invalid\": \"Not a valid time.\",\n        \"format\": '\"{input}\" cannot be formatted as a time.',\n    }\n\n    def _serialize(self, value, attr, obj, **kwargs):\n        if value is None:\n            return None\n        ret = value.isoformat()\n        if value.microsecond:\n            return ret[:15]\n        return ret\n\n    def _deserialize(self, value, attr, data, **kwargs):\n        \"\"\"Deserialize an ISO8601-formatted time to a :class:`datetime.time` object.\"\"\"\n        if not value:  # falsy values are invalid\n            raise self.make_error(\"invalid\")\n        try:\n            return utils.from_iso_time(value)\n        except (AttributeError, TypeError, ValueError) as error:\n            raise self.make_error(\"invalid\") from error",
        "file_path": "src/marshmallow/fields.py",
        "chunk_index": 21,
        "metadata": {
          "class_name": "Time",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "Field"
            ]
          },
          "parent_name": null,
          "private_methods_count": 2,
          "public_methods_count": 0,
          "node_count": 198,
          "chunk_size": 916
        }
      }
    ]
  },
  "marshmallow-code__marshmallow-1343": {
    "query": "[version 2.20.0] TypeError: 'NoneType' object is not subscriptable\nAfter update from version 2.19.5 to 2.20.0 I got error for code like:\r\n\r\n```python\r\nfrom marshmallow import Schema, fields, validates\r\n\r\n\r\nclass Bar(Schema):\r\n    value = fields.String()\r\n\r\n    @validates('value')  # <- issue here\r\n    def validate_value(self, value):\r\n        pass\r\n\r\n\r\nclass Foo(Schema):\r\n    bar = fields.Nested(Bar)\r\n\r\n\r\nsch = Foo()\r\n\r\nsch.validate({\r\n    'bar': 'invalid',\r\n})\r\n```\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/_/bug_mschema.py\", line 19, in <module>\r\n    'bar': 'invalid',\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/schema.py\", line 628, in validate\r\n    _, errors = self._do_load(data, many, partial=partial, postprocess=False)\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/schema.py\", line 670, in _do_load\r\n    index_errors=self.opts.index_errors,\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/marshalling.py\", line 292, in deserialize\r\n    index=(index if index_errors else None)\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/marshalling.py\", line 65, in call_and_store\r\n    value = getter_func(data)\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/marshalling.py\", line 285, in <lambda>\r\n    data\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/fields.py\", line 265, in deserialize\r\n    output = self._deserialize(value, attr, data)\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/fields.py\", line 465, in _deserialize\r\n    data, errors = self.schema.load(value)\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/schema.py\", line 588, in load\r\n    result, errors = self._do_load(data, many, partial=partial, postprocess=True)\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/schema.py\", line 674, in _do_load\r\n    self._invoke_field_validators(unmarshal, data=result, many=many)\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/schema.py\", line 894, in _invoke_field_validators\r\n    value = data[field_obj.attribute or field_name]\r\nTypeError: 'NoneType' object is not subscriptable\r\n```\n",
    "method": "hierarchical",
    "top_k": 10,
    "results": [
      {
        "rank": 1,
        "score": 0.5999484062194824,
        "content": "class Nested(Field):\n    \"\"\"Allows you to nest a :class:`Schema <marshmallow.Schema>`\n    inside a field.\n\n    Examples: ::\n\n        user = fields.Nested(UserSchema)\n        user2 = fields.Nested('UserSchema')  # Equivalent to above\n        collaborators = fields.Nested(UserSchema, many=True, only='id')\n        parent = fields.Nested('self')\n\n    When passing a `Schema <marshmallow.Schema>` instance as the first argument,\n    the instance's ``exclude``, ``only``, and ``many`` attributes will be respected.\n\n    Therefore, when passing the ``exclude``, ``only``, or ``many`` arguments to `fields.Nested`,\n    you should pass a `Schema <marshmallow.Schema>` class (not an instance) as the first argument.\n\n    ::\n\n        # Yes\n        author = fields.Nested(UserSchema, only=('id', 'name'))\n\n        # No\n        author = fields.Nested(UserSchema(), only=('id', 'name'))\n\n    :param Schema nested: The Schema class or class name (string)\n        to nest, or ``\"self\"`` to nest the :class:`Schema` within itself.\n    :param tuple exclude: A list or tuple of fields to exclude.\n    :param required: Raise an :exc:`ValidationError` during deserialization\n        if the field, *and* any required field values specified\n        in the `nested` schema, are not found in the data. If not a `bool`\n        (e.g. a `str`), the provided value will be used as the message of the\n        :exc:`ValidationError` instead of the default message.\n    :param only: A tuple or string of the field(s) to marshal. If `None`, all fields\n        will be marshalled. If a field name (string) is given, only a single\n        value will be returned as output instead of a dictionary.\n        This parameter takes precedence over ``exclude``.\n    :param bool many: Whether the field is a collection of objects.\n    :param kwargs: The same keyword arguments that :class:`Field` receives.\n    \"\"\"\n    def __init__(self, nested, default=missing_, exclude=tuple(), only=None, **kwargs):\n        self.nested = nested\n        self.only = only\n        self.exclude = exclude\n        self.many = kwargs.get('many', False)\n        self.__schema = None  # Cached Schema instance\n        self.__updated_fields = False\n        super(Nested, self).__init__(default=default, **kwargs)\n\n    @property\n    def schema(self):\n        \"\"\"The nested Schema object.\n\n        .. versionchanged:: 1.0.0\n            Renamed from `serializer` to `schema`\n        \"\"\"\n        if not self.__schema:\n            # Ensure that only parameter is a tuple\n            if isinstance(self.only, basestring):\n                only = (self.only,)\n            else:\n                only = self.only\n\n            # Inherit context from parent.\n            context = getattr(self.parent, 'context', {})\n            if isinstance(self.nested, SchemaABC):\n                self.__schema = self.nested\n                self.__schema.context.update(context)\n            elif isinstance(self.nested, type) and \\\n                    issubclass(self.nested, SchemaABC):\n                self.__schema = self.nested(many=self.many,\n                        only=only, exclude=self.exclude, context=context,\n                        load_only=self._nested_normalized_option('load_only'),\n                        dump_only=self._nested_normalized_option('dump_only'))\n            elif isinstance(self.nested, basestring):\n                if self.nested == _RECURSIVE_NESTED:\n                    parent_class = self.parent.__class__\n                    self.__schema = parent_class(many=self.many, only=only,\n                            exclude=self.exclude, context=context,\n                            load_only=self._nested_normalized_option('load_only'),\n                            dump_only=self._nested_normalized_option('dump_only'))\n                else:\n                    schema_class = class_registry.get_class(self.nested)\n                    self.__schema = schema_class(many=self.many,\n                            only=only, exclude=self.exclude, context=context,\n                            load_only=self._nested_normalized_option('load_only'),\n                            dump_only=self._nested_normalized_option('dump_only'))\n            else:\n                raise ValueError('Nested fields must be passed a '\n                                 'Schema, not {0}.'.format(self.nested.__class__))\n            self.__schema.ordered = getattr(self.parent, 'ordered', False)\n        return self.__schema\n\n    def _nested_normalized_option(self, option_name):\n        nested_field = '%s.' % self.name\n        return [field.split(nested_field, 1)[1]\n                for field in getattr(self.root, option_name, set())\n                if field.startswith(nested_field)]\n\n    def _serialize(self, nested_obj, attr, obj):\n        # Load up the schema first. This allows a RegistryError to be raised\n        # if an invalid schema name was passed\n        schema = self.schema\n        if nested_obj is None:\n            return None\n        if self.many and utils.is_iterable_but_not_string(nested_obj):\n            nested_obj = list(nested_obj)\n        if not self.__updated_fields:\n            schema._update_fields(obj=nested_obj, many=self.many)\n            self.__updated_fields = True\n        ret, errors = schema.dump(nested_obj, many=self.many,\n                update_fields=not self.__updated_fields)\n        if isinstance(self.only, basestring):  # self.only is a field name\n            only_field = self.schema.fields[self.only]\n            key = ''.join([self.schema.prefix or '', only_field.dump_to or self.only])\n            if self.many:\n                return utils.pluck(ret, key=key)\n            else:\n                return ret[key]\n        if errors:\n            raise ValidationError(errors, data=ret)\n        return ret\n\n    def _deserialize(self, value, attr, data):\n        if self.many and not utils.is_collection(value):\n            self.fail('type', input=value, type=value.__class__.__name__)\n\n        data, errors = self.schema.load(value)\n        if errors:\n            raise ValidationError(errors, data=data)\n        return data\n\n    def _validate_missing(self, value):\n        \"\"\"Validate missing values. Raise a :exc:`ValidationError` if\n        `value` should be considered missing.\n        \"\"\"\n        if value is missing_ and hasattr(self, 'required'):\n            if self.nested == _RECURSIVE_NESTED:\n                self.fail('required')\n            errors = self._check_required()\n            if errors:\n                raise ValidationError(errors)\n        else:\n            super(Nested, self)._validate_missing(value)\n\n    def _check_required(self):\n        errors = {}\n        if self.required:\n            for field_name, field in self.schema.fields.items():\n                if not field.required:\n                    continue\n                error_field_name = field.load_from or field_name\n                if (\n                    isinstance(field, Nested) and\n                    self.nested != _RECURSIVE_NESTED and\n                    field.nested != _RECURSIVE_NESTED\n                ):\n                    errors[error_field_name] = field._check_required()\n                else:\n                    try:\n                        field._validate_missing(field.missing)\n                    except ValidationError as ve:\n                        errors[error_field_name] = ve.messages\n            if self.many and errors:\n                errors = {0: errors}\n            # No inner errors; just raise required error like normal\n            if not errors:\n                self.fail('required')\n        return errors",
        "file_path": "src/marshmallow/fields.py",
        "chunk_index": 6,
        "metadata": {
          "class_name": "Nested",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "Field"
            ]
          },
          "parent_name": null,
          "private_methods_count": 6,
          "public_methods_count": 0,
          "node_count": 1393,
          "chunk_size": 7590
        }
      },
      {
        "rank": 2,
        "score": 0.5916412472724915,
        "content": "class MarshmallowError(Exception):\n    \"\"\"Base class for all marshmallow-related errors.\"\"\"\n    pass",
        "file_path": "src/marshmallow/exceptions.py",
        "chunk_index": 0,
        "metadata": {
          "class_name": "MarshmallowError",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "Exception"
            ]
          },
          "parent_name": null,
          "private_methods_count": 0,
          "public_methods_count": 0,
          "node_count": 16,
          "chunk_size": 100
        }
      },
      {
        "rank": 3,
        "score": 0.5833289623260498,
        "content": "class BaseSchema(base.SchemaABC):\n    \"\"\"Base schema class with which to define custom schemas.\n\n    Example usage:\n\n    .. code-block:: python\n\n        import datetime as dt\n        from marshmallow import Schema, fields\n\n        class Album(object):\n            def __init__(self, title, release_date):\n                self.title = title\n                self.release_date = release_date\n\n        class AlbumSchema(Schema):\n            title = fields.Str()\n            release_date = fields.Date()\n\n        # Or, equivalently\n        class AlbumSchema2(Schema):\n            class Meta:\n                fields = (\"title\", \"release_date\")\n\n        album = Album(\"Beggars Banquet\", dt.date(1968, 12, 6))\n        schema = AlbumSchema()\n        data, errors = schema.dump(album)\n        data  # {'release_date': '1968-12-06', 'title': 'Beggars Banquet'}\n\n    :param dict extra: A dict of extra attributes to bind to the serialized result.\n    :param tuple|list only: Whitelist of fields to select when instantiating the Schema.\n        If None, all fields are used.\n        Nested fields can be represented with dot delimiters.\n    :param tuple|list exclude: Blacklist of fields to exclude when instantiating the Schema.\n        If a field appears in both `only` and `exclude`, it is not used.\n        Nested fields can be represented with dot delimiters.\n    :param str prefix: Optional prefix that will be prepended to all the\n        serialized field names.\n    :param bool strict: If `True`, raise errors if invalid data are passed in\n        instead of failing silently and storing the errors.\n    :param bool many: Should be set to `True` if ``obj`` is a collection\n        so that the object will be serialized to a list.\n    :param dict context: Optional context passed to :class:`fields.Method` and\n        :class:`fields.Function` fields.\n    :param tuple|list load_only: Fields to skip during serialization (write-only fields)\n    :param tuple|list dump_only: Fields to skip during deserialization (read-only fields)\n    :param bool|tuple partial: Whether to ignore missing fields. If its value\n        is an iterable, only missing fields listed in that iterable will be\n        ignored.\n\n    .. versionchanged:: 2.0.0\n        `__validators__`, `__preprocessors__`, and `__data_handlers__` are removed in favor of\n        `marshmallow.decorators.validates_schema`,\n        `marshmallow.decorators.pre_load` and `marshmallow.decorators.post_dump`.\n        `__accessor__` and `__error_handler__` are deprecated. Implement the\n        `handle_error` and `get_attribute` methods instead.\n        \"\"\"\n    TYPE_MAPPING = {\n        text_type: fields.String,\n        binary_type: fields.String,\n        dt.datetime: fields.DateTime,\n        float: fields.Float,\n        bool: fields.Boolean,\n        tuple: fields.Raw,\n        list: fields.Raw,\n        set: fields.Raw,\n        int: fields.Integer,\n        uuid.UUID: fields.UUID,\n        dt.time: fields.Time,\n        dt.date: fields.Date,\n        dt.timedelta: fields.TimeDelta,\n        decimal.Decimal: fields.Decimal,\n    }\n\n    OPTIONS_CLASS = SchemaOpts\n\n    #: DEPRECATED: Custom error handler function. May be `None`.\n    __error_handler__ = None\n    #: DEPRECATED: Function used to get values of an object.\n    __accessor__ = None\n\n    class Meta(object):\n        \"\"\"Options object for a Schema.\n\n        Example usage: ::\n\n            class Meta:\n                fields = (\"id\", \"email\", \"date_created\")\n                exclude = (\"password\", \"secret_attribute\")\n\n        Available options:\n\n        - ``fields``: Tuple or list of fields to include in the serialized result.\n        - ``additional``: Tuple or list of fields to include *in addition* to the\n            explicitly declared fields. ``additional`` and ``fields`` are\n            mutually-exclusive options.\n        - ``include``: Dictionary of additional fields to include in the schema. It is\n            usually better to define fields as class variables, but you may need to\n            use this option, e.g., if your fields are Python keywords. May be an\n            `OrderedDict`.\n        - ``exclude``: Tuple or list of fields to exclude in the serialized result.\n            Nested fields can be represented with dot delimiters.\n        - ``dateformat``: Date format for all DateTime fields that do not have their\n            date format explicitly specified.\n        - ``strict``: If `True`, raise errors during marshalling rather than\n            storing them.\n        - ``json_module``: JSON module to use for `loads` and `dumps`.\n            Defaults to the ``json`` module in the stdlib.\n        - ``ordered``: If `True`, order serialization output according to the\n            order in which fields were declared. Output of `Schema.dump` will be a\n            `collections.OrderedDict`.\n        - ``index_errors``: If `True`, errors dictionaries will include the index\n            of invalid items in a collection.\n        - ``load_only``: Tuple or list of fields to exclude from serialized results.\n        - ``dump_only``: Tuple or list of fields to exclude from deserialization\n        \"\"\"\n        pass\n\n    def __init__(self, extra=None, only=None, exclude=(), prefix='', strict=None,\n                 many=False, context=None, load_only=(), dump_only=(),\n                 partial=False):\n        # copy declared fields from metaclass\n        self.declared_fields = copy.deepcopy(self._declared_fields)\n        self.many = many\n        self.only = only\n        self.exclude = set(self.opts.exclude) | set(exclude)\n        if prefix:\n            warnings.warn(\n                'The `prefix` argument is deprecated. Use a post_dump '\n                'method to insert a prefix instead.',\n                RemovedInMarshmallow3Warning\n            )\n        self.prefix = prefix\n        self.strict = strict if strict is not None else self.opts.strict\n        self.ordered = self.opts.ordered\n        self.load_only = set(load_only) or set(self.opts.load_only)\n        self.dump_only = set(dump_only) or set(self.opts.dump_only)\n        self.partial = partial\n        #: Dictionary mapping field_names -> :class:`Field` objects\n        self.fields = self.dict_class()\n        if extra:\n            warnings.warn(\n                'The `extra` argument is deprecated. Use a post_dump '\n                'method to add additional data instead.',\n                RemovedInMarshmallow3Warning\n            )\n        self.extra = extra\n        self.context = context or {}\n        self._normalize_nested_options()\n        self._types_seen = set()\n        self._update_fields(many=many)\n\n    def __repr__(self):\n        return '<{ClassName}(many={self.many}, strict={self.strict})>'.format(\n            ClassName=self.__class__.__name__, self=self\n        )\n\n    def _postprocess(self, data, many, obj):\n        if self.extra:\n            if many:\n                for each in data:\n                    each.update(self.extra)\n            else:\n                data.update(self.extra)\n        return data\n\n    @property\n    def dict_class(self):\n        return OrderedDict if self.ordered else dict\n\n    @property\n    def set_class(self):\n        return OrderedSet if self.ordered else set\n\n    ##### Override-able methods #####\n\n\n\n    ##### Handler decorators (deprecated) #####\n\n    @classmethod\n    def error_handler(cls, func):\n        \"\"\"Decorator that registers an error handler function for the schema.\n        The function receives the :class:`Schema` instance, a dictionary of errors,\n        and the serialized object (if serializing data) or data dictionary (if\n        deserializing data) as arguments.\n\n        Example: ::\n\n            class UserSchema(Schema):\n                email = fields.Email()\n\n            @UserSchema.error_handler\n            def handle_errors(schema, errors, obj):\n                raise ValueError('An error occurred while marshalling {}'.format(obj))\n\n            user = User(email='invalid')\n            UserSchema().dump(user)  # => raises ValueError\n            UserSchema().load({'email': 'bademail'})  # raises ValueError\n\n        .. versionadded:: 0.7.0\n        .. deprecated:: 2.0.0\n            Set the ``error_handler`` class Meta option instead.\n        \"\"\"\n        warnings.warn(\n            'Schema.error_handler is deprecated. Set the error_handler class Meta option '\n            'instead.', category=DeprecationWarning\n        )\n        cls.__error_handler__ = func\n        return func\n\n    @classmethod\n    def accessor(cls, func):\n        \"\"\"Decorator that registers a function for pulling values from an object\n        to serialize. The function receives the :class:`Schema` instance, the\n        ``key`` of the value to get, the ``obj`` to serialize, and an optional\n        ``default`` value.\n\n        .. deprecated:: 2.0.0\n            Set the ``error_handler`` class Meta option instead.\n        \"\"\"\n        warnings.warn(\n            'Schema.accessor is deprecated. Set the accessor class Meta option '\n            'instead.', category=DeprecationWarning\n        )\n        cls.__accessor__ = func\n        return func\n\n    ##### Serialization/Deserialization API #####\n\n\n\n\n\n\n    ##### Private Helpers #####\n\n    def _do_load(self, data, many=None, partial=None, postprocess=True):\n        \"\"\"Deserialize `data`, returning the deserialized result and a dictonary of\n        validation errors.\n\n        :param data: The data to deserialize.\n        :param bool many: Whether to deserialize `data` as a collection. If `None`, the\n            value for `self.many` is used.\n        :param bool|tuple partial: Whether to validate required fields. If its value is an iterable,\n            only fields listed in that iterable will be ignored will be allowed missing.\n            If `True`, all fields will be allowed missing.\n            If `None`, the value for `self.partial` is used.\n        :param bool postprocess: Whether to run post_load methods..\n        :return: A tuple of the form (`data`, `errors`)\n        \"\"\"\n        # Callable unmarshalling object\n        unmarshal = marshalling.Unmarshaller()\n        errors = {}\n        many = self.many if many is None else bool(many)\n        if partial is None:\n            partial = self.partial\n        try:\n            processed_data = self._invoke_load_processors(\n                PRE_LOAD,\n                data,\n                many,\n                original_data=data)\n        except ValidationError as err:\n            errors = err.normalized_messages()\n            result = None\n        if not errors:\n            try:\n                result = unmarshal(\n                    processed_data,\n                    self.fields,\n                    many=many,\n                    partial=partial,\n                    dict_class=self.dict_class,\n                    index_errors=self.opts.index_errors,\n                )\n            except ValidationError as error:\n                result = error.data\n            self._invoke_field_validators(unmarshal, data=result, many=many)\n            errors = unmarshal.errors\n            field_errors = bool(errors)\n            # Run schema-level migration\n            try:\n                self._invoke_validators(unmarshal, pass_many=True, data=result, original_data=data,\n                                        many=many, field_errors=field_errors)\n            except ValidationError as err:\n                errors.update(err.messages)\n            try:\n                self._invoke_validators(unmarshal, pass_many=False, data=result, original_data=data,\n                                        many=many, field_errors=field_errors)\n            except ValidationError as err:\n                errors.update(err.messages)\n        # Run post processors\n        if not errors and postprocess:\n            try:\n                result = self._invoke_load_processors(\n                    POST_LOAD,\n                    result,\n                    many,\n                    original_data=data)\n            except ValidationError as err:\n                errors = err.normalized_messages()\n        if errors:\n            # TODO: Remove self.__error_handler__ in a later release\n            if self.__error_handler__ and callable(self.__error_handler__):\n                self.__error_handler__(errors, data)\n            exc = ValidationError(\n                errors,\n                field_names=unmarshal.error_field_names,\n                fields=unmarshal.error_fields,\n                data=data,\n                **unmarshal.error_kwargs\n            )\n            self.handle_error(exc, data)\n            if self.strict:\n                raise exc\n\n        return result, errors\n\n    def _normalize_nested_options(self):\n        \"\"\"Apply then flatten nested schema options\"\"\"\n        if self.only is not None:\n            # Apply the only option to nested fields.\n            self.__apply_nested_option('only', self.only, 'intersection')\n            # Remove the child field names from the only option.\n            self.only = self.set_class(\n                [field.split('.', 1)[0] for field in self.only],\n            )\n        if self.exclude:\n            # Apply the exclude option to nested fields.\n            self.__apply_nested_option('exclude', self.exclude, 'union')\n            # Remove the parent field names from the exclude option.\n            self.exclude = self.set_class(\n                [field for field in self.exclude if '.' not in field],\n            )\n\n    def __apply_nested_option(self, option_name, field_names, set_operation):\n        \"\"\"Apply nested options to nested fields\"\"\"\n        # Split nested field names on the first dot.\n        nested_fields = [name.split('.', 1) for name in field_names if '.' in name]\n        # Partition the nested field names by parent field.\n        nested_options = defaultdict(list)\n        for parent, nested_names in nested_fields:\n            nested_options[parent].append(nested_names)\n        # Apply the nested field options.\n        for key, options in iter(nested_options.items()):\n            new_options = self.set_class(options)\n            original_options = getattr(self.declared_fields[key], option_name, ())\n            if original_options:\n                if set_operation == 'union':\n                    new_options |= self.set_class(original_options)\n                if set_operation == 'intersection':\n                    new_options &= self.set_class(original_options)\n            setattr(self.declared_fields[key], option_name, new_options)\n\n    def _update_fields(self, obj=None, many=False):\n        \"\"\"Update fields based on the passed in object.\"\"\"\n        if self.only is not None:\n            # Return only fields specified in only option\n            if self.opts.fields:\n                field_names = self.set_class(self.opts.fields) & self.set_class(self.only)\n            else:\n                field_names = self.set_class(self.only)\n        elif self.opts.fields:\n            # Return fields specified in fields option\n            field_names = self.set_class(self.opts.fields)\n        elif self.opts.additional:\n            # Return declared fields + additional fields\n            field_names = (self.set_class(self.declared_fields.keys()) |\n                            self.set_class(self.opts.additional))\n        else:\n            field_names = self.set_class(self.declared_fields.keys())\n\n        # If \"exclude\" option or param is specified, remove those fields\n        field_names -= self.exclude\n        ret = self.__filter_fields(field_names, obj, many=many)\n        # Set parents\n        self.__set_field_attrs(ret)\n        self.fields = ret\n        return self.fields\n\n\n    def __set_field_attrs(self, fields_dict):\n        \"\"\"Bind fields to the schema, setting any necessary attributes\n        on the fields (e.g. parent and name).\n\n        Also set field load_only and dump_only values if field_name was\n        specified in ``class Meta``.\n        \"\"\"\n        for field_name, field_obj in iteritems(fields_dict):\n            try:\n                if field_name in self.load_only:\n                    field_obj.load_only = True\n                if field_name in self.dump_only:\n                    field_obj.dump_only = True\n                field_obj._add_to_schema(field_name, self)\n                self.on_bind_field(field_name, field_obj)\n            except TypeError:\n                # field declared as a class, not an instance\n                if (isinstance(field_obj, type) and\n                        issubclass(field_obj, base.FieldABC)):\n                    msg = ('Field for \"{0}\" must be declared as a '\n                           'Field instance, not a class. '\n                           'Did you mean \"fields.{1}()\"?'\n                           .format(field_name, field_obj.__name__))\n                    raise TypeError(msg)\n        return fields_dict\n\n    def __filter_fields(self, field_names, obj, many=False):\n        \"\"\"Return only those field_name:field_obj pairs specified by\n        ``field_names``.\n\n        :param set field_names: Field names to include in the final\n            return dictionary.\n        :param object|Mapping|list obj The object to base filtered fields on.\n        :returns: An dict of field_name:field_obj pairs.\n        \"\"\"\n        if obj and many:\n            try:  # list\n                obj = obj[0]\n            except IndexError:  # Nothing to serialize\n                return dict((k, v) for k, v in self.declared_fields.items() if k in field_names)\n        ret = self.dict_class()\n        for key in field_names:\n            if key in self.declared_fields:\n                ret[key] = self.declared_fields[key]\n            else:  # Implicit field creation (class Meta 'fields' or 'additional')\n                if obj:\n                    attribute_type = None\n                    try:\n                        if isinstance(obj, Mapping):\n                            attribute_type = type(obj[key])\n                        else:\n                            attribute_type = type(getattr(obj, key))\n                    except (AttributeError, KeyError) as err:\n                        err_type = type(err)\n                        raise err_type(\n                            '\"{0}\" is not a valid field for {1}.'.format(key, obj))\n                    field_obj = self.TYPE_MAPPING.get(attribute_type, fields.Field)()\n                else:  # Object is None\n                    field_obj = fields.Field()\n                # map key -> field (default to Raw)\n                ret[key] = field_obj\n        return ret\n\n    def _invoke_dump_processors(self, tag_name, data, many, original_data=None):\n        # The pass_many post-dump processors may do things like add an envelope, so\n        # invoke those after invoking the non-pass_many processors which will expect\n        # to get a list of items.\n        data = self._invoke_processors(tag_name, pass_many=False,\n            data=data, many=many, original_data=original_data)\n        data = self._invoke_processors(tag_name, pass_many=True,\n            data=data, many=many, original_data=original_data)\n        return data\n\n    def _invoke_load_processors(self, tag_name, data, many, original_data=None):\n        # This has to invert the order of the dump processors, so run the pass_many\n        # processors first.\n        data = self._invoke_processors(tag_name, pass_many=True,\n            data=data, many=many, original_data=original_data)\n        data = self._invoke_processors(tag_name, pass_many=False,\n            data=data, many=many, original_data=original_data)\n        return data\n\n    def _invoke_field_validators(self, unmarshal, data, many):\n        for attr_name in self.__processors__[(VALIDATES, False)]:\n            validator = getattr(self, attr_name)\n            validator_kwargs = validator.__marshmallow_kwargs__[(VALIDATES, False)]\n            field_name = validator_kwargs['field_name']\n\n            try:\n                field_obj = self.fields[field_name]\n            except KeyError:\n                if field_name in self.declared_fields:\n                    continue\n                raise ValueError('\"{0}\" field does not exist.'.format(field_name))\n\n            if many:\n                for idx, item in enumerate(data):\n                    try:\n                        value = item[field_obj.attribute or field_name]\n                    except KeyError:\n                        pass\n                    else:\n                        validated_value = unmarshal.call_and_store(\n                            getter_func=validator,\n                            data=value,\n                            field_name=field_obj.load_from or field_name,\n                            field_obj=field_obj,\n                            index=(idx if self.opts.index_errors else None)\n                        )\n                        if validated_value is missing:\n                            data[idx].pop(field_name, None)\n            else:\n                try:\n                    value = data[field_obj.attribute or field_name]\n                except KeyError:\n                    pass\n                else:\n                    validated_value = unmarshal.call_and_store(\n                        getter_func=validator,\n                        data=value,\n                        field_name=field_obj.load_from or field_name,\n                        field_obj=field_obj\n                    )\n                    if validated_value is missing:\n                        data.pop(field_name, None)\n\n    def _invoke_validators(\n            self, unmarshal, pass_many, data, original_data, many, field_errors=False):\n        errors = {}\n        for attr_name in self.__processors__[(VALIDATES_SCHEMA, pass_many)]:\n            validator = getattr(self, attr_name)\n            validator_kwargs = validator.__marshmallow_kwargs__[(VALIDATES_SCHEMA, pass_many)]\n            pass_original = validator_kwargs.get('pass_original', False)\n\n            skip_on_field_errors = validator_kwargs['skip_on_field_errors']\n            if skip_on_field_errors and field_errors:\n                continue\n\n            if pass_many:\n                validator = functools.partial(validator, many=many)\n            if many and not pass_many:\n                for idx, item in enumerate(data):\n                    try:\n                        unmarshal.run_validator(validator,\n                                                item, original_data, self.fields, many=many,\n                                                index=idx, pass_original=pass_original)\n                    except ValidationError as err:\n                        errors.update(err.messages)\n            else:\n                try:\n                    unmarshal.run_validator(validator,\n                                            data, original_data, self.fields, many=many,\n                                            pass_original=pass_original)\n                except ValidationError as err:\n                    errors.update(err.messages)\n        if errors:\n            raise ValidationError(errors)\n        return None\n\n    def _invoke_processors(self, tag_name, pass_many, data, many, original_data=None):\n        for attr_name in self.__processors__[(tag_name, pass_many)]:\n            # This will be a bound method.\n            processor = getattr(self, attr_name)\n\n            processor_kwargs = processor.__marshmallow_kwargs__[(tag_name, pass_many)]\n            pass_original = processor_kwargs.get('pass_original', False)\n\n            if pass_many:\n                if pass_original:\n                    data = utils.if_none(processor(data, many, original_data), data)\n                else:\n                    data = utils.if_none(processor(data, many), data)\n            elif many:\n                if pass_original:\n                    data = [utils.if_none(processor(item, original_data), item)\n                            for item in data]\n                else:\n                    data = [utils.if_none(processor(item), item) for item in data]\n            else:\n                if pass_original:\n                    data = utils.if_none(processor(data, original_data), data)\n                else:\n                    data = utils.if_none(processor(data), data)\n        return data",
        "file_path": "src/marshmallow/schema.py",
        "chunk_index": 4,
        "metadata": {
          "class_name": "BaseSchema",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "base.SchemaABC"
            ]
          },
          "parent_name": null,
          "private_methods_count": 14,
          "public_methods_count": 8,
          "node_count": 4517,
          "chunk_size": 24224
        }
      },
      {
        "rank": 4,
        "score": 0.5747783184051514,
        "content": "class Unmarshaller(ErrorStore):\n    \"\"\"Callable class responsible for deserializing data and storing errors.\n\n    .. versionadded:: 1.0.0\n    \"\"\"\n\n    default_schema_validation_error = 'Invalid data.'\n\n    def run_validator(self, validator_func, output,\n            original_data, fields_dict, index=None,\n            many=False, pass_original=False):\n        try:\n            if pass_original:  # Pass original, raw data (before unmarshalling)\n                res = validator_func(output, original_data)\n            else:\n                res = validator_func(output)\n            if res is False:\n                raise ValidationError(self.default_schema_validation_error)\n        except ValidationError as err:\n            errors = self.get_errors(index=index)\n            self.error_kwargs.update(err.kwargs)\n            # Store or reraise errors\n            if err.field_names:\n                field_names = err.field_names\n                field_objs = [fields_dict[each] if each in fields_dict else None\n                              for each in field_names]\n            else:\n                field_names = [SCHEMA]\n                field_objs = []\n            self.error_field_names = field_names\n            self.error_fields = field_objs\n            for field_name in field_names:\n                if isinstance(err.messages, (list, tuple)):\n                    # self.errors[field_name] may be a dict if schemas are nested\n                    if isinstance(errors.get(field_name), dict):\n                        errors[field_name].setdefault(\n                            SCHEMA, []\n                        ).extend(err.messages)\n                    else:\n                        errors.setdefault(field_name, []).extend(err.messages)\n                elif isinstance(err.messages, dict):\n                    errors.setdefault(field_name, []).append(err.messages)\n                else:\n                    errors.setdefault(field_name, []).append(text_type(err))\n\n    def deserialize(self, data, fields_dict, many=False, partial=False,\n            dict_class=dict, index_errors=True, index=None):\n        \"\"\"Deserialize ``data`` based on the schema defined by ``fields_dict``.\n\n        :param dict data: The data to deserialize.\n        :param dict fields_dict: Mapping of field names to :class:`Field` objects.\n        :param bool many: Set to `True` if ``data`` should be deserialized as\n            a collection.\n        :param bool|tuple partial: Whether to ignore missing fields. If its\n            value is an iterable, only missing fields listed in that iterable\n            will be ignored.\n        :param type dict_class: Dictionary class used to construct the output.\n        :param bool index_errors: Whether to store the index of invalid items in\n            ``self.errors`` when ``many=True``.\n        :param int index: Index of the item being serialized (for storing errors) if\n            serializing a collection, otherwise `None`.\n        :return: A dictionary of the deserialized data.\n        \"\"\"\n        if many and data is not None:\n            if not is_collection(data):\n                errors = self.get_errors(index=index)\n                self.error_field_names.append(SCHEMA)\n                errors[SCHEMA] = ['Invalid input type.']\n                ret = []\n            else:\n                self._pending = True\n                ret = [self.deserialize(d, fields_dict, many=False,\n                            partial=partial, dict_class=dict_class,\n                            index=idx, index_errors=index_errors)\n                        for idx, d in enumerate(data)]\n\n                self._pending = False\n                if self.errors:\n                    raise ValidationError(\n                        self.errors,\n                        field_names=self.error_field_names,\n                        fields=self.error_fields,\n                        data=ret,\n                    )\n            return ret\n\n        ret = dict_class()\n\n        if not isinstance(data, collections.Mapping):\n            errors = self.get_errors(index=index)\n            msg = 'Invalid input type.'\n            self.error_field_names = [SCHEMA]\n            errors = self.get_errors()\n            errors.setdefault(SCHEMA, []).append(msg)\n            return None\n        else:\n            partial_is_collection = is_collection(partial)\n            for attr_name, field_obj in iteritems(fields_dict):\n                if field_obj.dump_only:\n                    continue\n                raw_value = data.get(attr_name, missing)\n                field_name = attr_name\n                if raw_value is missing and field_obj.load_from:\n                    field_name = field_obj.load_from\n                    raw_value = data.get(field_obj.load_from, missing)\n                if raw_value is missing:\n                    # Ignore missing field if we're allowed to.\n                    if (\n                        partial is True or\n                        (partial_is_collection and attr_name in partial)\n                    ):\n                        continue\n                    _miss = field_obj.missing\n                    raw_value = _miss() if callable(_miss) else _miss\n                if raw_value is missing and not field_obj.required:\n                    continue\n\n                getter = lambda val: field_obj.deserialize(\n                    val,\n                    field_obj.load_from or attr_name,\n                    data\n                )\n                value = self.call_and_store(\n                    getter_func=getter,\n                    data=raw_value,\n                    field_name=field_name,\n                    field_obj=field_obj,\n                    index=(index if index_errors else None)\n                )\n                if value is not missing:\n                    key = fields_dict[attr_name].attribute or attr_name\n                    set_value(ret, key, value)\n\n        if self.errors and not self._pending:\n            raise ValidationError(\n                self.errors,\n                field_names=self.error_field_names,\n                fields=self.error_fields,\n                data=ret,\n            )\n        return ret\n\n    # Make an instance callable\n    __call__ = deserialize",
        "file_path": "src/marshmallow/marshalling.py",
        "chunk_index": 5,
        "metadata": {
          "class_name": "Unmarshaller",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "ErrorStore"
            ]
          },
          "parent_name": null,
          "private_methods_count": 0,
          "public_methods_count": 2,
          "node_count": 1085,
          "chunk_size": 6231
        }
      },
      {
        "rank": 5,
        "score": 0.5690475702285767,
        "content": "def validates_schema(fn=None, pass_many=False, pass_original=False, skip_on_field_errors=False):\n    \"\"\"Register a schema-level validator.\n\n    By default, receives a single object at a time, regardless of whether ``many=True``\n    is passed to the `Schema`. If ``pass_many=True``, the raw data (which may be a collection)\n    and the value for ``many`` is passed.\n\n    If ``pass_original=True``, the original data (before unmarshalling) will be passed as\n    an additional argument to the method.\n\n    If ``skip_on_field_errors=True``, this validation method will be skipped whenever\n    validation errors have been detected when validating fields.\n    \"\"\"\n    return tag_processor(VALIDATES_SCHEMA, fn, pass_many, pass_original=pass_original,\n                         skip_on_field_errors=skip_on_field_errors)",
        "file_path": "src/marshmallow/decorators.py",
        "chunk_index": 1,
        "metadata": {
          "func_name": "validates_schema",
          "chunk_type": "function",
          "relationship": {
            "call": [
              "tag_processor"
            ]
          },
          "class_name": null
        }
      },
      {
        "rank": 6,
        "score": 0.5482392311096191,
        "content": "def call_and_store(self, getter_func, data, field_name, field_obj, index=None):\n        \"\"\"Call ``getter_func`` with ``data`` as its argument, and store any `ValidationErrors`.\n\n        :param callable getter_func: Function for getting the serialized/deserialized\n            value from ``data``.\n        :param data: The data passed to ``getter_func``.\n        :param str field_name: Field name.\n        :param FieldABC field_obj: Field object that performs the\n            serialization/deserialization behavior.\n        :param int index: Index of the item being validated, if validating a collection,\n            otherwise `None`.\n        \"\"\"\n        try:\n            value = getter_func(data)\n        except ValidationError as err:  # Store validation errors\n            self.error_kwargs.update(err.kwargs)\n            self.error_fields.append(field_obj)\n            self.error_field_names.append(field_name)\n            errors = self.get_errors(index=index)\n            # Warning: Mutation!\n            if isinstance(err.messages, dict):\n                errors[field_name] = err.messages\n            elif isinstance(errors.get(field_name), dict):\n                errors[field_name].setdefault(FIELD, []).extend(err.messages)\n            else:\n                errors.setdefault(field_name, []).extend(err.messages)\n            # When a Nested field fails validation, the marshalled data is stored\n            # on the ValidationError's data attribute\n            value = err.data or missing\n        return value",
        "file_path": "src/marshmallow/marshalling.py",
        "chunk_index": 2,
        "metadata": {
          "func_name": "call_and_store",
          "chunk_type": "function",
          "relationship": {
            "call": [
              "getter_func",
              "self.error_kwargs.update",
              "self.error_fields.append",
              "self.error_field_names.append",
              "self.get_errors",
              "errors.get",
              "errors[field_name].setdefault(FIELD, []).extend",
              "errors[field_name].setdefault",
              "errors.setdefault(field_name, []).extend",
              "errors.setdefault"
            ]
          },
          "class_name": "ErrorStore"
        }
      },
      {
        "rank": 7,
        "score": 0.5480475425720215,
        "content": "class Dict(Field):\n    \"\"\"A dict field. Supports dicts and dict-like objects.\n\n    .. note::\n        This field is only appropriate when the structure of\n        nested data is not known. For structured data, use\n        `Nested`.\n\n    .. versionadded:: 2.1.0\n    \"\"\"\n\n    default_error_messages = {\n        'invalid': 'Not a valid mapping type.'\n    }\n\n    def _deserialize(self, value, attr, data):\n        if isinstance(value, Mapping):\n            return value\n        else:\n            self.fail('invalid')",
        "file_path": "src/marshmallow/fields.py",
        "chunk_index": 22,
        "metadata": {
          "class_name": "Dict",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "Field"
            ]
          },
          "parent_name": null,
          "private_methods_count": 1,
          "public_methods_count": 0,
          "node_count": 78,
          "chunk_size": 511
        }
      },
      {
        "rank": 8,
        "score": 0.5448833703994751,
        "content": "class Field(FieldABC):\n    \"\"\"Basic field from which other fields should extend. It applies no\n    formatting by default, and should only be used in cases where\n    data does not need to be formatted before being serialized or deserialized.\n    On error, the name of the field will be returned.\n\n    :param default: If set, this value will be used during serialization if the input value\n        is missing. If not set, the field will be excluded from the serialized output if the\n        input value is missing. May be a value or a callable.\n    :param str attribute: The name of the attribute to get the value from. If\n        `None`, assumes the attribute has the same name as the field.\n    :param str load_from: Additional key to look for when deserializing. Will only\n        be checked if the field's name is not found on the input dictionary. If checked,\n        it will return this parameter on error.\n    :param str dump_to: Field name to use as a key when serializing.\n    :param callable validate: Validator or collection of validators that are called\n        during deserialization. Validator takes a field's input value as\n        its only parameter and returns a boolean.\n        If it returns `False`, an :exc:`ValidationError` is raised.\n    :param required: Raise a :exc:`ValidationError` if the field value\n        is not supplied during deserialization.\n    :param allow_none: Set this to `True` if `None` should be considered a valid value during\n        validation/deserialization. If ``missing=None`` and ``allow_none`` is unset,\n        will default to ``True``. Otherwise, the default is ``False``.\n    :param bool load_only: If `True` skip this field during serialization, otherwise\n        its value will be present in the serialized data.\n    :param bool dump_only: If `True` skip this field during deserialization, otherwise\n        its value will be present in the deserialized object. In the context of an\n        HTTP API, this effectively marks the field as \"read-only\".\n    :param missing: Default deserialization value for the field if the field is not\n        found in the input data. May be a value or a callable.\n    :param dict error_messages: Overrides for `Field.default_error_messages`.\n    :param metadata: Extra arguments to be stored as metadata.\n\n    .. versionchanged:: 2.0.0\n        Removed `error` parameter. Use ``error_messages`` instead.\n\n    .. versionchanged:: 2.0.0\n        Added `allow_none` parameter, which makes validation/deserialization of `None`\n        consistent across fields.\n\n    .. versionchanged:: 2.0.0\n        Added `load_only` and `dump_only` parameters, which allow field skipping\n        during the (de)serialization process.\n\n    .. versionchanged:: 2.0.0\n        Added `missing` parameter, which indicates the value for a field if the field\n        is not found during deserialization.\n\n    .. versionchanged:: 2.0.0\n        ``default`` value is only used if explicitly set. Otherwise, missing values\n        inputs are excluded from serialized output.\n    \"\"\"\n    # Some fields, such as Method fields and Function fields, are not expected\n    #  to exists as attributes on the objects to serialize. Set this to False\n    #  for those fields\n    _CHECK_ATTRIBUTE = True\n    _creation_index = 0  # Used for sorting\n\n    #: Default error messages for various kinds of errors. The keys in this dictionary\n    #: are passed to `Field.fail`. The values are error messages passed to\n    #: :exc:`marshmallow.ValidationError`.\n    default_error_messages = {\n        'required': 'Missing data for required field.',\n        'type': 'Invalid type.',  # used by Unmarshaller\n        'null': 'Field may not be null.',\n        'validator_failed': 'Invalid value.'\n    }\n\n    def __init__(self, default=missing_, attribute=None, load_from=None, dump_to=None,\n                 error=None, validate=None, required=False, allow_none=None, load_only=False,\n                 dump_only=False, missing=missing_, error_messages=None, **metadata):\n        self.default = default\n        self.attribute = attribute\n        self.load_from = load_from  # this flag is used by Unmarshaller\n        self.dump_to = dump_to  # this flag is used by Marshaller\n        self.validate = validate\n        if utils.is_iterable_but_not_string(validate):\n            if not utils.is_generator(validate):\n                self.validators = validate\n            else:\n                self.validators = list(validate)\n        elif callable(validate):\n            self.validators = [validate]\n        elif validate is None:\n            self.validators = []\n        else:\n            raise ValueError(\"The 'validate' parameter must be a callable \"\n                             \"or a collection of callables.\")\n\n        self.required = required\n        # If missing=None, None should be considered valid by default\n        if allow_none is None:\n            if missing is None:\n                self.allow_none = True\n            else:\n                self.allow_none = False\n        else:\n            self.allow_none = allow_none\n        self.load_only = load_only\n        self.dump_only = dump_only\n        self.missing = missing\n        self.metadata = metadata\n        self._creation_index = Field._creation_index\n        Field._creation_index += 1\n\n        # Collect default error message from self and parent classes\n        messages = {}\n        for cls in reversed(self.__class__.__mro__):\n            messages.update(getattr(cls, 'default_error_messages', {}))\n        messages.update(error_messages or {})\n        self.error_messages = messages\n\n    def __repr__(self):\n        return ('<fields.{ClassName}(default={self.default!r}, '\n                'attribute={self.attribute!r}, '\n                'validate={self.validate}, required={self.required}, '\n                'load_only={self.load_only}, dump_only={self.dump_only}, '\n                'missing={self.missing}, allow_none={self.allow_none}, '\n                'error_messages={self.error_messages})>'\n                .format(ClassName=self.__class__.__name__, self=self))\n\n\n    def _validate(self, value):\n        \"\"\"Perform validation on ``value``. Raise a :exc:`ValidationError` if validation\n        does not succeed.\n        \"\"\"\n        errors = []\n        kwargs = {}\n        for validator in self.validators:\n            try:\n                r = validator(value)\n                if not isinstance(validator, Validator) and r is False:\n                    self.fail('validator_failed')\n            except ValidationError as err:\n                kwargs.update(err.kwargs)\n                if isinstance(err.messages, dict):\n                    errors.append(err.messages)\n                else:\n                    errors.extend(err.messages)\n        if errors:\n            raise ValidationError(errors, **kwargs)\n\n    # Hat tip to django-rest-framework.\n\n    def _validate_missing(self, value):\n        \"\"\"Validate missing values. Raise a :exc:`ValidationError` if\n        `value` should be considered missing.\n        \"\"\"\n        if value is missing_:\n            if hasattr(self, 'required') and self.required:\n                self.fail('required')\n        if value is None:\n            if hasattr(self, 'allow_none') and self.allow_none is not True:\n                self.fail('null')\n\n\n\n    # Methods for concrete classes to override.\n\n    def _add_to_schema(self, field_name, schema):\n        \"\"\"Update field with values from its parent schema. Called by\n            :meth:`__set_field_attrs <marshmallow.Schema.__set_field_attrs>`.\n\n        :param str field_name: Field name set in schema.\n        :param Schema schema: Parent schema.\n        \"\"\"\n        self.parent = self.parent or schema\n        self.name = self.name or field_name\n\n    def _serialize(self, value, attr, obj):\n        \"\"\"Serializes ``value`` to a basic Python datatype. Noop by default.\n        Concrete :class:`Field` classes should implement this method.\n\n        Example: ::\n\n            class TitleCase(Field):\n                def _serialize(self, value, attr, obj):\n                    if not value:\n                        return ''\n                    return unicode(value).title()\n\n        :param value: The value to be serialized.\n        :param str attr: The attribute or key on the object to be serialized.\n        :param object obj: The object the value was pulled from.\n        :raise ValidationError: In case of formatting or validation failure.\n        :return: The serialized value\n        \"\"\"\n        return value\n\n    def _deserialize(self, value, attr, data):\n        \"\"\"Deserialize value. Concrete :class:`Field` classes should implement this method.\n\n        :param value: The value to be deserialized.\n        :param str attr: The attribute/key in `data` to be deserialized.\n        :param dict data: The raw input data passed to the `Schema.load`.\n        :raise ValidationError: In case of formatting or validation failure.\n        :return: The deserialized value.\n\n        .. versionchanged:: 2.0.0\n            Added ``attr`` and ``data`` parameters.\n        \"\"\"\n        return value\n\n    # Properties\n\n    @property\n    def context(self):\n        \"\"\"The context dictionary for the parent :class:`Schema`.\"\"\"\n        return self.parent.context\n\n    @property\n    def root(self):\n        \"\"\"Reference to the `Schema` that this field belongs to even if it is buried in a `List`.\n        Return `None` for unbound fields.\n        \"\"\"\n        ret = self\n        while hasattr(ret, 'parent') and ret.parent:\n            ret = ret.parent\n        return ret if isinstance(ret, SchemaABC) else None",
        "file_path": "src/marshmallow/fields.py",
        "chunk_index": 0,
        "metadata": {
          "class_name": "Field",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "FieldABC"
            ]
          },
          "parent_name": null,
          "private_methods_count": 7,
          "public_methods_count": 4,
          "node_count": 1443,
          "chunk_size": 9592
        }
      },
      {
        "rank": 9,
        "score": 0.5425887107849121,
        "content": "class ValidationError(MarshmallowError):\n    \"\"\"Raised when validation fails on a field. Validators and custom fields should\n    raise this exception.\n\n    :param message: An error message, list of error messages, or dict of\n        error messages.\n    :param list field_names: Field names to store the error on.\n        If `None`, the error is stored in its default location.\n    :param list fields: `Field` objects to which the error applies.\n    \"\"\"\n\n    def __init__(self, message, field_names=None, fields=None, data=None, **kwargs):\n        if not isinstance(message, dict) and not isinstance(message, list):\n            messages = [message]\n        else:\n            messages = message\n        #: String, list, or dictionary of error messages.\n        #: If a `dict`, the keys will be field names and the values will be lists of\n        #: messages.\n        self.messages = messages\n        #: List of field objects which failed validation.\n        self.fields = fields\n        if isinstance(field_names, basestring):\n            #: List of field_names which failed validation.\n            self.field_names = [field_names]\n        else:  # fields is a list or None\n            self.field_names = field_names or []\n        # Store nested data\n        self.data = data\n        self.kwargs = kwargs\n        MarshmallowError.__init__(self, message)\n",
        "file_path": "src/marshmallow/exceptions.py",
        "chunk_index": 1,
        "metadata": {
          "class_name": "ValidationError",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "MarshmallowError"
            ]
          },
          "parent_name": null,
          "private_methods_count": 1,
          "public_methods_count": 1,
          "node_count": 268,
          "chunk_size": 1352
        }
      },
      {
        "rank": 10,
        "score": 0.5413529276847839,
        "content": "class ChangedInMarshmallow3Warning(FutureWarning):\n    pass",
        "file_path": "src/marshmallow/warnings.py",
        "chunk_index": 1,
        "metadata": {
          "class_name": "ChangedInMarshmallow3Warning",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "FutureWarning"
            ]
          },
          "parent_name": null,
          "private_methods_count": 0,
          "public_methods_count": 0,
          "node_count": 11,
          "chunk_size": 59
        }
      }
    ]
  },
  "pylint-dev__astroid-1978": {
    "query": "Deprecation warnings from numpy\n### Steps to reproduce\r\n\r\n1. Run pylint over the following test case:\r\n\r\n```\r\n\"\"\"Test case\"\"\"\r\n\r\nimport numpy as np\r\nvalue = np.random.seed(1234)\r\n```\r\n\r\n### Current behavior\r\n```\r\n/home/bje/source/nemo/myenv/lib/python3.10/site-packages/astroid/raw_building.py:470: FutureWarning: In the future `np.long` will be defined as the corresponding NumPy scalar.  (This may have returned Python scalars in past versions.\r\n  getattr(sys.modules[modname], name)\r\n/home/bje/source/nemo/myenv/lib/python3.10/site-packages/astroid/raw_building.py:470: FutureWarning: In the future `np.long` will be defined as the corresponding NumPy scalar.  (This may have returned Python scalars in past versions.\r\n  getattr(sys.modules[modname], name)\r\n```\r\n\r\n### Expected behavior\r\nThere should be no future warnings.\r\n\r\n### python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\" output\r\n2.12.13\n",
    "method": "hierarchical",
    "top_k": 10,
    "results": [
      {
        "rank": 1,
        "score": 0.6080761551856995,
        "content": "def _get_numpy_version() -> tuple[str, str, str]:\n    \"\"\"\n    Return the numpy version number if numpy can be imported.\n\n    Otherwise returns ('0', '0', '0')\n    \"\"\"\n    try:\n        import numpy  # pylint: disable=import-outside-toplevel\n\n        return tuple(numpy.version.version.split(\".\"))\n    except (ImportError, AttributeError):\n        return (\"0\", \"0\", \"0\")",
        "file_path": "astroid/brain/brain_numpy_utils.py",
        "chunk_index": 1,
        "metadata": {
          "func_name": "_get_numpy_version",
          "chunk_type": "function",
          "relationship": {
            "call": [
              "numpy.version.version.split"
            ]
          },
          "class_name": null
        }
      },
      {
        "rank": 2,
        "score": 0.5938296318054199,
        "content": "def numpy_supports_type_hints() -> bool:\n    \"\"\"Returns True if numpy supports type hints.\"\"\"\n    np_ver = _get_numpy_version()\n    return np_ver and np_ver > NUMPY_VERSION_TYPE_HINTS_SUPPORT",
        "file_path": "astroid/brain/brain_numpy_utils.py",
        "chunk_index": 0,
        "metadata": {
          "func_name": "numpy_supports_type_hints",
          "chunk_type": "function",
          "relationship": {
            "call": [
              "_get_numpy_version"
            ]
          },
          "class_name": null
        }
      },
      {
        "rank": 3,
        "score": 0.5759243369102478,
        "content": "def deprecate_arguments(\n        astroid_version: str = \"3.0\", **arguments: str\n    ) -> Callable[[Callable[_P, _R]], Callable[_P, _R]]:\n        \"\"\"Passthrough decorator to improve performance if DeprecationWarnings are\n        disabled.\n        \"\"\"\n\n        def deco(func: Callable[_P, _R]) -> Callable[_P, _R]:\n            \"\"\"Decorator function.\"\"\"\n            return func\n\n        return deco",
        "file_path": "astroid/decorators.py",
        "chunk_index": 8,
        "metadata": {
          "func_name": "deprecate_arguments",
          "chunk_type": "function",
          "relationship": null,
          "class_name": null
        }
      },
      {
        "rank": 4,
        "score": 0.568659245967865,
        "content": "def deprecate_default_argument_values(\n        astroid_version: str = \"3.0\", **arguments: str\n    ) -> Callable[[Callable[_P, _R]], Callable[_P, _R]]:\n        \"\"\"Passthrough decorator to improve performance if DeprecationWarnings are\n        disabled.\n        \"\"\"\n\n        def deco(func: Callable[_P, _R]) -> Callable[_P, _R]:\n            \"\"\"Decorator function.\"\"\"\n            return func\n\n        return deco",
        "file_path": "astroid/decorators.py",
        "chunk_index": 7,
        "metadata": {
          "func_name": "deprecate_default_argument_values",
          "chunk_type": "function",
          "relationship": null,
          "class_name": null
        }
      },
      {
        "rank": 5,
        "score": 0.5637557506561279,
        "content": "def deprecate_arguments(\n        astroid_version: str = \"3.0\", **arguments: str\n    ) -> Callable[[Callable[_P, _R]], Callable[_P, _R]]:\n        \"\"\"Decorator which emits a DeprecationWarning if any arguments specified\n        are passed.\n\n        Arguments should be a key-value mapping, with the key being the argument to check\n        and the value being a string that explains what to do instead of passing the argument.\n\n        To improve performance, only used when DeprecationWarnings other than\n        the default one are enabled.\n        \"\"\"\n\n        def deco(func: Callable[_P, _R]) -> Callable[_P, _R]:\n            @functools.wraps(func)\n            def wrapper(*args: _P.args, **kwargs: _P.kwargs) -> _R:\n                keys = list(inspect.signature(func).parameters.keys())\n                for arg, note in arguments.items():\n                    try:\n                        index = keys.index(arg)\n                    except ValueError:\n                        raise ValueError(\n                            f\"Can't find argument '{arg}' for '{args[0].__class__.__qualname__}'\"\n                        ) from None\n                    if arg in kwargs or len(args) > index:\n                        warnings.warn(\n                            f\"The argument '{arg}' for \"\n                            f\"'{args[0].__class__.__qualname__}.{func.__name__}' is deprecated \"\n                            f\"and will be removed in astroid {astroid_version} ({note})\",\n                            DeprecationWarning,\n                        )\n                return func(*args, **kwargs)\n\n            return wrapper\n\n        return deco",
        "file_path": "astroid/decorators.py",
        "chunk_index": 6,
        "metadata": {
          "func_name": "deprecate_arguments",
          "chunk_type": "function",
          "relationship": {
            "call": [
              "functools.wraps",
              "inspect.signature(func).parameters.keys",
              "inspect.signature",
              "arguments.items",
              "keys.index",
              "warnings.warn",
              "func"
            ]
          },
          "class_name": null
        }
      },
      {
        "rank": 6,
        "score": 0.5599681735038757,
        "content": "def deprecate_default_argument_values(\n        astroid_version: str = \"3.0\", **arguments: str\n    ) -> Callable[[Callable[_P, _R]], Callable[_P, _R]]:\n        \"\"\"Decorator which emits a DeprecationWarning if any arguments specified\n        are None or not passed at all.\n\n        Arguments should be a key-value mapping, with the key being the argument to check\n        and the value being a type annotation as string for the value of the argument.\n\n        To improve performance, only used when DeprecationWarnings other than\n        the default one are enabled.\n        \"\"\"\n        # Helpful links\n        # Decorator for DeprecationWarning: https://stackoverflow.com/a/49802489\n        # Typing of stacked decorators: https://stackoverflow.com/a/68290080\n\n        def deco(func: Callable[_P, _R]) -> Callable[_P, _R]:\n            \"\"\"Decorator function.\"\"\"\n\n            @functools.wraps(func)\n            def wrapper(*args: _P.args, **kwargs: _P.kwargs) -> _R:\n                \"\"\"Emit DeprecationWarnings if conditions are met.\"\"\"\n\n                keys = list(inspect.signature(func).parameters.keys())\n                for arg, type_annotation in arguments.items():\n                    try:\n                        index = keys.index(arg)\n                    except ValueError:\n                        raise ValueError(\n                            f\"Can't find argument '{arg}' for '{args[0].__class__.__qualname__}'\"\n                        ) from None\n                    if (\n                        # Check kwargs\n                        # - if found, check it's not None\n                        (arg in kwargs and kwargs[arg] is None)\n                        # Check args\n                        # - make sure not in kwargs\n                        # - len(args) needs to be long enough, if too short\n                        #   arg can't be in args either\n                        # - args[index] should not be None\n                        or arg not in kwargs\n                        and (\n                            index == -1\n                            or len(args) <= index\n                            or (len(args) > index and args[index] is None)\n                        )\n                    ):\n                        warnings.warn(\n                            f\"'{arg}' will be a required argument for \"\n                            f\"'{args[0].__class__.__qualname__}.{func.__name__}'\"\n                            f\" in astroid {astroid_version} \"\n                            f\"('{arg}' should be of type: '{type_annotation}')\",\n                            DeprecationWarning,\n                        )\n                return func(*args, **kwargs)\n\n            return wrapper\n\n        return deco",
        "file_path": "astroid/decorators.py",
        "chunk_index": 5,
        "metadata": {
          "func_name": "deprecate_default_argument_values",
          "chunk_type": "function",
          "relationship": {
            "call": [
              "functools.wraps",
              "inspect.signature(func).parameters.keys",
              "inspect.signature",
              "arguments.items",
              "keys.index",
              "warnings.warn",
              "func"
            ]
          },
          "class_name": null
        }
      },
      {
        "rank": 7,
        "score": 0.5253008008003235,
        "content": "def _is_a_numpy_module(node: Name) -> bool:\n    \"\"\"\n    Returns True if the node is a representation of a numpy module.\n\n    For example in :\n        import numpy as np\n        x = np.linspace(1, 2)\n    The node <Name.np> is a representation of the numpy module.\n\n    :param node: node to test\n    :return: True if the node is a representation of the numpy module.\n    \"\"\"\n    module_nickname = node.name\n    potential_import_target = [\n        x for x in node.lookup(module_nickname)[1] if isinstance(x, Import)\n    ]\n    return any(\n        (\"numpy\", module_nickname) in target.names or (\"numpy\", None) in target.names\n        for target in potential_import_target\n    )",
        "file_path": "astroid/brain/brain_numpy_utils.py",
        "chunk_index": 3,
        "metadata": {
          "func_name": "_is_a_numpy_module",
          "chunk_type": "function",
          "relationship": {
            "call": [
              "node.lookup"
            ]
          },
          "class_name": null
        }
      },
      {
        "rank": 8,
        "score": 0.519788384437561,
        "content": "def check_warnings_filter() -> bool:\n    \"\"\"Return True if any other than the default DeprecationWarning filter is enabled.\n\n    https://docs.python.org/3/library/warnings.html#default-warning-filter\n    \"\"\"\n    return any(\n        issubclass(DeprecationWarning, filter[2])\n        and filter[0] != \"ignore\"\n        and filter[3] != \"__main__\"\n        for filter in warnings.filters\n    )",
        "file_path": "astroid/util.py",
        "chunk_index": 9,
        "metadata": {
          "func_name": "check_warnings_filter",
          "chunk_type": "function",
          "relationship": null,
          "class_name": null
        }
      },
      {
        "rank": 9,
        "score": 0.5142477750778198,
        "content": "def is_sys_guard(self) -> bool:\n        \"\"\"Return True if IF stmt is a sys.version_info guard.\n\n        >>> import astroid\n        >>> node = astroid.extract_node('''\n        import sys\n        if sys.version_info > (3, 8):\n            from typing import Literal\n        else:\n            from typing_extensions import Literal\n        ''')\n        >>> node.is_sys_guard()\n        True\n        \"\"\"\n        warnings.warn(\n            \"The 'is_sys_guard' function is deprecated and will be removed in astroid 3.0.0 \"\n            \"It has been moved to pylint and can be imported from 'pylint.checkers.utils' \"\n            \"starting with pylint 2.12\",\n            DeprecationWarning,\n        )\n        if isinstance(self.test, Compare):\n            value = self.test.left\n            if isinstance(value, Subscript):\n                value = value.value\n            if isinstance(value, Attribute) and value.as_string() == \"sys.version_info\":\n                return True\n\n        return False",
        "file_path": "astroid/nodes/node_classes.py",
        "chunk_index": 116,
        "metadata": {
          "func_name": "is_sys_guard",
          "chunk_type": "function",
          "relationship": {
            "call": [
              "warnings.warn",
              "value.as_string"
            ]
          },
          "class_name": "If"
        }
      },
      {
        "rank": 10,
        "score": 0.5134646892547607,
        "content": "def numpy_core_numeric_transform():\n    return parse(\n        \"\"\"\n    # different functions defined in numeric.py\n    import numpy\n    def zeros_like(a, dtype=None, order='K', subok=True, shape=None): return numpy.ndarray((0, 0))\n    def ones_like(a, dtype=None, order='K', subok=True, shape=None): return numpy.ndarray((0, 0))\n    def full_like(a, fill_value, dtype=None, order='K', subok=True, shape=None): return numpy.ndarray((0, 0))\n        \"\"\"\n    )",
        "file_path": "astroid/brain/brain_numpy_core_numeric.py",
        "chunk_index": 0,
        "metadata": {
          "func_name": "numpy_core_numeric_transform",
          "chunk_type": "function",
          "relationship": {
            "call": [
              "parse"
            ]
          },
          "class_name": null
        }
      }
    ]
  },
  "pylint-dev__astroid-1333": {
    "query": "astroid 2.9.1 breaks pylint with missing __init__.py: F0010: error while code parsing: Unable to load file __init__.py\n### Steps to reproduce\r\n> Steps provided are for Windows 11, but initial problem found in Ubuntu 20.04\r\n\r\n> Update 2022-01-04: Corrected repro steps and added more environment details\r\n\r\n1. Set up simple repo with following structure (all files can be empty):\r\n```\r\nroot_dir/\r\n|--src/\r\n|----project/ # Notice the missing __init__.py\r\n|------file.py # It can be empty, but I added `import os` at the top\r\n|----__init__.py\r\n```\r\n2. Open a command prompt\r\n3. `cd root_dir`\r\n4. `python -m venv venv`\r\n5. `venv/Scripts/activate`\r\n6. `pip install pylint astroid==2.9.1` # I also repro'd on the latest, 2.9.2\r\n7. `pylint src/project` # Updated from `pylint src`\r\n8. Observe failure:\r\n```\r\nsrc\\project\\__init__.py:1:0: F0010: error while code parsing: Unable to load file src\\project\\__init__.py:\r\n```\r\n\r\n### Current behavior\r\nFails with `src\\project\\__init__.py:1:0: F0010: error while code parsing: Unable to load file src\\project\\__init__.py:`\r\n\r\n### Expected behavior\r\nDoes not fail with error.\r\n> If you replace step 6 with `pip install pylint astroid==2.9.0`, you get no failure with an empty output - since no files have content\r\n\r\n### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\r\n2.9.1\r\n\r\n`python 3.9.1`\r\n`pylint 2.12.2 `\r\n\r\n\r\n\r\nThis issue has been observed with astroid `2.9.1` and `2.9.2`\n",
    "method": "hierarchical",
    "top_k": 10,
    "results": [
      {
        "rank": 1,
        "score": 0.6561583280563354,
        "content": "lass AstroidImportError(AstroidBuildingError):\n    \"\"\"Exception class used when a module can't be imported by astroid.\"\"\"\n",
        "file_path": "astroid/exceptions.py",
        "chunk_index": 2,
        "metadata": {
          "class_name": "AstroidImportError",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "AstroidBuildingError"
            ]
          },
          "parent_name": null,
          "private_methods_count": 0,
          "public_methods_count": 0,
          "node_count": 14,
          "chunk_size": 122
        }
      },
      {
        "rank": 2,
        "score": 0.6543624401092529,
        "content": "lass AstroidSyntaxError(AstroidBuildingError):\n    \"\"\"Exception class used when a module can't be parsed.\"\"\"\n",
        "file_path": "astroid/exceptions.py",
        "chunk_index": 4,
        "metadata": {
          "class_name": "AstroidSyntaxError",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "AstroidBuildingError"
            ]
          },
          "parent_name": null,
          "private_methods_count": 0,
          "public_methods_count": 0,
          "node_count": 14,
          "chunk_size": 109
        }
      },
      {
        "rank": 3,
        "score": 0.6061620116233826,
        "content": "ef ast_from_file(self, filepath, modname=None, fallback=True, source=False):\n        \"\"\"given a module name, return the astroid object\"\"\"\n        try:\n            filepath = get_source_file(filepath, include_no_ext=True)\n            source = True\n        except NoSourceFile:\n            pass\n        if modname is None:\n            try:\n                modname = \".\".join(modpath_from_file(filepath))\n            except ImportError:\n                modname = filepath\n        if (\n            modname in self.astroid_cache\n            and self.astroid_cache[modname].file == filepath\n        ):\n            return self.astroid_cache[modname]\n        if source:\n            # pylint: disable=import-outside-toplevel; circular import\n            from astroid.builder import AstroidBuilder\n\n            return AstroidBuilder(self).file_build(filepath, modname)\n        if fallback and modname:\n            return self.ast_from_module_name(modname)\n        raise AstroidBuildingError(\"Unable to build an AST for {path}.\", path=filepath)\n",
        "file_path": "astroid/manager.py",
        "chunk_index": 3,
        "metadata": {
          "func_name": "ast_from_file",
          "chunk_type": "function",
          "relationship": {
            "call": [
              "get_source_file",
              "\".\".join",
              "modpath_from_file",
              "AstroidBuilder(self).file_build",
              "AstroidBuilder",
              "self.ast_from_module_name",
              "AstroidBuildingError"
            ]
          },
          "class_name": "AstroidManager"
        }
      },
      {
        "rank": 4,
        "score": 0.5996749997138977,
        "content": "f file_build(self, path, modname=None):\n        \"\"\"Build astroid from a source code file (i.e. from an ast)\n\n        *path* is expected to be a python source file\n        \"\"\"\n        try:\n            stream, encoding, data = open_source_file(path)\n        except OSError as exc:\n            raise AstroidBuildingError(\n                \"Unable to load file {path}:\\n{error}\",\n                modname=modname,\n                path=path,\n                error=exc,\n            ) from exc\n        except (SyntaxError, LookupError) as exc:\n            raise AstroidSyntaxError(\n                \"Python 3 encoding specification error or unknown encoding:\\n\"\n                \"{error}\",\n                modname=modname,\n                path=path,\n                error=exc,\n            ) from exc\n        except UnicodeError as exc:  # wrong encoding\n            # detect_encoding returns utf-8 if no encoding specified\n            raise AstroidBuildingError(\n                \"Wrong or no encoding specified for {filename}.\", filename=path\n            ) from exc\n        with stream:\n            # get module name if necessary\n            if modname is None:\n                try:\n                    modname = \".\".join(modutils.modpath_from_file(path))\n                except ImportError:\n                    modname = os.path.splitext(os.path.basename(path))[0]\n            # build astroid representation\n            module = self._data_build(data, modname, path)\n            return self._post_build(module, encoding)\n\n",
        "file_path": "astroid/builder.py",
        "chunk_index": 4,
        "metadata": {
          "func_name": "file_build",
          "chunk_type": "function",
          "relationship": {
            "call": [
              "open_source_file",
              "AstroidBuildingError",
              "AstroidSyntaxError",
              "\".\".join",
              "modutils.modpath_from_file",
              "os.path.splitext",
              "os.path.basename",
              "self._data_build",
              "self._post_build"
            ]
          },
          "class_name": "AstroidBuilder"
        }
      },
      {
        "rank": 5,
        "score": 0.5963910818099976,
        "content": "ef ast_from_string(self, data, modname=\"\", filepath=None):\n        \"\"\"Given some source code as a string, return its corresponding astroid object\"\"\"\n        # pylint: disable=import-outside-toplevel; circular import\n        from astroid.builder import AstroidBuilder\n\n        return AstroidBuilder(self).string_build(data, modname, filepath)\n",
        "file_path": "astroid/manager.py",
        "chunk_index": 4,
        "metadata": {
          "func_name": "ast_from_string",
          "chunk_type": "function",
          "relationship": {
            "call": [
              "AstroidBuilder(self).string_build",
              "AstroidBuilder"
            ]
          },
          "class_name": "AstroidManager"
        }
      },
      {
        "rank": 6,
        "score": 0.5946141481399536,
        "content": "ef bootstrap(self):\n        \"\"\"Bootstrap the required AST modules needed for the manager to work\n\n        The bootstrap usually involves building the AST for the builtins\n        module, which is required by the rest of astroid to work correctly.\n        \"\"\"\n        from astroid import raw_building  # pylint: disable=import-outside-toplevel\n\n        raw_building._astroid_bootstrapping()\n",
        "file_path": "astroid/manager.py",
        "chunk_index": 13,
        "metadata": {
          "func_name": "bootstrap",
          "chunk_type": "function",
          "relationship": {
            "call": [
              "raw_building._astroid_bootstrapping"
            ]
          },
          "class_name": "AstroidManager"
        }
      },
      {
        "rank": 7,
        "score": 0.5857429504394531,
        "content": "lass AstroidTypeError(AstroidError):\n    \"\"\"Raised when a TypeError would be expected in Python code.\"\"\"\n",
        "file_path": "astroid/exceptions.py",
        "chunk_index": 17,
        "metadata": {
          "class_name": "AstroidTypeError",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "AstroidError"
            ]
          },
          "parent_name": null,
          "private_methods_count": 0,
          "public_methods_count": 0,
          "node_count": 14,
          "chunk_size": 105
        }
      },
      {
        "rank": 8,
        "score": 0.5810327529907227,
        "content": "lass AstroidBuildingError(AstroidError):\n    \"\"\"exception class when we are unable to build an astroid representation\n\n    Standard attributes:\n        modname: Name of the module that AST construction failed for.\n        error: Exception raised during construction.\n    \"\"\"\n\n    def __init__(self, message=\"Failed to import module {modname}.\", **kws):\n        super().__init__(message, **kws)",
        "file_path": "astroid/exceptions.py",
        "chunk_index": 1,
        "metadata": {
          "class_name": "AstroidBuildingError",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "AstroidError"
            ]
          },
          "parent_name": null,
          "private_methods_count": 1,
          "public_methods_count": 0,
          "node_count": 53,
          "chunk_size": 393
        }
      },
      {
        "rank": 9,
        "score": 0.5806784629821777,
        "content": "f parse(code, module_name=\"\", path=None, apply_transforms=True):\n    \"\"\"Parses a source string in order to obtain an astroid AST from it\n\n    :param str code: The code for the module.\n    :param str module_name: The name for the module, if any\n    :param str path: The path for the module\n    :param bool apply_transforms:\n        Apply the transforms for the give code. Use it if you\n        don't want the default transforms to be applied.\n    \"\"\"\n    code = textwrap.dedent(code)\n    builder = AstroidBuilder(\n        manager=AstroidManager(), apply_transforms=apply_transforms\n    )\n    return builder.string_build(code, modname=module_name, path=path)\n\n",
        "file_path": "astroid/builder.py",
        "chunk_index": 9,
        "metadata": {
          "func_name": "parse",
          "chunk_type": "function",
          "relationship": {
            "call": [
              "textwrap.dedent",
              "AstroidBuilder",
              "AstroidManager",
              "builder.string_build"
            ]
          },
          "class_name": null
        }
      },
      {
        "rank": 10,
        "score": 0.5763754844665527,
        "content": "ef ast_from_module(self, module: types.ModuleType, modname: Optional[str] = None):\n        \"\"\"given an imported module, return the astroid object\"\"\"\n        modname = modname or module.__name__\n        if modname in self.astroid_cache:\n            return self.astroid_cache[modname]\n        try:\n            # some builtin modules don't have __file__ attribute\n            filepath = module.__file__\n            if is_python_source(filepath):\n                return self.ast_from_file(filepath, modname)\n        except AttributeError:\n            pass\n\n        # pylint: disable=import-outside-toplevel; circular import\n        from astroid.builder import AstroidBuilder\n\n        return AstroidBuilder(self).module_build(module, modname)\n",
        "file_path": "astroid/manager.py",
        "chunk_index": 8,
        "metadata": {
          "func_name": "ast_from_module",
          "chunk_type": "function",
          "relationship": {
            "call": [
              "is_python_source",
              "self.ast_from_file",
              "AstroidBuilder(self).module_build",
              "AstroidBuilder"
            ]
          },
          "class_name": "AstroidManager"
        }
      }
    ]
  },
  "pylint-dev__astroid-1196": {
    "query": "getitem does not infer the actual unpacked value\nWhen trying to call `Dict.getitem()` on a context where we have a dict unpacking of anything beside a real dict, astroid currently raises an `AttributeError: 'getitem'`, which has 2 problems:\r\n\r\n- The object might be a reference against something constant, this pattern is usually seen when we have different sets of dicts that extend each other, and all of their values are inferrable. \r\n- We can have something that is uninferable, but in that case instead of an `AttributeError` I think it makes sense to raise the usual `AstroidIndexError` which is supposed to be already handled by the downstream.\r\n\r\n\r\nHere is a short reproducer;\r\n\r\n```py\r\nfrom astroid import parse\r\n\r\n\r\nsource = \"\"\"\r\nX = {\r\n    'A': 'B'\r\n}\r\n\r\nY = {\r\n    **X\r\n}\r\n\r\nKEY = 'A'\r\n\"\"\"\r\n\r\ntree = parse(source)\r\n\r\nfirst_dict = tree.body[0].value\r\nsecond_dict = tree.body[1].value\r\nkey = tree.body[2].value\r\n\r\nprint(f'{first_dict.getitem(key).value = }')\r\nprint(f'{second_dict.getitem(key).value = }')\r\n\r\n\r\n```\r\n\r\nThe current output;\r\n\r\n```\r\n $ python t1.py                                                                                                 3ms\r\nfirst_dict.getitem(key).value = 'B'\r\nTraceback (most recent call last):\r\n  File \"/home/isidentical/projects/astroid/t1.py\", line 23, in <module>\r\n    print(f'{second_dict.getitem(key).value = }')\r\n  File \"/home/isidentical/projects/astroid/astroid/nodes/node_classes.py\", line 2254, in getitem\r\n    return value.getitem(index, context)\r\nAttributeError: 'Name' object has no attribute 'getitem'\r\n```\r\n\r\nExpeceted output;\r\n```\r\n $ python t1.py                                                                                                 4ms\r\nfirst_dict.getitem(key).value = 'B'\r\nsecond_dict.getitem(key).value = 'B'\r\n\r\n```\r\n\n",
    "method": "hierarchical",
    "top_k": 10,
    "results": [
      {
        "rank": 1,
        "score": 0.7531801462173462,
        "content": "def getitem(self, index, context=None):\n        \"\"\"Get an item from this node.\n\n        :param index: The node to use as a subscript index.\n        :type index: Const or Slice\n\n        :raises AstroidTypeError: When the given index cannot be used as a\n            subscript index, or if this node is not subscriptable.\n        :raises AstroidIndexError: If the given index does not exist in the\n            dictionary.\n        \"\"\"\n        for key, value in self.items:\n            # TODO(cpopa): no support for overriding yet, {1:2, **{1: 3}}.\n            if isinstance(key, DictUnpack):\n                try:\n                    return value.getitem(index, context)\n                except (AstroidTypeError, AstroidIndexError):\n                    continue\n            for inferredkey in key.infer(context):\n                if inferredkey is util.Uninferable:\n                    continue\n                if isinstance(inferredkey, Const) and isinstance(index, Const):\n                    if inferredkey.value == index.value:\n                        return value\n\n        raise AstroidIndexError(index)",
        "file_path": "astroid/nodes/node_classes.py",
        "chunk_index": 89,
        "metadata": {
          "func_name": "getitem",
          "chunk_type": "function",
          "relationship": {
            "call": [
              "value.getitem",
              "key.infer",
              "AstroidIndexError"
            ]
          },
          "class_name": "Dict"
        }
      },
      {
        "rank": 2,
        "score": 0.6961362361907959,
        "content": "def getitem(self, index, context=None):\n        new_context = bind_context_to_node(context, self)\n        if not context:\n            context = new_context\n        method = next(self.igetattr(\"__getitem__\", context=context), None)\n        # Create a new CallContext for providing index as an argument.\n        new_context.callcontext = CallContext(args=[index], callee=method)\n        if not isinstance(method, BoundMethod):\n            raise InferenceError(\n                \"Could not find __getitem__ for {node!r}.\", node=self, context=context\n            )\n        if len(method.args.arguments) != 2:  # (self, index)\n            raise AstroidTypeError(\n                \"__getitem__ for {node!r} does not have correct signature\",\n                node=self,\n                context=context,\n            )\n        return next(method.infer_call_result(self, new_context), None)",
        "file_path": "astroid/bases.py",
        "chunk_index": 15,
        "metadata": {
          "func_name": "getitem",
          "chunk_type": "function",
          "relationship": {
            "call": [
              "bind_context_to_node",
              "self.igetattr",
              "CallContext",
              "InferenceError",
              "AstroidTypeError",
              "method.infer_call_result"
            ]
          },
          "class_name": "Instance"
        }
      },
      {
        "rank": 3,
        "score": 0.6714750528335571,
        "content": "def getitem(self, index, context=None):\n        \"\"\"Return the inference of a subscript.\n\n        This is basically looking up the method in the metaclass and calling it.\n\n        :returns: The inferred value of a subscript to this class.\n        :rtype: NodeNG\n\n        :raises AstroidTypeError: If this class does not define a\n            ``__getitem__`` method.\n        \"\"\"\n        try:\n            methods = lookup(self, \"__getitem__\")\n        except AttributeInferenceError as exc:\n            if isinstance(self, ClassDef):\n                # subscripting a class definition may be\n                # achieved thanks to __class_getitem__ method\n                # which is a classmethod defined in the class\n                # that supports subscript and not in the metaclass\n                try:\n                    methods = self.getattr(\"__class_getitem__\")\n                    # Here it is assumed that the __class_getitem__ node is\n                    # a FunctionDef. One possible improvement would be to deal\n                    # with more generic inference.\n                except AttributeInferenceError:\n                    raise AstroidTypeError(node=self, context=context) from exc\n            else:\n                raise AstroidTypeError(node=self, context=context) from exc\n\n        method = methods[0]\n\n        # Create a new callcontext for providing index as an argument.\n        new_context = bind_context_to_node(context, self)\n        new_context.callcontext = CallContext(args=[index], callee=method)\n\n        try:\n            return next(method.infer_call_result(self, new_context), util.Uninferable)\n        except AttributeError:\n            # Starting with python3.9, builtin types list, dict etc...\n            # are subscriptable thanks to __class_getitem___ classmethod.\n            # However in such case the method is bound to an EmptyNode and\n            # EmptyNode doesn't have infer_call_result method yielding to\n            # AttributeError\n            if (\n                isinstance(method, node_classes.EmptyNode)\n                and self.pytype() == \"builtins.type\"\n                and PY39_PLUS\n            ):\n                return self\n            raise\n        except InferenceError:\n            return util.Uninferable",
        "file_path": "astroid/nodes/scoped_nodes/scoped_nodes.py",
        "chunk_index": 96,
        "metadata": {
          "func_name": "getitem",
          "chunk_type": "function",
          "relationship": {
            "call": [
              "lookup",
              "self.getattr",
              "AstroidTypeError",
              "bind_context_to_node",
              "CallContext",
              "method.infer_call_result",
              "self.pytype"
            ]
          },
          "class_name": "ClassDef"
        }
      },
      {
        "rank": 4,
        "score": 0.6660448312759399,
        "content": "def infer_getattr(node, context=None):\n    \"\"\"Understand getattr calls\n\n    If one of the arguments is an Uninferable object, then the\n    result will be an Uninferable object. Otherwise, the normal attribute\n    lookup will be done.\n    \"\"\"\n    obj, attr = _infer_getattr_args(node, context)\n    if (\n        obj is util.Uninferable\n        or attr is util.Uninferable\n        or not hasattr(obj, \"igetattr\")\n    ):\n        return util.Uninferable\n\n    try:\n        return next(obj.igetattr(attr, context=context))\n    except (StopIteration, InferenceError, AttributeInferenceError):\n        if len(node.args) == 3:\n            # Try to infer the default and return it instead.\n            try:\n                return next(node.args[2].infer(context=context))\n            except (StopIteration, InferenceError) as exc:\n                raise UseInferenceDefault from exc\n\n    raise UseInferenceDefault",
        "file_path": "astroid/brain/brain_builtin_inference.py",
        "chunk_index": 11,
        "metadata": {
          "func_name": "infer_getattr",
          "chunk_type": "function",
          "relationship": {
            "call": [
              "_infer_getattr_args",
              "obj.igetattr",
              "node.args[2].infer"
            ]
          },
          "class_name": null
        }
      },
      {
        "rank": 5,
        "score": 0.6591811776161194,
        "content": "def getitem(self, index, context=None):\n        \"\"\"Get an item from this node if subscriptable.\n\n        :param index: The node to use as a subscript index.\n        :type index: Const or Slice\n\n        :raises AstroidTypeError: When the given index cannot be used as a\n            subscript index, or if this node is not subscriptable.\n        \"\"\"\n        if isinstance(index, Const):\n            index_value = index.value\n        elif isinstance(index, Slice):\n            index_value = _infer_slice(index, context=context)\n\n        else:\n            raise AstroidTypeError(\n                f\"Could not use type {type(index)} as subscript index\"\n            )\n\n        try:\n            if isinstance(self.value, (str, bytes)):\n                return Const(self.value[index_value])\n        except IndexError as exc:\n            raise AstroidIndexError(\n                message=\"Index {index!r} out of range\",\n                node=self,\n                index=index,\n                context=context,\n            ) from exc\n        except TypeError as exc:\n            raise AstroidTypeError(\n                message=\"Type error {error!r}\", node=self, index=index, context=context\n            ) from exc\n\n        raise AstroidTypeError(f\"{self!r} (value={self.value})\")",
        "file_path": "astroid/nodes/node_classes.py",
        "chunk_index": 67,
        "metadata": {
          "func_name": "getitem",
          "chunk_type": "function",
          "relationship": {
            "call": [
              "_infer_slice",
              "AstroidTypeError",
              "Const",
              "AstroidIndexError"
            ]
          },
          "class_name": "Const"
        }
      },
      {
        "rank": 6,
        "score": 0.6541242599487305,
        "content": "def infer_subscript(self, context=None):\n    \"\"\"Inference for subscripts\n\n    We're understanding if the index is a Const\n    or a slice, passing the result of inference\n    to the value's `getitem` method, which should\n    handle each supported index type accordingly.\n    \"\"\"\n\n    found_one = False\n    for value in self.value.infer(context):\n        if value is util.Uninferable:\n            yield util.Uninferable\n            return None\n        for index in self.slice.infer(context):\n            if index is util.Uninferable:\n                yield util.Uninferable\n                return None\n\n            # Try to deduce the index value.\n            index_value = _SUBSCRIPT_SENTINEL\n            if value.__class__ == bases.Instance:\n                index_value = index\n            elif index.__class__ == bases.Instance:\n                instance_as_index = helpers.class_instance_as_index(index)\n                if instance_as_index:\n                    index_value = instance_as_index\n            else:\n                index_value = index\n\n            if index_value is _SUBSCRIPT_SENTINEL:\n                raise InferenceError(node=self, context=context)\n\n            try:\n                assigned = value.getitem(index_value, context)\n            except (\n                AstroidTypeError,\n                AstroidIndexError,\n                AttributeInferenceError,\n                AttributeError,\n            ) as exc:\n                raise InferenceError(node=self, context=context) from exc\n\n            # Prevent inferring if the inferred subscript\n            # is the same as the original subscripted object.\n            if self is assigned or assigned is util.Uninferable:\n                yield util.Uninferable\n                return None\n            yield from assigned.infer(context)\n            found_one = True\n\n    if found_one:\n        return dict(node=self, context=context)\n    return None",
        "file_path": "astroid/inference.py",
        "chunk_index": 13,
        "metadata": {
          "func_name": "infer_subscript",
          "chunk_type": "function",
          "relationship": {
            "call": [
              "self.value.infer",
              "self.slice.infer",
              "helpers.class_instance_as_index",
              "InferenceError",
              "value.getitem",
              "assigned.infer"
            ]
          },
          "class_name": null
        }
      },
      {
        "rank": 7,
        "score": 0.6495541334152222,
        "content": "def _infer_map(node, context):\n    \"\"\"Infer all values based on Dict.items\"\"\"\n    values = {}\n    for name, value in node.items:\n        if isinstance(name, nodes.DictUnpack):\n            double_starred = helpers.safe_infer(value, context)\n            if not double_starred:\n                raise InferenceError\n            if not isinstance(double_starred, nodes.Dict):\n                raise InferenceError(node=node, context=context)\n            unpack_items = _infer_map(double_starred, context)\n            values = _update_with_replacement(values, unpack_items)\n        else:\n            key = helpers.safe_infer(name, context=context)\n            value = helpers.safe_infer(value, context=context)\n            if any(not elem for elem in (key, value)):\n                raise InferenceError(node=node, context=context)\n            values = _update_with_replacement(values, {key: value})\n    return values",
        "file_path": "astroid/inference.py",
        "chunk_index": 5,
        "metadata": {
          "func_name": "_infer_map",
          "chunk_type": "function",
          "relationship": {
            "call": [
              "helpers.safe_infer",
              "InferenceError",
              "_infer_map",
              "_update_with_replacement"
            ]
          },
          "class_name": null
        }
      },
      {
        "rank": 8,
        "score": 0.648688793182373,
        "content": "def infer_dict(node, context=None):\n    \"\"\"Try to infer a dict call to a Dict node.\n\n    The function treats the following cases:\n\n        * dict()\n        * dict(mapping)\n        * dict(iterable)\n        * dict(iterable, **kwargs)\n        * dict(mapping, **kwargs)\n        * dict(**kwargs)\n\n    If a case can't be inferred, we'll fallback to default inference.\n    \"\"\"\n    call = arguments.CallSite.from_call(node, context=context)\n    if call.has_invalid_arguments() or call.has_invalid_keywords():\n        raise UseInferenceDefault\n\n    args = call.positional_arguments\n    kwargs = list(call.keyword_arguments.items())\n\n    if not args and not kwargs:\n        # dict()\n        return nodes.Dict()\n    if kwargs and not args:\n        # dict(a=1, b=2, c=4)\n        items = [(nodes.Const(key), value) for key, value in kwargs]\n    elif len(args) == 1 and kwargs:\n        # dict(some_iterable, b=2, c=4)\n        elts = _get_elts(args[0], context)\n        keys = [(nodes.Const(key), value) for key, value in kwargs]\n        items = elts + keys\n    elif len(args) == 1:\n        items = _get_elts(args[0], context)\n    else:\n        raise UseInferenceDefault()\n    value = nodes.Dict(\n        col_offset=node.col_offset, lineno=node.lineno, parent=node.parent\n    )\n    value.postinit(items)\n    return value",
        "file_path": "astroid/brain/brain_builtin_inference.py",
        "chunk_index": 8,
        "metadata": {
          "func_name": "infer_dict",
          "chunk_type": "function",
          "relationship": {
            "call": [
              "arguments.CallSite.from_call",
              "call.has_invalid_arguments",
              "call.has_invalid_keywords",
              "call.keyword_arguments.items",
              "nodes.Dict",
              "nodes.Const",
              "_get_elts",
              "UseInferenceDefault",
              "value.postinit"
            ]
          },
          "class_name": null
        }
      },
      {
        "rank": 9,
        "score": 0.6324405670166016,
        "content": "def infer_attribute(self, context=None):\n    \"\"\"infer an Attribute node by using getattr on the associated object\"\"\"\n    for owner in self.expr.infer(context):\n        if owner is util.Uninferable:\n            yield owner\n            continue\n\n        if not context:\n            context = InferenceContext()\n        else:\n            context = copy_context(context)\n\n        old_boundnode = context.boundnode\n        try:\n            context.boundnode = owner\n            yield from owner.igetattr(self.attrname, context)\n        except (\n            AttributeInferenceError,\n            InferenceError,\n            AttributeError,\n        ):\n            pass\n        finally:\n            context.boundnode = old_boundnode\n    return dict(node=self, context=context)",
        "file_path": "astroid/inference.py",
        "chunk_index": 11,
        "metadata": {
          "func_name": "infer_attribute",
          "chunk_type": "function",
          "relationship": {
            "call": [
              "self.expr.infer",
              "InferenceContext",
              "copy_context",
              "owner.igetattr"
            ]
          },
          "class_name": null
        }
      },
      {
        "rank": 10,
        "score": 0.6305243372917175,
        "content": "def infer_dict_fromkeys(node, context=None):\n    \"\"\"Infer dict.fromkeys\n\n    :param nodes.Call node: dict.fromkeys() call to infer\n    :param context.InferenceContext context: node context\n    :rtype nodes.Dict:\n        a Dictionary containing the values that astroid was able to infer.\n        In case the inference failed for any reason, an empty dictionary\n        will be inferred instead.\n    \"\"\"\n\n    def _build_dict_with_elements(elements):\n        new_node = nodes.Dict(\n            col_offset=node.col_offset, lineno=node.lineno, parent=node.parent\n        )\n        new_node.postinit(elements)\n        return new_node\n\n    call = arguments.CallSite.from_call(node, context=context)\n    if call.keyword_arguments:\n        raise UseInferenceDefault(\"TypeError: int() must take no keyword arguments\")\n    if len(call.positional_arguments) not in {1, 2}:\n        raise UseInferenceDefault(\n            \"TypeError: Needs between 1 and 2 positional arguments\"\n        )\n\n    default = nodes.Const(None)\n    values = call.positional_arguments[0]\n    try:\n        inferred_values = next(values.infer(context=context))\n    except (InferenceError, StopIteration):\n        return _build_dict_with_elements([])\n    if inferred_values is util.Uninferable:\n        return _build_dict_with_elements([])\n\n    # Limit to a couple of potential values, as this can become pretty complicated\n    accepted_iterable_elements = (nodes.Const,)\n    if isinstance(inferred_values, (nodes.List, nodes.Set, nodes.Tuple)):\n        elements = inferred_values.elts\n        for element in elements:\n            if not isinstance(element, accepted_iterable_elements):\n                # Fallback to an empty dict\n                return _build_dict_with_elements([])\n\n        elements_with_value = [(element, default) for element in elements]\n        return _build_dict_with_elements(elements_with_value)\n    if isinstance(inferred_values, nodes.Const) and isinstance(\n        inferred_values.value, (str, bytes)\n    ):\n        elements = [\n            (nodes.Const(element), default) for element in inferred_values.value\n        ]\n        return _build_dict_with_elements(elements)\n    if isinstance(inferred_values, nodes.Dict):\n        keys = inferred_values.itered()\n        for key in keys:\n            if not isinstance(key, accepted_iterable_elements):\n                # Fallback to an empty dict\n                return _build_dict_with_elements([])\n\n        elements_with_value = [(element, default) for element in keys]\n        return _build_dict_with_elements(elements_with_value)\n\n    # Fallback to an empty dictionary\n    return _build_dict_with_elements([])",
        "file_path": "astroid/brain/brain_builtin_inference.py",
        "chunk_index": 26,
        "metadata": {
          "func_name": "infer_dict_fromkeys",
          "chunk_type": "function",
          "relationship": {
            "call": [
              "nodes.Dict",
              "new_node.postinit",
              "arguments.CallSite.from_call",
              "UseInferenceDefault",
              "nodes.Const",
              "values.infer",
              "_build_dict_with_elements",
              "inferred_values.itered"
            ]
          },
          "class_name": null
        }
      }
    ]
  },
  "pylint-dev__astroid-1866": {
    "query": "\"TypeError: unsupported format string passed to NoneType.__format__\" while running type inference in version 2.12.x\n### Steps to reproduce\r\n\r\nI have no concise reproducer. Exception happens every time I run pylint on some internal code, with astroid 2.12.10 and 2.12.12 (debian bookworm). It does _not_ happen with earlier versions of astroid (not with version 2.9). The pylinted code itself is \"valid\", it runs in production here.\r\n\r\n### Current behavior\r\n\r\nWhen running pylint on some code, I get this exception:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3/dist-packages/pylint/utils/ast_walker.py\", line 90, in walk\r\n    callback(astroid)\r\n  File \"/usr/lib/python3/dist-packages/pylint/checkers/classes/special_methods_checker.py\", line 183, in visit_functiondef\r\n    inferred = _safe_infer_call_result(node, node)\r\n  File \"/usr/lib/python3/dist-packages/pylint/checkers/classes/special_methods_checker.py\", line 42, in _safe_infer_call_result\r\n    value = next(inferit)\r\n  File \"/usr/lib/python3/dist-packages/astroid/nodes/scoped_nodes/scoped_nodes.py\", line 1749, in infer_call_result\r\n    yield from returnnode.value.infer(context)\r\n  File \"/usr/lib/python3/dist-packages/astroid/nodes/node_ng.py\", line 159, in infer\r\n    results = list(self._explicit_inference(self, context, **kwargs))\r\n  File \"/usr/lib/python3/dist-packages/astroid/inference_tip.py\", line 45, in _inference_tip_cached\r\n    result = _cache[func, node] = list(func(*args, **kwargs))\r\n  File \"/usr/lib/python3/dist-packages/astroid/brain/brain_builtin_inference.py\", line 956, in _infer_str_format_call\r\n    formatted_string = format_template.format(*pos_values, **keyword_values)\r\nTypeError: unsupported format string passed to NoneType.__format__\r\n```\r\n\r\n### Expected behavior\r\n\r\nTypeError exception should not happen\r\n\r\n### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\r\n\r\n2.12.10,\r\n2.12.12\n",
    "method": "hierarchical",
    "top_k": 10,
    "results": [
      {
        "rank": 1,
        "score": 0.698352038860321,
        "content": "def _infer_str_format_call(\n    node: nodes.Call, context: InferenceContext | None = None\n) -> Iterator[nodes.Const | type[util.Uninferable]]:\n    \"\"\"Return a Const node based on the template and passed arguments.\"\"\"\n    call = arguments.CallSite.from_call(node, context=context)\n    if isinstance(node.func.expr, nodes.Name):\n        value: nodes.Const = helpers.safe_infer(node.func.expr)\n    else:\n        value = node.func.expr\n\n    format_template = value.value\n\n    # Get the positional arguments passed\n    inferred_positional = [\n        helpers.safe_infer(i, context) for i in call.positional_arguments\n    ]\n    if not all(isinstance(i, nodes.Const) for i in inferred_positional):\n        return iter([util.Uninferable])\n    pos_values: list[str] = [i.value for i in inferred_positional]\n\n    # Get the keyword arguments passed\n    inferred_keyword = {\n        k: helpers.safe_infer(v, context) for k, v in call.keyword_arguments.items()\n    }\n    if not all(isinstance(i, nodes.Const) for i in inferred_keyword.values()):\n        return iter([util.Uninferable])\n    keyword_values: dict[str, str] = {k: v.value for k, v in inferred_keyword.items()}\n\n    try:\n        formatted_string = format_template.format(*pos_values, **keyword_values)\n    except (IndexError, KeyError):\n        # If there is an IndexError there are too few arguments to interpolate\n        return iter([util.Uninferable])\n\n    return iter([nodes.const_factory(formatted_string)])",
        "file_path": "astroid/brain/brain_builtin_inference.py",
        "chunk_index": 29,
        "metadata": {
          "func_name": "_infer_str_format_call",
          "chunk_type": "function",
          "relationship": {
            "call": [
              "arguments.CallSite.from_call",
              "helpers.safe_infer",
              "call.keyword_arguments.items",
              "inferred_keyword.values",
              "inferred_keyword.items",
              "format_template.format",
              "nodes.const_factory"
            ]
          },
          "class_name": null
        }
      },
      {
        "rank": 2,
        "score": 0.6912271976470947,
        "content": "def _infer_old_style_string_formatting(\n    instance: nodes.Const, other: nodes.NodeNG, context: InferenceContext\n) -> tuple[type[util.Uninferable] | nodes.Const]:\n    \"\"\"Infer the result of '\"string\" % ...'.\n\n    TODO: Instead of returning Uninferable we should rely\n    on the call to '%' to see if the result is actually uninferable.\n    \"\"\"\n    values = None\n    if isinstance(other, nodes.Tuple):\n        if util.Uninferable in other.elts:\n            return (util.Uninferable,)\n        inferred_positional = [helpers.safe_infer(i, context) for i in other.elts]\n        if all(isinstance(i, nodes.Const) for i in inferred_positional):\n            values = tuple(i.value for i in inferred_positional)\n    elif isinstance(other, nodes.Dict):\n        values: dict[Any, Any] = {}\n        for pair in other.items:\n            key = helpers.safe_infer(pair[0], context)\n            if not isinstance(key, nodes.Const):\n                return (util.Uninferable,)\n            value = helpers.safe_infer(pair[1], context)\n            if not isinstance(value, nodes.Const):\n                return (util.Uninferable,)\n            values[key.value] = value.value\n    elif isinstance(other, nodes.Const):\n        values = other.value\n    else:\n        return (util.Uninferable,)\n\n    try:\n        return (nodes.const_factory(instance.value % values),)\n    except (TypeError, KeyError, ValueError):\n        return (util.Uninferable,)",
        "file_path": "astroid/inference.py",
        "chunk_index": 19,
        "metadata": {
          "func_name": "_infer_old_style_string_formatting",
          "chunk_type": "function",
          "relationship": {
            "call": [
              "helpers.safe_infer",
              "nodes.const_factory"
            ]
          },
          "class_name": null
        }
      },
      {
        "rank": 3,
        "score": 0.6751132011413574,
        "content": "def _is_str_format_call(node: nodes.Call) -> bool:\n    \"\"\"Catch calls to str.format().\"\"\"\n    if not isinstance(node.func, nodes.Attribute) or not node.func.attrname == \"format\":\n        return False\n\n    if isinstance(node.func.expr, nodes.Name):\n        value = helpers.safe_infer(node.func.expr)\n    else:\n        value = node.func.expr\n\n    return isinstance(value, nodes.Const) and isinstance(value.value, str)",
        "file_path": "astroid/brain/brain_builtin_inference.py",
        "chunk_index": 28,
        "metadata": {
          "func_name": "_is_str_format_call",
          "chunk_type": "function",
          "relationship": {
            "call": [
              "helpers.safe_infer"
            ]
          },
          "class_name": null
        }
      },
      {
        "rank": 4,
        "score": 0.6418033242225647,
        "content": "class AstroidTypeError(AstroidError):\n    \"\"\"Raised when a TypeError would be expected in Python code.\"\"\"\n\n    def __init__(\n        self,\n        message: str = \"\",\n        node: nodes.NodeNG | bases.Instance | None = None,\n        index: nodes.Subscript | None = None,\n        context: InferenceContext | None = None,\n        **kws: Any,\n    ) -> None:\n        self.node = node\n        self.index = index\n        self.context = context\n        super().__init__(message, **kws)",
        "file_path": "astroid/exceptions.py",
        "chunk_index": 17,
        "metadata": {
          "class_name": "AstroidTypeError",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "AstroidError"
            ]
          },
          "parent_name": null,
          "private_methods_count": 1,
          "public_methods_count": 0,
          "node_count": 132,
          "chunk_size": 478
        }
      },
      {
        "rank": 5,
        "score": 0.622888445854187,
        "content": "def infer_str(node, context=None):\n    \"\"\"Infer str() calls\n\n    :param nodes.Call node: str() call to infer\n    :param context.InferenceContext: node context\n    :rtype nodes.Const: a Const containing an empty string\n    \"\"\"\n    call = arguments.CallSite.from_call(node, context=context)\n    if call.keyword_arguments:\n        raise UseInferenceDefault(\"TypeError: str() must take no keyword arguments\")\n    try:\n        return nodes.Const(\"\")\n    except (AstroidTypeError, InferenceError) as exc:\n        raise UseInferenceDefault(str(exc)) from exc",
        "file_path": "astroid/brain/brain_builtin_inference.py",
        "chunk_index": 24,
        "metadata": {
          "func_name": "infer_str",
          "chunk_type": "function",
          "relationship": {
            "call": [
              "arguments.CallSite.from_call",
              "UseInferenceDefault",
              "nodes.Const"
            ]
          },
          "class_name": null
        }
      },
      {
        "rank": 6,
        "score": 0.6132132411003113,
        "content": "class FormattedValue(NodeNG):\n    \"\"\"Class representing an :class:`ast.FormattedValue` node.\n\n    Represents a :pep:`498` format string.\n\n    >>> import astroid\n    >>> node = astroid.extract_node('f\"Format {type_}\"')\n    >>> node\n    <JoinedStr l.1 at 0x7f23b2e4ed30>\n    >>> node.values\n    [<Const.str l.1 at 0x7f23b2e4eda0>, <FormattedValue l.1 at 0x7f23b2e4edd8>]\n    \"\"\"\n\n    _astroid_fields = (\"value\", \"format_spec\")\n    _other_fields = (\"conversion\",)\n\n    def __init__(\n        self,\n        lineno: int | None = None,\n        col_offset: int | None = None,\n        parent: NodeNG | None = None,\n        *,\n        end_lineno: int | None = None,\n        end_col_offset: int | None = None,\n    ) -> None:\n        \"\"\"\n        :param lineno: The line that this node appears on in the source code.\n\n        :param col_offset: The column that this node appears on in the\n            source code.\n\n        :param parent: The parent node in the syntax tree.\n\n        :param end_lineno: The last line this node appears on in the source code.\n\n        :param end_col_offset: The end column this node appears on in the\n            source code. Note: This is after the last symbol.\n        \"\"\"\n        self.value: NodeNG\n        \"\"\"The value to be formatted into the string.\"\"\"\n\n        self.conversion: int | None = None  # can be None\n        \"\"\"The type of formatting to be applied to the value.\n\n        .. seealso::\n            :class:`ast.FormattedValue`\n        \"\"\"\n\n        self.format_spec: NodeNG | None = None  # can be None\n        \"\"\"The formatting to be applied to the value.\n\n        .. seealso::\n            :class:`ast.FormattedValue`\n\n        :type: JoinedStr or None\n        \"\"\"\n\n        super().__init__(\n            lineno=lineno,\n            col_offset=col_offset,\n            end_lineno=end_lineno,\n            end_col_offset=end_col_offset,\n            parent=parent,\n        )\n\n",
        "file_path": "astroid/nodes/node_classes.py",
        "chunk_index": 182,
        "metadata": {
          "class_name": "FormattedValue",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "NodeNG"
            ]
          },
          "parent_name": null,
          "private_methods_count": 1,
          "public_methods_count": 2,
          "node_count": 316,
          "chunk_size": 1902
        }
      },
      {
        "rank": 7,
        "score": 0.6031762957572937,
        "content": "def infer_typing_typevar_or_newtype(node, context_itton=None):\n    \"\"\"Infer a typing.TypeVar(...) or typing.NewType(...) call\"\"\"\n    try:\n        func = next(node.func.infer(context=context_itton))\n    except (InferenceError, StopIteration) as exc:\n        raise UseInferenceDefault from exc\n\n    if func.qname() not in TYPING_TYPEVARS_QUALIFIED:\n        raise UseInferenceDefault\n    if not node.args:\n        raise UseInferenceDefault\n    # Cannot infer from a dynamic class name (f-string)\n    if isinstance(node.args[0], JoinedStr):\n        raise UseInferenceDefault\n\n    typename = node.args[0].as_string().strip(\"'\")\n    node = extract_node(TYPING_TYPE_TEMPLATE.format(typename))\n    return node.infer(context=context_itton)",
        "file_path": "astroid/brain/brain_typing.py",
        "chunk_index": 1,
        "metadata": {
          "func_name": "infer_typing_typevar_or_newtype",
          "chunk_type": "function",
          "relationship": {
            "call": [
              "node.func.infer",
              "func.qname",
              "node.args[0].as_string().strip",
              "node.args[0].as_string",
              "extract_node",
              "TYPING_TYPE_TEMPLATE.format",
              "node.infer"
            ]
          },
          "class_name": null
        }
      },
      {
        "rank": 8,
        "score": 0.5935976505279541,
        "content": "class Unknown(_base_nodes.AssignTypeNode):\n    \"\"\"This node represents a node in a constructed AST where\n    introspection is not possible.  At the moment, it's only used in\n    the args attribute of FunctionDef nodes where function signature\n    introspection failed.\n    \"\"\"\n\n    name = \"Unknown\"\n\n\n    def _infer(self, context=None, **kwargs):\n        \"\"\"Inference on an Unknown node immediately terminates.\"\"\"\n        yield util.Uninferable",
        "file_path": "astroid/nodes/node_classes.py",
        "chunk_index": 193,
        "metadata": {
          "class_name": "Unknown",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "_base_nodes.AssignTypeNode"
            ]
          },
          "parent_name": null,
          "private_methods_count": 1,
          "public_methods_count": 1,
          "node_count": 82,
          "chunk_size": 444
        }
      },
      {
        "rank": 9,
        "score": 0.5932272672653198,
        "content": "def infer_typing_cast(\n    node: Call, ctx: context.InferenceContext | None = None\n) -> Iterator[NodeNG]:\n    \"\"\"Infer call to cast() returning same type as casted-from var\"\"\"\n    if not isinstance(node.func, (Name, Attribute)):\n        raise UseInferenceDefault\n\n    try:\n        func = next(node.func.infer(context=ctx))\n    except (InferenceError, StopIteration) as exc:\n        raise UseInferenceDefault from exc\n    if (\n        not isinstance(func, FunctionDef)\n        or func.qname() != \"typing.cast\"\n        or len(node.args) != 2\n    ):\n        raise UseInferenceDefault\n\n    return node.args[1].infer(context=ctx)",
        "file_path": "astroid/brain/brain_typing.py",
        "chunk_index": 13,
        "metadata": {
          "func_name": "infer_typing_cast",
          "chunk_type": "function",
          "relationship": {
            "call": [
              "node.func.infer",
              "func.qname",
              "node.args[1].infer"
            ]
          },
          "class_name": null
        }
      },
      {
        "rank": 10,
        "score": 0.5903385877609253,
        "content": "def visit_uninferable(self, node):\n        return str(node)",
        "file_path": "astroid/nodes/as_string.py",
        "chunk_index": 79,
        "metadata": {
          "func_name": "visit_uninferable",
          "chunk_type": "function",
          "relationship": null,
          "class_name": "AsStringVisitor"
        }
      }
    ]
  },
  "pylint-dev__astroid-1268": {
    "query": "'AsStringVisitor' object has no attribute 'visit_unknown'\n```python\r\n>>> import astroid\r\n>>> astroid.nodes.Unknown().as_string()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/tusharsadhwani/code/marvin-python/venv/lib/python3.9/site-packages/astroid/nodes/node_ng.py\", line 609, in as_string\r\n    return AsStringVisitor()(self)\r\n  File \"/Users/tusharsadhwani/code/marvin-python/venv/lib/python3.9/site-packages/astroid/nodes/as_string.py\", line 56, in __call__\r\n    return node.accept(self).replace(DOC_NEWLINE, \"\\n\")\r\n  File \"/Users/tusharsadhwani/code/marvin-python/venv/lib/python3.9/site-packages/astroid/nodes/node_ng.py\", line 220, in accept\r\n    func = getattr(visitor, \"visit_\" + self.__class__.__name__.lower())\r\nAttributeError: 'AsStringVisitor' object has no attribute 'visit_unknown'\r\n>>> \r\n```\r\n### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\r\n\r\n2.8.6-dev0\n",
    "method": "hierarchical",
    "top_k": 10,
    "results": [
      {
        "rank": 1,
        "score": 0.6261175870895386,
        "content": "ss AsStringVisitor:\n    \"\"\"Visitor to render an Astroid node as a valid python code string\"\"\"\n\n    def __init__(self, indent=\"    \"):\n        self.indent = indent\n\n    def __call__(self, node):\n        \"\"\"Makes this visitor behave as a simple function\"\"\"\n        return node.accept(self).replace(DOC_NEWLINE, \"\\n\")\n\n    def _docs_dedent(self, doc):\n        \"\"\"Stop newlines in docs being indented by self._stmt_list\"\"\"\n        return '\\n{}\"\"\"{}\"\"\"'.format(self.indent, doc.replace(\"\\n\", DOC_NEWLINE))\n\n    def _stmt_list(self, stmts, indent=True):\n        \"\"\"return a list of nodes to string\"\"\"\n        stmts = \"\\n\".join(nstr for nstr in [n.accept(self) for n in stmts] if nstr)\n        if indent:\n            return self.indent + stmts.replace(\"\\n\", \"\\n\" + self.indent)\n\n        return stmts\n\n    def _precedence_parens(self, node, child, is_left=True):\n        \"\"\"Wrap child in parens only if required to keep same semantics\"\"\"\n        if self._should_wrap(node, child, is_left):\n            return f\"({child.accept(self)})\"\n\n        return child.accept(self)\n\n    def _should_wrap(self, node, child, is_left):\n        \"\"\"Wrap child if:\n        - it has lower precedence\n        - same precedence with position opposite to associativity direction\n        \"\"\"\n        node_precedence = node.op_precedence()\n        child_precedence = child.op_precedence()\n\n        if node_precedence > child_precedence:\n            # 3 * (4 + 5)\n            return True\n\n        if (\n            node_precedence == child_precedence\n            and is_left != node.op_left_associative()\n        ):\n            # 3 - (4 - 5)\n            # (2**3)**4\n            return True\n\n        return False\n\n    # visit_<node> methods ###########################################\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _visit_dict(self, node):\n        for key, value in node.items:\n            key = key.accept(self)\n            value = value.accept(self)\n            if key == \"**\":\n                # It can only be a DictUnpack node.\n                yield key + value\n            else:\n                yield f\"{key}: {value}\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @staticmethod\n    def visit_matchsingleton(node: \"MatchSingleton\") -> str:\n        \"\"\"Return an astroid.MatchSingleton node as string.\"\"\"\n        return str(node.value)\n\n\n\n\n\n\n\n    # These aren't for real AST nodes, but for inference objects.\n\n\n\n\n\n\n",
        "file_path": "astroid/nodes/as_string.py",
        "chunk_index": 0,
        "metadata": {
          "class_name": "AsStringVisitor",
          "chunk_type": "class",
          "relationship": null,
          "parent_name": null,
          "private_methods_count": 7,
          "public_methods_count": 81,
          "node_count": 6538,
          "chunk_size": 2391
        }
      },
      {
        "rank": 2,
        "score": 0.609856367111206,
        "content": "s TreeRebuilder:\n    \"\"\"Rebuilds the _ast tree to become an Astroid tree\"\"\"\n\n    def __init__(\n        self, manager: AstroidManager, parser_module: Optional[ParserModule] = None\n    ):\n        self._manager = manager\n        self._global_names: List[Dict[str, List[nodes.Global]]] = []\n        self._import_from_nodes: List[nodes.ImportFrom] = []\n        self._delayed_assattr: List[nodes.AssignAttr] = []\n        self._visit_meths: Dict[\n            Type[\"ast.AST\"], Callable[[\"ast.AST\", NodeNG], NodeNG]\n        ] = {}\n\n        if parser_module is None:\n            self._parser_module = get_parser_module()\n        else:\n            self._parser_module = parser_module\n        self._module = self._parser_module.module\n\n    def _get_doc(self, node: T_Doc) -> Tuple[T_Doc, Optional[str]]:\n        try:\n            if PY37_PLUS and hasattr(node, \"docstring\"):\n                doc = node.docstring\n                return node, doc\n            if node.body and isinstance(node.body[0], self._module.Expr):\n\n                first_value = node.body[0].value\n                if isinstance(first_value, self._module.Str) or (\n                    PY38_PLUS\n                    and isinstance(first_value, self._module.Constant)\n                    and isinstance(first_value.value, str)\n                ):\n                    doc = first_value.value if PY38_PLUS else first_value.s\n                    node.body = node.body[1:]\n                    return node, doc\n        except IndexError:\n            pass  # ast built from scratch\n        return node, None\n\n    def _get_context(\n        self,\n        node: Union[\n            \"ast.Attribute\",\n            \"ast.List\",\n            \"ast.Name\",\n            \"ast.Subscript\",\n            \"ast.Starred\",\n            \"ast.Tuple\",\n        ],\n    ) -> Context:\n        return self._parser_module.context_classes.get(type(node.ctx), Context.Load)\n\n\n    if sys.version_info >= (3, 10):\n\n        @overload\n        def visit(self, node: \"ast.arg\", parent: NodeNG) -> nodes.AssignName:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.arguments\", parent: NodeNG) -> nodes.Arguments:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Assert\", parent: NodeNG) -> nodes.Assert:\n            ...\n\n        @overload\n        def visit(\n            self, node: \"ast.AsyncFunctionDef\", parent: NodeNG\n        ) -> nodes.AsyncFunctionDef:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.AsyncFor\", parent: NodeNG) -> nodes.AsyncFor:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Await\", parent: NodeNG) -> nodes.Await:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.AsyncWith\", parent: NodeNG) -> nodes.AsyncWith:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Assign\", parent: NodeNG) -> nodes.Assign:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.AnnAssign\", parent: NodeNG) -> nodes.AnnAssign:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.AugAssign\", parent: NodeNG) -> nodes.AugAssign:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.BinOp\", parent: NodeNG) -> nodes.BinOp:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.BoolOp\", parent: NodeNG) -> nodes.BoolOp:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Break\", parent: NodeNG) -> nodes.Break:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Call\", parent: NodeNG) -> nodes.Call:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.ClassDef\", parent: NodeNG) -> nodes.ClassDef:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Continue\", parent: NodeNG) -> nodes.Continue:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Compare\", parent: NodeNG) -> nodes.Compare:\n            ...\n\n        @overload\n        def visit(\n            self, node: \"ast.comprehension\", parent: NodeNG\n        ) -> nodes.Comprehension:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Delete\", parent: NodeNG) -> nodes.Delete:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Dict\", parent: NodeNG) -> nodes.Dict:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.DictComp\", parent: NodeNG) -> nodes.DictComp:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Expr\", parent: NodeNG) -> nodes.Expr:\n            ...\n\n        # Not used in Python 3.8+\n        @overload\n        def visit(self, node: \"ast.Ellipsis\", parent: NodeNG) -> nodes.Const:\n            ...\n\n        @overload\n        def visit(\n            self, node: \"ast.ExceptHandler\", parent: NodeNG\n        ) -> nodes.ExceptHandler:\n            ...\n\n        # Not used in Python 3.9+\n        @overload\n        def visit(self, node: \"ast.ExtSlice\", parent: nodes.Subscript) -> nodes.Tuple:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.For\", parent: NodeNG) -> nodes.For:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.ImportFrom\", parent: NodeNG) -> nodes.ImportFrom:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.FunctionDef\", parent: NodeNG) -> nodes.FunctionDef:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.GeneratorExp\", parent: NodeNG) -> nodes.GeneratorExp:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Attribute\", parent: NodeNG) -> nodes.Attribute:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Global\", parent: NodeNG) -> nodes.Global:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.If\", parent: NodeNG) -> nodes.If:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.IfExp\", parent: NodeNG) -> nodes.IfExp:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Import\", parent: NodeNG) -> nodes.Import:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.JoinedStr\", parent: NodeNG) -> nodes.JoinedStr:\n            ...\n\n        @overload\n        def visit(\n            self, node: \"ast.FormattedValue\", parent: NodeNG\n        ) -> nodes.FormattedValue:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.NamedExpr\", parent: NodeNG) -> nodes.NamedExpr:\n            ...\n\n        # Not used in Python 3.9+\n        @overload\n        def visit(self, node: \"ast.Index\", parent: nodes.Subscript) -> NodeNG:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.keyword\", parent: NodeNG) -> nodes.Keyword:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Lambda\", parent: NodeNG) -> nodes.Lambda:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.List\", parent: NodeNG) -> nodes.List:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.ListComp\", parent: NodeNG) -> nodes.ListComp:\n            ...\n\n        @overload\n        def visit(\n            self, node: \"ast.Name\", parent: NodeNG\n        ) -> Union[nodes.Name, nodes.Const, nodes.AssignName, nodes.DelName]:\n            ...\n\n        # Not used in Python 3.8+\n        @overload\n        def visit(self, node: \"ast.NameConstant\", parent: NodeNG) -> nodes.Const:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Nonlocal\", parent: NodeNG) -> nodes.Nonlocal:\n            ...\n\n        # Not used in Python 3.8+\n        @overload\n        def visit(self, node: \"ast.Str\", parent: NodeNG) -> nodes.Const:\n            ...\n\n        # Not used in Python 3.8+\n        @overload\n        def visit(self, node: \"ast.Bytes\", parent: NodeNG) -> nodes.Const:\n            ...\n\n        # Not used in Python 3.8+\n        @overload\n        def visit(self, node: \"ast.Num\", parent: NodeNG) -> nodes.Const:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Constant\", parent: NodeNG) -> nodes.Const:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Pass\", parent: NodeNG) -> nodes.Pass:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Raise\", parent: NodeNG) -> nodes.Raise:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Return\", parent: NodeNG) -> nodes.Return:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Set\", parent: NodeNG) -> nodes.Set:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.SetComp\", parent: NodeNG) -> nodes.SetComp:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Slice\", parent: nodes.Subscript) -> nodes.Slice:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Subscript\", parent: NodeNG) -> nodes.Subscript:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Starred\", parent: NodeNG) -> nodes.Starred:\n            ...\n\n        @overload\n        def visit(\n            self, node: \"ast.Try\", parent: NodeNG\n        ) -> Union[nodes.TryExcept, nodes.TryFinally]:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Tuple\", parent: NodeNG) -> nodes.Tuple:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.UnaryOp\", parent: NodeNG) -> nodes.UnaryOp:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.While\", parent: NodeNG) -> nodes.While:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.With\", parent: NodeNG) -> nodes.With:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Yield\", parent: NodeNG) -> nodes.Yield:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.YieldFrom\", parent: NodeNG) -> nodes.YieldFrom:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Match\", parent: NodeNG) -> nodes.Match:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.match_case\", parent: NodeNG) -> nodes.MatchCase:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.MatchValue\", parent: NodeNG) -> nodes.MatchValue:\n            ...\n\n        @overload\n        def visit(\n            self, node: \"ast.MatchSingleton\", parent: NodeNG\n        ) -> nodes.MatchSingleton:\n            ...\n\n        @overload\n        def visit(\n            self, node: \"ast.MatchSequence\", parent: NodeNG\n        ) -> nodes.MatchSequence:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.MatchMapping\", parent: NodeNG) -> nodes.MatchMapping:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.MatchClass\", parent: NodeNG) -> nodes.MatchClass:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.MatchStar\", parent: NodeNG) -> nodes.MatchStar:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.MatchAs\", parent: NodeNG) -> nodes.MatchAs:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.MatchOr\", parent: NodeNG) -> nodes.MatchOr:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.pattern\", parent: NodeNG) -> nodes.Pattern:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.AST\", parent: NodeNG) -> NodeNG:\n            ...\n\n        @overload\n        def visit(self, node: None, parent: NodeNG) -> None:\n            ...\n\n        def visit(self, node: Optional[\"ast.AST\"], parent: NodeNG) -> Optional[NodeNG]:\n            if node is None:\n                return None\n            cls = node.__class__\n            if cls in self._visit_meths:\n                visit_method = self._visit_meths[cls]\n            else:\n                cls_name = cls.__name__\n                visit_name = \"visit_\" + REDIRECT.get(cls_name, cls_name).lower()\n                visit_method = getattr(self, visit_name)\n                self._visit_meths[cls] = visit_method\n            return visit_method(node, parent)\n\n    else:\n\n        @overload\n        def visit(self, node: \"ast.arg\", parent: NodeNG) -> nodes.AssignName:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.arguments\", parent: NodeNG) -> nodes.Arguments:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Assert\", parent: NodeNG) -> nodes.Assert:\n            ...\n\n        @overload\n        def visit(\n            self, node: \"ast.AsyncFunctionDef\", parent: NodeNG\n        ) -> nodes.AsyncFunctionDef:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.AsyncFor\", parent: NodeNG) -> nodes.AsyncFor:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Await\", parent: NodeNG) -> nodes.Await:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.AsyncWith\", parent: NodeNG) -> nodes.AsyncWith:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Assign\", parent: NodeNG) -> nodes.Assign:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.AnnAssign\", parent: NodeNG) -> nodes.AnnAssign:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.AugAssign\", parent: NodeNG) -> nodes.AugAssign:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.BinOp\", parent: NodeNG) -> nodes.BinOp:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.BoolOp\", parent: NodeNG) -> nodes.BoolOp:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Break\", parent: NodeNG) -> nodes.Break:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Call\", parent: NodeNG) -> nodes.Call:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.ClassDef\", parent: NodeNG) -> nodes.ClassDef:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Continue\", parent: NodeNG) -> nodes.Continue:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Compare\", parent: NodeNG) -> nodes.Compare:\n            ...\n\n        @overload\n        def visit(\n            self, node: \"ast.comprehension\", parent: NodeNG\n        ) -> nodes.Comprehension:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Delete\", parent: NodeNG) -> nodes.Delete:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Dict\", parent: NodeNG) -> nodes.Dict:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.DictComp\", parent: NodeNG) -> nodes.DictComp:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Expr\", parent: NodeNG) -> nodes.Expr:\n            ...\n\n        # Not used in Python 3.8+\n        @overload\n        def visit(self, node: \"ast.Ellipsis\", parent: NodeNG) -> nodes.Const:\n            ...\n\n        @overload\n        def visit(\n            self, node: \"ast.ExceptHandler\", parent: NodeNG\n        ) -> nodes.ExceptHandler:\n            ...\n\n        # Not used in Python 3.9+\n        @overload\n        def visit(self, node: \"ast.ExtSlice\", parent: nodes.Subscript) -> nodes.Tuple:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.For\", parent: NodeNG) -> nodes.For:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.ImportFrom\", parent: NodeNG) -> nodes.ImportFrom:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.FunctionDef\", parent: NodeNG) -> nodes.FunctionDef:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.GeneratorExp\", parent: NodeNG) -> nodes.GeneratorExp:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Attribute\", parent: NodeNG) -> nodes.Attribute:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Global\", parent: NodeNG) -> nodes.Global:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.If\", parent: NodeNG) -> nodes.If:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.IfExp\", parent: NodeNG) -> nodes.IfExp:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Import\", parent: NodeNG) -> nodes.Import:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.JoinedStr\", parent: NodeNG) -> nodes.JoinedStr:\n            ...\n\n        @overload\n        def visit(\n            self, node: \"ast.FormattedValue\", parent: NodeNG\n        ) -> nodes.FormattedValue:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.NamedExpr\", parent: NodeNG) -> nodes.NamedExpr:\n            ...\n\n        # Not used in Python 3.9+\n        @overload\n        def visit(self, node: \"ast.Index\", parent: nodes.Subscript) -> NodeNG:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.keyword\", parent: NodeNG) -> nodes.Keyword:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Lambda\", parent: NodeNG) -> nodes.Lambda:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.List\", parent: NodeNG) -> nodes.List:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.ListComp\", parent: NodeNG) -> nodes.ListComp:\n            ...\n\n        @overload\n        def visit(\n            self, node: \"ast.Name\", parent: NodeNG\n        ) -> Union[nodes.Name, nodes.Const, nodes.AssignName, nodes.DelName]:\n            ...\n\n        # Not used in Python 3.8+\n        @overload\n        def visit(self, node: \"ast.NameConstant\", parent: NodeNG) -> nodes.Const:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Nonlocal\", parent: NodeNG) -> nodes.Nonlocal:\n            ...\n\n        # Not used in Python 3.8+\n        @overload\n        def visit(self, node: \"ast.Str\", parent: NodeNG) -> nodes.Const:\n            ...\n\n        # Not used in Python 3.8+\n        @overload\n        def visit(self, node: \"ast.Bytes\", parent: NodeNG) -> nodes.Const:\n            ...\n\n        # Not used in Python 3.8+\n        @overload\n        def visit(self, node: \"ast.Num\", parent: NodeNG) -> nodes.Const:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Constant\", parent: NodeNG) -> nodes.Const:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Pass\", parent: NodeNG) -> nodes.Pass:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Raise\", parent: NodeNG) -> nodes.Raise:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Return\", parent: NodeNG) -> nodes.Return:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Set\", parent: NodeNG) -> nodes.Set:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.SetComp\", parent: NodeNG) -> nodes.SetComp:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Slice\", parent: nodes.Subscript) -> nodes.Slice:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Subscript\", parent: NodeNG) -> nodes.Subscript:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Starred\", parent: NodeNG) -> nodes.Starred:\n            ...\n\n        @overload\n        def visit(\n            self, node: \"ast.Try\", parent: NodeNG\n        ) -> Union[nodes.TryExcept, nodes.TryFinally]:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Tuple\", parent: NodeNG) -> nodes.Tuple:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.UnaryOp\", parent: NodeNG) -> nodes.UnaryOp:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.While\", parent: NodeNG) -> nodes.While:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.With\", parent: NodeNG) -> nodes.With:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.Yield\", parent: NodeNG) -> nodes.Yield:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.YieldFrom\", parent: NodeNG) -> nodes.YieldFrom:\n            ...\n\n        @overload\n        def visit(self, node: \"ast.AST\", parent: NodeNG) -> NodeNG:\n            ...\n\n        @overload\n        def visit(self, node: None, parent: NodeNG) -> None:\n            ...\n\n        def visit(self, node: Optional[\"ast.AST\"], parent: NodeNG) -> Optional[NodeNG]:\n            if node is None:\n                return None\n            cls = node.__class__\n            if cls in self._visit_meths:\n                visit_method = self._visit_meths[cls]\n            else:\n                cls_name = cls.__name__\n                visit_name = \"visit_\" + REDIRECT.get(cls_name, cls_name).lower()\n                visit_method = getattr(self, visit_name)\n                self._visit_meths[cls] = visit_method\n            return visit_method(node, parent)\n\n    def _save_assignment(self, node: Union[nodes.AssignName, nodes.DelName]) -> None:\n        \"\"\"save assignment situation since node.parent is not available yet\"\"\"\n        if self._global_names and node.name in self._global_names[-1]:\n            node.root().set_local(node.name, node)\n        else:\n            node.parent.set_local(node.name, node)\n\n\n\n\n\n\n\n\n\n\n\n\n    @overload\n    def visit_assignname(\n        self, node: \"ast.AST\", parent: NodeNG, node_name: str\n    ) -> nodes.AssignName:\n        ...\n\n    @overload\n    def visit_assignname(\n        self, node: \"ast.AST\", parent: NodeNG, node_name: None\n    ) -> None:\n        ...\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _visit_dict_items(\n        self, node: \"ast.Dict\", parent: NodeNG, newnode: nodes.Dict\n    ) -> Generator[Tuple[NodeNG, NodeNG], None, None]:\n        for key, value in zip(node.keys, node.values):\n            rebuilt_key: NodeNG\n            rebuilt_value = self.visit(value, newnode)\n            if not key:\n                # Extended unpacking\n                rebuilt_key = nodes.DictUnpack(\n                    rebuilt_value.lineno, rebuilt_value.col_offset, parent\n                )\n            else:\n                rebuilt_key = self.visit(key, newnode)\n            yield rebuilt_key, rebuilt_value\n\n\n\n\n    # Not used in Python 3.8+.\n\n\n    # Not used in Python 3.9+.\n\n    @overload\n    def _visit_for(\n        self, cls: Type[nodes.For], node: \"ast.For\", parent: NodeNG\n    ) -> nodes.For:\n        ...\n\n    @overload\n    def _visit_for(\n        self, cls: Type[nodes.AsyncFor], node: \"ast.AsyncFor\", parent: NodeNG\n    ) -> nodes.AsyncFor:\n        ...\n\n    def _visit_for(\n        self, cls: Type[T_For], node: Union[\"ast.For\", \"ast.AsyncFor\"], parent: NodeNG\n    ) -> T_For:\n        \"\"\"visit a For node by returning a fresh instance of it\"\"\"\n        newnode = cls(node.lineno, node.col_offset, parent)\n        type_annotation = self.check_type_comment(node, parent=newnode)\n        newnode.postinit(\n            target=self.visit(node.target, newnode),\n            iter=self.visit(node.iter, newnode),\n            body=[self.visit(child, newnode) for child in node.body],\n            orelse=[self.visit(child, newnode) for child in node.orelse],\n            type_annotation=type_annotation,\n        )\n        return newnode\n\n\n\n    @overload\n    def _visit_functiondef(\n        self, cls: Type[nodes.FunctionDef], node: \"ast.FunctionDef\", parent: NodeNG\n    ) -> nodes.FunctionDef:\n        ...\n\n    @overload\n    def _visit_functiondef(\n        self,\n        cls: Type[nodes.AsyncFunctionDef],\n        node: \"ast.AsyncFunctionDef\",\n        parent: NodeNG,\n    ) -> nodes.AsyncFunctionDef:\n        ...\n\n    def _visit_functiondef(\n        self,\n        cls: Type[T_Function],\n        node: Union[\"ast.FunctionDef\", \"ast.AsyncFunctionDef\"],\n        parent: NodeNG,\n    ) -> T_Function:\n        \"\"\"visit an FunctionDef node to become astroid\"\"\"\n        self._global_names.append({})\n        node, doc = self._get_doc(node)\n\n        lineno = node.lineno\n        if PY38_PLUS and node.decorator_list:\n            # Python 3.8 sets the line number of a decorated function\n            # to be the actual line number of the function, but the\n            # previous versions expected the decorator's line number instead.\n            # We reset the function's line number to that of the\n            # first decorator to maintain backward compatibility.\n            # It's not ideal but this discrepancy was baked into\n            # the framework for *years*.\n            lineno = node.decorator_list[0].lineno\n\n        newnode = cls(node.name, doc, lineno, node.col_offset, parent)\n        decorators = self.visit_decorators(node, newnode)\n        returns: Optional[NodeNG]\n        if node.returns:\n            returns = self.visit(node.returns, newnode)\n        else:\n            returns = None\n\n        type_comment_args = type_comment_returns = None\n        type_comment_annotation = self.check_function_type_comment(node, newnode)\n        if type_comment_annotation:\n            type_comment_returns, type_comment_args = type_comment_annotation\n        newnode.postinit(\n            args=self.visit(node.args, newnode),\n            body=[self.visit(child, newnode) for child in node.body],\n            decorators=decorators,\n            returns=returns,\n            type_comment_returns=type_comment_returns,\n            type_comment_args=type_comment_args,\n        )\n        self._global_names.pop()\n        return newnode\n\n\n\n\n\n\n\n\n\n\n\n    # Not used in Python 3.9+.\n\n\n\n\n\n\n    # Not used in Python 3.8+.\n\n\n\n    # Not used in Python 3.8+.\n\n    # Not used in Python 3.8+\n    visit_bytes = visit_str\n\n    # Not used in Python 3.8+.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @overload\n    def _visit_with(\n        self, cls: Type[nodes.With], node: \"ast.With\", parent: NodeNG\n    ) -> nodes.With:\n        ...\n\n    @overload\n    def _visit_with(\n        self, cls: Type[nodes.AsyncWith], node: \"ast.AsyncWith\", parent: NodeNG\n    ) -> nodes.AsyncWith:\n        ...\n\n    def _visit_with(\n        self,\n        cls: Type[T_With],\n        node: Union[\"ast.With\", \"ast.AsyncWith\"],\n        parent: NodeNG,\n    ) -> T_With:\n        newnode = cls(node.lineno, node.col_offset, parent)\n\n        def visit_child(child: \"ast.withitem\") -> Tuple[NodeNG, Optional[NodeNG]]:\n            expr = self.visit(child.context_expr, newnode)\n            var = self.visit(child.optional_vars, newnode)\n            return expr, var\n\n        type_annotation = self.check_type_comment(node, parent=newnode)\n        newnode.postinit(\n            items=[visit_child(child) for child in node.items],\n            body=[self.visit(child, newnode) for child in node.body],\n            type_annotation=type_annotation,\n        )\n        return newnode\n\n\n\n\n    if sys.version_info >= (3, 10):\n\n        def visit_match(self, node: \"ast.Match\", parent: NodeNG) -> nodes.Match:\n            newnode = nodes.Match(node.lineno, node.col_offset, parent)\n            newnode.postinit(\n                subject=self.visit(node.subject, newnode),\n                cases=[self.visit(case, newnode) for case in node.cases],\n            )\n            return newnode\n\n        def visit_matchcase(\n            self, node: \"ast.match_case\", parent: NodeNG\n        ) -> nodes.MatchCase:\n            newnode = nodes.MatchCase(parent=parent)\n            newnode.postinit(\n                pattern=self.visit(node.pattern, newnode),\n                guard=self.visit(node.guard, newnode),\n                body=[self.visit(child, newnode) for child in node.body],\n            )\n            return newnode\n\n        def visit_matchvalue(\n            self, node: \"ast.MatchValue\", parent: NodeNG\n        ) -> nodes.MatchValue:\n            newnode = nodes.MatchValue(node.lineno, node.col_offset, parent)\n            newnode.postinit(value=self.visit(node.value, newnode))\n            return newnode\n\n        def visit_matchsingleton(\n            self, node: \"ast.MatchSingleton\", parent: NodeNG\n        ) -> nodes.MatchSingleton:\n            return nodes.MatchSingleton(\n                value=node.value,\n                lineno=node.lineno,\n                col_offset=node.col_offset,\n                parent=parent,\n            )\n\n        def visit_matchsequence(\n            self, node: \"ast.MatchSequence\", parent: NodeNG\n        ) -> nodes.MatchSequence:\n            newnode = nodes.MatchSequence(node.lineno, node.col_offset, parent)\n            newnode.postinit(\n                patterns=[self.visit(pattern, newnode) for pattern in node.patterns]\n            )\n            return newnode\n\n        def visit_matchmapping(\n            self, node: \"ast.MatchMapping\", parent: NodeNG\n        ) -> nodes.MatchMapping:\n            newnode = nodes.MatchMapping(node.lineno, node.col_offset, parent)\n            # Add AssignName node for 'node.name'\n            # https://bugs.python.org/issue43994\n            newnode.postinit(\n                keys=[self.visit(child, newnode) for child in node.keys],\n                patterns=[self.visit(pattern, newnode) for pattern in node.patterns],\n                rest=self.visit_assignname(node, newnode, node.rest),\n            )\n            return newnode\n\n        def visit_matchclass(\n            self, node: \"ast.MatchClass\", parent: NodeNG\n        ) -> nodes.MatchClass:\n            newnode = nodes.MatchClass(node.lineno, node.col_offset, parent)\n            newnode.postinit(\n                cls=self.visit(node.cls, newnode),\n                patterns=[self.visit(pattern, newnode) for pattern in node.patterns],\n                kwd_attrs=node.kwd_attrs,\n                kwd_patterns=[\n                    self.visit(pattern, newnode) for pattern in node.kwd_patterns\n                ],\n            )\n            return newnode\n\n        def visit_matchstar(\n            self, node: \"ast.MatchStar\", parent: NodeNG\n        ) -> nodes.MatchStar:\n            newnode = nodes.MatchStar(node.lineno, node.col_offset, parent)\n            # Add AssignName node for 'node.name'\n            # https://bugs.python.org/issue43994\n            newnode.postinit(name=self.visit_assignname(node, newnode, node.name))\n            return newnode\n\n        def visit_matchas(self, node: \"ast.MatchAs\", parent: NodeNG) -> nodes.MatchAs:\n            newnode = nodes.MatchAs(node.lineno, node.col_offset, parent)\n            # Add AssignName node for 'node.name'\n            # https://bugs.python.org/issue43994\n            newnode.postinit(\n                pattern=self.visit(node.pattern, newnode),\n                name=self.visit_assignname(node, newnode, node.name),\n            )\n            return newnode\n\n        def visit_matchor(self, node: \"ast.MatchOr\", parent: NodeNG) -> nodes.MatchOr:\n            newnode = nodes.MatchOr(node.lineno, node.col_offset, parent)\n            newnode.postinit(\n                patterns=[self.visit(pattern, newnode) for pattern in node.patterns]\n            )\n            return newnode",
        "file_path": "astroid/rebuilder.py",
        "chunk_index": 0,
        "metadata": {
          "class_name": "TreeRebuilder",
          "chunk_type": "class",
          "relationship": null,
          "parent_name": null,
          "private_methods_count": 8,
          "public_methods_count": 70,
          "node_count": 16937,
          "chunk_size": 30248
        }
      },
      {
        "rank": 3,
        "score": 0.6093612313270569,
        "content": " visit_assignattr(self, node):\n        \"\"\"return an astroid.AssAttr node as string\"\"\"\n        return self.visit_attribute(node)\n\n ",
        "file_path": "astroid/nodes/as_string.py",
        "chunk_index": 5,
        "metadata": {
          "func_name": "visit_assignattr",
          "chunk_type": "function",
          "relationship": {
            "call": [
              "self.visit_attribute"
            ]
          },
          "class_name": "AsStringVisitor"
        }
      },
      {
        "rank": 4,
        "score": 0.6079879999160767,
        "content": "ss Unknown(mixins.AssignTypeMixin, NodeNG):\n    \"\"\"This node represents a node in a constructed AST where\n    introspection is not possible.  At the moment, it's only used in\n    the args attribute of FunctionDef nodes where function signature\n    introspection failed.\n    \"\"\"\n\n    name = \"Unknown\"\n\n\n    def _infer(self, context=None, **kwargs):\n        \"\"\"Inference on an Unknown node immediately terminates.\"\"\"\n        yield util.Uninferable\n\n",
        "file_path": "astroid/nodes/node_classes.py",
        "chunk_index": 196,
        "metadata": {
          "class_name": "Unknown",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "mixins.AssignTypeMixin",
              "NodeNG"
            ]
          },
          "parent_name": null,
          "private_methods_count": 1,
          "public_methods_count": 1,
          "node_count": 72,
          "chunk_size": 447
        }
      },
      {
        "rank": 5,
        "score": 0.5998989939689636,
        "content": "ef ast_from_string(self, data, modname=\"\", filepath=None):\n        \"\"\"Given some source code as a string, return its corresponding astroid object\"\"\"\n        # pylint: disable=import-outside-toplevel; circular import\n        from astroid.builder import AstroidBuilder\n\n        return AstroidBuilder(self).string_build(data, modname, filepath)\n",
        "file_path": "astroid/manager.py",
        "chunk_index": 4,
        "metadata": {
          "func_name": "ast_from_string",
          "chunk_type": "function",
          "relationship": {
            "call": [
              "AstroidBuilder(self).string_build",
              "AstroidBuilder"
            ]
          },
          "class_name": "AstroidManager"
        }
      },
      {
        "rank": 6,
        "score": 0.5935428738594055,
        "content": "lass AstroidImportError(AstroidBuildingError):\n    \"\"\"Exception class used when a module can't be imported by astroid.\"\"\"\n",
        "file_path": "astroid/exceptions.py",
        "chunk_index": 2,
        "metadata": {
          "class_name": "AstroidImportError",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "AstroidBuildingError"
            ]
          },
          "parent_name": null,
          "private_methods_count": 0,
          "public_methods_count": 0,
          "node_count": 14,
          "chunk_size": 122
        }
      },
      {
        "rank": 7,
        "score": 0.5929057002067566,
        "content": "lass AstroidSyntaxError(AstroidBuildingError):\n    \"\"\"Exception class used when a module can't be parsed.\"\"\"\n",
        "file_path": "astroid/exceptions.py",
        "chunk_index": 4,
        "metadata": {
          "class_name": "AstroidSyntaxError",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "AstroidBuildingError"
            ]
          },
          "parent_name": null,
          "private_methods_count": 0,
          "public_methods_count": 0,
          "node_count": 14,
          "chunk_size": 109
        }
      },
      {
        "rank": 8,
        "score": 0.582190752029419,
        "content": "ef ast_from_module(self, module: types.ModuleType, modname: str = None):\n        \"\"\"given an imported module, return the astroid object\"\"\"\n        modname = modname or module.__name__\n        if modname in self.astroid_cache:\n            return self.astroid_cache[modname]\n        try:\n            # some builtin modules don't have __file__ attribute\n            filepath = module.__file__\n            if is_python_source(filepath):\n                return self.ast_from_file(filepath, modname)\n        except AttributeError:\n            pass\n\n        # pylint: disable=import-outside-toplevel; circular import\n        from astroid.builder import AstroidBuilder\n\n        return AstroidBuilder(self).module_build(module, modname)\n",
        "file_path": "astroid/manager.py",
        "chunk_index": 8,
        "metadata": {
          "func_name": "ast_from_module",
          "chunk_type": "function",
          "relationship": {
            "call": [
              "is_python_source",
              "self.ast_from_file",
              "AstroidBuilder(self).module_build",
              "AstroidBuilder"
            ]
          },
          "class_name": "AstroidManager"
        }
      },
      {
        "rank": 9,
        "score": 0.5786409378051758,
        "content": "lass AstroidTypeError(AstroidError):\n    \"\"\"Raised when a TypeError would be expected in Python code.\"\"\"\n",
        "file_path": "astroid/exceptions.py",
        "chunk_index": 17,
        "metadata": {
          "class_name": "AstroidTypeError",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "AstroidError"
            ]
          },
          "parent_name": null,
          "private_methods_count": 0,
          "public_methods_count": 0,
          "node_count": 14,
          "chunk_size": 105
        }
      },
      {
        "rank": 10,
        "score": 0.575061559677124,
        "content": " visit_dict(self, node):\n        \"\"\"return an astroid.Dict node as string\"\"\"\n        return \"{%s}\" % \", \".join(self._visit_dict(node))\n\n ",
        "file_path": "astroid/nodes/as_string.py",
        "chunk_index": 24,
        "metadata": {
          "func_name": "visit_dict",
          "chunk_type": "function",
          "relationship": {
            "call": [
              "\", \".join",
              "self._visit_dict"
            ]
          },
          "class_name": "AsStringVisitor"
        }
      }
    ]
  },
  "pyvista__pyvista-4315": {
    "query": "Rectilinear grid does not allow Sequences as inputs\n### Describe the bug, what's wrong, and what you expected.\r\n\r\nRectilinear grid gives an error when `Sequence`s are passed in, but `ndarray` are ok.\r\n\r\n### Steps to reproduce the bug.\r\n\r\nThis doesn't work\r\n```python\r\nimport pyvista as pv\r\npv.RectilinearGrid([0, 1], [0, 1], [0, 1])\r\n```\r\n\r\nThis works\r\n```py\r\nimport pyvista as pv\r\nimport numpy as np\r\npv.RectilinearGrid(np.ndarray([0, 1]), np.ndarray([0, 1]), np.ndarray([0, 1]))\r\n```\r\n### System Information\r\n\r\n```shell\r\n--------------------------------------------------------------------------------\r\n  Date: Wed Apr 19 20:15:10 2023 UTC\r\n\r\n                OS : Linux\r\n            CPU(s) : 2\r\n           Machine : x86_64\r\n      Architecture : 64bit\r\n       Environment : IPython\r\n        GPU Vendor : Mesa/X.org\r\n      GPU Renderer : llvmpipe (LLVM 11.0.1, 256 bits)\r\n       GPU Version : 4.5 (Core Profile) Mesa 20.3.5\r\n\r\n  Python 3.11.2 (main, Mar 23 2023, 17:12:29) [GCC 10.2.1 20210110]\r\n\r\n           pyvista : 0.38.5\r\n               vtk : 9.2.6\r\n             numpy : 1.24.2\r\n           imageio : 2.27.0\r\n            scooby : 0.7.1\r\n             pooch : v1.7.0\r\n        matplotlib : 3.7.1\r\n           IPython : 8.12.0\r\n--------------------------------------------------------------------------------\r\n```\r\n\r\n\r\n### Screenshots\r\n\r\n_No response_\n",
    "method": "hierarchical",
    "top_k": 10,
    "results": [
      {
        "rank": 1,
        "score": 0.7194738388061523,
        "content": "class RectilinearGrid(_vtk.vtkRectilinearGrid, Grid, RectilinearGridFilters):\n    \"\"\"Dataset with variable spacing in the three coordinate directions.\n\n    Can be initialized in several ways:\n\n    * Create empty grid\n    * Initialize from a ``vtk.vtkRectilinearGrid`` object\n    * Initialize directly from the point arrays\n\n    Parameters\n    ----------\n    uinput : str, pathlib.Path, vtk.vtkRectilinearGrid, numpy.ndarray, optional\n        Filename, dataset, or array to initialize the rectilinear grid from. If a\n        filename is passed, pyvista will attempt to load it as a\n        :class:`RectilinearGrid`. If passed a ``vtk.vtkRectilinearGrid``, it\n        will be wrapped. If a :class:`numpy.ndarray` is passed, this will be\n        loaded as the x range.\n\n    y : numpy.ndarray, optional\n        Coordinates of the points in y direction. If this is passed, ``uinput``\n        must be a :class:`numpy.ndarray`.\n\n    z : numpy.ndarray, optional\n        Coordinates of the points in z direction. If this is passed, ``uinput``\n        and ``y`` must be a :class:`numpy.ndarray`.\n\n    check_duplicates : bool, optional\n        Check for duplications in any arrays that are passed. Defaults to\n        ``False``. If ``True``, an error is raised if there are any duplicate\n        values in any of the array-valued input arguments.\n\n    deep : bool, optional\n        Whether to deep copy a ``vtk.vtkRectilinearGrid`` object.\n        Default is ``False``.  Keyword only.\n\n    Examples\n    --------\n    >>> import pyvista\n    >>> import vtk\n    >>> import numpy as np\n\n    Create an empty grid.\n\n    >>> grid = pyvista.RectilinearGrid()\n\n    Initialize from a vtk.vtkRectilinearGrid object\n\n    >>> vtkgrid = vtk.vtkRectilinearGrid()\n    >>> grid = pyvista.RectilinearGrid(vtkgrid)\n\n    Create from NumPy arrays.\n\n    >>> xrng = np.arange(-10, 10, 2)\n    >>> yrng = np.arange(-10, 10, 5)\n    >>> zrng = np.arange(-10, 10, 1)\n    >>> grid = pyvista.RectilinearGrid(xrng, yrng, zrng)\n    >>> grid.plot(show_edges=True)\n\n    \"\"\"\n\n    _WRITERS = {'.vtk': _vtk.vtkRectilinearGridWriter, '.vtr': _vtk.vtkXMLRectilinearGridWriter}\n\n    def __init__(self, *args, check_duplicates=False, deep=False, **kwargs):\n        \"\"\"Initialize the rectilinear grid.\"\"\"\n        super().__init__()\n\n        if len(args) == 1:\n            if isinstance(args[0], _vtk.vtkRectilinearGrid):\n                if deep:\n                    self.deep_copy(args[0])\n                else:\n                    self.shallow_copy(args[0])\n            elif isinstance(args[0], (str, pathlib.Path)):\n                self._from_file(args[0], **kwargs)\n            elif isinstance(args[0], np.ndarray):\n                self._from_arrays(args[0], None, None, check_duplicates)\n            else:\n                raise TypeError(f'Type ({type(args[0])}) not understood by `RectilinearGrid`')\n\n        elif len(args) == 3 or len(args) == 2:\n            arg0_is_arr = isinstance(args[0], np.ndarray)\n            arg1_is_arr = isinstance(args[1], np.ndarray)\n            if len(args) == 3:\n                arg2_is_arr = isinstance(args[2], np.ndarray)\n            else:\n                arg2_is_arr = False\n\n            if all([arg0_is_arr, arg1_is_arr, arg2_is_arr]):\n                self._from_arrays(args[0], args[1], args[2], check_duplicates)\n            elif all([arg0_is_arr, arg1_is_arr]):\n                self._from_arrays(args[0], args[1], None, check_duplicates)\n            else:\n                raise TypeError(\"Arguments not understood by `RectilinearGrid`.\")\n\n    def __repr__(self):\n        \"\"\"Return the default representation.\"\"\"\n        return DataSet.__repr__(self)\n\n    def __str__(self):\n        \"\"\"Return the str representation.\"\"\"\n        return DataSet.__str__(self)\n\n    def _update_dimensions(self):\n        \"\"\"Update the dimensions if coordinates have changed.\"\"\"\n        return self.SetDimensions(len(self.x), len(self.y), len(self.z))\n\n    def _from_arrays(\n        self, x: np.ndarray, y: np.ndarray, z: np.ndarray, check_duplicates: bool = False\n    ):\n        \"\"\"Create VTK rectilinear grid directly from numpy arrays.\n\n        Each array gives the uniques coordinates of the mesh along each axial\n        direction. To help ensure you are using this correctly, we take the unique\n        values of each argument.\n\n        Parameters\n        ----------\n        x : numpy.ndarray\n            Coordinates of the points in x direction.\n\n        y : numpy.ndarray\n            Coordinates of the points in y direction.\n\n        z : numpy.ndarray\n            Coordinates of the points in z direction.\n\n        check_duplicates : bool, optional\n            Check for duplications in any arrays that are passed.\n\n        \"\"\"\n        # Set the coordinates along each axial direction\n        # Must at least be an x array\n        if check_duplicates:\n            raise_has_duplicates(x)\n\n        # edges are shown as triangles if x is not floating point\n        if not np.issubdtype(x.dtype, np.floating):\n            x = x.astype(float)\n        self.SetXCoordinates(helpers.convert_array(x.ravel()))\n        if y is not None:\n            if check_duplicates:\n                raise_has_duplicates(y)\n            if not np.issubdtype(y.dtype, np.floating):\n                y = y.astype(float)\n            self.SetYCoordinates(helpers.convert_array(y.ravel()))\n        if z is not None:\n            if check_duplicates:\n                raise_has_duplicates(z)\n            if not np.issubdtype(z.dtype, np.floating):\n                z = z.astype(float)\n            self.SetZCoordinates(helpers.convert_array(z.ravel()))\n        # Ensure dimensions are properly set\n        self._update_dimensions()\n\n    @property\n    def meshgrid(self) -> list:\n        \"\"\"Return a meshgrid of numpy arrays for this mesh.\n\n        This simply returns a :func:`numpy.meshgrid` of the\n        coordinates for this mesh in ``ij`` indexing. These are a copy\n        of the points of this mesh.\n\n        \"\"\"\n        return np.meshgrid(self.x, self.y, self.z, indexing='ij')\n\n    @property  # type: ignore\n    def points(self) -> np.ndarray:  # type: ignore\n        \"\"\"Return a copy of the points as an n by 3 numpy array.\n\n        Notes\n        -----\n        Points of a :class:`pyvista.RectilinearGrid` cannot be\n        set. Set point coordinates with :attr:`RectilinearGrid.x`,\n        :attr:`RectilinearGrid.y`, or :attr:`RectilinearGrid.z`.\n\n        Examples\n        --------\n        >>> import numpy as np\n        >>> import pyvista\n        >>> xrng = np.arange(-10, 10, 10, dtype=float)\n        >>> yrng = np.arange(-10, 10, 10, dtype=float)\n        >>> zrng = np.arange(-10, 10, 10, dtype=float)\n        >>> grid = pyvista.RectilinearGrid(xrng, yrng, zrng)\n        >>> grid.points\n        array([[-10., -10., -10.],\n               [  0., -10., -10.],\n               [-10.,   0., -10.],\n               [  0.,   0., -10.],\n               [-10., -10.,   0.],\n               [  0., -10.,   0.],\n               [-10.,   0.,   0.],\n               [  0.,   0.,   0.]])\n\n        \"\"\"\n        xx, yy, zz = self.meshgrid\n        return np.c_[xx.ravel(order='F'), yy.ravel(order='F'), zz.ravel(order='F')]\n\n    @points.setter\n    def points(self, points):\n        \"\"\"Raise an AttributeError.\n\n        This setter overrides the base class's setter to ensure a user\n        does not attempt to set them.\n        \"\"\"\n        raise AttributeError(\n            \"The points cannot be set. The points of \"\n            \"`RectilinearGrid` are defined in each axial direction. Please \"\n            \"use the `x`, `y`, and `z` setters individually.\"\n        )\n\n    @property\n    def x(self) -> np.ndarray:\n        \"\"\"Return or set the coordinates along the X-direction.\n\n        Examples\n        --------\n        Return the x coordinates of a RectilinearGrid.\n\n        >>> import numpy as np\n        >>> import pyvista\n        >>> xrng = np.arange(-10, 10, 10, dtype=float)\n        >>> yrng = np.arange(-10, 10, 10, dtype=float)\n        >>> zrng = np.arange(-10, 10, 10, dtype=float)\n        >>> grid = pyvista.RectilinearGrid(xrng, yrng, zrng)\n        >>> grid.x\n        array([-10.,   0.])\n\n        Set the x coordinates of a RectilinearGrid.\n\n        >>> grid.x = [-10.0, 0.0, 10.0]\n        >>> grid.x\n        array([-10.,   0.,  10.])\n\n        \"\"\"\n        return helpers.convert_array(self.GetXCoordinates())\n\n    @x.setter\n    def x(self, coords: Sequence):\n        \"\"\"Set the coordinates along the X-direction.\"\"\"\n        self.SetXCoordinates(helpers.convert_array(coords))\n        self._update_dimensions()\n        self.Modified()\n\n    @property\n    def y(self) -> np.ndarray:\n        \"\"\"Return or set the coordinates along the Y-direction.\n\n        Examples\n        --------\n        Return the y coordinates of a RectilinearGrid.\n\n        >>> import numpy as np\n        >>> import pyvista\n        >>> xrng = np.arange(-10, 10, 10, dtype=float)\n        >>> yrng = np.arange(-10, 10, 10, dtype=float)\n        >>> zrng = np.arange(-10, 10, 10, dtype=float)\n        >>> grid = pyvista.RectilinearGrid(xrng, yrng, zrng)\n        >>> grid.y\n        array([-10.,   0.])\n\n        Set the y coordinates of a RectilinearGrid.\n\n        >>> grid.y = [-10.0, 0.0, 10.0]\n        >>> grid.y\n        array([-10.,   0.,  10.])\n\n        \"\"\"\n        return helpers.convert_array(self.GetYCoordinates())\n\n    @y.setter\n    def y(self, coords: Sequence):\n        \"\"\"Set the coordinates along the Y-direction.\"\"\"\n        self.SetYCoordinates(helpers.convert_array(coords))\n        self._update_dimensions()\n        self.Modified()\n\n    @property\n    def z(self) -> np.ndarray:\n        \"\"\"Return or set the coordinates along the Z-direction.\n\n        Examples\n        --------\n        Return the z coordinates of a RectilinearGrid.\n\n        >>> import numpy as np\n        >>> import pyvista\n        >>> xrng = np.arange(-10, 10, 10, dtype=float)\n        >>> yrng = np.arange(-10, 10, 10, dtype=float)\n        >>> zrng = np.arange(-10, 10, 10, dtype=float)\n        >>> grid = pyvista.RectilinearGrid(xrng, yrng, zrng)\n        >>> grid.z\n        array([-10.,   0.])\n\n        Set the z coordinates of a RectilinearGrid.\n\n        >>> grid.z = [-10.0, 0.0, 10.0]\n        >>> grid.z\n        array([-10.,   0.,  10.])\n\n        \"\"\"\n        return helpers.convert_array(self.GetZCoordinates())\n\n    @z.setter\n    def z(self, coords: Sequence):\n        \"\"\"Set the coordinates along the Z-direction.\"\"\"\n        self.SetZCoordinates(helpers.convert_array(coords))\n        self._update_dimensions()\n        self.Modified()\n\n    @Grid.dimensions.setter  # type: ignore\n    def dimensions(self, dims):\n        \"\"\"Do not let the dimensions of the RectilinearGrid be set.\"\"\"\n        raise AttributeError(\n            \"The dimensions of a `RectilinearGrid` are implicitly \"\n            \"defined and thus cannot be set.\"\n        )\n",
        "file_path": "pyvista/core/grid.py",
        "chunk_index": 1,
        "metadata": {
          "class_name": "RectilinearGrid",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "_vtk.vtkRectilinearGrid",
              "Grid",
              "RectilinearGridFilters"
            ]
          },
          "parent_name": null,
          "private_methods_count": 5,
          "public_methods_count": 1,
          "node_count": 1487,
          "chunk_size": 10834
        }
      },
      {
        "rank": 2,
        "score": 0.6751937866210938,
        "content": "def cast_to_structured_grid(self) -> 'pyvista.StructuredGrid':\n        \"\"\"Cast this rectilinear grid to a structured grid.\n\n        Returns\n        -------\n        pyvista.StructuredGrid\n            This grid as a structured grid.\n\n        \"\"\"\n        alg = _vtk.vtkRectilinearGridToPointSet()\n        alg.SetInputData(self)\n        alg.Update()\n        return _get_output(alg)",
        "file_path": "pyvista/core/grid.py",
        "chunk_index": 2,
        "metadata": {
          "func_name": "cast_to_structured_grid",
          "chunk_type": "function",
          "relationship": {
            "call": [
              "_vtk.vtkRectilinearGridToPointSet",
              "alg.SetInputData",
              "alg.Update",
              "_get_output"
            ]
          },
          "class_name": "RectilinearGrid"
        }
      },
      {
        "rank": 3,
        "score": 0.6711309552192688,
        "content": "def load_rectilinear():\n    \"\"\"Load a sample uniform grid.\n\n    Returns\n    -------\n    pyvista.RectilinearGrid\n        Dataset.\n\n    Examples\n    --------\n    >>> from pyvista import examples\n    >>> dataset = examples.load_rectilinear()\n    >>> dataset.plot()\n\n    \"\"\"\n    return pyvista.RectilinearGrid(rectfile)",
        "file_path": "pyvista/examples/examples.py",
        "chunk_index": 4,
        "metadata": {
          "func_name": "load_rectilinear",
          "chunk_type": "function",
          "relationship": {
            "call": [
              "pyvista.RectilinearGrid"
            ]
          },
          "class_name": null
        }
      },
      {
        "rank": 4,
        "score": 0.6596612930297852,
        "content": "def cast_to_rectilinear_grid(self) -> 'RectilinearGrid':\n        \"\"\"Cast this uniform grid to a rectilinear grid.\n\n        Returns\n        -------\n        pyvista.RectilinearGrid\n            This uniform grid as a rectilinear grid.\n\n        \"\"\"\n\n        def gen_coords(i):\n            coords = (\n                np.cumsum(np.insert(np.full(self.dimensions[i] - 1, self.spacing[i]), 0, 0))\n                + self.origin[i]\n            )\n            return coords\n\n        xcoords = gen_coords(0)\n        ycoords = gen_coords(1)\n        zcoords = gen_coords(2)\n        grid = pyvista.RectilinearGrid(xcoords, ycoords, zcoords)\n        grid.point_data.update(self.point_data)\n        grid.cell_data.update(self.cell_data)\n        grid.field_data.update(self.field_data)\n        grid.copy_meta_from(self, deep=True)\n        return grid",
        "file_path": "pyvista/core/grid.py",
        "chunk_index": 5,
        "metadata": {
          "func_name": "cast_to_rectilinear_grid",
          "chunk_type": "function",
          "relationship": {
            "call": [
              "np.cumsum",
              "np.insert",
              "np.full",
              "gen_coords",
              "pyvista.RectilinearGrid",
              "grid.point_data.update",
              "grid.cell_data.update",
              "grid.field_data.update",
              "grid.copy_meta_from"
            ]
          },
          "class_name": "UniformGrid"
        }
      },
      {
        "rank": 5,
        "score": 0.6595473289489746,
        "content": "class StructuredGrid(_vtk.vtkStructuredGrid, PointGrid, StructuredGridFilters):\n    \"\"\"Dataset used for topologically regular arrays of data.\n\n    Can be initialized in one of the following several ways:\n\n    * Create empty grid.\n    * Initialize from a filename.\n    * Initialize from a ``vtk.vtkStructuredGrid`` object.\n    * Initialize directly from one or more :class:`numpy.ndarray`. See the\n      example or the documentation of ``uinput``.\n\n    Parameters\n    ----------\n    uinput : str, pathlib.Path, vtk.vtkStructuredGrid, numpy.ndarray, optional\n        Filename, dataset, or array to initialize the structured grid from. If\n        a filename is passed, pyvista will attempt to load it as a\n        :class:`StructuredGrid`. If passed a ``vtk.vtkStructuredGrid``, it will\n        be wrapped as a deep copy.\n\n        If a :class:`numpy.ndarray` is provided and ``y`` and ``z`` are empty,\n        this array will define the points of this :class:`StructuredGrid`.\n        Set the dimensions with :attr:`StructuredGrid.dimensions`.\n\n        Otherwise, this parameter will be loaded as the ``x`` points, and ``y``\n        and ``z`` points must be set. The shape of this array defines the shape\n        of the structured data and the shape should be ``(dimx, dimy,\n        dimz)``. Missing trailing dimensions are assumed to be ``1``.\n\n    y : numpy.ndarray, optional\n        Coordinates of the points in y direction. If this is passed, ``uinput``\n        must be a :class:`numpy.ndarray` and match the shape of ``y``.\n\n    z : numpy.ndarray, optional\n        Coordinates of the points in z direction. If this is passed, ``uinput``\n        and ``y`` must be a :class:`numpy.ndarray` and match the shape of ``z``.\n\n    deep : optional\n        Whether to deep copy a StructuredGrid object.\n        Default is ``False``.  Keyword only.\n\n    **kwargs : dict, optional\n        Additional keyword arguments passed when reading from a file or loading\n        from arrays.\n\n    Examples\n    --------\n    >>> import pyvista\n    >>> import vtk\n    >>> import numpy as np\n\n    Create an empty structured grid.\n\n    >>> grid = pyvista.StructuredGrid()\n\n    Initialize from a ``vtk.vtkStructuredGrid`` object\n\n    >>> vtkgrid = vtk.vtkStructuredGrid()\n    >>> grid = pyvista.StructuredGrid(vtkgrid)\n\n    Create from NumPy arrays.\n\n    >>> xrng = np.arange(-10, 10, 2, dtype=np.float32)\n    >>> yrng = np.arange(-10, 10, 2, dtype=np.float32)\n    >>> zrng = np.arange(-10, 10, 2, dtype=np.float32)\n    >>> x, y, z = np.meshgrid(xrng, yrng, zrng)\n    >>> grid = pyvista.StructuredGrid(x, y, z)\n    >>> grid  # doctest:+SKIP\n    StructuredGrid (0x7fb18f2a8580)\n    N Cells:    729\n    N Points:   1000\n    X Bounds:   -1.000e+01, 8.000e+00\n    Y Bounds:   -1.000e+01, 8.000e+00\n    Z Bounds:   -1.000e+01, 8.000e+00\n    Dimensions: 10, 10, 10\n    N Arrays:   0\n\n    \"\"\"\n\n    _WRITERS = {'.vtk': _vtk.vtkStructuredGridWriter, '.vts': _vtk.vtkXMLStructuredGridWriter}\n\n    def __init__(self, uinput=None, y=None, z=None, *args, deep=False, **kwargs) -> None:\n        \"\"\"Initialize the structured grid.\"\"\"\n        super().__init__()\n\n        if args:\n            raise ValueError(\"Too many args to create StructuredGrid.\")\n\n        if isinstance(uinput, _vtk.vtkStructuredGrid):\n            if deep:\n                self.deep_copy(uinput)\n            else:\n                self.shallow_copy(uinput)\n        elif isinstance(uinput, (str, pathlib.Path)):\n            self._from_file(uinput, **kwargs)\n        elif (\n            isinstance(uinput, np.ndarray)\n            and isinstance(y, np.ndarray)\n            and isinstance(z, np.ndarray)\n        ):\n            self._from_arrays(uinput, y, z, **kwargs)\n        elif isinstance(uinput, np.ndarray) and y is None and z is None:\n            self.points = uinput  # type: ignore\n        elif uinput is None:\n            # do nothing, initialize as empty structured grid\n            pass\n        else:\n            raise TypeError(\n                \"Invalid parameters. Expecting one of the following:\\n\"\n                \" - No arguments\\n\"\n                \" - Filename as the only argument\\n\"\n                \" - StructuredGrid as the only argument\\n\"\n                \" - Single `numpy.ndarray` as the only argument\"\n                \" - Three `numpy.ndarray` as the first three arguments\"\n            )\n\n    def __repr__(self):\n        \"\"\"Return the standard representation.\"\"\"\n        return DataSet.__repr__(self)\n\n    def __str__(self):\n        \"\"\"Return the standard str representation.\"\"\"\n        return DataSet.__str__(self)\n\n    def _from_arrays(self, x, y, z, force_float=True):\n        \"\"\"Create VTK structured grid directly from numpy arrays.\n\n        Parameters\n        ----------\n        x : numpy.ndarray\n            Position of the points in x direction.\n\n        y : numpy.ndarray\n            Position of the points in y direction.\n\n        z : numpy.ndarray\n            Position of the points in z direction.\n\n        force_float : bool, optional\n            Casts the datatype to ``float32`` if points datatype is\n            non-float.  Default ``True``. Set this to ``False`` to allow\n            non-float types, though this may lead to truncation of\n            intermediate floats when transforming datasets.\n\n        \"\"\"\n        if not (x.shape == y.shape == z.shape):\n            raise ValueError('Input point array shapes must match exactly')\n\n        # make the output points the same precision as the input arrays\n        points = np.empty((x.size, 3), x.dtype)\n        points[:, 0] = x.ravel('F')\n        points[:, 1] = y.ravel('F')\n        points[:, 2] = z.ravel('F')\n\n        # ensure that the inputs are 3D\n        dim = list(x.shape)\n        while len(dim) < 3:\n            dim.append(1)\n\n        # Create structured grid\n        self.SetDimensions(dim)\n        self.SetPoints(pyvista.vtk_points(points, force_float=force_float))\n\n    @property\n    def dimensions(self):\n        \"\"\"Return a length 3 tuple of the grid's dimensions.\n\n        Returns\n        -------\n        tuple\n            Grid dimensions.\n\n        Examples\n        --------\n        >>> import pyvista\n        >>> import numpy as np\n        >>> xrng = np.arange(-10, 10, 1, dtype=np.float32)\n        >>> yrng = np.arange(-10, 10, 2, dtype=np.float32)\n        >>> zrng = np.arange(-10, 10, 5, dtype=np.float32)\n        >>> x, y, z = np.meshgrid(xrng, yrng, zrng)\n        >>> grid = pyvista.StructuredGrid(x, y, z)\n        >>> grid.dimensions\n        (10, 20, 4)\n\n        \"\"\"\n        return tuple(self.GetDimensions())\n\n    @dimensions.setter\n    def dimensions(self, dims):\n        nx, ny, nz = dims[0], dims[1], dims[2]\n        self.SetDimensions(nx, ny, nz)\n        self.Modified()\n\n    @property\n    def x(self):\n        \"\"\"Return the X coordinates of all points.\n\n        Returns\n        -------\n        numpy.ndarray\n            Numpy array of all X coordinates.\n\n        Examples\n        --------\n        >>> import pyvista\n        >>> import numpy as np\n        >>> xrng = np.arange(-10, 10, 1, dtype=np.float32)\n        >>> yrng = np.arange(-10, 10, 2, dtype=np.float32)\n        >>> zrng = np.arange(-10, 10, 5, dtype=np.float32)\n        >>> x, y, z = np.meshgrid(xrng, yrng, zrng)\n        >>> grid = pyvista.StructuredGrid(x, y, z)\n        >>> grid.x.shape\n        (10, 20, 4)\n\n        \"\"\"\n        return self._reshape_point_array(self.points[:, 0])\n\n    @property\n    def y(self):\n        \"\"\"Return the Y coordinates of all points.\"\"\"\n        return self._reshape_point_array(self.points[:, 1])\n\n    @property\n    def z(self):\n        \"\"\"Return the Z coordinates of all points.\"\"\"\n        return self._reshape_point_array(self.points[:, 2])\n\n    @property\n    def points_matrix(self):\n        \"\"\"Points as a 4-D matrix, with x/y/z along the last dimension.\"\"\"\n        return self.points.reshape((*self.dimensions, 3), order='F')\n\n    def _get_attrs(self):\n        \"\"\"Return the representation methods (internal helper).\"\"\"\n        attrs = PointGrid._get_attrs(self)\n        attrs.append((\"Dimensions\", self.dimensions, \"{:d}, {:d}, {:d}\"))\n        return attrs\n\n    def __getitem__(self, key):\n        \"\"\"Slice subsets of the StructuredGrid, or extract an array field.\"\"\"\n        # legacy behavior which looks for a point or cell array\n        if not isinstance(key, tuple):\n            return super().__getitem__(key)\n\n        # convert slice to VOI specification - only \"basic indexing\" is supported\n        voi = []\n        rate = []\n        if len(key) != 3:\n            raise RuntimeError('Slices must have exactly 3 dimensions.')\n        for i, k in enumerate(key):\n            if isinstance(k, collections.abc.Iterable):\n                raise RuntimeError('Fancy indexing is not supported.')\n            if isinstance(k, numbers.Integral):\n                start = stop = k\n                step = 1\n            elif isinstance(k, slice):\n                start = k.start if k.start is not None else 0\n                stop = k.stop - 1 if k.stop is not None else self.dimensions[i]\n                step = k.step if k.step is not None else 1\n            voi.extend((start, stop))\n            rate.append(step)\n\n        return self.extract_subset(voi, rate, boundary=False)\n\n\n\n    def _reshape_point_array(self, array):\n        \"\"\"Reshape point data to a 3-D matrix.\"\"\"\n        return array.reshape(self.dimensions, order='F')\n\n    def _reshape_cell_array(self, array):\n        \"\"\"Reshape cell data to a 3-D matrix.\"\"\"\n        cell_dims = np.array(self.dimensions) - 1\n        cell_dims[cell_dims == 0] = 1\n        return array.reshape(cell_dims, order='F')",
        "file_path": "pyvista/core/pointset.py",
        "chunk_index": 31,
        "metadata": {
          "class_name": "StructuredGrid",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "_vtk.vtkStructuredGrid",
              "PointGrid",
              "StructuredGridFilters"
            ]
          },
          "parent_name": null,
          "private_methods_count": 8,
          "public_methods_count": 2,
          "node_count": 1659,
          "chunk_size": 9601
        }
      },
      {
        "rank": 6,
        "score": 0.6456193923950195,
        "content": "class UnstructuredGrid(_vtk.vtkUnstructuredGrid, PointGrid, UnstructuredGridFilters):\n    \"\"\"Dataset used for arbitrary combinations of all possible cell types.\n\n    Can be initialized by the following:\n\n    - Creating an empty grid\n    - From a ``vtk.vtkPolyData`` or ``vtk.vtkStructuredGrid`` object\n    - From cell, offset, and node arrays\n    - From a file\n\n    Parameters\n    ----------\n    args : str, vtk.vtkUnstructuredGrid, iterable\n        See examples below.\n    deep : bool, default: False\n        Whether to deep copy a vtkUnstructuredGrid object.\n        Default is ``False``.  Keyword only.\n\n    Examples\n    --------\n    >>> import pyvista\n    >>> from pyvista import examples\n    >>> import vtk\n\n    Create an empty grid\n\n    >>> grid = pyvista.UnstructuredGrid()\n\n    Copy a vtk.vtkUnstructuredGrid\n\n    >>> vtkgrid = vtk.vtkUnstructuredGrid()\n    >>> grid = pyvista.UnstructuredGrid(vtkgrid)\n\n    From a filename.\n\n    >>> grid = pyvista.UnstructuredGrid(examples.hexbeamfile)\n    >>> grid.plot(show_edges=True)\n\n    From arrays. Here we create a single tetrahedron.\n\n    >>> cells = [4, 0, 1, 2, 3]\n    >>> celltypes = [pyvista.CellType.TETRA]\n    >>> points = [\n    ...     [1.0, 1.0, 1.0],\n    ...     [1.0, -1.0, -1.0],\n    ...     [-1.0, 1.0, -1.0],\n    ...     [-1.0, -1.0, 1.0],\n    ... ]\n    >>> grid = pyvista.UnstructuredGrid(cells, celltypes, points)\n    >>> grid.plot(show_edges=True)\n\n    See the :ref:`create_unstructured_example` example for more details\n    on creating unstructured grids within PyVista.\n\n    \"\"\"\n\n    _WRITERS = {'.vtu': _vtk.vtkXMLUnstructuredGridWriter, '.vtk': _vtk.vtkUnstructuredGridWriter}\n\n    def __init__(self, *args, deep=False, **kwargs) -> None:\n        \"\"\"Initialize the unstructured grid.\"\"\"\n        super().__init__()\n\n        if not len(args):\n            return\n        if len(args) == 1:\n            if isinstance(args[0], _vtk.vtkUnstructuredGrid):\n                if deep:\n                    self.deep_copy(args[0])\n                else:\n                    self.shallow_copy(args[0])\n\n            elif isinstance(args[0], (str, pathlib.Path)):\n                self._from_file(args[0], **kwargs)\n\n            elif isinstance(args[0], (_vtk.vtkStructuredGrid, _vtk.vtkPolyData)):\n                vtkappend = _vtk.vtkAppendFilter()\n                vtkappend.AddInputData(args[0])\n                vtkappend.Update()\n                self.shallow_copy(vtkappend.GetOutput())\n\n            else:\n                itype = type(args[0])\n                raise TypeError(f'Cannot work with input type {itype}')\n\n        # Cell dictionary creation\n        elif len(args) == 2 and isinstance(args[0], dict) and isinstance(args[1], np.ndarray):\n            self._from_cells_dict(args[0], args[1], deep)\n            self._check_for_consistency()\n\n        elif len(args) == 3:\n            arg0_is_seq = isinstance(args[0], (np.ndarray, collections.abc.Sequence))\n            arg1_is_seq = isinstance(args[1], (np.ndarray, collections.abc.Sequence))\n            arg2_is_seq = isinstance(args[2], (np.ndarray, collections.abc.Sequence))\n\n            if all([arg0_is_seq, arg1_is_seq, arg2_is_seq]):\n                self._from_arrays(None, args[0], args[1], args[2], deep, **kwargs)\n                self._check_for_consistency()\n            else:\n                raise TypeError('All input types must be sequences.')\n\n        elif len(args) == 4:  # pragma: no cover\n            arg0_is_arr = isinstance(args[0], (np.ndarray, collections.abc.Sequence))\n            arg1_is_arr = isinstance(args[1], (np.ndarray, collections.abc.Sequence))\n            arg2_is_arr = isinstance(args[2], (np.ndarray, collections.abc.Sequence))\n            arg3_is_arr = isinstance(args[3], (np.ndarray, collections.abc.Sequence))\n\n            if all([arg0_is_arr, arg1_is_arr, arg2_is_arr, arg3_is_arr]):\n                self._from_arrays(args[0], args[1], args[2], args[3], deep)\n                self._check_for_consistency()\n            else:\n                raise TypeError('All input types must be sequences.')\n        else:\n            raise TypeError(\n                'Invalid parameters.  Initialization with arrays requires the '\n                'following arrays:\\n`cells`, `cell_type`, `points`'\n            )\n\n    def __repr__(self):\n        \"\"\"Return the standard representation.\"\"\"\n        return DataSet.__repr__(self)\n\n    def __str__(self):\n        \"\"\"Return the standard str representation.\"\"\"\n        return DataSet.__str__(self)\n\n    def _from_cells_dict(self, cells_dict, points, deep=True):\n        if points.ndim != 2 or points.shape[-1] != 3:\n            raise ValueError(\"Points array must be a [M, 3] array\")\n\n        nr_points = points.shape[0]\n        cell_types, cells = create_mixed_cells(cells_dict, nr_points)\n        self._from_arrays(None, cells, cell_types, points, deep=deep)\n\n    def _from_arrays(\n        self,\n        offset,\n        cells,\n        cell_type,\n        points,\n        deep=True,\n        force_float=True,\n    ):\n        \"\"\"Create VTK unstructured grid from numpy arrays.\n\n        Parameters\n        ----------\n        offset : any, default None\n            Ignored (this is a pre-VTK9 legacy).\n\n        cells : sequence[int]\n            Array of cells.  Each cell contains the number of points in the\n            cell and the node numbers of the cell.\n\n        cell_type : sequence[int]\n            Cell types of each cell.  Each cell type numbers can be found from\n            vtk documentation.  More efficient if using ``np.uint8``. See\n            example below.\n\n        points : sequence[float]\n            Numpy array containing point locations.\n\n        deep : bool, default: True\n            When ``True``, makes a copy of the points array.  Default\n            ``False``.  Cells and cell types are always copied.\n\n        force_float : bool, default: True\n            Casts the datatype to ``float32`` if points datatype is\n            non-float.  Set this to ``False`` to allow non-float types,\n            though this may lead to truncation of intermediate floats when\n            transforming datasets.\n\n        Examples\n        --------\n        >>> import numpy as np\n        >>> from pyvista import CellType\n        >>> import pyvista\n        >>> cell0_ids = [8, 0, 1, 2, 3, 4, 5, 6, 7]\n        >>> cell1_ids = [8, 8, 9, 10, 11, 12, 13, 14, 15]\n        >>> cells = np.hstack((cell0_ids, cell1_ids))\n        >>> cell_type = np.array(\n        ...     [CellType.HEXAHEDRON, CellType.HEXAHEDRON], np.int8\n        ... )\n\n        >>> cell1 = np.array(\n        ...     [\n        ...         [0, 0, 0],\n        ...         [1, 0, 0],\n        ...         [1, 1, 0],\n        ...         [0, 1, 0],\n        ...         [0, 0, 1],\n        ...         [1, 0, 1],\n        ...         [1, 1, 1],\n        ...         [0, 1, 1],\n        ...     ],\n        ...     dtype=np.float32,\n        ... )\n\n        >>> cell2 = np.array(\n        ...     [\n        ...         [0, 0, 2],\n        ...         [1, 0, 2],\n        ...         [1, 1, 2],\n        ...         [0, 1, 2],\n        ...         [0, 0, 3],\n        ...         [1, 0, 3],\n        ...         [1, 1, 3],\n        ...         [0, 1, 3],\n        ...     ],\n        ...     dtype=np.float32,\n        ... )\n\n        >>> points = np.vstack((cell1, cell2))\n\n        >>> grid = pyvista.UnstructuredGrid(cells, cell_type, points)\n\n        \"\"\"\n        if offset is not None:\n            warnings.warn('VTK 9 no longer accepts an offset array', stacklevel=3)\n        # convert to arrays upfront\n        cells = np.asarray(cells)\n        cell_type = np.asarray(cell_type)\n        points = np.asarray(points)\n\n        # Convert to vtk arrays\n        vtkcells = CellArray(cells, cell_type.size, deep)\n        if cell_type.dtype != np.uint8:\n            cell_type = cell_type.astype(np.uint8)\n        cell_type = _vtk.numpy_to_vtk(cell_type, deep=deep)\n\n        points = pyvista.vtk_points(points, deep, force_float)\n        self.SetPoints(points)\n\n        self.SetCells(cell_type, vtkcells)\n\n    def _check_for_consistency(self):\n        \"\"\"Check if size of offsets and celltypes match the number of cells.\n\n        Checks if the number of offsets and celltypes correspond to\n        the number of cells.  Called after initialization of the self\n        from arrays.\n        \"\"\"\n        if self.n_cells != self.celltypes.size:\n            raise ValueError(\n                f'Number of cell types ({self.celltypes.size}) '\n                f'must match the number of cells {self.n_cells})'\n            )\n\n        if self.n_cells != self.offset.size - 1:  # pragma: no cover\n            raise ValueError(\n                f'Size of the offset ({self.offset.size}) '\n                f'must be one greater than the number of cells ({self.n_cells})'\n            )\n\n    @property\n    def cells(self) -> np.ndarray:\n        \"\"\"Return a pointer to the cells as a numpy object.\n\n        See Also\n        --------\n        pyvista.DataSet.get_cell\n\n        Examples\n        --------\n        Return the indices of the first two cells from the example hex\n        beam.  Note how the cells have \"padding\" indicating the number\n        of points per cell.\n\n        >>> import pyvista\n        >>> from pyvista import examples\n        >>> hex_beam = pyvista.read(examples.hexbeamfile)\n        >>> hex_beam.cells[:18]  # doctest:+SKIP\n        array([ 8,  0,  2,  8,  7, 27, 36, 90, 81,  8,  2,  1,  4,\n                8, 36, 18, 54, 90])\n\n        \"\"\"\n        return _vtk.vtk_to_numpy(self.GetCells().GetData())\n\n    @property\n    def cells_dict(self) -> dict:\n        \"\"\"Return a dictionary that contains all cells mapped from cell types.\n\n        This function returns a :class:`numpy.ndarray` for each cell\n        type in an ordered fashion.  Note that this function only\n        works with element types of fixed sizes.\n\n        Returns\n        -------\n        dict\n            A dictionary mapping containing all cells of this unstructured grid.\n            Structure: vtk_enum_type (int) -> cells (:class:`numpy.ndarray`).\n\n        See Also\n        --------\n        pyvista.DataSet.get_cell\n\n        Examples\n        --------\n        Return the cells dictionary of the sample hex beam.  Note how\n        there is only one key/value pair as the hex beam example is\n        composed of only all hexahedral cells, which is\n        ``CellType.HEXAHEDRON``, which evaluates to 12.\n\n        Also note how there is no padding for the cell array.  This\n        approach may be more helpful than the ``cells`` property when\n        extracting cells.\n\n        >>> import pyvista\n        >>> from pyvista import examples\n        >>> hex_beam = pyvista.read(examples.hexbeamfile)\n        >>> hex_beam.cells_dict  # doctest:+SKIP\n        {12: array([[ 0,  2,  8,  7, 27, 36, 90, 81],\n                [ 2,  1,  4,  8, 36, 18, 54, 90],\n                [ 7,  8,  6,  5, 81, 90, 72, 63],\n                ...\n                [44, 26, 62, 98, 11, 10, 13, 17],\n                [89, 98, 80, 71, 16, 17, 15, 14],\n                [98, 62, 53, 80, 17, 13, 12, 15]])}\n        \"\"\"\n        return get_mixed_cells(self)\n\n    @property\n    def cell_connectivity(self) -> np.ndarray:\n        \"\"\"Return a the vtk cell connectivity as a numpy array.\n\n        This is effectively :attr:`UnstructuredGrid.cells` without the\n        padding.\n\n        Returns\n        -------\n        numpy.ndarray\n            Connectivity array.\n\n        See Also\n        --------\n        pyvista.DataSet.get_cell\n\n        Examples\n        --------\n        Return the cell connectivity for the first two cells.\n\n        >>> import pyvista\n        >>> from pyvista import examples\n        >>> hex_beam = pyvista.read(examples.hexbeamfile)\n        >>> hex_beam.cell_connectivity[:16]\n        array([ 0,  2,  8,  7, 27, 36, 90, 81,  2,  1,  4,  8, 36, 18, 54, 90])\n\n        \"\"\"\n        carr = self.GetCells()\n        return _vtk.vtk_to_numpy(carr.GetConnectivityArray())\n\n\n    @property\n    def celltypes(self) -> np.ndarray:\n        \"\"\"Return the cell types array.\n\n        Returns\n        -------\n        numpy.ndarray\n            Array of cell types.\n\n        Notes\n        -----\n        Here are some of the most popular cell types:\n\n        * ``EMPTY_CELL = 0``\n        * ``VERTEX = 1``\n        * ``POLY_VERTEX = 2``\n        * ``LINE = 3``\n        * ``POLY_LINE = 4``\n        * ``TRIANGLE = 5``\n        * ``TRIANGLE_STRIP = 6``\n        * ``POLYGON = 7``\n        * ``PIXEL = 8``\n        * ``QUAD = 9``\n        * ``TETRA = 10``\n        * ``VOXEL = 11``\n        * ``HEXAHEDRON = 12``\n        * ``WEDGE = 13``\n        * ``PYRAMID = 14``\n        * ``PENTAGONAL_PRISM = 15``\n        * ``HEXAGONAL_PRISM = 16``\n        * ``QUADRATIC_EDGE = 21``\n        * ``QUADRATIC_TRIANGLE = 22``\n        * ``QUADRATIC_QUAD = 23``\n        * ``QUADRATIC_POLYGON = 36``\n        * ``QUADRATIC_TETRA = 24``\n        * ``QUADRATIC_HEXAHEDRON = 25``\n        * ``QUADRATIC_WEDGE = 26``\n        * ``QUADRATIC_PYRAMID = 27``\n        * ``BIQUADRATIC_QUAD = 28``\n        * ``TRIQUADRATIC_HEXAHEDRON = 29``\n        * ``QUADRATIC_LINEAR_QUAD = 30``\n        * ``QUADRATIC_LINEAR_WEDGE = 31``\n        * ``BIQUADRATIC_QUADRATIC_WEDGE = 32``\n        * ``BIQUADRATIC_QUADRATIC_HEXAHEDRON = 33``\n        * ``BIQUADRATIC_TRIANGLE = 34``\n\n        See `vtkCellType.h\n        <https://vtk.org/doc/nightly/html/vtkCellType_8h_source.html>`_ for all\n        cell types.\n\n        Examples\n        --------\n        This mesh contains only linear hexahedral cells, type\n        ``CellType.HEXAHEDRON``, which evaluates to 12.\n\n        >>> import pyvista\n        >>> from pyvista import examples\n        >>> hex_beam = pyvista.read(examples.hexbeamfile)\n        >>> hex_beam.celltypes  # doctest:+SKIP\n        array([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n               12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n               12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12],\n               dtype=uint8)\n\n        \"\"\"\n        return _vtk.vtk_to_numpy(self.GetCellTypesArray())\n\n    @property\n    def offset(self) -> np.ndarray:\n        \"\"\"Return the cell locations array.\n\n        This is the location of the start of each cell in\n        :attr:`cell_connectivity`.\n\n        Returns\n        -------\n        numpy.ndarray\n            Array of cell offsets indicating the start of each cell.\n\n        Examples\n        --------\n        Return the cell offset array within ``vtk==9``.  Since this\n        mesh is composed of all hexahedral cells, note how each cell\n        starts at 8 greater than the prior cell.\n\n        >>> import pyvista\n        >>> from pyvista import examples\n        >>> hex_beam = pyvista.read(examples.hexbeamfile)\n        >>> hex_beam.offset\n        array([  0,   8,  16,  24,  32,  40,  48,  56,  64,  72,  80,  88,  96,\n               104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200,\n               208, 216, 224, 232, 240, 248, 256, 264, 272, 280, 288, 296, 304,\n               312, 320])\n\n        \"\"\"\n        carr = self.GetCells()\n        # This will be the number of cells + 1.\n        return _vtk.vtk_to_numpy(carr.GetOffsetsArray())\n",
        "file_path": "pyvista/core/pointset.py",
        "chunk_index": 28,
        "metadata": {
          "class_name": "UnstructuredGrid",
          "chunk_type": "class",
          "relationship": {
            "inherit": [
              "_vtk.vtkUnstructuredGrid",
              "PointGrid",
              "UnstructuredGridFilters"
            ]
          },
          "parent_name": null,
          "private_methods_count": 6,
          "public_methods_count": 2,
          "node_count": 2169,
          "chunk_size": 15129
        }
      },
      {
        "rank": 7,
        "score": 0.6372284889221191,
        "content": "def cast_to_structured_grid(self) -> 'pyvista.StructuredGrid':\n        \"\"\"Cast this uniform grid to a structured grid.\n\n        Returns\n        -------\n        pyvista.StructuredGrid\n            This grid as a structured grid.\n\n        \"\"\"\n        alg = _vtk.vtkImageToStructuredGrid()\n        alg.SetInputData(self)\n        alg.Update()\n        return _get_output(alg)",
        "file_path": "pyvista/core/grid.py",
        "chunk_index": 4,
        "metadata": {
          "func_name": "cast_to_structured_grid",
          "chunk_type": "function",
          "relationship": {
            "call": [
              "_vtk.vtkImageToStructuredGrid",
              "alg.SetInputData",
              "alg.Update",
              "_get_output"
            ]
          },
          "class_name": "UniformGrid"
        }
      },
      {
        "rank": 8,
        "score": 0.6367930769920349,
        "content": "def wrap_image_array(arr):\n    \"\"\"Wrap a numpy array as a pyvista.UniformGrid.\n\n    Parameters\n    ----------\n    arr : np.ndarray\n        A ``np.uint8`` ``(X, Y, (3 or 4)`` array.  For example\n        ``(768, 1024, 3)``.\n\n    \"\"\"\n    if arr.ndim != 3:\n        raise ValueError('Expecting a X by Y by (3 or 4) array')\n    if arr.shape[2] not in [3, 4]:\n        raise ValueError('Expecting a X by Y by (3 or 4) array')\n    if arr.dtype != np.uint8:\n        raise ValueError('Expecting a np.uint8 array')\n\n    img = _vtk.vtkImageData()\n    img.SetDimensions(arr.shape[1], arr.shape[0], 1)\n    wrap_img = pyvista.wrap(img)\n    wrap_img.point_data['PNGImage'] = arr[::-1].reshape(-1, arr.shape[2])\n    return wrap_img",
        "file_path": "pyvista/utilities/regression.py",
        "chunk_index": 1,
        "metadata": {
          "func_name": "wrap_image_array",
          "chunk_type": "function",
          "relationship": {
            "call": [
              "_vtk.vtkImageData",
              "img.SetDimensions",
              "pyvista.wrap",
              "arr[::-1].reshape"
            ]
          },
          "class_name": null
        }
      },
      {
        "rank": 9,
        "score": 0.6360129117965698,
        "content": "def create_grid(dataset, dimensions=(101, 101, 101)):\n    \"\"\"Create a uniform grid surrounding the given dataset.\n\n    The output grid will have the specified dimensions and is commonly used\n    for interpolating the input dataset.\n\n    \"\"\"\n    bounds = np.array(dataset.bounds)\n    if dimensions is None:\n        # TODO: we should implement an algorithm to automatically determine an\n        # \"optimal\" grid size by looking at the sparsity of the points in the\n        # input dataset - I actually think VTK might have this implemented\n        # somewhere\n        raise NotImplementedError('Please specify dimensions.')\n    dimensions = np.array(dimensions, dtype=int)\n    image = pyvista.UniformGrid()\n    image.dimensions = dimensions\n    dims = dimensions - 1\n    dims[dims == 0] = 1\n    image.spacing = (bounds[1::2] - bounds[:-1:2]) / dims\n    image.origin = bounds[::2]\n    return image",
        "file_path": "pyvista/utilities/features.py",
        "chunk_index": 1,
        "metadata": {
          "func_name": "create_grid",
          "chunk_type": "function",
          "relationship": {
            "call": [
              "np.array",
              "pyvista.UniformGrid"
            ]
          },
          "class_name": null
        }
      },
      {
        "rank": 10,
        "score": 0.634823203086853,
        "content": "def cast_to_unstructured_grid(self) -> pyvista.UnstructuredGrid:\n        \"\"\"Get a new representation of this object as a :class:`pyvista.UnstructuredGrid`.\n\n        Returns\n        -------\n        pyvista.UnstructuredGrid\n            Dataset cast into a :class:`pyvista.UnstructuredGrid`.\n\n        Examples\n        --------\n        Cast a :class:`pyvista.PolyData` to a\n        :class:`pyvista.UnstructuredGrid`.\n\n        >>> import pyvista\n        >>> mesh = pyvista.Sphere()\n        >>> type(mesh)\n        <class 'pyvista.core.pointset.PolyData'>\n        >>> grid = mesh.cast_to_unstructured_grid()\n        >>> type(grid)\n        <class 'pyvista.core.pointset.UnstructuredGrid'>\n\n        \"\"\"\n        alg = _vtk.vtkAppendFilter()\n        alg.AddInputData(self)\n        alg.Update()\n        return _get_output(alg)",
        "file_path": "pyvista/core/dataset.py",
        "chunk_index": 27,
        "metadata": {
          "func_name": "cast_to_unstructured_grid",
          "chunk_type": "function",
          "relationship": {
            "call": [
              "_vtk.vtkAppendFilter",
              "alg.AddInputData",
              "alg.Update",
              "_get_output"
            ]
          },
          "class_name": "DataSet"
        }
      }
    ]
  }
}