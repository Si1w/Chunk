{
  "sqlfluff__sqlfluff-1625": {
    "query": "TSQL - L031 incorrectly triggers \"Avoid using aliases in join condition\" when no join present\n## Expected Behaviour\r\n\r\nBoth of these queries should pass, the only difference is the addition of a table alias 'a':\r\n\r\n1/ no alias\r\n\r\n```\r\nSELECT [hello]\r\nFROM\r\n    mytable\r\n```\r\n\r\n2/ same query with alias\r\n\r\n```\r\nSELECT a.[hello]\r\nFROM\r\n    mytable AS a\r\n```\r\n\r\n## Observed Behaviour\r\n\r\n1/ passes\r\n2/ fails with: L031: Avoid using aliases in join condition.\r\n\r\nBut there is no join condition :-)\r\n\r\n## Steps to Reproduce\r\n\r\nLint queries above\r\n\r\n## Dialect\r\n\r\nTSQL\r\n\r\n## Version\r\n\r\nsqlfluff 0.6.9\r\nPython 3.6.9\r\n\r\n## Configuration\r\n\r\nN/A\n",
    "method": "function",
    "top_k": 10,
    "results": [
      {
        "rank": 1,
        "score": 0.7295547127723694,
        "content": "def _lint_aliases_in_join(\n        self, base_table, from_expression_elements, column_reference_segments, segment\n    ):\n        \"\"\"Lint and fix all aliases in joins - except for self-joins.\"\"\"\n        # A buffer to keep any violations.\n        violation_buff = []\n\n        to_check = list(\n            self._filter_table_expressions(base_table, from_expression_elements)\n        )\n\n        # How many times does each table appear in the FROM clause?\n        table_counts = Counter(ai.table_ref.raw for ai in to_check)\n\n        # What is the set of aliases used for each table? (We are mainly\n        # interested in the NUMBER of different aliases used.)\n        table_aliases = defaultdict(set)\n        for ai in to_check:\n            table_aliases[ai.table_ref.raw].add(ai.alias_identifier_ref.raw)\n\n        # For each aliased table, check whether to keep or remove it.\n        for alias_info in to_check:\n            # If the same table appears more than once in the FROM clause with\n            # different alias names, do not consider removing its aliases.\n            # The aliases may have been introduced simply to make each\n            # occurrence of the table independent within the query.\n            if (\n                table_counts[alias_info.table_ref.raw] > 1\n                and len(table_aliases[alias_info.table_ref.raw]) > 1\n            ):\n                continue\n\n            select_clause = segment.get_child(\"select_clause\")\n\n            ids_refs = []\n\n            # Find all references to alias in select clause\n            alias_name = alias_info.alias_identifier_ref.raw\n            for alias_with_column in select_clause.recursive_crawl(\"object_reference\"):\n                used_alias_ref = alias_with_column.get_child(\"identifier\")\n                if used_alias_ref and used_alias_ref.raw == alias_name:\n                    ids_refs.append(used_alias_ref)\n\n            # Find all references to alias in column references\n            for exp_ref in column_reference_segments:\n                used_alias_ref = exp_ref.get_child(\"identifier\")\n                # exp_ref.get_child('dot') ensures that the column reference includes a table reference\n                if used_alias_ref.raw == alias_name and exp_ref.get_child(\"dot\"):\n                    ids_refs.append(used_alias_ref)\n\n            # Fixes for deleting ` as sth` and for editing references to aliased tables\n            fixes = [\n                *[\n                    LintFix(\"delete\", d)\n                    for d in [alias_info.alias_exp_ref, alias_info.whitespace_ref]\n                ],\n                *[\n                    LintFix(\"edit\", alias, alias.edit(alias_info.table_ref.raw))\n                    for alias in [alias_info.alias_identifier_ref, *ids_refs]\n                ],\n            ]\n\n            violation_buff.append(\n                LintResult(\n                    anchor=alias_info.alias_identifier_ref,\n                    description=\"Avoid using aliases in join condition\",\n                    fixes=fixes,\n                )\n            )\n\n        return violation_buff or None",
        "file_path": "src/sqlfluff/rules/L031.py",
        "chunk_index": 2,
        "metadata": {
          "filepath": "src/sqlfluff/rules/L031.py",
          "function_name": "_lint_aliases_in_join",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 71,
          "start_line_no": 149,
          "end_line_no": 219
        }
      },
      {
        "rank": 2,
        "score": 0.6655069589614868,
        "content": "def _eval(self, segment, **kwargs):\n        \"\"\"Identify aliases in from clause and join conditions.\n\n        Find base table, table expressions in join, and other expressions in select clause\n        and decide if it's needed to report them.\n        \"\"\"\n        if segment.is_type(\"select_statement\"):\n            # A buffer for all table expressions in join conditions\n            from_expression_elements = []\n            column_reference_segments = []\n\n            from_clause_segment = segment.get_child(\"from_clause\")\n\n            if not from_clause_segment:\n                return None\n\n            from_expression = from_clause_segment.get_child(\"from_expression\")\n            from_expression_element = None\n            if from_expression:\n                from_expression_element = from_expression.get_child(\n                    \"from_expression_element\"\n                )\n\n            if not from_expression_element:\n                return None\n            from_expression_element = from_expression_element.get_child(\n                \"table_expression\"\n            )\n\n            # Find base table\n            base_table = None\n            if from_expression_element:\n                base_table = from_expression_element.get_child(\"object_reference\")\n\n            from_clause_index = segment.segments.index(from_clause_segment)\n            from_clause_and_after = segment.segments[from_clause_index:]\n\n            for clause in from_clause_and_after:\n                for from_expression_element in clause.recursive_crawl(\n                    \"from_expression_element\"\n                ):\n                    from_expression_elements.append(from_expression_element)\n                for column_reference in clause.recursive_crawl(\"column_reference\"):\n                    column_reference_segments.append(column_reference)\n\n            return (\n                self._lint_aliases_in_join(\n                    base_table,\n                    from_expression_elements,\n                    column_reference_segments,\n                    segment,\n                )\n                or None\n            )\n        return None",
        "file_path": "src/sqlfluff/rules/L031.py",
        "chunk_index": 0,
        "metadata": {
          "filepath": "src/sqlfluff/rules/L031.py",
          "function_name": "_eval",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 55,
          "start_line_no": 49,
          "end_line_no": 103
        }
      },
      {
        "rank": 3,
        "score": 0.6287769675254822,
        "content": "def _lint_references_and_aliases(\n        self,\n        table_aliases,\n        standalone_aliases,\n        references,\n        col_aliases,\n        using_cols,\n        parent_select,\n    ):\n        \"\"\"Check all aliased references against tables referenced in the query.\"\"\"\n        # A buffer to keep any violations.\n        violation_buff = []\n        # Check all the references that we have, keep track of which aliases we refer to.\n        tbl_refs = set()\n        for r in references:\n            tbl_refs.update(\n                tr.part\n                for tr in r.extract_possible_references(\n                    level=r.ObjectReferenceLevel.TABLE\n                )\n            )\n\n        alias: AliasInfo\n        for alias in table_aliases:\n            if alias.aliased and alias.ref_str not in tbl_refs:\n                fixes = [LintFix(\"delete\", alias.alias_expression)]\n                found_alias_segment = False\n                # Walk back to remove indents/whitespaces\n                for segment in reversed(alias.from_expression_element.segments):\n                    if not found_alias_segment:\n                        if segment is alias.alias_expression:\n                            found_alias_segment = True\n                    else:\n                        if (\n                            segment.name == \"whitespace\"\n                            or segment.name == \"newline\"\n                            or segment.is_meta\n                        ):\n                            fixes.append(LintFix(\"delete\", segment))\n                        else:\n                            # Stop once we reach an other, \"regular\" segment.\n                            break\n                violation_buff.append(\n                    LintResult(\n                        anchor=alias.segment,\n                        description=\"Alias {!r} is never used in SELECT statement.\".format(\n                            alias.ref_str\n                        ),\n                        fixes=fixes,\n                    )\n                )\n        return violation_buff or None",
        "file_path": "src/sqlfluff/rules/L025.py",
        "chunk_index": 0,
        "metadata": {
          "filepath": "src/sqlfluff/rules/L025.py",
          "function_name": "_lint_references_and_aliases",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 52,
          "start_line_no": 39,
          "end_line_no": 90
        }
      },
      {
        "rank": 4,
        "score": 0.6016850471496582,
        "content": "def get_eventual_alias(self) -> AliasInfo:\n        \"\"\"Return the eventual table name referred to by this join clause.\"\"\"\n        from_expression_element = self.get_child(\"from_expression_element\")\n        return from_expression_element.get_eventual_alias()",
        "file_path": "src/sqlfluff/dialects/dialect_ansi.py",
        "chunk_index": 8,
        "metadata": {
          "filepath": "src/sqlfluff/dialects/dialect_ansi.py",
          "function_name": "get_eventual_alias",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 4,
          "start_line_no": 1299,
          "end_line_no": 1302
        }
      },
      {
        "rank": 5,
        "score": 0.5996900796890259,
        "content": "def _eval(self, segment, **kwargs):\n        \"\"\"Look for USING in a join clause.\"\"\"\n        if segment.is_type(\"join_clause\"):\n            for seg in segment.segments:\n                if seg.is_type(\"keyword\") and seg.name == \"using\":\n                    return [\n                        LintResult(\n                            # Reference the element, not the string.\n                            anchor=seg,\n                            description=(\n                                \"Found USING statement. Expected only ON statements.\"\n                            ),\n                        )\n                    ]\n        return None",
        "file_path": "src/sqlfluff/rules/L032.py",
        "chunk_index": 0,
        "metadata": {
          "filepath": "src/sqlfluff/rules/L032.py",
          "function_name": "_eval",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 15,
          "start_line_no": 35,
          "end_line_no": 49
        }
      },
      {
        "rank": 6,
        "score": 0.5978361368179321,
        "content": "def _eval(self, segment, parent_stack, raw_stack, **kwargs):\n        \"\"\"Implicit aliasing of table/column not allowed. Use explicit `AS` clause.\n\n        We look for the alias segment, and then evaluate its parent and whether\n        it contains an AS keyword. This is the _eval function for both L011 and L012.\n\n        The use of `raw_stack` is just for working out how much whitespace to add.\n\n        \"\"\"\n        fixes = []\n\n        if segment.is_type(\"alias_expression\"):\n            if parent_stack[-1].is_type(*self._target_elems):\n                if any(e.name.lower() == \"as\" for e in segment.segments):\n                    if self.aliasing == \"implicit\":\n                        if segment.segments[0].name.lower() == \"as\":\n\n                            # Remove the AS as we're using implict aliasing\n                            fixes.append(LintFix(\"delete\", segment.segments[0]))\n                            anchor = raw_stack[-1]\n\n                            # Remove whitespace before (if exists) or after (if not)\n                            if (\n                                len(raw_stack) > 0\n                                and raw_stack[-1].type == \"whitespace\"\n                            ):\n                                fixes.append(LintFix(\"delete\", raw_stack[-1]))\n                            elif (\n                                len(segment.segments) > 0\n                                and segment.segments[1].type == \"whitespace\"\n                            ):\n                                fixes.append(LintFix(\"delete\", segment.segments[1]))\n\n                            return LintResult(anchor=anchor, fixes=fixes)\n\n                else:\n                    insert_buff = []\n\n                    # Add initial whitespace if we need to...\n                    if raw_stack[-1].name not in [\"whitespace\", \"newline\"]:\n                        insert_buff.append(WhitespaceSegment())\n\n                    # Add an AS (Uppercase for now, but could be corrected later)\n                    insert_buff.append(KeywordSegment(\"AS\"))\n\n                    # Add a trailing whitespace if we need to\n                    if segment.segments[0].name not in [\"whitespace\", \"newline\"]:\n                        insert_buff.append(WhitespaceSegment())\n\n                    return LintResult(\n                        anchor=segment,\n                        fixes=[LintFix(\"create\", segment.segments[0], insert_buff)],\n                    )\n        return None",
        "file_path": "src/sqlfluff/rules/L011.py",
        "chunk_index": 0,
        "metadata": {
          "filepath": "src/sqlfluff/rules/L011.py",
          "function_name": "_eval",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 54,
          "start_line_no": 40,
          "end_line_no": 93
        }
      },
      {
        "rank": 7,
        "score": 0.597643256187439,
        "content": "def _lint_references_and_aliases(\n        self,\n        table_aliases,\n        standalone_aliases,\n        references,\n        col_aliases,\n        using_cols,\n        parent_select,\n    ):\n        \"\"\"Iterate through references and check consistency.\"\"\"\n        # How many aliases are there? If more than one then abort.\n        if len(table_aliases) > 1:\n            return None\n        # A buffer to keep any violations.\n        violation_buff = []\n        # Check all the references that we have.\n        seen_ref_types = set()\n        for ref in references:\n            # We skip any unqualified wildcard references (i.e. *). They shouldn't count.\n            if not ref.is_qualified() and ref.is_type(\"wildcard_identifier\"):\n                continue\n            # Oddball case: Column aliases provided via function calls in by\n            # FROM or JOIN. References to these don't need to be qualified.\n            # Note there could be a table with a column by the same name as\n            # this alias, so avoid bogus warnings by just skipping them\n            # entirely rather than trying to enforce anything.\n            if ref.raw in standalone_aliases:\n                continue\n            this_ref_type = ref.qualification()\n            if self.single_table_references == \"consistent\":\n                if seen_ref_types and this_ref_type not in seen_ref_types:\n                    violation_buff.append(\n                        LintResult(\n                            anchor=ref,\n                            description=f\"{this_ref_type.capitalize()} reference \"\n                            f\"{ref.raw!r} found in single table select which is \"\n                            \"inconsistent with previous references.\",\n                        )\n                    )\n            elif self.single_table_references != this_ref_type:\n                violation_buff.append(\n                    LintResult(\n                        anchor=ref,\n                        description=\"{} reference {!r} found in single table select.\".format(\n                            this_ref_type.capitalize(), ref.raw\n                        ),\n                    )\n                )\n            seen_ref_types.add(this_ref_type)\n\n        return violation_buff or None",
        "file_path": "src/sqlfluff/rules/L028.py",
        "chunk_index": 0,
        "metadata": {
          "filepath": "src/sqlfluff/rules/L028.py",
          "function_name": "_lint_references_and_aliases",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 51,
          "start_line_no": 47,
          "end_line_no": 97
        }
      },
      {
        "rank": 8,
        "score": 0.594584584236145,
        "content": "def _eval(self, segment, parent_stack, **kwargs):\n        \"\"\"Column expression without alias. Use explicit `AS` clause.\n\n        We look for the select_clause_element segment, and then evaluate\n        whether it has an alias segment or not and whether the expression\n        is complicated enough. `parent_stack` is to assess how many other\n        elements there are.\n\n        \"\"\"\n        if segment.is_type(\"select_clause_element\"):\n            if not any(e.is_type(\"alias_expression\") for e in segment.segments):\n                types = {e.get_type() for e in segment.segments if e.name != \"star\"}\n                unallowed_types = types - {\n                    \"whitespace\",\n                    \"newline\",\n                    \"column_reference\",\n                    \"wildcard_expression\",\n                }\n                if len(unallowed_types) > 0:\n                    # No fixes, because we don't know what the alias should be,\n                    # the user should document it themselves.\n                    if self.allow_scalar:\n                        # Check *how many* elements there are in the select\n                        # statement. If this is the only one, then we won't\n                        # report an error.\n                        num_elements = sum(\n                            e.is_type(\"select_clause_element\")\n                            for e in parent_stack[-1].segments\n                        )\n                        if num_elements > 1:\n                            return LintResult(anchor=segment)\n                        else:\n                            return None\n                    else:\n                        # Just error if we don't care.\n                        return LintResult(anchor=segment)\n        return None",
        "file_path": "src/sqlfluff/rules/L013.py",
        "chunk_index": 0,
        "metadata": {
          "filepath": "src/sqlfluff/rules/L013.py",
          "function_name": "_eval",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 37,
          "start_line_no": 35,
          "end_line_no": 71
        }
      },
      {
        "rank": 9,
        "score": 0.5915414094924927,
        "content": "def _is_bad_tbl_ref(table_aliases, parent_select, tbl_ref):\n        \"\"\"Given a table reference, try to find what it's referring to.\"\"\"\n        # Is it referring to one of the table aliases?\n        if tbl_ref[0] in [a.ref_str for a in table_aliases]:\n            # Yes. Therefore okay.\n            return False\n\n        # Not a table alias. It it referring to a correlated subquery?\n        if parent_select:\n            parent_aliases, _ = get_aliases_from_select(parent_select)\n            if parent_aliases and tbl_ref[0] in [a[0] for a in parent_aliases]:\n                # Yes. Therefore okay.\n                return False\n\n        # It's not referring to an alias or a correlated subquery. Looks like a\n        # bad reference (i.e. referring to something unknown.)\n        return True",
        "file_path": "src/sqlfluff/rules/L026.py",
        "chunk_index": 0,
        "metadata": {
          "filepath": "src/sqlfluff/rules/L026.py",
          "function_name": "_is_bad_tbl_ref",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 17,
          "start_line_no": 40,
          "end_line_no": 56
        }
      },
      {
        "rank": 10,
        "score": 0.5877885818481445,
        "content": "def _eval(self, segment, **kwargs):\n        \"\"\"Join/From clauses should not contain subqueries. Use CTEs instead.\n\n        NB: No fix for this routine because it would be very complex to\n        implement reliably.\n        \"\"\"\n        parent_types = self._config_mapping[self.forbid_subquery_in]\n        for parent_type in parent_types:\n            if segment.is_type(parent_type):\n                # Get the referenced table segment\n                from_expression_element = segment.get_child(\"from_expression_element\")\n                if not from_expression_element:  # pragma: no cover\n                    return None  # There isn't one. We're done.\n                # Get the main bit\n                from_expression_element = from_expression_element.get_child(\n                    \"table_expression\"\n                )\n                if not from_expression_element:  # pragma: no cover\n                    return None  # There isn't one. We're done.\n                # Is it bracketed?\n                bracketed_expression = from_expression_element.get_child(\"bracketed\")\n                # If it is, lint that instead\n                if bracketed_expression:\n                    from_expression_element = bracketed_expression\n                # If any of the following are found, raise an issue.\n                # If not, we're fine.\n                problem_children = [\n                    \"with_compound_statement\",\n                    \"set_expression\",\n                    \"select_statement\",\n                ]\n                for seg_type in problem_children:\n                    seg = from_expression_element.get_child(seg_type)\n                    if seg:\n                        return LintResult(\n                            anchor=seg,\n                            description=f\"{parent_type} clauses should not contain subqueries. Use CTEs instead\",\n                        )",
        "file_path": "src/sqlfluff/rules/L042.py",
        "chunk_index": 0,
        "metadata": {
          "filepath": "src/sqlfluff/rules/L042.py",
          "function_name": "_eval",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 38,
          "start_line_no": 52,
          "end_line_no": 89
        }
      }
    ]
  },
  "sqlfluff__sqlfluff-2419": {
    "query": "Rule L060 could give a specific error message\nAt the moment rule L060 flags something like this:\r\n\r\n```\r\nL:  21 | P:   9 | L060 | Use 'COALESCE' instead of 'IFNULL' or 'NVL'.\r\n```\r\n\r\nSince we likely know the wrong word, it might be nice to actually flag that instead of both `IFNULL` and `NVL` - like most of the other rules do.\r\n\r\nThat is it should flag this:\r\n\r\n```\r\nL:  21 | P:   9 | L060 | Use 'COALESCE' instead of 'IFNULL'.\r\n```\r\n Or this:\r\n\r\n```\r\nL:  21 | P:   9 | L060 | Use 'COALESCE' instead of 'NVL'.\r\n```\r\n\r\nAs appropriate.\r\n\r\nWhat do you think @jpy-git ?\r\n\n",
    "method": "function",
    "top_k": 10,
    "results": [
      {
        "rank": 1,
        "score": 0.7197498083114624,
        "content": "def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Use ``COALESCE`` instead of ``IFNULL`` or ``NVL``.\"\"\"\n        # We only care about function names.\n        if context.segment.name != \"function_name_identifier\":\n            return None\n\n        # Only care if the function is ``IFNULL`` or ``NVL``.\n        if context.segment.raw_upper not in {\"IFNULL\", \"NVL\"}:\n            return None\n\n        # Create fix to replace ``IFNULL`` or ``NVL`` with ``COALESCE``.\n        fix = LintFix.replace(\n            context.segment,\n            [\n                CodeSegment(\n                    raw=\"COALESCE\",\n                    name=\"function_name_identifier\",\n                    type=\"function_name_identifier\",\n                )\n            ],\n        )\n\n        return LintResult(context.segment, [fix])",
        "file_path": "src/sqlfluff/rules/L060.py",
        "chunk_index": 0,
        "metadata": {
          "filepath": "src/sqlfluff/rules/L060.py",
          "function_name": "_eval",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 23,
          "start_line_no": 40,
          "end_line_no": 62
        }
      },
      {
        "rank": 2,
        "score": 0.5920895338058472,
        "content": "def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Unnecessary CASE statement.\"\"\"\n        # Look for CASE expression.\n        if (\n            context.segment.is_type(\"case_expression\")\n            and context.segment.segments[0].name == \"case\"\n        ):\n            # Find all 'WHEN' clauses and the optional 'ELSE' clause.\n            children = context.functional.segment.children()\n            when_clauses = children.select(sp.is_type(\"when_clause\"))\n            else_clauses = children.select(sp.is_type(\"else_clause\"))\n\n            # Can't fix if multiple WHEN clauses.\n            if len(when_clauses) > 1:\n                return None\n\n            # Find condition and then expressions.\n            condition_expression = when_clauses.children(sp.is_type(\"expression\"))[0]\n            then_expression = when_clauses.children(sp.is_type(\"expression\"))[1]\n\n            # Method 1: Check if THEN/ELSE expressions are both Boolean and can\n            # therefore be reduced.\n            if else_clauses:\n                else_expression = else_clauses.children(sp.is_type(\"expression\"))[0]\n                upper_bools = [\"TRUE\", \"FALSE\"]\n                if (\n                    (then_expression.raw_upper in upper_bools)\n                    and (else_expression.raw_upper in upper_bools)\n                    and (then_expression.raw_upper != else_expression.raw_upper)\n                ):\n                    coalesce_arg_1 = condition_expression\n                    coalesce_arg_2 = KeywordSegment(\"false\")\n                    preceding_not = then_expression.raw_upper == \"FALSE\"\n\n                    fixes = self._coalesce_fix_list(\n                        context,\n                        coalesce_arg_1,\n                        coalesce_arg_2,\n                        preceding_not,\n                    )\n\n                    return LintResult(\n                        anchor=condition_expression,\n                        fixes=fixes,\n                        description=\"Unnecessary CASE statement. \"\n                        \"Use COALESCE function instead.\",\n                    )\n\n            # Method 2: Check if the condition expression is comparing a column\n            # reference to NULL and whether that column reference is also in either the\n            # THEN/ELSE expression. We can only apply this method when there is only\n            # one condition in the condition expression.\n            condition_expression_segments_raw = {\n                segment.raw_upper for segment in condition_expression.segments\n            }\n            if {\"IS\", \"NULL\"}.issubset(condition_expression_segments_raw) and (\n                not condition_expression_segments_raw.intersection({\"AND\", \"OR\"})\n            ):\n                # Check if the comparison is to NULL or NOT NULL.\n                is_not_prefix = \"NOT\" in condition_expression_segments_raw\n\n                # Locate column reference in condition expression.\n                column_reference_segment = (\n                    Segments(condition_expression)\n                    .children(sp.is_type(\"column_reference\"))\n                    .get()\n                )\n\n                # Return None if none found (this condition does not apply to functions)\n                if not column_reference_segment:\n                    return None\n\n                if else_clauses:\n                    else_expression = else_clauses.children(sp.is_type(\"expression\"))[0]\n                    # Check if we can reduce the CASE expression to a single coalesce\n                    # function.\n                    if (\n                        not is_not_prefix\n                        and column_reference_segment.raw_upper\n                        == else_expression.raw_upper\n                    ):\n                        coalesce_arg_1 = else_expression\n                        coalesce_arg_2 = then_expression\n                    elif (\n                        is_not_prefix\n                        and column_reference_segment.raw_upper\n                        == then_expression.raw_upper\n                    ):\n                        coalesce_arg_1 = then_expression\n                        coalesce_arg_2 = else_expression\n                    else:\n                        return None\n\n                    if coalesce_arg_2.raw_upper == \"NULL\":\n                        # Can just specify the column on it's own\n                        # rather than using a COALESCE function.\n                        return LintResult(\n                            anchor=condition_expression,\n                            fixes=self._column_only_fix_list(\n                                context,\n                                column_reference_segment,\n                            ),\n                            description=\"Unnecessary CASE statement. \"\n                            f\"Just use column '{column_reference_segment.raw}'.\",\n                        )\n\n                    return LintResult(\n                        anchor=condition_expression,\n                        fixes=self._coalesce_fix_list(\n                            context,\n                            coalesce_arg_1,\n                            coalesce_arg_2,\n                        ),\n                        description=\"Unnecessary CASE statement. \"\n                        \"Use COALESCE function instead.\",\n                    )\n                elif (\n                    column_reference_segment.raw_segments_upper\n                    == then_expression.raw_segments_upper\n                ):\n                    # Can just specify the column on it's own\n                    # rather than using a COALESCE function.\n                    # In this case no ELSE statement is equivalent to ELSE NULL.\n                    return LintResult(\n                        anchor=condition_expression,\n                        fixes=self._column_only_fix_list(\n                            context,\n                            column_reference_segment,\n                        ),\n                        description=\"Unnecessary CASE statement. \"\n                        f\"Just use column '{column_reference_segment.raw}'.\",\n                    )\n\n        return None",
        "file_path": "src/sqlfluff/rules/L043.py",
        "chunk_index": 2,
        "metadata": {
          "filepath": "src/sqlfluff/rules/L043.py",
          "function_name": "_eval",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 134,
          "start_line_no": 125,
          "end_line_no": 258
        }
      },
      {
        "rank": 3,
        "score": 0.5890392661094666,
        "content": "def _eval(self, context: RuleContext) -> LintResult:\n        \"\"\"Relational operators should not be used to check for NULL values.\"\"\"\n        # Context/motivation for this rule:\n        # https://news.ycombinator.com/item?id=28772289\n        # https://stackoverflow.com/questions/9581745/sql-is-null-and-null\n        if len(context.segment.segments) <= 2:\n            return LintResult()\n\n        # Allow assignments in SET clauses\n        if context.parent_stack and context.parent_stack[-1].is_type(\n            \"set_clause_list\", \"execute_script_statement\"\n        ):\n            return LintResult()\n\n        # Allow assignments in EXEC clauses\n        if context.segment.is_type(\"set_clause_list\", \"execute_script_statement\"):\n            return LintResult()\n\n        # Iterate through children of this segment looking for equals or \"not\n        # equals\". Once found, check if the next code segment is a NULL literal.\n        idx_operator = None\n        operator = None\n        for idx, sub_seg in enumerate(context.segment.segments):\n            # Skip anything which is whitespace or non-code.\n            if sub_seg.is_whitespace or not sub_seg.is_code:\n                continue\n\n            # Look for \"=\" or \"<>\".\n            if not operator and sub_seg.name in (\"equals\", \"not_equal_to\"):\n                self.logger.debug(\n                    \"Found equals/not equals @%s: %r\", sub_seg.pos_marker, sub_seg.raw\n                )\n                idx_operator = idx\n                operator = sub_seg\n            elif operator:\n                # Look for a \"NULL\" literal.\n                if sub_seg.name == \"null_literal\":\n                    self.logger.debug(\n                        \"Found NULL literal following equals/not equals @%s: %r\",\n                        sub_seg.pos_marker,\n                        sub_seg.raw,\n                    )\n                    if sub_seg.raw[0] == \"N\":\n                        is_seg = KeywordSegment(\"IS\")\n                        not_seg = KeywordSegment(\"NOT\")\n                    else:\n                        is_seg = KeywordSegment(\"is\")\n                        not_seg = KeywordSegment(\"not\")\n\n                    edit: List[Union[WhitespaceSegment, KeywordSegment]] = (\n                        [is_seg]\n                        if operator.name == \"equals\"\n                        else [\n                            is_seg,\n                            WhitespaceSegment(),\n                            not_seg,\n                        ]\n                    )\n                    prev_seg = self._find_segment(\n                        idx_operator, context.segment.segments, before=True\n                    )\n                    next_seg = self._find_segment(\n                        idx_operator, context.segment.segments, before=False\n                    )\n                    if self._missing_whitespace(prev_seg, before=True):\n                        whitespace_segment: List[\n                            Union[WhitespaceSegment, KeywordSegment]\n                        ] = [WhitespaceSegment()]\n                        edit = whitespace_segment + edit\n                    if self._missing_whitespace(next_seg, before=False):\n                        edit = edit + [WhitespaceSegment()]\n                    return LintResult(\n                        anchor=operator,\n                        fixes=[\n                            LintFix.replace(\n                                operator,\n                                edit,\n                            )\n                        ],\n                    )\n        # If we get to here, it's not a violation\n        return LintResult()",
        "file_path": "src/sqlfluff/rules/L049.py",
        "chunk_index": 0,
        "metadata": {
          "filepath": "src/sqlfluff/rules/L049.py",
          "function_name": "_eval",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 82,
          "start_line_no": 36,
          "end_line_no": 117
        }
      },
      {
        "rank": 4,
        "score": 0.5738241672515869,
        "content": "def _coalesce_fix_list(\n        context: RuleContext,\n        coalesce_arg_1: BaseSegment,\n        coalesce_arg_2: BaseSegment,\n        preceding_not: bool = False,\n    ) -> List[LintFix]:\n        \"\"\"Generate list of fixes to convert CASE statement to COALESCE function.\"\"\"\n        # Add coalesce and opening parenthesis.\n        edits = [\n            KeywordSegment(\"coalesce\"),\n            SymbolSegment(\"(\", name=\"start_bracket\", type=\"start_bracket\"),\n            coalesce_arg_1,\n            SymbolSegment(\",\", name=\"comma\", type=\"comma\"),\n            WhitespaceSegment(),\n            coalesce_arg_2,\n            SymbolSegment(\")\", name=\"end_bracket\", type=\"end_bracket\"),\n        ]\n\n        if preceding_not:\n            not_edits: List[BaseSegment] = [\n                KeywordSegment(\"not\"),\n                WhitespaceSegment(),\n            ]\n            edits = not_edits + edits\n\n        fixes = [\n            LintFix.replace(\n                context.segment,\n                edits,\n            )\n        ]\n        return fixes",
        "file_path": "src/sqlfluff/rules/L043.py",
        "chunk_index": 0,
        "metadata": {
          "filepath": "src/sqlfluff/rules/L043.py",
          "function_name": "_coalesce_fix_list",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 32,
          "start_line_no": 78,
          "end_line_no": 109
        }
      },
      {
        "rank": 5,
        "score": 0.5420689582824707,
        "content": "def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Find rule violations and provide fixes.\n\n        0. Look for a case expression\n        1. Look for \"ELSE\"\n        2. Mark \"ELSE\" for deletion (populate \"fixes\")\n        3. Backtrack and mark all newlines/whitespaces for deletion\n        4. Look for a raw \"NULL\" segment\n        5.a. The raw \"NULL\" segment is found, we mark it for deletion and return\n        5.b. We reach the end of case when without matching \"NULL\": the rule passes\n        \"\"\"\n        if context.segment.is_type(\"case_expression\"):\n            children = context.functional.segment.children()\n            else_clause = children.first(sp.is_type(\"else_clause\"))\n\n            # Does the \"ELSE\" have a \"NULL\"? NOTE: Here, it's safe to look for\n            # \"NULL\", as an expression would *contain* NULL but not be == NULL.\n            if else_clause and else_clause.children(\n                lambda child: child.raw_upper == \"NULL\"\n            ):\n                # Found ELSE with NULL. Delete the whole else clause as well as\n                # indents/whitespaces/meta preceding the ELSE. :TRICKY: Note\n                # the use of reversed() to make select() effectively search in\n                # reverse.\n                before_else = children.reversed().select(\n                    start_seg=else_clause[0],\n                    loop_while=sp.or_(\n                        sp.is_name(\"whitespace\", \"newline\"), sp.is_meta()\n                    ),\n                )\n                return LintResult(\n                    anchor=context.segment,\n                    fixes=[LintFix.delete(else_clause[0])]\n                    + [LintFix.delete(seg) for seg in before_else],\n                )\n        return None",
        "file_path": "src/sqlfluff/rules/L035.py",
        "chunk_index": 0,
        "metadata": {
          "filepath": "src/sqlfluff/rules/L035.py",
          "function_name": "_eval",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 36,
          "start_line_no": 38,
          "end_line_no": 73
        }
      },
      {
        "rank": 6,
        "score": 0.5292851328849792,
        "content": "def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Inconsistent column references in GROUP BY/ORDER BY clauses.\"\"\"\n        # Config type hints\n        self.group_by_and_order_by_style: str\n\n        # We only care about GROUP BY/ORDER BY clauses.\n        if not context.segment.is_type(\"groupby_clause\", \"orderby_clause\"):\n            return None\n\n        # Look at child segments and map column references to either the implict or\n        # explicit category.\n        # N.B. segment names are used as the numeric literal type is 'raw', so best to\n        # be specific with the name.\n        column_reference_category_map = {\n            \"ColumnReferenceSegment\": \"explicit\",\n            \"ExpressionSegment\": \"explicit\",\n            \"numeric_literal\": \"implicit\",\n        }\n        column_reference_category_set = {\n            column_reference_category_map[segment.name]\n            for segment in context.segment.segments\n            if segment.name in column_reference_category_map\n        }\n\n        # If there are no column references then just return\n        if not column_reference_category_set:\n            return LintResult(memory=context.memory)\n\n        if self.group_by_and_order_by_style == \"consistent\":\n            # If consistent naming then raise lint error if either:\n\n            if len(column_reference_category_set) > 1:\n                # 1. Both implicit and explicit column references are found in the same\n                # clause.\n                return LintResult(\n                    anchor=context.segment,\n                    memory=context.memory,\n                )\n            else:\n                # 2. A clause is found to contain column name references that\n                #    contradict the precedent set in earlier clauses.\n                current_group_by_order_by_convention = (\n                    column_reference_category_set.pop()\n                )\n                prior_group_by_order_by_convention = context.memory.get(\n                    \"prior_group_by_order_by_convention\"\n                )\n\n                if prior_group_by_order_by_convention and (\n                    prior_group_by_order_by_convention\n                    != current_group_by_order_by_convention\n                ):\n                    return LintResult(\n                        anchor=context.segment,\n                        memory=context.memory,\n                    )\n\n                context.memory[\n                    \"prior_group_by_order_by_convention\"\n                ] = current_group_by_order_by_convention\n        else:\n            # If explicit or implicit naming then raise lint error\n            # if the opposite reference type is detected.\n            if any(\n                category != self.group_by_and_order_by_style\n                for category in column_reference_category_set\n            ):\n                return LintResult(\n                    anchor=context.segment,\n                    memory=context.memory,\n                )\n\n        # Return memory for later clauses.\n        return LintResult(memory=context.memory)",
        "file_path": "src/sqlfluff/rules/L054.py",
        "chunk_index": 0,
        "metadata": {
          "filepath": "src/sqlfluff/rules/L054.py",
          "function_name": "_eval",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 74,
          "start_line_no": 82,
          "end_line_no": 155
        }
      },
      {
        "rank": 7,
        "score": 0.5246068835258484,
        "content": "def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Column expression without alias. Use explicit `AS` clause.\n\n        We look for the select_clause_element segment, and then evaluate\n        whether it has an alias segment or not and whether the expression\n        is complicated enough. `parent_stack` is to assess how many other\n        elements there are.\n\n        \"\"\"\n        segment = context.functional.segment\n        children = segment.children()\n        if segment.all(sp.is_type(\"select_clause_element\")) and not children.any(\n            sp.is_type(\"alias_expression\")\n        ):\n            # Ignore if it's a function with EMITS clause as EMITS is equivalent to AS\n            if (\n                children.select(sp.is_type(\"function\"))\n                .children()\n                .select(sp.is_type(\"emits_segment\"))\n            ):\n                return None\n\n            types = set(\n                children.select(sp.not_(sp.is_name(\"star\"))).apply(sp.get_type())\n            )\n            unallowed_types = types - {\n                \"whitespace\",\n                \"newline\",\n                \"column_reference\",\n                \"wildcard_expression\",\n            }\n            if unallowed_types:\n                # No fixes, because we don't know what the alias should be,\n                # the user should document it themselves.\n                if self.allow_scalar:  # type: ignore\n                    # Check *how many* elements there are in the select\n                    # statement. If this is the only one, then we won't\n                    # report an error.\n                    immediate_parent = context.functional.parent_stack.last()\n                    num_elements = len(\n                        immediate_parent.children(sp.is_type(\"select_clause_element\"))\n                    )\n                    if num_elements > 1:\n                        return LintResult(anchor=context.segment)\n                    else:\n                        return None\n                else:\n                    # Just error if we don't care.\n                    return LintResult(anchor=context.segment)\n        return None",
        "file_path": "src/sqlfluff/rules/L013.py",
        "chunk_index": 0,
        "metadata": {
          "filepath": "src/sqlfluff/rules/L013.py",
          "function_name": "_eval",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 50,
          "start_line_no": 37,
          "end_line_no": 86
        }
      },
      {
        "rank": 8,
        "score": 0.5222163200378418,
        "content": "def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Implicit aliasing of table/column not allowed. Use explicit `AS` clause.\n\n        We look for the alias segment, and then evaluate its parent and whether\n        it contains an AS keyword. This is the _eval function for both L011 and L012.\n\n        The use of `raw_stack` is just for working out how much whitespace to add.\n\n        \"\"\"\n        fixes = []\n\n        if context.segment.is_type(\"alias_expression\"):\n            if context.parent_stack[-1].is_type(*self._target_elems):\n                if any(e.name.lower() == \"as\" for e in context.segment.segments):\n                    if self.aliasing == \"implicit\":  # type: ignore\n                        if context.segment.segments[0].name.lower() == \"as\":\n\n                            # Remove the AS as we're using implict aliasing\n                            fixes.append(LintFix.delete(context.segment.segments[0]))\n                            anchor = context.raw_stack[-1]\n\n                            # Remove whitespace before (if exists) or after (if not)\n                            if (\n                                len(context.raw_stack) > 0\n                                and context.raw_stack[-1].type == \"whitespace\"\n                            ):\n                                fixes.append(LintFix.delete(context.raw_stack[-1]))\n                            elif (\n                                len(context.segment.segments) > 0\n                                and context.segment.segments[1].type == \"whitespace\"\n                            ):\n                                fixes.append(\n                                    LintFix.delete(context.segment.segments[1])\n                                )\n\n                            return LintResult(anchor=anchor, fixes=fixes)\n\n                else:\n                    insert_buff: List[Union[WhitespaceSegment, KeywordSegment]] = []\n\n                    # Add initial whitespace if we need to...\n                    if context.raw_stack[-1].name not in [\"whitespace\", \"newline\"]:\n                        insert_buff.append(WhitespaceSegment())\n\n                    # Add an AS (Uppercase for now, but could be corrected later)\n                    insert_buff.append(KeywordSegment(\"AS\"))\n\n                    # Add a trailing whitespace if we need to\n                    if context.segment.segments[0].name not in [\n                        \"whitespace\",\n                        \"newline\",\n                    ]:\n                        insert_buff.append(WhitespaceSegment())\n\n                    return LintResult(\n                        anchor=context.segment,\n                        fixes=[\n                            LintFix.create_before(\n                                context.segment.segments[0],\n                                insert_buff,\n                            )\n                        ],\n                    )\n        return None",
        "file_path": "src/sqlfluff/rules/L011.py",
        "chunk_index": 0,
        "metadata": {
          "filepath": "src/sqlfluff/rules/L011.py",
          "function_name": "_eval",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 64,
          "start_line_no": 44,
          "end_line_no": 107
        }
      },
      {
        "rank": 9,
        "score": 0.5108767747879028,
        "content": "def _eval(self, context: RuleContext) -> Optional[List[LintResult]]:\n        \"\"\"Look for USING in a join clause.\"\"\"\n        if context.segment.is_type(\"join_clause\"):\n            for seg in context.segment.segments:\n                if seg.is_type(\"keyword\") and seg.name == \"using\":\n                    return [\n                        LintResult(\n                            # Reference the element, not the string.\n                            anchor=seg,\n                            description=(\n                                \"Found USING statement. Expected only ON statements.\"\n                            ),\n                        )\n                    ]\n        return None",
        "file_path": "src/sqlfluff/rules/L032.py",
        "chunk_index": 0,
        "metadata": {
          "filepath": "src/sqlfluff/rules/L032.py",
          "function_name": "_eval",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 15,
          "start_line_no": 36,
          "end_line_no": 50
        }
      },
      {
        "rank": 10,
        "score": 0.4960564076900482,
        "content": "def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Identify aliases in from clause and join conditions.\n\n        Find base table, table expressions in join, and other expressions in select\n        clause and decide if it's needed to report them.\n        \"\"\"\n        if context.segment.is_type(\"select_statement\"):\n            children = context.functional.segment.children()\n            from_clause_segment = children.select(sp.is_type(\"from_clause\")).first()\n            base_table = (\n                from_clause_segment.children(sp.is_type(\"from_expression\"))\n                .first()\n                .children(sp.is_type(\"from_expression_element\"))\n                .first()\n                .children(sp.is_type(\"table_expression\"))\n                .first()\n                .children(sp.is_type(\"object_reference\"))\n                .first()\n            )\n            if not base_table:\n                return None\n\n            # A buffer for all table expressions in join conditions\n            from_expression_elements = []\n            column_reference_segments = []\n\n            after_from_clause = children.select(start_seg=from_clause_segment[0])\n            for clause in from_clause_segment + after_from_clause:\n                for from_expression_element in clause.recursive_crawl(\n                    \"from_expression_element\"\n                ):\n                    from_expression_elements.append(from_expression_element)\n                for column_reference in clause.recursive_crawl(\"column_reference\"):\n                    column_reference_segments.append(column_reference)\n\n            return (\n                self._lint_aliases_in_join(\n                    base_table[0] if base_table else None,\n                    from_expression_elements,\n                    column_reference_segments,\n                    context.segment,\n                )\n                or None\n            )\n        return None",
        "file_path": "src/sqlfluff/rules/L031.py",
        "chunk_index": 0,
        "metadata": {
          "filepath": "src/sqlfluff/rules/L031.py",
          "function_name": "_eval",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 45,
          "start_line_no": 61,
          "end_line_no": 105
        }
      }
    ]
  },
  "sqlfluff__sqlfluff-1733": {
    "query": "Extra space when first field moved to new line in a WITH statement\nNote, the query below uses a `WITH` statement. If I just try to fix the SQL within the CTE, this works fine.\r\n\r\nGiven the following SQL:\r\n\r\n```sql\r\nWITH example AS (\r\n    SELECT my_id,\r\n        other_thing,\r\n        one_more\r\n    FROM\r\n        my_table\r\n)\r\n\r\nSELECT *\r\nFROM example\r\n```\r\n\r\n## Expected Behaviour\r\n\r\nafter running `sqlfluff fix` I'd expect (`my_id` gets moved down and indented properly):\r\n\r\n```sql\r\nWITH example AS (\r\n    SELECT\r\n        my_id,\r\n        other_thing,\r\n        one_more\r\n    FROM\r\n        my_table\r\n)\r\n\r\nSELECT *\r\nFROM example\r\n```\r\n\r\n## Observed Behaviour\r\n\r\nafter running `sqlfluff fix` we get (notice that `my_id` is indented one extra space)\r\n\r\n```sql\r\nWITH example AS (\r\n    SELECT\r\n         my_id,\r\n        other_thing,\r\n        one_more\r\n    FROM\r\n        my_table\r\n)\r\n\r\nSELECT *\r\nFROM example\r\n```\r\n\r\n## Steps to Reproduce\r\n\r\nNoted above. Create a file with the initial SQL and fun `sqfluff fix` on it.\r\n\r\n## Dialect\r\n\r\nRunning with default config.\r\n\r\n## Version\r\nInclude the output of `sqlfluff --version` along with your Python version\r\n\r\nsqlfluff, version 0.7.0\r\nPython 3.7.5\r\n\r\n## Configuration\r\n\r\nDefault config.\r\n\n",
    "method": "function",
    "top_k": 10,
    "results": [
      {
        "rank": 1,
        "score": 0.6755046844482422,
        "content": "self, context: RuleContext) -> LintResult:\n        \"\"\"WITH clause closing bracket should be aligned with WITH keyword.\n\n        Look for a with clause and evaluate the position of closing brackets.\n        \"\"\"\n        # We only trigger on start_bracket (open parenthesis)\n        if context.segment.is_type(\"with_compound_statement\"):\n            raw_stack_buff = list(context.raw_stack)\n            # Look for the with keyword\n            for seg in context.segment.segments:\n                if seg.name.lower() == \"with\":\n                    seg_line_no = seg.pos_marker.line_no\n                    break\n            else:  # pragma: no cover\n                # This *could* happen if the with statement is unparsable,\n                # in which case then the user will have to fix that first.\n                if any(s.is_type(\"unparsable\") for s in context.segment.segments):\n                    return LintResult()\n                # If it's parsable but we still didn't find a with, then\n                # we should raise that.\n                raise RuntimeError(\"Didn't find WITH keyword!\")\n\n            def indent_size_up_to(segs):\n                seg_buff = []\n                # Get any segments running up to the WITH\n                for elem in reversed(segs):\n                    if elem.is_type(\"newline\"):\n                        break\n                    elif elem.is_meta:\n                        continue\n                    else:\n                        seg_buff.append(elem)\n                # reverse the indent if we have one\n                if seg_buff:\n                    seg_buff = list(reversed(seg_buff))\n                indent_str = \"\".join(seg.raw for seg in seg_buff).replace(\n                    \"\\t\", \" \" * self.tab_space_size\n                )\n                indent_size = len(indent_str)\n                return indent_size, indent_str\n\n            balance = 0\n            with_indent, with_indent_str = indent_size_up_to(raw_stack_buff)\n            for seg in context.segment.iter_segments(\n                expanding=[\"common_table_expression\", \"bracketed\"], pass_through=True\n            ):\n                if seg.name == \"start_bracket\":\n                    balance += 1\n                elif seg.name == \"end_bracket\":\n                    balance -= 1\n                    if balance == 0:\n                        closing_bracket_indent, _ = indent_size_up_to(raw_stack_buff)\n                        indent_diff = closing_bracket_indent - with_indent\n                        # Is indent of closing bracket not the same as\n                        # indent of WITH keyword.\n                        if seg.pos_marker.line_no == seg_line_no:\n                            # Skip if it's the one-line version. That's ok\n                            pass\n                        elif indent_diff < 0:\n                            return LintResult(\n                                anchor=seg,\n                                fixes=[\n                                    LintFix(\n                                        \"create\",\n                                        seg,\n                                        WhitespaceSegment(\" \" * (-indent_diff)),\n                                    )\n                                ],\n                            )\n                        elif indent_diff > 0:\n                            # Is it all whitespace before the bracket on this line?\n                            prev_segs_on_line = [\n                                elem\n                                for elem in context.segment.iter_segments(\n                                    expanding=[\"common_table_expression\", \"bracketed\"],\n                                    pass_through=True,\n                                )\n                                if elem.pos_marker.line_no == seg.pos_marker.line_no\n                                and elem.pos_marker.line_pos < seg.pos_marker.line_pos\n                            ]\n                            if all(\n                                elem.is_type(\"whitespace\") for elem in prev_segs_on_line\n                            ):\n                                # We can move it back, it's all whitespace\n                                fixes = [\n                                    LintFix(\n                                        \"create\",\n                                        seg,\n                                        [WhitespaceSegment(with_indent_str)],\n                                    )\n                                ] + [\n                                    LintFix(\"delete\", elem)\n                                    for elem in prev_segs_on_line\n                                ]\n                            else:\n                                # We have to move it to a newline\n                                fixes = [\n                                    LintFix(\n                                        \"create\",\n                                        seg,\n                                        [\n                                            NewlineSegment(),\n                                            WhitespaceSegment(with_indent_str),\n                                        ],\n                                    )\n                                ]\n                            return LintResult(anchor=seg, fixes=fixes)\n                else:\n                    raw_stack_buff.append(seg)\n        return LintResult()\n",
        "file_path": "src/sqlfluff/rules/L018.py",
        "chunk_index": 0,
        "metadata": {
          "filepath": "src/sqlfluff/rules/L018.py",
          "function_name": "_eval",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 110,
          "start_line_no": 42,
          "end_line_no": 151
        }
      },
      {
        "rank": 2,
        "score": 0.6121230125427246,
        "content": "uleContext) -> LintResult:\n        \"\"\"Incorrect indentation found in file.\"\"\"\n        # Config type hints\n        self.tab_space_size: int\n        self.indent_unit: str\n\n        tab = \"\\t\"\n        space = \" \"\n        correct_indent = (\n            space * self.tab_space_size if self.indent_unit == \"space\" else tab\n        )\n        wrong_indent = (\n            tab if self.indent_unit == \"space\" else space * self.tab_space_size\n        )\n        if (\n            context.segment.is_type(\"whitespace\")\n            and wrong_indent in context.segment.raw\n        ):\n            fixes = []\n            description = \"Incorrect indentation type found in file.\"\n            edit_indent = context.segment.raw.replace(wrong_indent, correct_indent)\n            # Ensure that the number of space indents is a multiple of tab_space_size\n            # before attempting to convert spaces to tabs to avoid mixed indents\n            # unless we are converted tabs to spaces (indent_unit = space)\n            if (\n                (\n                    self.indent_unit == \"space\"\n                    or context.segment.raw.count(space) % self.tab_space_size == 0\n                )\n                # Only attempt a fix at the start of a newline for now\n                and (\n                    len(context.raw_stack) == 0\n                    or context.raw_stack[-1].is_type(\"newline\")\n                )\n            ):\n                fixes = [\n                    LintFix(\n                        \"edit\",\n                        context.segment,\n                        WhitespaceSegment(raw=edit_indent),\n                    )\n                ]\n            elif not (\n                len(context.raw_stack) == 0 or context.raw_stack[-1].is_type(\"newline\")\n            ):\n                # give a helpful message if the wrong indent has been found and is not at the start of a newline\n                description += (\n                    \" The indent occurs after other text, so a manual fix is needed.\"\n                )\n            else:\n                # If we get here, the indent_unit is tabs, and the number of spaces is not a multiple of tab_space_size\n                description += \" The number of spaces is not a multiple of tab_space_size, so a manual fix is needed.\"\n            return LintResult(\n                anchor=context.segment, fixes=fixes, description=description\n            )\n        return LintResult()\n",
        "file_path": "src/sqlfluff/rules/L004.py",
        "chunk_index": 0,
        "metadata": {
          "filepath": "src/sqlfluff/rules/L004.py",
          "function_name": "_eval",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 56,
          "start_line_no": 46,
          "end_line_no": 101
        }
      },
      {
        "rank": 3,
        "score": 0.6026754379272461,
        "content": "def fix(sql, dialect=\"ansi\", rules=None):\n    \"\"\"Fix a sql string or file.\n\n    Args:\n        sql (:obj:`str` or file-like object): The sql to be linted\n            either as a string or a subclass of :obj:`TextIOBase`.\n        dialect (:obj:`str`, optional): A reference to the dialect of the sql\n            to be linted. Defaults to `ansi`.\n        rules (:obj:`str` or iterable of :obj:`str`, optional): A subset of rule\n            reference to lint for.\n\n    Returns:\n        :obj:`str` for the fixed sql if possible.\n    \"\"\"\n    sql = _unify_str_or_file(sql)\n    linter = Linter(dialect=dialect, rules=rules)\n\n    result = linter.lint_string_wrapped(sql, fix=True)\n    fixed_string = result.paths[0].files[0].fix_string()[0]\n    return fixed_string",
        "file_path": "src/sqlfluff/api/simple.py",
        "chunk_index": 3,
        "metadata": {
          "filepath": "src/sqlfluff/api/simple.py",
          "function_name": "fix",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 20,
          "start_line_no": 50,
          "end_line_no": 69
        }
      },
      {
        "rank": 4,
        "score": 0.5808979272842407,
        "content": "def _eval(self, context: RuleContext) -> Optional[List[LintResult]]:\n        \"\"\"Blank line expected but not found after CTE definition.\"\"\"\n        # Config type hints\n        self.comma_style: str\n\n        error_buffer = []\n        if context.segment.is_type(\"with_compound_statement\"):\n            # First we need to find all the commas, the end brackets, the\n            # things that come after that and the blank lines in between.\n\n            # Find all the closing brackets. They are our anchor points.\n            bracket_indices = []\n            expanded_segments = list(\n                context.segment.iter_segments(expanding=[\"common_table_expression\"])\n            )\n            for idx, seg in enumerate(expanded_segments):\n                if seg.is_type(\"bracketed\"):\n                    bracket_indices.append(idx)\n\n            # Work through each point and deal with it individually\n            for bracket_idx in bracket_indices:\n                forward_slice = expanded_segments[bracket_idx:]\n                seg_idx = 1\n                line_idx = 0\n                comma_seg_idx = 0\n                blank_lines = 0\n                comma_line_idx = None\n                line_blank = False\n                comma_style = None\n                line_starts = {}\n                comment_lines = []\n\n                self.logger.info(\n                    \"## CTE closing bracket found at %s, idx: %s. Forward slice: %.20r\",\n                    forward_slice[0].pos_marker,\n                    bracket_idx,\n                    \"\".join(elem.raw for elem in forward_slice),\n                )\n\n                # Work forward to map out the following segments.\n                while (\n                    forward_slice[seg_idx].is_type(\"comma\")\n                    or not forward_slice[seg_idx].is_code\n                ):\n                    if forward_slice[seg_idx].is_type(\"newline\"):\n                        if line_blank:\n                            # It's a blank line!\n                            blank_lines += 1\n                        line_blank = True\n                        line_idx += 1\n                        line_starts[line_idx] = seg_idx + 1\n                    elif forward_slice[seg_idx].is_type(\"comment\"):\n                        # Lines with comments aren't blank\n                        line_blank = False\n                        comment_lines.append(line_idx)\n                    elif forward_slice[seg_idx].is_type(\"comma\"):\n                        # Keep track of where the comma is.\n                        # We'll evaluate it later.\n                        comma_line_idx = line_idx\n                        comma_seg_idx = seg_idx\n                    seg_idx += 1\n\n                # Infer the comma style (NB this could be different for each case!)\n                if comma_line_idx is None:\n                    comma_style = \"final\"\n                elif line_idx == 0:\n                    comma_style = \"oneline\"\n                elif comma_line_idx == 0:\n                    comma_style = \"trailing\"\n                elif comma_line_idx == line_idx:\n                    comma_style = \"leading\"\n                else:\n                    comma_style = \"floating\"\n\n                # Readout of findings\n                self.logger.info(\n                    \"blank_lines: %s, comma_line_idx: %s. final_line_idx: %s, final_seg_idx: %s\",\n                    blank_lines,\n                    comma_line_idx,\n                    line_idx,\n                    seg_idx,\n                )\n                self.logger.info(\n                    \"comma_style: %r, line_starts: %r, comment_lines: %r\",\n                    comma_style,\n                    line_starts,\n                    comment_lines,\n                )\n\n                if blank_lines < 1:\n                    # We've got an issue\n                    self.logger.info(\"!! Found CTE without enough blank lines.\")\n\n                    # Based on the current location of the comma we insert newlines\n                    # to correct the issue.\n                    fix_type = \"create\"  # In most cases we just insert newlines.\n                    if comma_style == \"oneline\":\n                        # Here we respect the target comma style to insert at the relevant point.\n                        if self.comma_style == \"trailing\":\n                            # Add a blank line after the comma\n                            fix_point = forward_slice[comma_seg_idx + 1]\n                            # Optionally here, if the segment we've landed on is\n                            # whitespace then we REPLACE it rather than inserting.\n                            if forward_slice[comma_seg_idx + 1].is_type(\"whitespace\"):\n                                fix_type = \"edit\"\n                        elif self.comma_style == \"leading\":\n                            # Add a blank line before the comma\n                            fix_point = forward_slice[comma_seg_idx]\n                        # In both cases it's a double newline.\n                        num_newlines = 2\n                    else:\n                        # In the following cases we only care which one we're in\n                        # when comments don't get in the way. If they *do*, then\n                        # we just work around them.\n                        if not comment_lines or line_idx - 1 not in comment_lines:\n                            self.logger.info(\"Comment routines not applicable\")\n                            if comma_style in (\"trailing\", \"final\", \"floating\"):\n                                # Detected an existing trailing comma or it's a final CTE,\n                                # OR the comma isn't leading or trailing.\n                                # If the preceding segment is whitespace, replace it\n                                if forward_slice[seg_idx - 1].is_type(\"whitespace\"):\n                                    fix_point = forward_slice[seg_idx - 1]\n                                    fix_type = \"edit\"\n                                else:\n                                    # Otherwise add a single newline before the end content.\n                                    fix_point = forward_slice[seg_idx]\n                            elif comma_style == \"leading\":\n                                # Detected an existing leading comma.\n                                fix_point = forward_slice[comma_seg_idx]\n                        else:\n                            self.logger.info(\"Handling preceding comments\")\n                            offset = 1\n                            while line_idx - offset in comment_lines:\n                                offset += 1\n                            fix_point = forward_slice[\n                                line_starts[line_idx - (offset - 1)]\n                            ]\n                        # Note: There is an edge case where this isn't enough, if\n                        # comments are in strange places, but we'll catch them on\n                        # the next iteration.\n                        num_newlines = 1\n\n                    fixes = [\n                        LintFix(\n                            fix_type,\n                            fix_point,\n                            [NewlineSegment()] * num_newlines,\n                        )\n                    ]\n                    # Create a result, anchored on the start of the next content.\n                    error_buffer.append(\n                        LintResult(anchor=forward_slice[seg_idx], fixes=fixes)\n                    )\n        # Return the buffer if we have one.\n        return error_buffer or None",
        "file_path": "src/sqlfluff/rules/L022.py",
        "chunk_index": 0,
        "metadata": {
          "filepath": "src/sqlfluff/rules/L022.py",
          "function_name": "_eval",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 155,
          "start_line_no": 40,
          "end_line_no": 194
        }
      },
      {
        "rank": 5,
        "score": 0.5733485817909241,
        "content": "def cli():\n    \"\"\"Sqlfluff is a modular sql linter for humans.\"\"\"",
        "file_path": "src/sqlfluff/cli/commands.py",
        "chunk_index": 6,
        "metadata": {
          "filepath": "src/sqlfluff/cli/commands.py",
          "function_name": "cli",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 2,
          "start_line_no": 269,
          "end_line_no": 270
        }
      },
      {
        "rank": 6,
        "score": 0.5678036212921143,
        "content": "def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Select clause modifiers must appear on same line as SELECT.\"\"\"\n        if context.segment.is_type(\"select_clause\"):\n            # Does the select clause have modifiers?\n            select_modifier = context.segment.get_child(\"select_clause_modifier\")\n            if not select_modifier:\n                return None  # No. We're done.\n            select_modifier_idx = context.segment.segments.index(select_modifier)\n\n            # Does the select clause contain a newline?\n            newline = context.segment.get_child(\"newline\")\n            if not newline:\n                return None  # No. We're done.\n            newline_idx = context.segment.segments.index(newline)\n\n            # Is there a newline before the select modifier?\n            if newline_idx > select_modifier_idx:\n                return None  # No, we're done.\n\n            # Yes to all the above. We found an issue.\n\n            # E.g.: \" DISTINCT\\n\"\n            replace_newline_with = [\n                WhitespaceSegment(),\n                select_modifier,\n                NewlineSegment(),\n            ]\n            fixes = [\n                # E.g. \"\\n\" -> \" DISTINCT\\n.\n                LintFix(\"edit\", newline, replace_newline_with),\n                # E.g. \"DISTINCT\" -> X\n                LintFix(\"delete\", select_modifier),\n            ]\n\n            # E.g. \" \" after \"DISTINCT\"\n            ws_to_delete = context.segment.select_children(\n                start_seg=select_modifier,\n                select_if=lambda s: s.is_type(\"whitespace\"),\n                loop_while=lambda s: s.is_type(\"whitespace\") or s.is_meta,\n            )\n\n            # E.g. \" \" -> X\n            fixes += [LintFix(\"delete\", ws) for ws in ws_to_delete]\n            return LintResult(\n                anchor=context.segment,\n                fixes=fixes,\n            )\n\n        return None",
        "file_path": "src/sqlfluff/rules/L041.py",
        "chunk_index": 0,
        "metadata": {
          "filepath": "src/sqlfluff/rules/L041.py",
          "function_name": "_eval",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 49,
          "start_line_no": 35,
          "end_line_no": 83
        }
      },
      {
        "rank": 7,
        "score": 0.56224125623703,
        "content": "val(self, context: RuleContext) -> LintResult:\n        \"\"\"Unnecessary trailing whitespace.\n\n        Look for newline segments, and then evaluate what\n        it was preceded by.\n        \"\"\"\n        # We only trigger on newlines\n        if (\n            context.segment.is_type(\"newline\")\n            and len(context.raw_stack) > 0\n            and context.raw_stack[-1].is_type(\"whitespace\")\n        ):\n            # If we find a newline, which is preceded by whitespace, then bad\n            deletions = []\n            idx = -1\n            while abs(idx) <= len(context.raw_stack) and context.raw_stack[idx].is_type(\n                \"whitespace\"\n            ):\n                deletions.append(context.raw_stack[idx])\n                idx -= 1\n            last_deletion_slice = deletions[-1].pos_marker.source_slice\n\n            # Check the raw source (before template expansion) immediately\n            # following the whitespace we want to delete. Often, what looks\n            # like trailing whitespace in rendered SQL is actually a line like:\n            # \"    {% for elem in elements %}\\n\", in which case the code is\n            # fine -- it's not trailing whitespace from a source code\n            # perspective.\n            if context.templated_file:\n                next_raw_slice = (\n                    context.templated_file.raw_slices_spanning_source_slice(\n                        slice(last_deletion_slice.stop, last_deletion_slice.stop)\n                    )\n                )\n                # If the next slice is literal, that means it's regular code, so\n                # it's safe to delete the trailing whitespace. If it's anything\n                # else, it's template code, so don't delete the whitespace because\n                # it's not REALLY trailing whitespace in terms of the raw source\n                # code.\n                if next_raw_slice[0].slice_type != \"literal\":\n                    return LintResult()\n            return LintResult(\n                anchor=deletions[-1],\n                fixes=[LintFix(\"delete\", d) for d in deletions],\n            )\n        return LintResult()\n",
        "file_path": "src/sqlfluff/rules/L001.py",
        "chunk_index": 0,
        "metadata": {
          "filepath": "src/sqlfluff/rules/L001.py",
          "function_name": "_eval",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 46,
          "start_line_no": 30,
          "end_line_no": 75
        }
      },
      {
        "rank": 8,
        "score": 0.5613899230957031,
        "content": "\n        desired_indent: str,\n        current_indent_buffer: Tuple[RawSegment, ...],\n        current_anchor: BaseSegment,\n    ) -> List[LintFix]:\n        \"\"\"Generate fixes to make an indent a certain size.\"\"\"\n        # If there shouldn't be an indent at all, just delete.\n        if len(desired_indent) == 0:\n            fixes = [LintFix(\"delete\", elem) for elem in current_indent_buffer]\n        # If we don't have any indent and we should, then add a single\n        elif len(\"\".join(elem.raw for elem in current_indent_buffer)) == 0:\n            fixes = [\n                LintFix(\n                    \"create\",\n                    current_anchor,\n                    WhitespaceSegment(\n                        raw=desired_indent,\n                    ),\n                )\n            ]\n        # Otherwise edit the first element to be the right size\n        else:\n            # Edit the first element of this line's indent.\n            fixes = [\n                LintFix(\n                    \"edit\",\n                    current_indent_buffer[0],\n                    WhitespaceSegment(\n                        raw=desired_indent,\n                    ),\n                )\n            ]\n        return fixes\n\n    @staticmethod\n    def _strip_b",
        "file_path": "src/sqlfluff/rules/L003.py",
        "chunk_index": 6,
        "metadata": {
          "filepath": "src/sqlfluff/rules/L003.py",
          "function_name": "_coerce_indent_to",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 36,
          "start_line_no": 254,
          "end_line_no": 287
        }
      },
      {
        "rank": 9,
        "score": 0.5570734739303589,
        "content": "ntext: RuleContext) -> Optional[LintResult]:\n        \"\"\"Mixed Tabs and Spaces in single whitespace.\n\n        Only trigger from whitespace segments if they contain\n        multiple kinds of whitespace.\n        \"\"\"\n        # Config type hints\n        self.tab_space_size: int\n\n        if context.segment.is_type(\"whitespace\"):\n            if \" \" in context.segment.raw and \"\\t\" in context.segment.raw:\n                if len(context.raw_stack) == 0 or context.raw_stack[-1].is_type(\n                    \"newline\"\n                ):\n                    # We've got a single whitespace at the beginning of a line.\n                    # It's got a mix of spaces and tabs. Replace each tab with\n                    # a multiple of spaces\n                    return LintResult(\n                        anchor=context.segment,\n                        fixes=[\n                            LintFix(\n                                \"edit\",\n                                context.segment,\n                                context.segment.edit(\n                                    context.segment.raw.replace(\n                                        \"\\t\", \" \" * self.tab_space_size\n                                    )\n                                ),\n                            )\n                        ],\n                    )\n        return None\n",
        "file_path": "src/sqlfluff/rules/L002.py",
        "chunk_index": 0,
        "metadata": {
          "filepath": "src/sqlfluff/rules/L002.py",
          "function_name": "_eval",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 32,
          "start_line_no": 44,
          "end_line_no": 75
        }
      },
      {
        "rank": 10,
        "score": 0.5570052862167358,
        "content": "def fix(\n    force: bool,\n    paths: Tuple[str],\n    processes: int,\n    bench: bool = False,\n    fixed_suffix: str = \"\",\n    logger: Optional[logging.Logger] = None,\n    **kwargs,\n) -> NoReturn:\n    \"\"\"Fix SQL files.\n\n    PATH is the path to a sql file or directory to lint. This can be either a\n    file ('path/to/file.sql'), a path ('directory/of/sql/files'), a single ('-')\n    character to indicate reading from *stdin* or a dot/blank ('.'/' ') which will\n    be interpreted like passing the current working directory as a path argument.\n    \"\"\"\n    # some quick checks\n    fixing_stdin = (\"-\",) == paths\n\n    config = get_config(**kwargs)\n    lnt, formatter = get_linter_and_formatter(config, silent=fixing_stdin)\n    verbose = config.get(\"verbose\")\n    exit_code = 0\n\n    formatter.dispatch_config(lnt)\n\n    # Set up logging.\n    set_logging_level(verbosity=verbose, logger=logger, stderr_output=fixing_stdin)\n\n    # handle stdin case. should output formatted sql to stdout and nothing else.\n    if fixing_stdin:\n        stdin = sys.stdin.read()\n\n        result = lnt.lint_string_wrapped(stdin, fname=\"stdin\", fix=True)\n        templater_error = result.num_violations(types=SQLTemplaterError) > 0\n        unfixable_error = result.num_violations(types=SQLLintError, fixable=False) > 0\n\n        if result.num_violations(types=SQLLintError, fixable=True) > 0:\n            stdout = result.paths[0].files[0].fix_string()[0]\n        else:\n            stdout = stdin\n\n        if templater_error:\n            click.echo(\n                colorize(\n                    \"Fix aborted due to unparseable template variables.\",\n                    Color.red,\n                ),\n                err=True,\n            )\n            click.echo(\n                colorize(\n                    \"Use '--ignore templating' to attempt to fix anyway.\",\n                    Color.red,\n                ),\n                err=True,\n            )\n        if unfixable_error:\n            click.echo(colorize(\"Unfixable violations detected.\", Color.red), err=True)\n\n        click.echo(stdout, nl=False)\n        sys.exit(1 if templater_error or unfixable_error else 0)\n\n    # Lint the paths (not with the fix argument at this stage), outputting as we go.\n    click.echo(\"==== finding fixable violations ====\")\n    try:\n        result = lnt.lint_paths(\n            paths, fix=True, ignore_non_existent_files=False, processes=processes\n        )\n    except OSError:\n        click.echo(\n            colorize(\n                f\"The path(s) '{paths}' could not be accessed. Check it/they exist(s).\",\n                Color.red,\n            ),\n            err=True,\n        )\n        sys.exit(1)\n\n    # NB: We filter to linting violations here, because they're\n    # the only ones which can be potentially fixed.\n    if result.num_violations(types=SQLLintError, fixable=True) > 0:\n        click.echo(\"==== fixing violations ====\")\n        click.echo(\n            f\"{result.num_violations(types=SQLLintError, fixable=True)} fixable linting violations found\"\n        )\n        if force:\n            click.echo(f\"{colorize('FORCE MODE', Color.red)}: Attempting fixes...\")\n            success = do_fixes(\n                lnt,\n                result,\n                formatter,\n                types=SQLLintError,\n                fixed_file_suffix=fixed_suffix,\n            )\n            if not success:\n                sys.exit(1)  # pragma: no cover\n        else:\n            click.echo(\n                \"Are you sure you wish to attempt to fix these? [Y/n] \", nl=False\n            )\n            c = click.getchar().lower()\n            click.echo(\"...\")\n            if c in (\"y\", \"\\r\", \"\\n\"):\n                click.echo(\"Attempting fixes...\")\n                success = do_fixes(\n                    lnt,\n                    result,\n                    formatter,\n                    types=SQLLintError,\n                    fixed_file_suffix=fixed_suffix,\n                )\n                if not success:\n                    sys.exit(1)  # pragma: no cover\n                else:\n                    _completion_message(config)\n            elif c == \"n\":\n                click.echo(\"Aborting...\")\n                exit_code = 1\n            else:  # pragma: no cover\n                click.echo(\"Invalid input, please enter 'Y' or 'N'\")\n                click.echo(\"Aborting...\")\n                exit_code = 1\n    else:\n        click.echo(\"==== no fixable linting violations found ====\")\n        _completion_message(config)\n\n    if result.num_violations(types=SQLLintError, fixable=False) > 0:\n        click.echo(\n            f\"  [{result.num_violations(types=SQLLintError, fixable=False)} unfixable linting violations found]\"\n        )\n        exit_code = 1\n\n    if result.num_violations(types=SQLTemplaterError) > 0:\n        click.echo(\n            f\"  [{result.num_violations(types=SQLTemplaterError)} templating errors found]\"\n        )\n        exit_code = 1\n\n    if bench:\n        click.echo(\"==== overall timings ====\")\n        click.echo(cli_table([(\"Clock time\", result.total_time)]))\n        timing_summary = result.timing_summary()\n        for step in timing_summary:\n            click.echo(f\"=== {step} ===\")\n            click.echo(cli_table(timing_summary[step].items()))\n\n    sys.exit(exit_code)",
        "file_path": "src/sqlfluff/cli/commands.py",
        "chunk_index": 12,
        "metadata": {
          "filepath": "src/sqlfluff/cli/commands.py",
          "function_name": "fix",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 148,
          "start_line_no": 489,
          "end_line_no": 636
        }
      }
    ]
  },
  "sqlfluff__sqlfluff-1517": {
    "query": "\"Dropped elements in sequence matching\" when doubled semicolon\n## Expected Behaviour\r\nFrankly, I'm not sure whether it (doubled `;`) should be just ignored or rather some specific rule should be triggered.\r\n## Observed Behaviour\r\n```console\r\n(.venv) ?master ~/prod/_inne/sqlfluff> echo \"select id from tbl;;\" | sqlfluff lint -\r\nTraceback (most recent call last):\r\n  File \"/home/adam/prod/_inne/sqlfluff/.venv/bin/sqlfluff\", line 11, in <module>\r\n    load_entry_point('sqlfluff', 'console_scripts', 'sqlfluff')()\r\n  File \"/home/adam/prod/_inne/sqlfluff/.venv/lib/python3.9/site-packages/click/core.py\", line 1137, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/home/adam/prod/_inne/sqlfluff/.venv/lib/python3.9/site-packages/click/core.py\", line 1062, in main\r\n    rv = self.invoke(ctx)\r\n  File \"/home/adam/prod/_inne/sqlfluff/.venv/lib/python3.9/site-packages/click/core.py\", line 1668, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"/home/adam/prod/_inne/sqlfluff/.venv/lib/python3.9/site-packages/click/core.py\", line 1404, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"/home/adam/prod/_inne/sqlfluff/.venv/lib/python3.9/site-packages/click/core.py\", line 763, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/cli/commands.py\", line 347, in lint\r\n    result = lnt.lint_string_wrapped(sys.stdin.read(), fname=\"stdin\")\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/linter/linter.py\", line 789, in lint_string_wrapped\r\n    linted_path.add(self.lint_string(string, fname=fname, fix=fix))\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/linter/linter.py\", line 668, in lint_string\r\n    parsed = self.parse_string(in_str=in_str, fname=fname, config=config)\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/linter/linter.py\", line 607, in parse_string\r\n    return self.parse_rendered(rendered, recurse=recurse)\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/linter/linter.py\", line 313, in parse_rendered\r\n    parsed, pvs = cls._parse_tokens(\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/linter/linter.py\", line 190, in _parse_tokens\r\n    parsed: Optional[BaseSegment] = parser.parse(\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/parser/parser.py\", line 32, in parse\r\n    parsed = root_segment.parse(parse_context=ctx)\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/parser/segments/base.py\", line 821, in parse\r\n    check_still_complete(segments, m.matched_segments, m.unmatched_segments)\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/parser/helpers.py\", line 30, in check_still_complete\r\n    raise RuntimeError(\r\nRuntimeError: Dropped elements in sequence matching! 'select id from tbl;;' != ';'\r\n\r\n```\r\n## Steps to Reproduce\r\nRun \r\n```console\r\necho \"select id from tbl;;\" | sqlfluff lint -\r\n```\r\n## Dialect\r\ndefault (ansi)\r\n## Version\r\n```\r\nsqlfluff, version 0.6.6\r\nPython 3.9.5\r\n```\r\n## Configuration\r\nNone\r\n\n",
    "method": "function",
    "top_k": 10,
    "results": [
      {
        "rank": 1,
        "score": 0.6411839723587036,
        "content": "def _eval(self, segment, parent_stack, **kwargs):\n        \"\"\"Trailing commas within select clause.\"\"\"\n        if segment.is_type(\"select_clause\"):\n            # Iterate content to find last element\n            last_content = None\n            for seg in segment.segments:\n                if seg.is_code:\n                    last_content = seg\n\n            # What mode are we in?\n            if self.select_clause_trailing_comma == \"forbid\":\n                # Is it a comma?\n                if last_content.is_type(\"comma\"):\n                    return LintResult(\n                        anchor=last_content,\n                        fixes=[LintFix(\"delete\", last_content)],\n                        description=\"Trailing comma in select statement forbidden\",\n                    )\n            elif self.select_clause_trailing_comma == \"require\":\n                if not last_content.is_type(\"comma\"):\n                    new_comma = SymbolSegment(\",\", name=\"comma\", type=\"comma\")\n                    return LintResult(\n                        anchor=last_content,\n                        fixes=[\n                            LintFix(\"edit\", last_content, [last_content, new_comma])\n                        ],\n                        description=\"Trailing comma in select statement required\",\n                    )\n        return None",
        "file_path": "src/sqlfluff/rules/L038.py",
        "chunk_index": 0,
        "metadata": {
          "filepath": "src/sqlfluff/rules/L038.py",
          "function_name": "_eval",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 29,
          "start_line_no": 42,
          "end_line_no": 70
        }
      },
      {
        "rank": 2,
        "score": 0.6173782348632812,
        "content": "def _eval(self, segment, **kwargs):\n        \"\"\"Select clause modifiers must appear on same line as SELECT.\"\"\"\n        if segment.is_type(\"select_clause\"):\n            # Does the select clause have modifiers?\n            select_modifier = segment.get_child(\"select_clause_modifier\")\n            if not select_modifier:\n                return None  # No. We're done.\n            select_modifier_idx = segment.segments.index(select_modifier)\n\n            # Does the select clause contain a newline?\n            newline = segment.get_child(\"newline\")\n            if not newline:\n                return None  # No. We're done.\n            newline_idx = segment.segments.index(newline)\n\n            # Is there a newline before the select modifier?\n            if newline_idx > select_modifier_idx:\n                return None  # No, we're done.\n\n            # Yes to all the above. We found an issue.\n\n            # E.g.: \" DISTINCT\\n\"\n            replace_newline_with = [\n                WhitespaceSegment(),\n                select_modifier,\n                NewlineSegment(),\n            ]\n            fixes = [\n                # E.g. \"\\n\" -> \" DISTINCT\\n.\n                LintFix(\"edit\", newline, replace_newline_with),\n                # E.g. \"DISTINCT\" -> X\n                LintFix(\"delete\", select_modifier),\n            ]\n\n            # E.g. \" \" after \"DISTINCT\"\n            ws_to_delete = segment.select_children(\n                start_seg=select_modifier,\n                select_if=lambda s: s.is_type(\"whitespace\"),\n                loop_while=lambda s: s.is_type(\"whitespace\") or s.is_meta,\n            )\n\n            # E.g. \" \" -> X\n            fixes += [LintFix(\"delete\", ws) for ws in ws_to_delete]\n            return LintResult(\n                anchor=segment,\n                fixes=fixes,\n            )",
        "file_path": "src/sqlfluff/rules/L041.py",
        "chunk_index": 0,
        "metadata": {
          "filepath": "src/sqlfluff/rules/L041.py",
          "function_name": "_eval",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 47,
          "start_line_no": 34,
          "end_line_no": 80
        }
      },
      {
        "rank": 3,
        "score": 0.5995234251022339,
        "content": "def _eval(self, segment, raw_stack, **kwargs):\n        \"\"\"Looking for DISTINCT before a bracket.\n\n        Look for DISTINCT keyword immediately followed by open parenthesis.\n        \"\"\"\n        # We trigger on `select_clause` and look for `select_clause_modifier`\n        if segment.is_type(\"select_clause\"):\n            modifier = segment.get_child(\"select_clause_modifier\")\n            if not modifier:\n                return None\n            first_element = segment.get_child(\"select_clause_element\")\n            if not first_element:\n                return None  # pragma: no cover\n            # is the first element only an expression with only brackets?\n            expression = first_element.get_child(\"expression\")\n            if not expression:\n                expression = first_element\n            bracketed = expression.get_child(\"bracketed\")\n            if not bracketed:\n                return None\n            fixes = []\n            # If there's nothing else in the expression, remove the brackets.\n            if len(expression.segments) == 1:\n                # Remove the brackets, and strip any meta segments.\n                fixes = [\n                    LintFix(\n                        \"edit\", bracketed, self.filter_meta(bracketed.segments)[1:-1]\n                    ),\n                ]\n            # Is there any whitespace between DISTINCT and the expression?\n            distinct_idx = segment.segments.index(modifier)\n            elem_idx = segment.segments.index(first_element)\n            if not any(\n                seg.is_whitespace for seg in segment.segments[distinct_idx:elem_idx]\n            ):\n                fixes.append(\n                    LintFix(\n                        \"create\",\n                        first_element,\n                        WhitespaceSegment(),\n                    )\n                )\n            # If no fixes, no problem.\n            if fixes:\n                return LintResult(anchor=modifier, fixes=fixes)\n        return None",
        "file_path": "src/sqlfluff/rules/L015.py",
        "chunk_index": 0,
        "metadata": {
          "filepath": "src/sqlfluff/rules/L015.py",
          "function_name": "_eval",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 46,
          "start_line_no": 32,
          "end_line_no": 77
        }
      },
      {
        "rank": 4,
        "score": 0.5946242809295654,
        "content": "def _eval(self, segment, parent_stack, dialect, **kwargs):\n        \"\"\"Override Rule L020 for dialects that use structs.\n\n        Some dialects use structs (e.g. column.field) which look like\n        table references and so incorrectly trigger this rule.\n        \"\"\"\n        if dialect.name in [\"bigquery\"] and not self.force_enable:\n            return LintResult()\n\n        return super()._eval(segment, parent_stack, dialect, **kwargs)",
        "file_path": "src/sqlfluff/rules/L026.py",
        "chunk_index": 2,
        "metadata": {
          "filepath": "src/sqlfluff/rules/L026.py",
          "function_name": "_eval",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 10,
          "start_line_no": 88,
          "end_line_no": 97
        }
      },
      {
        "rank": 5,
        "score": 0.5900259613990784,
        "content": "def _eval(self, segment, parent_stack, dialect, **kwargs):\n        \"\"\"Override Rule L025 for dialects that use structs.\n\n        Some dialects use structs (e.g. column.field) which look like\n        table references and so incorrectly trigger this rule.\n        \"\"\"\n        if dialect.name in [\"bigquery\"] and not self.force_enable:\n            return LintResult()\n\n        return super()._eval(segment, parent_stack, dialect, **kwargs)",
        "file_path": "src/sqlfluff/rules/L028.py",
        "chunk_index": 1,
        "metadata": {
          "filepath": "src/sqlfluff/rules/L028.py",
          "function_name": "_eval",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 10,
          "start_line_no": 100,
          "end_line_no": 109
        }
      },
      {
        "rank": 6,
        "score": 0.5757226347923279,
        "content": "def match(\n        self, segments: Tuple[\"BaseSegment\", ...], parse_context: ParseContext\n    ) -> MatchResult:\n        \"\"\"Match if this is a bracketed sequence, with content that matches one of the elements.\n\n        1. work forwards to find the first bracket.\n           If we find something other that whitespace, then fail out.\n        2. Once we have the first bracket, we need to bracket count forward to find its partner.\n        3. Assuming we find its partner then we try and match what goes between them\n           using the match method of Sequence.\n           If we match, great. If not, then we return an empty match.\n           If we never find its partner then we return an empty match but should probably\n           log a parsing warning, or error?\n\n        \"\"\"\n        # Trim ends if allowed.\n        if self.allow_gaps:\n            pre_nc, seg_buff, post_nc = trim_non_code_segments(segments)\n        else:\n            seg_buff = segments  # pragma: no cover TODO?\n\n        # Rehydrate the bracket segments in question.\n        # bracket_persits controls whether we make a BracketedSegment or not.\n        start_bracket, end_bracket, bracket_persists = self.get_bracket_from_dialect(\n            parse_context\n        )\n        # Allow optional override for special bracket-like things\n        start_bracket = self.start_bracket or start_bracket\n        end_bracket = self.end_bracket or end_bracket\n\n        # Are we dealing with a pre-existing BracketSegment?\n        if seg_buff[0].is_type(\"bracketed\"):\n            seg: BracketedSegment = cast(BracketedSegment, seg_buff[0])\n            content_segs = seg.segments[len(seg.start_bracket) : -len(seg.end_bracket)]\n            bracket_segment = seg\n            trailing_segments = seg_buff[1:]\n        # Otherwise try and match the segments directly.\n        else:\n            # Look for the first bracket\n            with parse_context.deeper_match() as ctx:\n                start_match = start_bracket.match(seg_buff, parse_context=ctx)\n            if start_match:\n                seg_buff = start_match.unmatched_segments\n            else:\n                # Can't find the opening bracket. No Match.\n                return MatchResult.from_unmatched(segments)\n\n            # Look for the closing bracket\n            content_segs, end_match, _ = self._bracket_sensitive_look_ahead_match(\n                segments=seg_buff,\n                matchers=[end_bracket],\n                parse_context=parse_context,\n                start_bracket=start_bracket,\n                end_bracket=end_bracket,\n                bracket_pairs_set=self.bracket_pairs_set,\n            )\n            if not end_match:  # pragma: no cover\n                raise SQLParseError(\n                    \"Couldn't find closing bracket for opening bracket.\",\n                    segment=start_match.matched_segments[0],\n                )\n\n            # Construct a bracket segment\n            bracket_segment = BracketedSegment(\n                segments=(\n                    start_match.matched_segments\n                    + content_segs\n                    + end_match.matched_segments\n                ),\n                start_bracket=start_match.matched_segments,\n                end_bracket=end_match.matched_segments,\n            )\n            trailing_segments = end_match.unmatched_segments\n\n        # Then trim whitespace and deal with the case of non-code content e.g. \"(   )\"\n        if self.allow_gaps:\n            pre_segs, content_segs, post_segs = trim_non_code_segments(content_segs)\n        else:  # pragma: no cover TODO?\n            pre_segs = ()\n            post_segs = ()\n\n        # If we've got a case of empty brackets check whether that is allowed.\n        if not content_segs:\n            if not self._elements or (\n                all(e.is_optional() for e in self._elements)\n                and (self.allow_gaps or (not pre_segs and not post_segs))\n            ):\n                return MatchResult(\n                    (bracket_segment,)\n                    if bracket_persists\n                    else bracket_segment.segments,\n                    trailing_segments,\n                )\n            else:\n                return MatchResult.from_unmatched(segments)\n\n        # Match the content using super. Sequence will interpret the content of the elements.\n        with parse_context.deeper_match() as ctx:\n            content_match = super().match(content_segs, parse_context=ctx)\n\n        # We require a complete match for the content (hopefully for obvious reasons)\n        if content_match.is_complete():\n            # Reconstruct the bracket segment post match.\n            # We need to realign the meta segments so the pos markers are correct.\n            # Have we already got indents?\n            meta_idx = None\n            for idx, seg in enumerate(bracket_segment.segments):\n                if seg.is_meta and cast(MetaSegment, seg).indent_val > 0:\n                    meta_idx = idx\n                    break\n            # If we've already got indents, don't add more.\n            if meta_idx:\n                bracket_segment.segments = BaseSegment._position_segments(\n                    bracket_segment.start_bracket\n                    + pre_segs\n                    + content_match.all_segments()\n                    + post_segs\n                    + bracket_segment.end_bracket\n                )\n            # Append some indent and dedent tokens at the start and the end.\n            else:\n                bracket_segment.segments = BaseSegment._position_segments(\n                    # NB: The nc segments go *outside* the indents.\n                    bracket_segment.start_bracket\n                    + (Indent(),)  # Add a meta indent here\n                    + pre_segs\n                    + content_match.all_segments()\n                    + post_segs\n                    + (Dedent(),)  # Add a meta indent here\n                    + bracket_segment.end_bracket\n                )\n            return MatchResult(\n                (bracket_segment,) if bracket_persists else bracket_segment.segments,\n                trailing_segments,\n            )\n        # No complete match. Fail.\n        else:\n            return MatchResult.from_unmatched(segments)",
        "file_path": "src/sqlfluff/core/parser/grammar/sequence.py",
        "chunk_index": 5,
        "metadata": {
          "filepath": "src/sqlfluff/core/parser/grammar/sequence.py",
          "function_name": "match",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 138,
          "start_line_no": 233,
          "end_line_no": 370
        }
      },
      {
        "rank": 7,
        "score": 0.5747143030166626,
        "content": "def _eval(self, segment, **kwargs):\n        \"\"\"Join/From clauses should not contain subqueries. Use CTEs instead.\n\n        NB: No fix for this routine because it would be very complex to\n        implement reliably.\n        \"\"\"\n        parent_types = self._config_mapping[self.forbid_subquery_in]\n        for parent_type in parent_types:\n            if segment.is_type(parent_type):\n                # Get the referenced table segment\n                from_expression_element = segment.get_child(\"from_expression_element\")\n                if not from_expression_element:  # pragma: no cover\n                    return None  # There isn't one. We're done.\n                # Get the main bit\n                from_expression_element = from_expression_element.get_child(\n                    \"table_expression\"\n                )\n                if not from_expression_element:  # pragma: no cover\n                    return None  # There isn't one. We're done.\n                # Is it bracketed?\n                bracketed_expression = from_expression_element.get_child(\"bracketed\")\n                # If it is, lint that instead\n                if bracketed_expression:\n                    from_expression_element = bracketed_expression\n                # If any of the following are found, raise an issue.\n                # If not, we're fine.\n                problem_children = [\n                    \"with_compound_statement\",\n                    \"set_expression\",\n                    \"select_statement\",\n                ]\n                for seg_type in problem_children:\n                    seg = from_expression_element.get_child(seg_type)\n                    if seg:\n                        return LintResult(\n                            anchor=seg,\n                            description=f\"{parent_type} clauses should not contain subqueries. Use CTEs instead\",\n                        )",
        "file_path": "src/sqlfluff/rules/L042.py",
        "chunk_index": 0,
        "metadata": {
          "filepath": "src/sqlfluff/rules/L042.py",
          "function_name": "_eval",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 38,
          "start_line_no": 52,
          "end_line_no": 89
        }
      },
      {
        "rank": 8,
        "score": 0.5715382099151611,
        "content": "def lint(sql, dialect=\"ansi\", rules=None):\n    \"\"\"Lint a sql string or file.\n\n    Args:\n        sql (:obj:`str` or file-like object): The sql to be linted\n            either as a string or a subclass of :obj:`TextIOBase`.\n        dialect (:obj:`str`, optional): A reference to the dialect of the sql\n            to be linted. Defaults to `ansi`.\n        rules (:obj:`str` or iterable of :obj:`str`, optional): A subset of rule\n            reference to lint for.\n\n    Returns:\n        :obj:`list` of :obj:`dict` for each violation found.\n    \"\"\"\n    sql = _unify_str_or_file(sql)\n    linter = Linter(dialect=dialect, rules=rules)\n\n    result = linter.lint_string_wrapped(sql)\n    result_records = result.as_records()\n    # Return just the violations for this file\n    return [] if not result_records else result_records[0][\"violations\"]",
        "file_path": "src/sqlfluff/api/simple.py",
        "chunk_index": 2,
        "metadata": {
          "filepath": "src/sqlfluff/api/simple.py",
          "function_name": "lint",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 21,
          "start_line_no": 27,
          "end_line_no": 47
        }
      },
      {
        "rank": 9,
        "score": 0.5665560364723206,
        "content": "def lint(\n        self,\n        tree: BaseSegment,\n        config: Optional[FluffConfig] = None,\n        fname: Optional[str] = None,\n        templated_file: Optional[TemplatedFile] = None,\n    ) -> List[SQLBaseError]:\n        \"\"\"Return just the violations from lintfix when we're only linting.\"\"\"\n        config = config or self.config\n        rule_set = self.get_ruleset(config=config)\n        _, violations, _ = self.lint_fix_parsed(\n            tree,\n            config,\n            rule_set,\n            fix=False,\n            fname=fname,\n            templated_file=templated_file,\n            formatter=self.formatter,\n        )\n        return violations",
        "file_path": "src/sqlfluff/core/linter/linter.py",
        "chunk_index": 19,
        "metadata": {
          "filepath": "src/sqlfluff/core/linter/linter.py",
          "function_name": "lint",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 20,
          "start_line_no": 630,
          "end_line_no": 649
        }
      },
      {
        "rank": 10,
        "score": 0.5641638040542603,
        "content": "def _eval(self, segment, parent_stack, **kwargs):\n        \"\"\"Column expression without alias. Use explicit `AS` clause.\n\n        We look for the select_clause_element segment, and then evaluate\n        whether it has an alias segment or not and whether the expression\n        is complicated enough. `parent_stack` is to assess how many other\n        elements there are.\n\n        \"\"\"\n        if segment.is_type(\"select_clause_element\"):\n            if not any(e.is_type(\"alias_expression\") for e in segment.segments):\n                types = {e.get_type() for e in segment.segments if e.name != \"star\"}\n                unallowed_types = types - {\n                    \"whitespace\",\n                    \"newline\",\n                    \"column_reference\",\n                    \"wildcard_expression\",\n                }\n                if len(unallowed_types) > 0:\n                    # No fixes, because we don't know what the alias should be,\n                    # the user should document it themselves.\n                    if self.allow_scalar:\n                        # Check *how many* elements there are in the select\n                        # statement. If this is the only one, then we won't\n                        # report an error.\n                        num_elements = sum(\n                            e.is_type(\"select_clause_element\")\n                            for e in parent_stack[-1].segments\n                        )\n                        if num_elements > 1:\n                            return LintResult(anchor=segment)\n                        else:\n                            return None\n                    else:\n                        # Just error if we don't care.\n                        return LintResult(anchor=segment)\n        return None",
        "file_path": "src/sqlfluff/rules/L013.py",
        "chunk_index": 0,
        "metadata": {
          "filepath": "src/sqlfluff/rules/L013.py",
          "function_name": "_eval",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 37,
          "start_line_no": 35,
          "end_line_no": 71
        }
      }
    ]
  },
  "sqlfluff__sqlfluff-1763": {
    "query": "dbt postgres fix command errors with UnicodeEncodeError and also wipes the .sql file\n_If this is a parsing or linting issue, please include a minimal SQL example which reproduces the issue, along with the `sqlfluff parse` output, `sqlfluff lint` output and `sqlfluff fix` output when relevant._\r\n\r\n## Expected Behaviour\r\nViolation failure notice at a minimum, without wiping the file. Would like a way to ignore the known error at a minimum as --noqa is not getting past this. Actually would expect --noqa to totally ignore this.\r\n\r\n## Observed Behaviour\r\nReported error: `UnicodeEncodeError: 'charmap' codec can't encode character '\\u2192' in position 120: character maps to <undefined>`\r\n\r\n## Steps to Reproduce\r\nSQL file:\r\n```sql\r\nSELECT\r\n    reacted_table_name_right.descendant_id AS category_id,\r\n    string_agg(redacted_table_name_left.name, '  ' ORDER BY reacted_table_name_right.generations DESC) AS breadcrumbs -- noqa\r\nFROM {{ ref2('redacted_schema_name', 'redacted_table_name_left') }} AS redacted_table_name_left\r\nINNER JOIN {{ ref2('redacted_schema_name', 'reacted_table_name_right') }} AS reacted_table_name_right\r\n    ON redacted_table_name_left.id = order_issue_category_hierarchies.ancestor_id\r\nGROUP BY reacted_table_name_right.descendant_id\r\n```\r\nRunning `sqlfluff fix --ignore templating,parsing,lexing -vvvv` and accepting proposed fixes for linting violations.\r\n\r\n## Dialect\r\n`postgres`, with `dbt` templater\r\n\r\n## Version\r\n`python 3.7.12`\r\n`sqlfluff 0.7.0`\r\n`sqlfluff-templater-dbt 0.7.0`\r\n\r\n## Configuration\r\nI've tried a few, here's one:\r\n```\r\n[sqlfluff]\r\nverbose = 2\r\ndialect = postgres\r\ntemplater = dbt\r\nexclude_rules = None\r\noutput_line_length = 80\r\nrunaway_limit = 10\r\nignore_templated_areas = True\r\nprocesses = 3\r\n# Comma separated list of file extensions to lint.\r\n\r\n# NB: This config will only apply in the root folder.\r\nsql_file_exts = .sql\r\n\r\n[sqlfluff:indentation]\r\nindented_joins = False\r\nindented_using_on = True\r\ntemplate_blocks_indent = True\r\n\r\n[sqlfluff:templater]\r\nunwrap_wrapped_queries = True\r\n\r\n[sqlfluff:templater:jinja]\r\napply_dbt_builtins = True\r\n\r\n[sqlfluff:templater:jinja:macros]\r\n# Macros provided as builtins for dbt projects\r\ndbt_ref = {% macro ref(model_ref) %}{{model_ref}}{% endmacro %}\r\ndbt_source = {% macro source(source_name, table) %}{{source_name}}_{{table}}{% endmacro %}\r\ndbt_config = {% macro config() %}{% for k in kwargs %}{% endfor %}{% endmacro %}\r\ndbt_var = {% macro var(variable, default='') %}item{% endmacro %}\r\ndbt_is_incremental = {% macro is_incremental() %}True{% endmacro %}\r\n\r\n# Common config across rules\r\n[sqlfluff:rules]\r\ntab_space_size = 4\r\nindent_unit = space\r\nsingle_table_references = consistent\r\nunquoted_identifiers_policy = all\r\n\r\n# L001 - Remove trailing whitespace (fix)\r\n# L002 - Single section of whitespace should not contain both tabs and spaces (fix)\r\n# L003 - Keep consistent indentation (fix)\r\n# L004 - We use 4 spaces for indentation just for completeness (fix)\r\n# L005 - Remove space before commas (fix)\r\n# L006 - Operators (+, -, *, /) will be wrapped by a single space each side (fix)\r\n\r\n# L007 - Operators should not be at the end of a line\r\n[sqlfluff:rules:L007]  # Keywords\r\noperator_new_lines = after\r\n\r\n# L008 - Always use a single whitespace after a comma (fix)\r\n# L009 - Files will always end with a trailing newline\r\n\r\n# L010 - All keywords will use full upper case (fix)\r\n[sqlfluff:rules:L010]  # Keywords\r\ncapitalisation_policy = upper\r\n\r\n# L011 - Always explicitly alias tables (fix)\r\n[sqlfluff:rules:L011]  # Aliasing\r\naliasing = explicit\r\n\r\n# L012 - Do not have to explicitly alias all columns\r\n[sqlfluff:rules:L012]  # Aliasing\r\naliasing = explicit\r\n\r\n# L013 - Always explicitly alias a column with an expression in it (fix)\r\n[sqlfluff:rules:L013]  # Aliasing\r\nallow_scalar = False\r\n\r\n# L014 - Always user full lower case for 'quoted identifiers' -> column refs. without an alias (fix)\r\n[sqlfluff:rules:L014]  # Unquoted identifiers\r\nextended_capitalisation_policy = lower\r\n\r\n# L015 - Always remove parenthesis when using DISTINCT to be clear that DISTINCT applies to all columns (fix)\r\n\r\n# L016 - Lines should be 120 characters of less. Comment lines should not be ignored (fix)\r\n[sqlfluff:rules:L016]\r\nignore_comment_lines = False\r\nmax_line_length = 120\r\n\r\n# L017 - There should not be whitespace between function name and brackets (fix)\r\n# L018 - Always align closing bracket of WITH to the WITH keyword (fix)\r\n\r\n# L019 - Always use trailing commas / commas at the end of the line (fix)\r\n[sqlfluff:rules:L019]\r\ncomma_style = trailing\r\n\r\n# L020 - Table aliases will always be unique per statement\r\n# L021 - Remove any use of ambiguous DISTINCT and GROUP BY combinations. Lean on removing the GROUP BY.\r\n# L022 - Add blank lines after common table expressions (CTE) / WITH.\r\n# L023 - Always add a single whitespace after AS in a WITH clause (fix)\r\n\r\n[sqlfluff:rules:L026]\r\nforce_enable = False\r\n\r\n# L027 - Always add references if more than one referenced table or view is used\r\n\r\n[sqlfluff:rules:L028]\r\nforce_enable = False\r\n\r\n[sqlfluff:rules:L029]  # Keyword identifiers\r\nunquoted_identifiers_policy = aliases\r\n\r\n[sqlfluff:rules:L030]  # Function names\r\ncapitalisation_policy = upper\r\n\r\n# L032 - We prefer use of join keys rather than USING\r\n# L034 - We prefer ordering of columns in select statements as (fix):\r\n# 1. wildcards\r\n# 2. single identifiers\r\n# 3. calculations and aggregates\r\n\r\n# L035 - Omit 'else NULL'; it is redundant (fix)\r\n# L036 - Move select targets / identifiers onto new lines each (fix)\r\n# L037 - When using ORDER BY, make the direction explicit (fix)\r\n\r\n# L038 - Never use trailing commas at the end of the SELECT clause\r\n[sqlfluff:rules:L038]\r\nselect_clause_trailing_comma = forbid\r\n\r\n# L039 - Remove unnecessary whitespace (fix)\r\n\r\n[sqlfluff:rules:L040]  # Null & Boolean Literals\r\ncapitalisation_policy = upper\r\n\r\n# L042 - Join clauses should not contain subqueries. Use common tables expressions (CTE) instead.\r\n[sqlfluff:rules:L042]\r\n# By default, allow subqueries in from clauses, but not join clauses.\r\nforbid_subquery_in = join\r\n\r\n# L043 - Reduce CASE WHEN conditions to COALESCE (fix)\r\n# L044 - Prefer a known number of columns along the path to the source data\r\n# L045 - Remove unused common tables expressions (CTE) / WITH statements (fix)\r\n# L046 - Jinja tags should have a single whitespace on both sides\r\n\r\n# L047 - Use COUNT(*) instead of COUNT(0) or COUNT(1) alternatives (fix)\r\n[sqlfluff:rules:L047]  # Consistent syntax to count all rows\r\nprefer_count_1 = False\r\nprefer_count_0 = False\r\n\r\n# L048 - Quoted literals should be surrounded by a single whitespace (fix)\r\n# L049 - Always use IS or IS NOT for comparisons with NULL (fix)\r\n```\r\n\n",
    "method": "function",
    "top_k": 10,
    "results": [
      {
        "rank": 1,
        "score": 0.6809530258178711,
        "content": "def fix(sql, dialect=\"ansi\", rules=None):\n    \"\"\"Fix a sql string or file.\n\n    Args:\n        sql (:obj:`str` or file-like object): The sql to be linted\n            either as a string or a subclass of :obj:`TextIOBase`.\n        dialect (:obj:`str`, optional): A reference to the dialect of the sql\n            to be linted. Defaults to `ansi`.\n        rules (:obj:`str` or iterable of :obj:`str`, optional): A subset of rule\n            reference to lint for.\n\n    Returns:\n        :obj:`str` for the fixed sql if possible.\n    \"\"\"\n    sql = _unify_str_or_file(sql)\n    linter = Linter(dialect=dialect, rules=rules)\n\n    result = linter.lint_string_wrapped(sql, fix=True)\n    fixed_string = result.paths[0].files[0].fix_string()[0]\n    return fixed_string",
        "file_path": "src/sqlfluff/api/simple.py",
        "chunk_index": 3,
        "metadata": {
          "filepath": "src/sqlfluff/api/simple.py",
          "function_name": "fix",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 20,
          "start_line_no": 50,
          "end_line_no": 69
        }
      },
      {
        "rank": 2,
        "score": 0.6608344316482544,
        "content": "def fix(\n    force: bool,\n    paths: Tuple[str],\n    processes: int,\n    bench: bool = False,\n    fixed_suffix: str = \"\",\n    logger: Optional[logging.Logger] = None,\n    **kwargs,\n) -> NoReturn:\n    \"\"\"Fix SQL files.\n\n    PATH is the path to a sql file or directory to lint. This can be either a\n    file ('path/to/file.sql'), a path ('directory/of/sql/files'), a single ('-')\n    character to indicate reading from *stdin* or a dot/blank ('.'/' ') which will\n    be interpreted like passing the current working directory as a path argument.\n    \"\"\"\n    # some quick checks\n    fixing_stdin = (\"-\",) == paths\n\n    config = get_config(**kwargs)\n    lnt, formatter = get_linter_and_formatter(config, silent=fixing_stdin)\n    verbose = config.get(\"verbose\")\n    exit_code = 0\n\n    formatter.dispatch_config(lnt)\n\n    # Set up logging.\n    set_logging_level(verbosity=verbose, logger=logger, stderr_output=fixing_stdin)\n\n    # handle stdin case. should output formatted sql to stdout and nothing else.\n    if fixing_stdin:\n        stdin = sys.stdin.read()\n\n        result = lnt.lint_string_wrapped(stdin, fname=\"stdin\", fix=True)\n        templater_error = result.num_violations(types=SQLTemplaterError) > 0\n        unfixable_error = result.num_violations(types=SQLLintError, fixable=False) > 0\n\n        if result.num_violations(types=SQLLintError, fixable=True) > 0:\n            stdout = result.paths[0].files[0].fix_string()[0]\n        else:\n            stdout = stdin\n\n        if templater_error:\n            click.echo(\n                colorize(\n                    \"Fix aborted due to unparseable template variables.\",\n                    Color.red,\n                ),\n                err=True,\n            )\n            click.echo(\n                colorize(\n                    \"Use '--ignore templating' to attempt to fix anyway.\",\n                    Color.red,\n                ),\n                err=True,\n            )\n        if unfixable_error:\n            click.echo(colorize(\"Unfixable violations detected.\", Color.red), err=True)\n\n        click.echo(stdout, nl=False)\n        sys.exit(1 if templater_error or unfixable_error else 0)\n\n    # Lint the paths (not with the fix argument at this stage), outputting as we go.\n    click.echo(\"==== finding fixable violations ====\")\n    try:\n        result = lnt.lint_paths(\n            paths, fix=True, ignore_non_existent_files=False, processes=processes\n        )\n    except OSError:\n        click.echo(\n            colorize(\n                f\"The path(s) '{paths}' could not be accessed. Check it/they exist(s).\",\n                Color.red,\n            ),\n            err=True,\n        )\n        sys.exit(1)\n\n    # NB: We filter to linting violations here, because they're\n    # the only ones which can be potentially fixed.\n    if result.num_violations(types=SQLLintError, fixable=True) > 0:\n        click.echo(\"==== fixing violations ====\")\n        click.echo(\n            f\"{result.num_violations(types=SQLLintError, fixable=True)} fixable linting violations found\"\n        )\n        if force:\n            click.echo(f\"{colorize('FORCE MODE', Color.red)}: Attempting fixes...\")\n            success = do_fixes(\n                lnt,\n                result,\n                formatter,\n                types=SQLLintError,\n                fixed_file_suffix=fixed_suffix,\n            )\n            if not success:\n                sys.exit(1)  # pragma: no cover\n        else:\n            click.echo(\n                \"Are you sure you wish to attempt to fix these? [Y/n] \", nl=False\n            )\n            c = click.getchar().lower()\n            click.echo(\"...\")\n            if c in (\"y\", \"\\r\", \"\\n\"):\n                click.echo(\"Attempting fixes...\")\n                success = do_fixes(\n                    lnt,\n                    result,\n                    formatter,\n                    types=SQLLintError,\n                    fixed_file_suffix=fixed_suffix,\n                )\n                if not success:\n                    sys.exit(1)  # pragma: no cover\n                else:\n                    _completion_message(config)\n            elif c == \"n\":\n                click.echo(\"Aborting...\")\n                exit_code = 1\n            else:  # pragma: no cover\n                click.echo(\"Invalid input, please enter 'Y' or 'N'\")\n                click.echo(\"Aborting...\")\n                exit_code = 1\n    else:\n        click.echo(\"==== no fixable linting violations found ====\")\n        _completion_message(config)\n\n    if result.num_violations(types=SQLLintError, fixable=False) > 0:\n        click.echo(\n            f\"  [{result.num_violations(types=SQLLintError, fixable=False)} unfixable linting violations found]\"\n        )\n        exit_code = 1\n\n    if result.num_violations(types=SQLTemplaterError) > 0:\n        click.echo(\n            f\"  [{result.num_violations(types=SQLTemplaterError)} templating errors found]\"\n        )\n        exit_code = 1\n\n    if bench:\n        click.echo(\"==== overall timings ====\")\n        click.echo(cli_table([(\"Clock time\", result.total_time)]))\n        timing_summary = result.timing_summary()\n        for step in timing_summary:\n            click.echo(f\"=== {step} ===\")\n            click.echo(cli_table(timing_summary[step].items()))\n\n    sys.exit(exit_code)",
        "file_path": "src/sqlfluff/cli/commands.py",
        "chunk_index": 12,
        "metadata": {
          "filepath": "src/sqlfluff/cli/commands.py",
          "function_name": "fix",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 148,
          "start_line_no": 489,
          "end_line_no": 636
        }
      },
      {
        "rank": 3,
        "score": 0.622316300868988,
        "content": "def lint(sql, dialect=\"ansi\", rules=None):\n    \"\"\"Lint a sql string or file.\n\n    Args:\n        sql (:obj:`str` or file-like object): The sql to be linted\n            either as a string or a subclass of :obj:`TextIOBase`.\n        dialect (:obj:`str`, optional): A reference to the dialect of the sql\n            to be linted. Defaults to `ansi`.\n        rules (:obj:`str` or iterable of :obj:`str`, optional): A subset of rule\n            reference to lint for.\n\n    Returns:\n        :obj:`list` of :obj:`dict` for each violation found.\n    \"\"\"\n    sql = _unify_str_or_file(sql)\n    linter = Linter(dialect=dialect, rules=rules)\n\n    result = linter.lint_string_wrapped(sql)\n    result_records = result.as_records()\n    # Return just the violations for this file\n    return [] if not result_records else result_records[0][\"violations\"]",
        "file_path": "src/sqlfluff/api/simple.py",
        "chunk_index": 2,
        "metadata": {
          "filepath": "src/sqlfluff/api/simple.py",
          "function_name": "lint",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 21,
          "start_line_no": 27,
          "end_line_no": 47
        }
      },
      {
        "rank": 4,
        "score": 0.6076274514198303,
        "content": "def lint(\n    paths: Tuple[str],\n    processes: int,\n    format: str,\n    annotation_level: str,\n    nofail: bool,\n    disregard_sqlfluffignores: bool,\n    logger: Optional[logging.Logger] = None,\n    bench: bool = False,\n    **kwargs,\n) -> NoReturn:\n    \"\"\"Lint SQL files via passing a list of files or using stdin.\n\n    PATH is the path to a sql file or directory to lint. This can be either a\n    file ('path/to/file.sql'), a path ('directory/of/sql/files'), a single ('-')\n    character to indicate reading from *stdin* or a dot/blank ('.'/' ') which will\n    be interpreted like passing the current working directory as a path argument.\n\n    Linting SQL files:\n\n        sqlfluff lint path/to/file.sql\n        sqlfluff lint directory/of/sql/files\n\n    Linting a file via stdin (note the lone '-' character):\n\n        cat path/to/file.sql | sqlfluff lint -\n        echo 'select col from tbl' | sqlfluff lint -\n\n    \"\"\"\n    config = get_config(**kwargs)\n    non_human_output = format != FormatType.human.value\n    lnt, formatter = get_linter_and_formatter(config, silent=non_human_output)\n    verbose = config.get(\"verbose\")\n\n    formatter.dispatch_config(lnt)\n\n    # Set up logging.\n    set_logging_level(verbosity=verbose, logger=logger, stderr_output=non_human_output)\n    # add stdin if specified via lone '-'\n    if (\"-\",) == paths:\n        result = lnt.lint_string_wrapped(sys.stdin.read(), fname=\"stdin\")\n    else:\n        # Output the results as we go\n        if verbose >= 1:\n            click.echo(format_linting_result_header())\n        try:\n            result = lnt.lint_paths(\n                paths,\n                ignore_non_existent_files=False,\n                ignore_files=not disregard_sqlfluffignores,\n                processes=processes,\n            )\n        except OSError:\n            click.echo(\n                colorize(\n                    f\"The path(s) '{paths}' could not be accessed. Check it/they exist(s).\",\n                    Color.red,\n                )\n            )\n            sys.exit(1)\n        # Output the final stats\n        if verbose >= 1:\n            click.echo(format_linting_stats(result, verbose=verbose))\n\n    if format == FormatType.json.value:\n        click.echo(json.dumps(result.as_records()))\n    elif format == FormatType.yaml.value:\n        click.echo(yaml.dump(result.as_records()))\n    elif format == FormatType.github_annotation.value:\n        github_result = []\n        for record in result.as_records():\n            filepath = record[\"filepath\"]\n            for violation in record[\"violations\"]:\n                # NOTE: The output format is designed for this GitHub action:\n                # https://github.com/yuzutech/annotations-action\n                # It is similar, but not identical, to the native GitHub format:\n                # https://docs.github.com/en/rest/reference/checks#annotations-items\n                github_result.append(\n                    {\n                        \"file\": filepath,\n                        \"line\": violation[\"line_no\"],\n                        \"start_column\": violation[\"line_pos\"],\n                        \"end_column\": violation[\"line_pos\"],\n                        \"title\": \"SQLFluff\",\n                        \"message\": f\"{violation['code']}: {violation['description']}\",\n                        \"annotation_level\": annotation_level,\n                    }\n                )\n        click.echo(json.dumps(github_result))\n\n    if bench:\n        click.echo(\"==== overall timings ====\")\n        click.echo(cli_table([(\"Clock time\", result.total_time)]))\n        timing_summary = result.timing_summary()\n        for step in timing_summary:\n            click.echo(f\"=== {step} ===\")\n            click.echo(cli_table(timing_summary[step].items()))\n\n    if not nofail:\n        if not non_human_output:\n            _completion_message(config)\n        sys.exit(result.stats()[\"exit code\"])\n    else:\n        sys.exit(0)",
        "file_path": "src/sqlfluff/cli/commands.py",
        "chunk_index": 10,
        "metadata": {
          "filepath": "src/sqlfluff/cli/commands.py",
          "function_name": "lint",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 104,
          "start_line_no": 343,
          "end_line_no": 446
        }
      },
      {
        "rank": 5,
        "score": 0.6057137250900269,
        "content": "def process(self, *, fname, in_str=None, config=None, formatter=None):\n        \"\"\"Compile a dbt model and return the compiled SQL.\n\n        Args:\n            fname (:obj:`str`): Path to dbt model(s)\n            in_str (:obj:`str`, optional): This is ignored for dbt\n            config (:obj:`FluffConfig`, optional): A specific config to use for this\n                templating operation. Only necessary for some templaters.\n            formatter (:obj:`CallbackFormatter`): Optional object for output.\n        \"\"\"\n        # Stash the formatter if provided to use in cached methods.\n        self.formatter = formatter\n        self.sqlfluff_config = config\n        self.project_dir = self._get_project_dir()\n        self.profiles_dir = self._get_profiles_dir()\n        fname_absolute_path = os.path.abspath(fname)\n\n        try:\n            os.chdir(self.project_dir)\n            processed_result = self._unsafe_process(fname_absolute_path, in_str, config)\n            # Reset the fail counter\n            self._sequential_fails = 0\n            return processed_result\n        except DbtCompilationException as e:\n            # Increment the counter\n            self._sequential_fails += 1\n            if e.node:\n                return None, [\n                    SQLTemplaterError(\n                        f\"dbt compilation error on file '{e.node.original_file_path}', {e.msg}\",\n                        # It's fatal if we're over the limit\n                        fatal=self._sequential_fails > self.sequential_fail_limit,\n                    )\n                ]\n            else:\n                raise  # pragma: no cover\n        except DbtFailedToConnectException as e:\n            return None, [\n                SQLTemplaterError(\n                    \"dbt tried to connect to the database and failed: \"\n                    \"you could use 'execute' https://docs.getdbt.com/reference/dbt-jinja-functions/execute/ \"\n                    f\"to skip the database calls. Error: {e.msg}\",\n                    fatal=True,\n                )\n            ]\n        # If a SQLFluff error is raised, just pass it through\n        except SQLTemplaterError as e:  # pragma: no cover\n            return None, [e]\n        finally:\n            os.chdir(self.working_dir)",
        "file_path": "plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py",
        "chunk_index": 13,
        "metadata": {
          "filepath": "plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py",
          "function_name": "process",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 50,
          "start_line_no": 289,
          "end_line_no": 338
        }
      },
      {
        "rank": 6,
        "score": 0.5993829369544983,
        "content": "def violations(src_path: str) -> List[Violation]:\n        \"\"\"Return list of violations.\n\n        Given the path to a .sql file, analyze it and return a list of\n        violations (i.e. formatting or style issues).\n        \"\"\"\n        linter = Linter(config=FluffConfig.from_root())\n        linted_path = linter.lint_path(src_path, ignore_non_existent_files=True)\n        result = []\n        for violation in linted_path.get_violations():\n            try:\n                # Normal SQLFluff warnings\n                message = f\"{violation.rule_code()}: {violation.description}\"\n            except AttributeError:\n                # Parse errors\n                message = str(violation)\n            result.append(Violation(violation.line_no, message))\n        return result",
        "file_path": "src/sqlfluff/diff_quality_plugin.py",
        "chunk_index": 1,
        "metadata": {
          "filepath": "src/sqlfluff/diff_quality_plugin.py",
          "function_name": "violations",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 18,
          "start_line_no": 20,
          "end_line_no": 37
        }
      },
      {
        "rank": 7,
        "score": 0.5991039276123047,
        "content": "def to_linting_error(self, rule) -> Optional[SQLLintError]:\n        \"\"\"Convert a linting result to a :exc:`SQLLintError` if appropriate.\"\"\"\n        if self.anchor:\n            # Allow description override from the LintResult\n            description = self.description or rule.description\n            return SQLLintError(\n                rule=rule,\n                segment=self.anchor,\n                fixes=self.fixes,\n                description=description,\n            )\n        else:\n            return None",
        "file_path": "src/sqlfluff/core/rules/base.py",
        "chunk_index": 2,
        "metadata": {
          "filepath": "src/sqlfluff/core/rules/base.py",
          "function_name": "to_linting_error",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 13,
          "start_line_no": 82,
          "end_line_no": 94
        }
      },
      {
        "rank": 8,
        "score": 0.5950978398323059,
        "content": "def lint(\n        self,\n        tree: BaseSegment,\n        config: Optional[FluffConfig] = None,\n        fname: Optional[str] = None,\n        templated_file: Optional[TemplatedFile] = None,\n    ) -> List[SQLBaseError]:\n        \"\"\"Return just the violations from lintfix when we're only linting.\"\"\"\n        config = config or self.config\n        rule_set = self.get_ruleset(config=config)\n        _, violations, _ = self.lint_fix_parsed(\n            tree,\n            config,\n            rule_set,\n            fix=False,\n            fname=fname,\n            templated_file=templated_file,\n            formatter=self.formatter,\n        )\n        return violations",
        "file_path": "src/sqlfluff/core/linter/linter.py",
        "chunk_index": 19,
        "metadata": {
          "filepath": "src/sqlfluff/core/linter/linter.py",
          "function_name": "lint",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 20,
          "start_line_no": 628,
          "end_line_no": 647
        }
      },
      {
        "rank": 9,
        "score": 0.5839689373970032,
        "content": "def remove_templated_errors(\n        linting_errors: List[SQLBaseError],\n    ) -> List[SQLBaseError]:\n        \"\"\"Filter a list of lint errors, removing those which only occur in templated slices.\"\"\"\n        # Filter out any linting errors in templated sections if relevant.\n        result: List[SQLBaseError] = []\n        for e in linting_errors:\n            if isinstance(e, SQLLintError):\n                if (\n                    # Is it in a literal section?\n                    e.segment.pos_marker.is_literal()\n                    # Is it a rule that is designed to work on templated sections?\n                    or e.rule.targets_templated\n                ):\n                    result.append(e)\n            else:\n                # If it's another type, just keep it. (E.g. SQLParseError from\n                # malformed \"noqa\" comment).\n                result.append(e)\n        return result",
        "file_path": "src/sqlfluff/core/linter/linter.py",
        "chunk_index": 7,
        "metadata": {
          "filepath": "src/sqlfluff/core/linter/linter.py",
          "function_name": "remove_templated_errors",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 20,
          "start_line_no": 263,
          "end_line_no": 282
        }
      },
      {
        "rank": 10,
        "score": 0.5822690725326538,
        "content": "def _unsafe_process(self, fname, in_str=None, config=None):\n        node = self._find_node(fname, config)\n\n        node = self.dbt_compiler.compile_node(\n            node=node,\n            manifest=self.dbt_manifest,\n        )\n\n        if hasattr(node, \"injected_sql\"):\n            # If injected SQL is present, it contains a better picture\n            # of what will actually hit the database (e.g. with tests).\n            # However it's not always present.\n            compiled_sql = node.injected_sql\n        else:\n            compiled_sql = node.compiled_sql\n\n        if not compiled_sql:  # pragma: no cover\n            raise SQLTemplaterError(\n                \"dbt templater compilation failed silently, check your configuration \"\n                \"by running `dbt compile` directly.\"\n            )\n\n        with open(fname) as source_dbt_model:\n            source_dbt_sql = source_dbt_model.read()\n\n        n_trailing_newlines = len(source_dbt_sql) - len(source_dbt_sql.rstrip(\"\\n\"))\n\n        templater_logger.debug(\n            \"    Trailing newline count in source dbt model: %r\", n_trailing_newlines\n        )\n        templater_logger.debug(\"    Raw SQL before compile: %r\", source_dbt_sql)\n        templater_logger.debug(\"    Node raw SQL: %r\", node.raw_sql)\n        templater_logger.debug(\"    Node compiled SQL: %r\", compiled_sql)\n\n        # When using dbt-templater, trailing newlines are ALWAYS REMOVED during\n        # compiling. Unless fixed (like below), this will cause:\n        #    1. L009 linting errors when running \"sqlfluff lint foo_bar.sql\"\n        #       since the linter will use the compiled code with the newlines\n        #       removed.\n        #    2. \"No newline at end of file\" warnings in Git/GitHub since\n        #       sqlfluff uses the compiled SQL to write fixes back to the\n        #       source SQL in the dbt model.\n        # The solution is:\n        #    1. Check for trailing newlines before compiling by looking at the\n        #       raw SQL in the source dbt file, store the count of trailing newlines.\n        #    2. Append the count from #1 above to the node.raw_sql and\n        #       compiled_sql objects, both of which have had the trailing\n        #       newlines removed by the dbt-templater.\n        node.raw_sql = node.raw_sql + \"\\n\" * n_trailing_newlines\n        compiled_sql = compiled_sql + \"\\n\" * n_trailing_newlines\n\n        raw_sliced, sliced_file, templated_sql = self.slice_file(\n            node.raw_sql,\n            compiled_sql,\n            config=config,\n        )\n\n        return (\n            TemplatedFile(\n                source_str=node.raw_sql,\n                templated_str=templated_sql,\n                fname=fname,\n                sliced_file=sliced_file,\n                raw_sliced=raw_sliced,\n            ),\n            # No violations returned in this way.\n            [],\n        )",
        "file_path": "plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py",
        "chunk_index": 15,
        "metadata": {
          "filepath": "plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py",
          "function_name": "_unsafe_process",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 68,
          "start_line_no": 374,
          "end_line_no": 441
        }
      }
    ]
  },
  "marshmallow-code__marshmallow-1359": {
    "query": "3.0: DateTime fields cannot be used as inner field for List or Tuple fields\nBetween releases 3.0.0rc8 and 3.0.0rc9, `DateTime` fields have started throwing an error when being instantiated as inner fields of container fields like `List` or `Tuple`. The snippet below works in <=3.0.0rc8 and throws the error below in >=3.0.0rc9 (and, worryingly, 3.0.0):\r\n\r\n```python\r\nfrom marshmallow import fields, Schema\r\n\r\nclass MySchema(Schema):\r\n    times = fields.List(fields.DateTime())\r\n\r\ns = MySchema()\r\n```\r\n\r\nTraceback:\r\n```\r\nTraceback (most recent call last):\r\n  File \"test-mm.py\", line 8, in <module>\r\n    s = MySchema()\r\n  File \"/Users/victor/.pyenv/versions/marshmallow/lib/python3.6/site-packages/marshmallow/schema.py\", line 383, in __init__\r\n    self.fields = self._init_fields()\r\n  File \"/Users/victor/.pyenv/versions/marshmallow/lib/python3.6/site-packages/marshmallow/schema.py\", line 913, in _init_fields\r\n    self._bind_field(field_name, field_obj)\r\n  File \"/Users/victor/.pyenv/versions/marshmallow/lib/python3.6/site-packages/marshmallow/schema.py\", line 969, in _bind_field\r\n    field_obj._bind_to_schema(field_name, self)\r\n  File \"/Users/victor/.pyenv/versions/marshmallow/lib/python3.6/site-packages/marshmallow/fields.py\", line 636, in _bind_to_schema\r\n    self.inner._bind_to_schema(field_name, self)\r\n  File \"/Users/victor/.pyenv/versions/marshmallow/lib/python3.6/site-packages/marshmallow/fields.py\", line 1117, in _bind_to_schema\r\n    or getattr(schema.opts, self.SCHEMA_OPTS_VAR_NAME)\r\nAttributeError: 'List' object has no attribute 'opts'\r\n```\r\n\r\nIt seems like it's treating the parent field as a Schema without checking that it is indeed a schema, so the `schema.opts` statement fails as fields don't have an `opts` attribute.\n",
    "method": "function",
    "top_k": 10,
    "results": [
      {
        "rank": 1,
        "score": 0.5796363353729248,
        "content": "def resolve_field_instance(cls_or_instance):\n    \"\"\"Return a Schema instance from a Schema class or instance.\n\n    :param type|Schema cls_or_instance: Marshmallow Schema class or instance.\n    \"\"\"\n    if isinstance(cls_or_instance, type):\n        if not issubclass(cls_or_instance, FieldABC):\n            raise FieldInstanceResolutionError\n        return cls_or_instance()\n    else:\n        if not isinstance(cls_or_instance, FieldABC):\n            raise FieldInstanceResolutionError\n        return cls_or_instance",
        "file_path": "src/marshmallow/utils.py",
        "chunk_index": 28,
        "metadata": {
          "filepath": "src/marshmallow/utils.py",
          "function_name": "resolve_field_instance",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 13,
          "start_line_no": 302,
          "end_line_no": 314
        }
      },
      {
        "rank": 2,
        "score": 0.554298996925354,
        "content": "def _bind_to_schema(self, field_name, schema):\n        \"\"\"Update field with values from its parent schema. Called by\n        :meth:`Schema._bind_field <marshmallow.Schema._bind_field>`.\n\n        :param str field_name: Field name set in schema.\n        :param Schema schema: Parent schema.\n        \"\"\"\n        self.parent = self.parent or schema\n        self.name = self.name or field_name",
        "file_path": "src/marshmallow/fields.py",
        "chunk_index": 10,
        "metadata": {
          "filepath": "src/marshmallow/fields.py",
          "function_name": "_bind_to_schema",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 9,
          "start_line_no": 335,
          "end_line_no": 343
        }
      },
      {
        "rank": 3,
        "score": 0.5466262102127075,
        "content": "def root(self):\n        \"\"\"Reference to the `Schema` that this field belongs to even if it is buried in a\n        container field (e.g. `List`).\n        Return `None` for unbound fields.\n        \"\"\"\n        ret = self\n        while hasattr(ret, \"parent\"):\n            ret = ret.parent\n        return ret if isinstance(ret, SchemaABC) else None",
        "file_path": "src/marshmallow/fields.py",
        "chunk_index": 14,
        "metadata": {
          "filepath": "src/marshmallow/fields.py",
          "function_name": "root",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 9,
          "start_line_no": 391,
          "end_line_no": 399
        }
      },
      {
        "rank": 4,
        "score": 0.5431730151176453,
        "content": "def _bind_to_schema(self, field_name, schema):\n        super()._bind_to_schema(field_name, schema)\n        self.inner = copy.deepcopy(self.inner)\n        self.inner._bind_to_schema(field_name, self)\n        if isinstance(self.inner, Nested):\n            self.inner.only = self.only\n            self.inner.exclude = self.exclude",
        "file_path": "src/marshmallow/fields.py",
        "chunk_index": 27,
        "metadata": {
          "filepath": "src/marshmallow/fields.py",
          "function_name": "_bind_to_schema",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 7,
          "start_line_no": 633,
          "end_line_no": 639
        }
      },
      {
        "rank": 5,
        "score": 0.5403856039047241,
        "content": "def _bind_to_schema(self, field_name, schema):\n        super()._bind_to_schema(field_name, schema)\n        new_tuple_fields = []\n        for field in self.tuple_fields:\n            field = copy.deepcopy(field)\n            field._bind_to_schema(field_name, self)\n            new_tuple_fields.append(field)\n\n        self.tuple_fields = new_tuple_fields",
        "file_path": "src/marshmallow/fields.py",
        "chunk_index": 31,
        "metadata": {
          "filepath": "src/marshmallow/fields.py",
          "function_name": "_bind_to_schema",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 9,
          "start_line_no": 712,
          "end_line_no": 720
        }
      },
      {
        "rank": 6,
        "score": 0.535642147064209,
        "content": "def schema(self):\n        \"\"\"The nested Schema object.\n\n        .. versionchanged:: 1.0.0\n            Renamed from `serializer` to `schema`.\n        \"\"\"\n        if not self._schema:\n            # Inherit context from parent.\n            context = getattr(self.parent, \"context\", {})\n            if isinstance(self.nested, SchemaABC):\n                self._schema = self.nested\n                self._schema.context.update(context)\n            else:\n                if isinstance(self.nested, type) and issubclass(self.nested, SchemaABC):\n                    schema_class = self.nested\n                elif not isinstance(self.nested, (str, bytes)):\n                    raise ValueError(\n                        \"Nested fields must be passed a \"\n                        \"Schema, not {}.\".format(self.nested.__class__)\n                    )\n                elif self.nested == \"self\":\n                    ret = self\n                    while not isinstance(ret, SchemaABC):\n                        ret = ret.parent\n                    schema_class = ret.__class__\n                else:\n                    schema_class = class_registry.get_class(self.nested)\n                self._schema = schema_class(\n                    many=self.many,\n                    only=self.only,\n                    exclude=self.exclude,\n                    context=context,\n                    load_only=self._nested_normalized_option(\"load_only\"),\n                    dump_only=self._nested_normalized_option(\"dump_only\"),\n                )\n        return self._schema",
        "file_path": "src/marshmallow/fields.py",
        "chunk_index": 16,
        "metadata": {
          "filepath": "src/marshmallow/fields.py",
          "function_name": "schema",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 36,
          "start_line_no": 465,
          "end_line_no": 500
        }
      },
      {
        "rank": 7,
        "score": 0.5354287624359131,
        "content": "def _bind_field(self, field_name, field_obj):\n        \"\"\"Bind field to the schema, setting any necessary attributes on the\n        field (e.g. parent and name).\n\n        Also set field load_only and dump_only values if field_name was\n        specified in ``class Meta``.\n        \"\"\"\n        try:\n            if field_name in self.load_only:\n                field_obj.load_only = True\n            if field_name in self.dump_only:\n                field_obj.dump_only = True\n            field_obj._bind_to_schema(field_name, self)\n            self.on_bind_field(field_name, field_obj)\n        except TypeError as error:\n            # field declared as a class, not an instance\n            if isinstance(field_obj, type) and issubclass(field_obj, base.FieldABC):\n                msg = (\n                    'Field for \"{}\" must be declared as a '\n                    \"Field instance, not a class. \"\n                    'Did you mean \"fields.{}()\"?'.format(field_name, field_obj.__name__)\n                )\n                raise TypeError(msg) from error",
        "file_path": "src/marshmallow/schema.py",
        "chunk_index": 28,
        "metadata": {
          "filepath": "src/marshmallow/schema.py",
          "function_name": "_bind_field",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 23,
          "start_line_no": 957,
          "end_line_no": 979
        }
      },
      {
        "rank": 8,
        "score": 0.5315070152282715,
        "content": "def __init__(self, cls_or_instance, **kwargs):\n        super().__init__(**kwargs)\n        try:\n            self.inner = resolve_field_instance(cls_or_instance)\n        except FieldInstanceResolutionError as error:\n            raise ValueError(\n                \"The list elements must be a subclass or instance of \"\n                \"marshmallow.base.FieldABC.\"\n            ) from error\n        if isinstance(self.inner, Nested):\n            self.only = self.inner.only\n            self.exclude = self.inner.exclude",
        "file_path": "src/marshmallow/fields.py",
        "chunk_index": 26,
        "metadata": {
          "filepath": "src/marshmallow/fields.py",
          "function_name": "__init__",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 12,
          "start_line_no": 620,
          "end_line_no": 631
        }
      },
      {
        "rank": 9,
        "score": 0.5307262539863586,
        "content": "def from_dict(\n        cls, fields: typing.Dict[str, ma_fields.Field], *, name: str = \"GeneratedSchema\"\n    ) -> typing.Type[\"Schema\"]:\n        \"\"\"Generate a `Schema` class given a dictionary of fields.\n\n        .. code-block:: python\n\n            from marshmallow import Schema, fields\n\n            PersonSchema = Schema.from_dict({\"name\": fields.Str()})\n            print(PersonSchema().load({\"name\": \"David\"}))  # => {'name': 'David'}\n\n        Generated schemas are not added to the class registry and therefore cannot\n        be referred to by name in `Nested` fields.\n\n        :param dict fields: Dictionary mapping field names to field instances.\n        :param str name: Optional name for the class, which will appear in\n            the ``repr`` for the class.\n\n        .. versionadded:: 3.0.0\n        \"\"\"\n        attrs = fields.copy()\n        attrs[\"Meta\"] = type(\n            \"GeneratedMeta\", (getattr(cls, \"Meta\", object),), {\"register\": False}\n        )\n        schema_cls = type(name, (cls,), attrs)\n        return schema_cls",
        "file_path": "src/marshmallow/schema.py",
        "chunk_index": 11,
        "metadata": {
          "filepath": "src/marshmallow/schema.py",
          "function_name": "from_dict",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 27,
          "start_line_no": 414,
          "end_line_no": 440
        }
      },
      {
        "rank": 10,
        "score": 0.5253565311431885,
        "content": "def on_bind_field(self, field_name, field_obj):\n        \"\"\"Hook to modify a field when it is bound to the `Schema`.\n\n        No-op by default.\n        \"\"\"\n        return None",
        "file_path": "src/marshmallow/schema.py",
        "chunk_index": 27,
        "metadata": {
          "filepath": "src/marshmallow/schema.py",
          "function_name": "on_bind_field",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 6,
          "start_line_no": 950,
          "end_line_no": 955
        }
      }
    ]
  },
  "marshmallow-code__marshmallow-1343": {
    "query": "[version 2.20.0] TypeError: 'NoneType' object is not subscriptable\nAfter update from version 2.19.5 to 2.20.0 I got error for code like:\r\n\r\n```python\r\nfrom marshmallow import Schema, fields, validates\r\n\r\n\r\nclass Bar(Schema):\r\n    value = fields.String()\r\n\r\n    @validates('value')  # <- issue here\r\n    def validate_value(self, value):\r\n        pass\r\n\r\n\r\nclass Foo(Schema):\r\n    bar = fields.Nested(Bar)\r\n\r\n\r\nsch = Foo()\r\n\r\nsch.validate({\r\n    'bar': 'invalid',\r\n})\r\n```\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/_/bug_mschema.py\", line 19, in <module>\r\n    'bar': 'invalid',\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/schema.py\", line 628, in validate\r\n    _, errors = self._do_load(data, many, partial=partial, postprocess=False)\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/schema.py\", line 670, in _do_load\r\n    index_errors=self.opts.index_errors,\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/marshalling.py\", line 292, in deserialize\r\n    index=(index if index_errors else None)\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/marshalling.py\", line 65, in call_and_store\r\n    value = getter_func(data)\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/marshalling.py\", line 285, in <lambda>\r\n    data\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/fields.py\", line 265, in deserialize\r\n    output = self._deserialize(value, attr, data)\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/fields.py\", line 465, in _deserialize\r\n    data, errors = self.schema.load(value)\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/schema.py\", line 588, in load\r\n    result, errors = self._do_load(data, many, partial=partial, postprocess=True)\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/schema.py\", line 674, in _do_load\r\n    self._invoke_field_validators(unmarshal, data=result, many=many)\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/schema.py\", line 894, in _invoke_field_validators\r\n    value = data[field_obj.attribute or field_name]\r\nTypeError: 'NoneType' object is not subscriptable\r\n```\n",
    "method": "function",
    "top_k": 10,
    "results": [
      {
        "rank": 1,
        "score": 0.5871920585632324,
        "content": "def _add_to_schema(self, field_name, schema):\n        \"\"\"Update field with values from its parent schema. Called by\n            :meth:`__set_field_attrs <marshmallow.Schema.__set_field_attrs>`.\n\n        :param str field_name: Field name set in schema.\n        :param Schema schema: Parent schema.\n        \"\"\"\n        self.parent = self.parent or schema\n        self.name = self.name or field_name",
        "file_path": "src/marshmallow/fields.py",
        "chunk_index": 8,
        "metadata": {
          "filepath": "src/marshmallow/fields.py",
          "function_name": "_add_to_schema",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 9,
          "start_line_no": 271,
          "end_line_no": 279
        }
      },
      {
        "rank": 2,
        "score": 0.5690475702285767,
        "content": "def validates_schema(fn=None, pass_many=False, pass_original=False, skip_on_field_errors=False):\n    \"\"\"Register a schema-level validator.\n\n    By default, receives a single object at a time, regardless of whether ``many=True``\n    is passed to the `Schema`. If ``pass_many=True``, the raw data (which may be a collection)\n    and the value for ``many`` is passed.\n\n    If ``pass_original=True``, the original data (before unmarshalling) will be passed as\n    an additional argument to the method.\n\n    If ``skip_on_field_errors=True``, this validation method will be skipped whenever\n    validation errors have been detected when validating fields.\n    \"\"\"\n    return tag_processor(VALIDATES_SCHEMA, fn, pass_many, pass_original=pass_original,\n                         skip_on_field_errors=skip_on_field_errors)",
        "file_path": "src/marshmallow/decorators.py",
        "chunk_index": 1,
        "metadata": {
          "filepath": "src/marshmallow/decorators.py",
          "function_name": "validates_schema",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 15,
          "start_line_no": 72,
          "end_line_no": 86
        }
      },
      {
        "rank": 3,
        "score": 0.5674552917480469,
        "content": "def root(self):\n        \"\"\"Reference to the `Schema` that this field belongs to even if it is buried in a `List`.\n        Return `None` for unbound fields.\n        \"\"\"\n        ret = self\n        while hasattr(ret, 'parent') and ret.parent:\n            ret = ret.parent\n        return ret if isinstance(ret, SchemaABC) else None",
        "file_path": "src/marshmallow/fields.py",
        "chunk_index": 12,
        "metadata": {
          "filepath": "src/marshmallow/fields.py",
          "function_name": "root",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 8,
          "start_line_no": 323,
          "end_line_no": 330
        }
      },
      {
        "rank": 4,
        "score": 0.5600955486297607,
        "content": "def schema(self):\n        \"\"\"The nested Schema object.\n\n        .. versionchanged:: 1.0.0\n            Renamed from `serializer` to `schema`\n        \"\"\"\n        if not self.__schema:\n            # Ensure that only parameter is a tuple\n            if isinstance(self.only, basestring):\n                only = (self.only,)\n            else:\n                only = self.only\n\n            # Inherit context from parent.\n            context = getattr(self.parent, 'context', {})\n            if isinstance(self.nested, SchemaABC):\n                self.__schema = self.nested\n                self.__schema.context.update(context)\n            elif isinstance(self.nested, type) and \\\n                    issubclass(self.nested, SchemaABC):\n                self.__schema = self.nested(many=self.many,\n                        only=only, exclude=self.exclude, context=context,\n                        load_only=self._nested_normalized_option('load_only'),\n                        dump_only=self._nested_normalized_option('dump_only'))\n            elif isinstance(self.nested, basestring):\n                if self.nested == _RECURSIVE_NESTED:\n                    parent_class = self.parent.__class__\n                    self.__schema = parent_class(many=self.many, only=only,\n                            exclude=self.exclude, context=context,\n                            load_only=self._nested_normalized_option('load_only'),\n                            dump_only=self._nested_normalized_option('dump_only'))\n                else:\n                    schema_class = class_registry.get_class(self.nested)\n                    self.__schema = schema_class(many=self.many,\n                            only=only, exclude=self.exclude, context=context,\n                            load_only=self._nested_normalized_option('load_only'),\n                            dump_only=self._nested_normalized_option('dump_only'))\n            else:\n                raise ValueError('Nested fields must be passed a '\n                                 'Schema, not {0}.'.format(self.nested.__class__))\n            self.__schema.ordered = getattr(self.parent, 'ordered', False)\n        return self.__schema",
        "file_path": "src/marshmallow/fields.py",
        "chunk_index": 14,
        "metadata": {
          "filepath": "src/marshmallow/fields.py",
          "function_name": "schema",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 42,
          "start_line_no": 388,
          "end_line_no": 429
        }
      },
      {
        "rank": 5,
        "score": 0.5482391715049744,
        "content": "def call_and_store(self, getter_func, data, field_name, field_obj, index=None):\n        \"\"\"Call ``getter_func`` with ``data`` as its argument, and store any `ValidationErrors`.\n\n        :param callable getter_func: Function for getting the serialized/deserialized\n            value from ``data``.\n        :param data: The data passed to ``getter_func``.\n        :param str field_name: Field name.\n        :param FieldABC field_obj: Field object that performs the\n            serialization/deserialization behavior.\n        :param int index: Index of the item being validated, if validating a collection,\n            otherwise `None`.\n        \"\"\"\n        try:\n            value = getter_func(data)\n        except ValidationError as err:  # Store validation errors\n            self.error_kwargs.update(err.kwargs)\n            self.error_fields.append(field_obj)\n            self.error_field_names.append(field_name)\n            errors = self.get_errors(index=index)\n            # Warning: Mutation!\n            if isinstance(err.messages, dict):\n                errors[field_name] = err.messages\n            elif isinstance(errors.get(field_name), dict):\n                errors[field_name].setdefault(FIELD, []).extend(err.messages)\n            else:\n                errors.setdefault(field_name, []).extend(err.messages)\n            # When a Nested field fails validation, the marshalled data is stored\n            # on the ValidationError's data attribute\n            value = err.data or missing\n        return value",
        "file_path": "src/marshmallow/marshalling.py",
        "chunk_index": 2,
        "metadata": {
          "filepath": "src/marshmallow/marshalling.py",
          "function_name": "call_and_store",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 30,
          "start_line_no": 52,
          "end_line_no": 81
        }
      },
      {
        "rank": 6,
        "score": 0.5442743301391602,
        "content": "def error_handler(cls, func):\n        \"\"\"Decorator that registers an error handler function for the schema.\n        The function receives the :class:`Schema` instance, a dictionary of errors,\n        and the serialized object (if serializing data) or data dictionary (if\n        deserializing data) as arguments.\n\n        Example: ::\n\n            class UserSchema(Schema):\n                email = fields.Email()\n\n            @UserSchema.error_handler\n            def handle_errors(schema, errors, obj):\n                raise ValueError('An error occurred while marshalling {}'.format(obj))\n\n            user = User(email='invalid')\n            UserSchema().dump(user)  # => raises ValueError\n            UserSchema().load({'email': 'bademail'})  # raises ValueError\n\n        .. versionadded:: 0.7.0\n        .. deprecated:: 2.0.0\n            Set the ``error_handler`` class Meta option instead.\n        \"\"\"\n        warnings.warn(\n            'Schema.error_handler is deprecated. Set the error_handler class Meta option '\n            'instead.', category=DeprecationWarning\n        )\n        cls.__error_handler__ = func\n        return func",
        "file_path": "src/marshmallow/schema.py",
        "chunk_index": 14,
        "metadata": {
          "filepath": "src/marshmallow/schema.py",
          "function_name": "error_handler",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 29,
          "start_line_no": 420,
          "end_line_no": 448
        }
      },
      {
        "rank": 7,
        "score": 0.5423014760017395,
        "content": "def _validate_missing(self, value):\n        \"\"\"Validate missing values. Raise a :exc:`ValidationError` if\n        `value` should be considered missing.\n        \"\"\"\n        if value is missing_ and hasattr(self, 'required'):\n            if self.nested == _RECURSIVE_NESTED:\n                self.fail('required')\n            errors = self._check_required()\n            if errors:\n                raise ValidationError(errors)\n        else:\n            super(Nested, self)._validate_missing(value)",
        "file_path": "src/marshmallow/fields.py",
        "chunk_index": 18,
        "metadata": {
          "filepath": "src/marshmallow/fields.py",
          "function_name": "_validate_missing",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 12,
          "start_line_no": 470,
          "end_line_no": 481
        }
      },
      {
        "rank": 8,
        "score": 0.5399460792541504,
        "content": "def on_bind_field(self, field_name, field_obj):\n        \"\"\"Hook to modify a field when it is bound to the `Schema`. No-op by default.\"\"\"\n        return None",
        "file_path": "src/marshmallow/schema.py",
        "chunk_index": 25,
        "metadata": {
          "filepath": "src/marshmallow/schema.py",
          "function_name": "on_bind_field",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 3,
          "start_line_no": 777,
          "end_line_no": 779
        }
      },
      {
        "rank": 9,
        "score": 0.5341358184814453,
        "content": "def _do_load(self, data, many=None, partial=None, postprocess=True):\n        \"\"\"Deserialize `data`, returning the deserialized result and a dictonary of\n        validation errors.\n\n        :param data: The data to deserialize.\n        :param bool many: Whether to deserialize `data` as a collection. If `None`, the\n            value for `self.many` is used.\n        :param bool|tuple partial: Whether to validate required fields. If its value is an iterable,\n            only fields listed in that iterable will be ignored will be allowed missing.\n            If `True`, all fields will be allowed missing.\n            If `None`, the value for `self.partial` is used.\n        :param bool postprocess: Whether to run post_load methods..\n        :return: A tuple of the form (`data`, `errors`)\n        \"\"\"\n        # Callable unmarshalling object\n        unmarshal = marshalling.Unmarshaller()\n        errors = {}\n        many = self.many if many is None else bool(many)\n        if partial is None:\n            partial = self.partial\n        try:\n            processed_data = self._invoke_load_processors(\n                PRE_LOAD,\n                data,\n                many,\n                original_data=data)\n        except ValidationError as err:\n            errors = err.normalized_messages()\n            result = None\n        if not errors:\n            try:\n                result = unmarshal(\n                    processed_data,\n                    self.fields,\n                    many=many,\n                    partial=partial,\n                    dict_class=self.dict_class,\n                    index_errors=self.opts.index_errors,\n                )\n            except ValidationError as error:\n                result = error.data\n            self._invoke_field_validators(unmarshal, data=result, many=many)\n            errors = unmarshal.errors\n            field_errors = bool(errors)\n            # Run schema-level migration\n            try:\n                self._invoke_validators(unmarshal, pass_many=True, data=result, original_data=data,\n                                        many=many, field_errors=field_errors)\n            except ValidationError as err:\n                errors.update(err.messages)\n            try:\n                self._invoke_validators(unmarshal, pass_many=False, data=result, original_data=data,\n                                        many=many, field_errors=field_errors)\n            except ValidationError as err:\n                errors.update(err.messages)\n        # Run post processors\n        if not errors and postprocess:\n            try:\n                result = self._invoke_load_processors(\n                    POST_LOAD,\n                    result,\n                    many,\n                    original_data=data)\n            except ValidationError as err:\n                errors = err.normalized_messages()\n        if errors:\n            # TODO: Remove self.__error_handler__ in a later release\n            if self.__error_handler__ and callable(self.__error_handler__):\n                self.__error_handler__(errors, data)\n            exc = ValidationError(\n                errors,\n                field_names=unmarshal.error_field_names,\n                fields=unmarshal.error_fields,\n                data=data,\n                **unmarshal.error_kwargs\n            )\n            self.handle_error(exc, data)\n            if self.strict:\n                raise exc\n\n        return result, errors",
        "file_path": "src/marshmallow/schema.py",
        "chunk_index": 21,
        "metadata": {
          "filepath": "src/marshmallow/schema.py",
          "function_name": "_do_load",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 81,
          "start_line_no": 633,
          "end_line_no": 713
        }
      },
      {
        "rank": 10,
        "score": 0.5330426692962646,
        "content": "def validate(self, data, many=None, partial=None):\n        \"\"\"Validate `data` against the schema, returning a dictionary of\n        validation errors.\n\n        :param dict data: The data to validate.\n        :param bool many: Whether to validate `data` as a collection. If `None`, the\n            value for `self.many` is used.\n        :param bool|tuple partial: Whether to ignore missing fields. If `None`,\n            the value for `self.partial` is used. If its value is an iterable,\n            only missing fields listed in that iterable will be ignored.\n        :return: A dictionary of validation errors.\n        :rtype: dict\n\n        .. versionadded:: 1.1.0\n        \"\"\"\n        _, errors = self._do_load(data, many, partial=partial, postprocess=False)\n        return errors",
        "file_path": "src/marshmallow/schema.py",
        "chunk_index": 20,
        "metadata": {
          "filepath": "src/marshmallow/schema.py",
          "function_name": "validate",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 17,
          "start_line_no": 613,
          "end_line_no": 629
        }
      }
    ]
  },
  "pylint-dev__astroid-1978": {
    "query": "Deprecation warnings from numpy\n### Steps to reproduce\r\n\r\n1. Run pylint over the following test case:\r\n\r\n```\r\n\"\"\"Test case\"\"\"\r\n\r\nimport numpy as np\r\nvalue = np.random.seed(1234)\r\n```\r\n\r\n### Current behavior\r\n```\r\n/home/bje/source/nemo/myenv/lib/python3.10/site-packages/astroid/raw_building.py:470: FutureWarning: In the future `np.long` will be defined as the corresponding NumPy scalar.  (This may have returned Python scalars in past versions.\r\n  getattr(sys.modules[modname], name)\r\n/home/bje/source/nemo/myenv/lib/python3.10/site-packages/astroid/raw_building.py:470: FutureWarning: In the future `np.long` will be defined as the corresponding NumPy scalar.  (This may have returned Python scalars in past versions.\r\n  getattr(sys.modules[modname], name)\r\n```\r\n\r\n### Expected behavior\r\nThere should be no future warnings.\r\n\r\n### python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\" output\r\n2.12.13\n",
    "method": "function",
    "top_k": 10,
    "results": [
      {
        "rank": 1,
        "score": 0.6080761551856995,
        "content": "def _get_numpy_version() -> tuple[str, str, str]:\n    \"\"\"\n    Return the numpy version number if numpy can be imported.\n\n    Otherwise returns ('0', '0', '0')\n    \"\"\"\n    try:\n        import numpy  # pylint: disable=import-outside-toplevel\n\n        return tuple(numpy.version.version.split(\".\"))\n    except (ImportError, AttributeError):\n        return (\"0\", \"0\", \"0\")",
        "file_path": "astroid/brain/brain_numpy_utils.py",
        "chunk_index": 1,
        "metadata": {
          "filepath": "astroid/brain/brain_numpy_utils.py",
          "function_name": "_get_numpy_version",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 12,
          "start_line_no": 23,
          "end_line_no": 34
        }
      },
      {
        "rank": 2,
        "score": 0.5938296318054199,
        "content": "def numpy_supports_type_hints() -> bool:\n    \"\"\"Returns True if numpy supports type hints.\"\"\"\n    np_ver = _get_numpy_version()\n    return np_ver and np_ver > NUMPY_VERSION_TYPE_HINTS_SUPPORT",
        "file_path": "astroid/brain/brain_numpy_utils.py",
        "chunk_index": 0,
        "metadata": {
          "filepath": "astroid/brain/brain_numpy_utils.py",
          "function_name": "numpy_supports_type_hints",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 4,
          "start_line_no": 17,
          "end_line_no": 20
        }
      },
      {
        "rank": 3,
        "score": 0.5759243965148926,
        "content": "def deprecate_arguments(\n        astroid_version: str = \"3.0\", **arguments: str\n    ) -> Callable[[Callable[_P, _R]], Callable[_P, _R]]:\n        \"\"\"Passthrough decorator to improve performance if DeprecationWarnings are\n        disabled.\n        \"\"\"\n\n        def deco(func: Callable[_P, _R]) -> Callable[_P, _R]:\n            \"\"\"Decorator function.\"\"\"\n            return func\n\n        return deco",
        "file_path": "astroid/decorators.py",
        "chunk_index": 16,
        "metadata": {
          "filepath": "astroid/decorators.py",
          "function_name": "deprecate_arguments",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 12,
          "start_line_no": 276,
          "end_line_no": 287
        }
      },
      {
        "rank": 4,
        "score": 0.568659245967865,
        "content": "def deprecate_default_argument_values(\n        astroid_version: str = \"3.0\", **arguments: str\n    ) -> Callable[[Callable[_P, _R]], Callable[_P, _R]]:\n        \"\"\"Passthrough decorator to improve performance if DeprecationWarnings are\n        disabled.\n        \"\"\"\n\n        def deco(func: Callable[_P, _R]) -> Callable[_P, _R]:\n            \"\"\"Decorator function.\"\"\"\n            return func\n\n        return deco",
        "file_path": "astroid/decorators.py",
        "chunk_index": 14,
        "metadata": {
          "filepath": "astroid/decorators.py",
          "function_name": "deprecate_default_argument_values",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 12,
          "start_line_no": 263,
          "end_line_no": 274
        }
      },
      {
        "rank": 5,
        "score": 0.5637557506561279,
        "content": "def deprecate_arguments(\n        astroid_version: str = \"3.0\", **arguments: str\n    ) -> Callable[[Callable[_P, _R]], Callable[_P, _R]]:\n        \"\"\"Decorator which emits a DeprecationWarning if any arguments specified\n        are passed.\n\n        Arguments should be a key-value mapping, with the key being the argument to check\n        and the value being a string that explains what to do instead of passing the argument.\n\n        To improve performance, only used when DeprecationWarnings other than\n        the default one are enabled.\n        \"\"\"\n\n        def deco(func: Callable[_P, _R]) -> Callable[_P, _R]:\n            @functools.wraps(func)\n            def wrapper(*args: _P.args, **kwargs: _P.kwargs) -> _R:\n                keys = list(inspect.signature(func).parameters.keys())\n                for arg, note in arguments.items():\n                    try:\n                        index = keys.index(arg)\n                    except ValueError:\n                        raise ValueError(\n                            f\"Can't find argument '{arg}' for '{args[0].__class__.__qualname__}'\"\n                        ) from None\n                    if arg in kwargs or len(args) > index:\n                        warnings.warn(\n                            f\"The argument '{arg}' for \"\n                            f\"'{args[0].__class__.__qualname__}.{func.__name__}' is deprecated \"\n                            f\"and will be removed in astroid {astroid_version} ({note})\",\n                            DeprecationWarning,\n                        )\n                return func(*args, **kwargs)\n\n            return wrapper\n\n        return deco",
        "file_path": "astroid/decorators.py",
        "chunk_index": 11,
        "metadata": {
          "filepath": "astroid/decorators.py",
          "function_name": "deprecate_arguments",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 36,
          "start_line_no": 224,
          "end_line_no": 259
        }
      },
      {
        "rank": 6,
        "score": 0.5599681735038757,
        "content": "def deprecate_default_argument_values(\n        astroid_version: str = \"3.0\", **arguments: str\n    ) -> Callable[[Callable[_P, _R]], Callable[_P, _R]]:\n        \"\"\"Decorator which emits a DeprecationWarning if any arguments specified\n        are None or not passed at all.\n\n        Arguments should be a key-value mapping, with the key being the argument to check\n        and the value being a type annotation as string for the value of the argument.\n\n        To improve performance, only used when DeprecationWarnings other than\n        the default one are enabled.\n        \"\"\"\n        # Helpful links\n        # Decorator for DeprecationWarning: https://stackoverflow.com/a/49802489\n        # Typing of stacked decorators: https://stackoverflow.com/a/68290080\n\n        def deco(func: Callable[_P, _R]) -> Callable[_P, _R]:\n            \"\"\"Decorator function.\"\"\"\n\n            @functools.wraps(func)\n            def wrapper(*args: _P.args, **kwargs: _P.kwargs) -> _R:\n                \"\"\"Emit DeprecationWarnings if conditions are met.\"\"\"\n\n                keys = list(inspect.signature(func).parameters.keys())\n                for arg, type_annotation in arguments.items():\n                    try:\n                        index = keys.index(arg)\n                    except ValueError:\n                        raise ValueError(\n                            f\"Can't find argument '{arg}' for '{args[0].__class__.__qualname__}'\"\n                        ) from None\n                    if (\n                        # Check kwargs\n                        # - if found, check it's not None\n                        (arg in kwargs and kwargs[arg] is None)\n                        # Check args\n                        # - make sure not in kwargs\n                        # - len(args) needs to be long enough, if too short\n                        #   arg can't be in args either\n                        # - args[index] should not be None\n                        or arg not in kwargs\n                        and (\n                            index == -1\n                            or len(args) <= index\n                            or (len(args) > index and args[index] is None)\n                        )\n                    ):\n                        warnings.warn(\n                            f\"'{arg}' will be a required argument for \"\n                            f\"'{args[0].__class__.__qualname__}.{func.__name__}'\"\n                            f\" in astroid {astroid_version} \"\n                            f\"('{arg}' should be of type: '{type_annotation}')\",\n                            DeprecationWarning,\n                        )\n                return func(*args, **kwargs)\n\n            return wrapper\n\n        return deco",
        "file_path": "astroid/decorators.py",
        "chunk_index": 8,
        "metadata": {
          "filepath": "astroid/decorators.py",
          "function_name": "deprecate_default_argument_values",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 59,
          "start_line_no": 164,
          "end_line_no": 222
        }
      },
      {
        "rank": 7,
        "score": 0.5253008008003235,
        "content": "def _is_a_numpy_module(node: Name) -> bool:\n    \"\"\"\n    Returns True if the node is a representation of a numpy module.\n\n    For example in :\n        import numpy as np\n        x = np.linspace(1, 2)\n    The node <Name.np> is a representation of the numpy module.\n\n    :param node: node to test\n    :return: True if the node is a representation of the numpy module.\n    \"\"\"\n    module_nickname = node.name\n    potential_import_target = [\n        x for x in node.lookup(module_nickname)[1] if isinstance(x, Import)\n    ]\n    return any(\n        (\"numpy\", module_nickname) in target.names or (\"numpy\", None) in target.names\n        for target in potential_import_target\n    )",
        "file_path": "astroid/brain/brain_numpy_utils.py",
        "chunk_index": 3,
        "metadata": {
          "filepath": "astroid/brain/brain_numpy_utils.py",
          "function_name": "_is_a_numpy_module",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 20,
          "start_line_no": 42,
          "end_line_no": 61
        }
      },
      {
        "rank": 8,
        "score": 0.5237811803817749,
        "content": "def wrapper(*args: _P.args, **kwargs: _P.kwargs) -> _R:\n                \"\"\"Emit DeprecationWarnings if conditions are met.\"\"\"\n\n                keys = list(inspect.signature(func).parameters.keys())\n                for arg, type_annotation in arguments.items():\n                    try:\n                        index = keys.index(arg)\n                    except ValueError:\n                        raise ValueError(\n                            f\"Can't find argument '{arg}' for '{args[0].__class__.__qualname__}'\"\n                        ) from None\n                    if (\n                        # Check kwargs\n                        # - if found, check it's not None\n                        (arg in kwargs and kwargs[arg] is None)\n                        # Check args\n                        # - make sure not in kwargs\n                        # - len(args) needs to be long enough, if too short\n                        #   arg can't be in args either\n                        # - args[index] should not be None\n                        or arg not in kwargs\n                        and (\n                            index == -1\n                            or len(args) <= index\n                            or (len(args) > index and args[index] is None)\n                        )\n                    ):\n                        warnings.warn(\n                            f\"'{arg}' will be a required argument for \"\n                            f\"'{args[0].__class__.__qualname__}.{func.__name__}'\"\n                            f\" in astroid {astroid_version} \"\n                            f\"('{arg}' should be of type: '{type_annotation}')\",\n                            DeprecationWarning,\n                        )\n                return func(*args, **kwargs)",
        "file_path": "astroid/decorators.py",
        "chunk_index": 10,
        "metadata": {
          "filepath": "astroid/decorators.py",
          "function_name": "wrapper",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 35,
          "start_line_no": 184,
          "end_line_no": 218
        }
      },
      {
        "rank": 9,
        "score": 0.5216134190559387,
        "content": "def deco(func: Callable[_P, _R]) -> Callable[_P, _R]:\n            \"\"\"Decorator function.\"\"\"\n\n            @functools.wraps(func)\n            def wrapper(*args: _P.args, **kwargs: _P.kwargs) -> _R:\n                \"\"\"Emit DeprecationWarnings if conditions are met.\"\"\"\n\n                keys = list(inspect.signature(func).parameters.keys())\n                for arg, type_annotation in arguments.items():\n                    try:\n                        index = keys.index(arg)\n                    except ValueError:\n                        raise ValueError(\n                            f\"Can't find argument '{arg}' for '{args[0].__class__.__qualname__}'\"\n                        ) from None\n                    if (\n                        # Check kwargs\n                        # - if found, check it's not None\n                        (arg in kwargs and kwargs[arg] is None)\n                        # Check args\n                        # - make sure not in kwargs\n                        # - len(args) needs to be long enough, if too short\n                        #   arg can't be in args either\n                        # - args[index] should not be None\n                        or arg not in kwargs\n                        and (\n                            index == -1\n                            or len(args) <= index\n                            or (len(args) > index and args[index] is None)\n                        )\n                    ):\n                        warnings.warn(\n                            f\"'{arg}' will be a required argument for \"\n                            f\"'{args[0].__class__.__qualname__}.{func.__name__}'\"\n                            f\" in astroid {astroid_version} \"\n                            f\"('{arg}' should be of type: '{type_annotation}')\",\n                            DeprecationWarning,\n                        )\n                return func(*args, **kwargs)\n\n            return wrapper",
        "file_path": "astroid/decorators.py",
        "chunk_index": 9,
        "metadata": {
          "filepath": "astroid/decorators.py",
          "function_name": "deco",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 41,
          "start_line_no": 180,
          "end_line_no": 220
        }
      },
      {
        "rank": 10,
        "score": 0.519788384437561,
        "content": "def check_warnings_filter() -> bool:\n    \"\"\"Return True if any other than the default DeprecationWarning filter is enabled.\n\n    https://docs.python.org/3/library/warnings.html#default-warning-filter\n    \"\"\"\n    return any(\n        issubclass(DeprecationWarning, filter[2])\n        and filter[0] != \"ignore\"\n        and filter[3] != \"__main__\"\n        for filter in warnings.filters\n    )",
        "file_path": "astroid/util.py",
        "chunk_index": 16,
        "metadata": {
          "filepath": "astroid/util.py",
          "function_name": "check_warnings_filter",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 11,
          "start_line_no": 144,
          "end_line_no": 154
        }
      }
    ]
  },
  "pylint-dev__astroid-1333": {
    "query": "astroid 2.9.1 breaks pylint with missing __init__.py: F0010: error while code parsing: Unable to load file __init__.py\n### Steps to reproduce\r\n> Steps provided are for Windows 11, but initial problem found in Ubuntu 20.04\r\n\r\n> Update 2022-01-04: Corrected repro steps and added more environment details\r\n\r\n1. Set up simple repo with following structure (all files can be empty):\r\n```\r\nroot_dir/\r\n|--src/\r\n|----project/ # Notice the missing __init__.py\r\n|------file.py # It can be empty, but I added `import os` at the top\r\n|----__init__.py\r\n```\r\n2. Open a command prompt\r\n3. `cd root_dir`\r\n4. `python -m venv venv`\r\n5. `venv/Scripts/activate`\r\n6. `pip install pylint astroid==2.9.1` # I also repro'd on the latest, 2.9.2\r\n7. `pylint src/project` # Updated from `pylint src`\r\n8. Observe failure:\r\n```\r\nsrc\\project\\__init__.py:1:0: F0010: error while code parsing: Unable to load file src\\project\\__init__.py:\r\n```\r\n\r\n### Current behavior\r\nFails with `src\\project\\__init__.py:1:0: F0010: error while code parsing: Unable to load file src\\project\\__init__.py:`\r\n\r\n### Expected behavior\r\nDoes not fail with error.\r\n> If you replace step 6 with `pip install pylint astroid==2.9.0`, you get no failure with an empty output - since no files have content\r\n\r\n### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\r\n2.9.1\r\n\r\n`python 3.9.1`\r\n`pylint 2.12.2 `\r\n\r\n\r\n\r\nThis issue has been observed with astroid `2.9.1` and `2.9.2`\n",
    "method": "function",
    "top_k": 10,
    "results": [
      {
        "rank": 1,
        "score": 0.6061620116233826,
        "content": "ef ast_from_file(self, filepath, modname=None, fallback=True, source=False):\n        \"\"\"given a module name, return the astroid object\"\"\"\n        try:\n            filepath = get_source_file(filepath, include_no_ext=True)\n            source = True\n        except NoSourceFile:\n            pass\n        if modname is None:\n            try:\n                modname = \".\".join(modpath_from_file(filepath))\n            except ImportError:\n                modname = filepath\n        if (\n            modname in self.astroid_cache\n            and self.astroid_cache[modname].file == filepath\n        ):\n            return self.astroid_cache[modname]\n        if source:\n            # pylint: disable=import-outside-toplevel; circular import\n            from astroid.builder import AstroidBuilder\n\n            return AstroidBuilder(self).file_build(filepath, modname)\n        if fallback and modname:\n            return self.ast_from_module_name(modname)\n        raise AstroidBuildingError(\"Unable to build an AST for {path}.\", path=filepath)\n",
        "file_path": "astroid/manager.py",
        "chunk_index": 6,
        "metadata": {
          "filepath": "astroid/manager.py",
          "function_name": "ast_from_file",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 25,
          "start_line_no": 103,
          "end_line_no": 127
        }
      },
      {
        "rank": 2,
        "score": 0.5996747612953186,
        "content": "f file_build(self, path, modname=None):\n        \"\"\"Build astroid from a source code file (i.e. from an ast)\n\n        *path* is expected to be a python source file\n        \"\"\"\n        try:\n            stream, encoding, data = open_source_file(path)\n        except OSError as exc:\n            raise AstroidBuildingError(\n                \"Unable to load file {path}:\\n{error}\",\n                modname=modname,\n                path=path,\n                error=exc,\n            ) from exc\n        except (SyntaxError, LookupError) as exc:\n            raise AstroidSyntaxError(\n                \"Python 3 encoding specification error or unknown encoding:\\n\"\n                \"{error}\",\n                modname=modname,\n                path=path,\n                error=exc,\n            ) from exc\n        except UnicodeError as exc:  # wrong encoding\n            # detect_encoding returns utf-8 if no encoding specified\n            raise AstroidBuildingError(\n                \"Wrong or no encoding specified for {filename}.\", filename=path\n            ) from exc\n        with stream:\n            # get module name if necessary\n            if modname is None:\n                try:\n                    modname = \".\".join(modutils.modpath_from_file(path))\n                except ImportError:\n                    modname = os.path.splitext(os.path.basename(path))[0]\n            # build astroid representation\n            module = self._data_build(data, modname, path)\n            return self._post_build(module, encoding)\n\n",
        "file_path": "astroid/builder.py",
        "chunk_index": 4,
        "metadata": {
          "filepath": "astroid/builder.py",
          "function_name": "file_build",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 38,
          "start_line_no": 115,
          "end_line_no": 151
        }
      },
      {
        "rank": 3,
        "score": 0.5963910818099976,
        "content": "ef ast_from_string(self, data, modname=\"\", filepath=None):\n        \"\"\"Given some source code as a string, return its corresponding astroid object\"\"\"\n        # pylint: disable=import-outside-toplevel; circular import\n        from astroid.builder import AstroidBuilder\n\n        return AstroidBuilder(self).string_build(data, modname, filepath)\n",
        "file_path": "astroid/manager.py",
        "chunk_index": 7,
        "metadata": {
          "filepath": "astroid/manager.py",
          "function_name": "ast_from_string",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 6,
          "start_line_no": 129,
          "end_line_no": 134
        }
      },
      {
        "rank": 4,
        "score": 0.5946140885353088,
        "content": "ef bootstrap(self):\n        \"\"\"Bootstrap the required AST modules needed for the manager to work\n\n        The bootstrap usually involves building the AST for the builtins\n        module, which is required by the rest of astroid to work correctly.\n        \"\"\"\n        from astroid import raw_building  # pylint: disable=import-outside-toplevel\n\n        raw_building._astroid_bootstrapping()\n",
        "file_path": "astroid/manager.py",
        "chunk_index": 19,
        "metadata": {
          "filepath": "astroid/manager.py",
          "function_name": "bootstrap",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 9,
          "start_line_no": 362,
          "end_line_no": 370
        }
      },
      {
        "rank": 5,
        "score": 0.5806784629821777,
        "content": "f parse(code, module_name=\"\", path=None, apply_transforms=True):\n    \"\"\"Parses a source string in order to obtain an astroid AST from it\n\n    :param str code: The code for the module.\n    :param str module_name: The name for the module, if any\n    :param str path: The path for the module\n    :param bool apply_transforms:\n        Apply the transforms for the give code. Use it if you\n        don't want the default transforms to be applied.\n    \"\"\"\n    code = textwrap.dedent(code)\n    builder = AstroidBuilder(\n        manager=AstroidManager(), apply_transforms=apply_transforms\n    )\n    return builder.string_build(code, modname=module_name, path=path)\n\n",
        "file_path": "astroid/builder.py",
        "chunk_index": 13,
        "metadata": {
          "filepath": "astroid/builder.py",
          "function_name": "parse",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 16,
          "start_line_no": 282,
          "end_line_no": 296
        }
      },
      {
        "rank": 6,
        "score": 0.5763754844665527,
        "content": "ef ast_from_module(self, module: types.ModuleType, modname: Optional[str] = None):\n        \"\"\"given an imported module, return the astroid object\"\"\"\n        modname = modname or module.__name__\n        if modname in self.astroid_cache:\n            return self.astroid_cache[modname]\n        try:\n            # some builtin modules don't have __file__ attribute\n            filepath = module.__file__\n            if is_python_source(filepath):\n                return self.ast_from_file(filepath, modname)\n        except AttributeError:\n            pass\n\n        # pylint: disable=import-outside-toplevel; circular import\n        from astroid.builder import AstroidBuilder\n\n        return AstroidBuilder(self).module_build(module, modname)\n",
        "file_path": "astroid/manager.py",
        "chunk_index": 14,
        "metadata": {
          "filepath": "astroid/manager.py",
          "function_name": "ast_from_module",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 17,
          "start_line_no": 271,
          "end_line_no": 287
        }
      },
      {
        "rank": 7,
        "score": 0.5701638460159302,
        "content": " _has_init(directory):\n    \"\"\"if the given directory has a valid __init__ file, return its path,\n    else return None\n    \"\"\"\n    mod_or_pack = os.path.join(directory, \"__init__\")\n    for ext in PY_SOURCE_EXTS + (\"pyc\", \"pyo\"):\n        if os.path.exists(mod_or_pack + \".\" + ext):\n            return mod_or_pack + \".\" + ext\n    return None\n\n\n",
        "file_path": "astroid/modutils.py",
        "chunk_index": 22,
        "metadata": {
          "filepath": "astroid/modutils.py",
          "function_name": "_has_init",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 11,
          "start_line_no": 643,
          "end_line_no": 651
        }
      },
      {
        "rank": 8,
        "score": 0.5553967952728271,
        "content": "def pytest_transform():\n    return AstroidBuilder(AstroidManager()).string_build(\n        \"\"\"\n\ntry:\n    import _pytest.mark\n    import _pytest.recwarn\n    import _pytest.runner\n    import _pytest.python\n    import _pytest.skipping\n    import _pytest.assertion\nexcept ImportError:\n    pass\nelse:\n    deprecated_call = _pytest.recwarn.deprecated_call\n    warns = _pytest.recwarn.warns\n\n    exit = _pytest.runner.exit\n    fail = _pytest.runner.fail\n    skip = _pytest.runner.skip\n    importorskip = _pytest.runner.importorskip\n\n    xfail = _pytest.skipping.xfail\n    mark = _pytest.mark.MarkGenerator()\n    raises = _pytest.python.raises\n\n    # New in pytest 3.0\n    try:\n        approx = _pytest.python.approx\n        register_assert_rewrite = _pytest.assertion.register_assert_rewrite\n    except AttributeError:\n        pass\n\n\n# Moved in pytest 3.0\n\ntry:\n    import _pytest.freeze_support\n    freeze_includes = _pytest.freeze_support.freeze_includes\nexcept ImportError:\n    try:\n        import _pytest.genscript\n        freeze_includes = _pytest.genscript.freeze_includes\n    except ImportError:\n        pass\n\ntry:\n    import _pytest.debugging\n    set_trace = _pytest.debugging.pytestPDB().set_trace\nexcept ImportError:\n    try:\n        import _pytest.pdb\n        set_trace = _pytest.pdb.pytestPDB().set_trace\n    except ImportError:\n        pass\n\ntry:\n    import _pytest.fixtures\n    fixture = _pytest.fixtures.fixture\n    yield_fixture = _pytest.fixtures.yield_fixture\nexcept ImportError:\n    try:\n        import _pytest.python\n        fixture = _pytest.python.fixture\n        yield_fixture = _pytest.python.yield_fixture\n    except ImportError:\n        pass\n\"\"\"\n    )",
        "file_path": "astroid/brain/brain_pytest.py",
        "chunk_index": 0,
        "metadata": {
          "filepath": "astroid/brain/brain_pytest.py",
          "function_name": "pytest_transform",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 69,
          "start_line_no": 19,
          "end_line_no": 87
        }
      },
      {
        "rank": 9,
        "score": 0.5552796721458435,
        "content": "ef ast_from_module_name(self, modname, context_file=None):\n        \"\"\"given a module name, return the astroid object\"\"\"\n        if modname in self.astroid_cache:\n            return self.astroid_cache[modname]\n        if modname == \"__main__\":\n            return self._build_stub_module(modname)\n        if context_file:\n            old_cwd = os.getcwd()\n            os.chdir(os.path.dirname(context_file))\n        try:\n            found_spec = self.file_from_module_name(modname, context_file)\n            if found_spec.type == spec.ModuleType.PY_ZIPMODULE:\n                module = self.zip_import_data(found_spec.location)\n                if module is not None:\n                    return module\n\n            elif found_spec.type in (\n                spec.ModuleType.C_BUILTIN,\n                spec.ModuleType.C_EXTENSION,\n            ):\n                if (\n                    found_spec.type == spec.ModuleType.C_EXTENSION\n                    and not self._can_load_extension(modname)\n                ):\n                    return self._build_stub_module(modname)\n                try:\n                    module = load_module_from_name(modname)\n                except Exception as e:\n                    raise AstroidImportError(\n                        \"Loading {modname} failed with:\\n{error}\",\n                        modname=modname,\n                        path=found_spec.location,\n                    ) from e\n                return self.ast_from_module(module, modname)\n\n            elif found_spec.type == spec.ModuleType.PY_COMPILED:\n                raise AstroidImportError(\n                    \"Unable to load compiled module {modname}.\",\n                    modname=modname,\n                    path=found_spec.location,\n                )\n\n            elif found_spec.type == spec.ModuleType.PY_NAMESPACE:\n                return self._build_namespace_module(\n                    modname, found_spec.submodule_search_locations\n                )\n            elif found_spec.type == spec.ModuleType.PY_FROZEN:\n                return self._build_stub_module(modname)\n\n            if found_spec.location is None:\n                raise AstroidImportError(\n                    \"Can't find a file for module {modname}.\", modname=modname\n                )\n\n            return self.ast_from_file(found_spec.location, modname, fallback=False)\n        except AstroidBuildingError as e:\n            for hook in self._failed_import_hooks:\n                try:\n                    return hook(modname)\n                except AstroidBuildingError:\n                    pass\n            raise e\n        finally:\n            if context_file:\n                os.chdir(old_cwd)\n",
        "file_path": "astroid/manager.py",
        "chunk_index": 11,
        "metadata": {
          "filepath": "astroid/manager.py",
          "function_name": "ast_from_module_name",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 65,
          "start_line_no": 157,
          "end_line_no": 221
        }
      },
      {
        "rank": 10,
        "score": 0.5520001649856567,
        "content": "f string_build(self, data, modname=\"\", path=None):\n        \"\"\"Build astroid from source code string.\"\"\"\n        module = self._data_build(data, modname, path)\n        module.file_bytes = data.encode(\"utf-8\")\n        return self._post_build(module, \"utf-8\")\n\n",
        "file_path": "astroid/builder.py",
        "chunk_index": 5,
        "metadata": {
          "filepath": "astroid/builder.py",
          "function_name": "string_build",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 6,
          "start_line_no": 153,
          "end_line_no": 157
        }
      }
    ]
  },
  "pylint-dev__astroid-1196": {
    "query": "getitem does not infer the actual unpacked value\nWhen trying to call `Dict.getitem()` on a context where we have a dict unpacking of anything beside a real dict, astroid currently raises an `AttributeError: 'getitem'`, which has 2 problems:\r\n\r\n- The object might be a reference against something constant, this pattern is usually seen when we have different sets of dicts that extend each other, and all of their values are inferrable. \r\n- We can have something that is uninferable, but in that case instead of an `AttributeError` I think it makes sense to raise the usual `AstroidIndexError` which is supposed to be already handled by the downstream.\r\n\r\n\r\nHere is a short reproducer;\r\n\r\n```py\r\nfrom astroid import parse\r\n\r\n\r\nsource = \"\"\"\r\nX = {\r\n    'A': 'B'\r\n}\r\n\r\nY = {\r\n    **X\r\n}\r\n\r\nKEY = 'A'\r\n\"\"\"\r\n\r\ntree = parse(source)\r\n\r\nfirst_dict = tree.body[0].value\r\nsecond_dict = tree.body[1].value\r\nkey = tree.body[2].value\r\n\r\nprint(f'{first_dict.getitem(key).value = }')\r\nprint(f'{second_dict.getitem(key).value = }')\r\n\r\n\r\n```\r\n\r\nThe current output;\r\n\r\n```\r\n $ python t1.py                                                                                                 3ms\r\nfirst_dict.getitem(key).value = 'B'\r\nTraceback (most recent call last):\r\n  File \"/home/isidentical/projects/astroid/t1.py\", line 23, in <module>\r\n    print(f'{second_dict.getitem(key).value = }')\r\n  File \"/home/isidentical/projects/astroid/astroid/nodes/node_classes.py\", line 2254, in getitem\r\n    return value.getitem(index, context)\r\nAttributeError: 'Name' object has no attribute 'getitem'\r\n```\r\n\r\nExpeceted output;\r\n```\r\n $ python t1.py                                                                                                 4ms\r\nfirst_dict.getitem(key).value = 'B'\r\nsecond_dict.getitem(key).value = 'B'\r\n\r\n```\r\n\n",
    "method": "function",
    "top_k": 10,
    "results": [
      {
        "rank": 1,
        "score": 0.7531803250312805,
        "content": "def getitem(self, index, context=None):\n        \"\"\"Get an item from this node.\n\n        :param index: The node to use as a subscript index.\n        :type index: Const or Slice\n\n        :raises AstroidTypeError: When the given index cannot be used as a\n            subscript index, or if this node is not subscriptable.\n        :raises AstroidIndexError: If the given index does not exist in the\n            dictionary.\n        \"\"\"\n        for key, value in self.items:\n            # TODO(cpopa): no support for overriding yet, {1:2, **{1: 3}}.\n            if isinstance(key, DictUnpack):\n                try:\n                    return value.getitem(index, context)\n                except (AstroidTypeError, AstroidIndexError):\n                    continue\n            for inferredkey in key.infer(context):\n                if inferredkey is util.Uninferable:\n                    continue\n                if isinstance(inferredkey, Const) and isinstance(index, Const):\n                    if inferredkey.value == index.value:\n                        return value\n\n        raise AstroidIndexError(index)",
        "file_path": "astroid/nodes/node_classes.py",
        "chunk_index": 102,
        "metadata": {
          "filepath": "astroid/nodes/node_classes.py",
          "function_name": "getitem",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 26,
          "start_line_no": 2349,
          "end_line_no": 2374
        }
      },
      {
        "rank": 2,
        "score": 0.6961362361907959,
        "content": "def getitem(self, index, context=None):\n        new_context = bind_context_to_node(context, self)\n        if not context:\n            context = new_context\n        method = next(self.igetattr(\"__getitem__\", context=context), None)\n        # Create a new CallContext for providing index as an argument.\n        new_context.callcontext = CallContext(args=[index], callee=method)\n        if not isinstance(method, BoundMethod):\n            raise InferenceError(\n                \"Could not find __getitem__ for {node!r}.\", node=self, context=context\n            )\n        if len(method.args.arguments) != 2:  # (self, index)\n            raise AstroidTypeError(\n                \"__getitem__ for {node!r} does not have correct signature\",\n                node=self,\n                context=context,\n            )\n        return next(method.infer_call_result(self, new_context), None)",
        "file_path": "astroid/bases.py",
        "chunk_index": 17,
        "metadata": {
          "filepath": "astroid/bases.py",
          "function_name": "getitem",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 18,
          "start_line_no": 321,
          "end_line_no": 338
        }
      },
      {
        "rank": 3,
        "score": 0.6714750528335571,
        "content": "def getitem(self, index, context=None):\n        \"\"\"Return the inference of a subscript.\n\n        This is basically looking up the method in the metaclass and calling it.\n\n        :returns: The inferred value of a subscript to this class.\n        :rtype: NodeNG\n\n        :raises AstroidTypeError: If this class does not define a\n            ``__getitem__`` method.\n        \"\"\"\n        try:\n            methods = lookup(self, \"__getitem__\")\n        except AttributeInferenceError as exc:\n            if isinstance(self, ClassDef):\n                # subscripting a class definition may be\n                # achieved thanks to __class_getitem__ method\n                # which is a classmethod defined in the class\n                # that supports subscript and not in the metaclass\n                try:\n                    methods = self.getattr(\"__class_getitem__\")\n                    # Here it is assumed that the __class_getitem__ node is\n                    # a FunctionDef. One possible improvement would be to deal\n                    # with more generic inference.\n                except AttributeInferenceError:\n                    raise AstroidTypeError(node=self, context=context) from exc\n            else:\n                raise AstroidTypeError(node=self, context=context) from exc\n\n        method = methods[0]\n\n        # Create a new callcontext for providing index as an argument.\n        new_context = bind_context_to_node(context, self)\n        new_context.callcontext = CallContext(args=[index], callee=method)\n\n        try:\n            return next(method.infer_call_result(self, new_context), util.Uninferable)\n        except AttributeError:\n            # Starting with python3.9, builtin types list, dict etc...\n            # are subscriptable thanks to __class_getitem___ classmethod.\n            # However in such case the method is bound to an EmptyNode and\n            # EmptyNode doesn't have infer_call_result method yielding to\n            # AttributeError\n            if (\n                isinstance(method, node_classes.EmptyNode)\n                and self.pytype() == \"builtins.type\"\n                and PY39_PLUS\n            ):\n                return self\n            raise\n        except InferenceError:\n            return util.Uninferable",
        "file_path": "astroid/nodes/scoped_nodes/scoped_nodes.py",
        "chunk_index": 119,
        "metadata": {
          "filepath": "astroid/nodes/scoped_nodes/scoped_nodes.py",
          "function_name": "getitem",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 52,
          "start_line_no": 2721,
          "end_line_no": 2772
        }
      },
      {
        "rank": 4,
        "score": 0.6660448312759399,
        "content": "def infer_getattr(node, context=None):\n    \"\"\"Understand getattr calls\n\n    If one of the arguments is an Uninferable object, then the\n    result will be an Uninferable object. Otherwise, the normal attribute\n    lookup will be done.\n    \"\"\"\n    obj, attr = _infer_getattr_args(node, context)\n    if (\n        obj is util.Uninferable\n        or attr is util.Uninferable\n        or not hasattr(obj, \"igetattr\")\n    ):\n        return util.Uninferable\n\n    try:\n        return next(obj.igetattr(attr, context=context))\n    except (StopIteration, InferenceError, AttributeInferenceError):\n        if len(node.args) == 3:\n            # Try to infer the default and return it instead.\n            try:\n                return next(node.args[2].infer(context=context))\n            except (StopIteration, InferenceError) as exc:\n                raise UseInferenceDefault from exc\n\n    raise UseInferenceDefault",
        "file_path": "astroid/brain/brain_builtin_inference.py",
        "chunk_index": 13,
        "metadata": {
          "filepath": "astroid/brain/brain_builtin_inference.py",
          "function_name": "infer_getattr",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 26,
          "start_line_no": 461,
          "end_line_no": 486
        }
      },
      {
        "rank": 5,
        "score": 0.6591811776161194,
        "content": "def getitem(self, index, context=None):\n        \"\"\"Get an item from this node if subscriptable.\n\n        :param index: The node to use as a subscript index.\n        :type index: Const or Slice\n\n        :raises AstroidTypeError: When the given index cannot be used as a\n            subscript index, or if this node is not subscriptable.\n        \"\"\"\n        if isinstance(index, Const):\n            index_value = index.value\n        elif isinstance(index, Slice):\n            index_value = _infer_slice(index, context=context)\n\n        else:\n            raise AstroidTypeError(\n                f\"Could not use type {type(index)} as subscript index\"\n            )\n\n        try:\n            if isinstance(self.value, (str, bytes)):\n                return Const(self.value[index_value])\n        except IndexError as exc:\n            raise AstroidIndexError(\n                message=\"Index {index!r} out of range\",\n                node=self,\n                index=index,\n                context=context,\n            ) from exc\n        except TypeError as exc:\n            raise AstroidTypeError(\n                message=\"Type error {error!r}\", node=self, index=index, context=context\n            ) from exc\n\n        raise AstroidTypeError(f\"{self!r} (value={self.value})\")",
        "file_path": "astroid/nodes/node_classes.py",
        "chunk_index": 80,
        "metadata": {
          "filepath": "astroid/nodes/node_classes.py",
          "function_name": "getitem",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 35,
          "start_line_no": 1942,
          "end_line_no": 1976
        }
      },
      {
        "rank": 6,
        "score": 0.6541242599487305,
        "content": "def infer_subscript(self, context=None):\n    \"\"\"Inference for subscripts\n\n    We're understanding if the index is a Const\n    or a slice, passing the result of inference\n    to the value's `getitem` method, which should\n    handle each supported index type accordingly.\n    \"\"\"\n\n    found_one = False\n    for value in self.value.infer(context):\n        if value is util.Uninferable:\n            yield util.Uninferable\n            return None\n        for index in self.slice.infer(context):\n            if index is util.Uninferable:\n                yield util.Uninferable\n                return None\n\n            # Try to deduce the index value.\n            index_value = _SUBSCRIPT_SENTINEL\n            if value.__class__ == bases.Instance:\n                index_value = index\n            elif index.__class__ == bases.Instance:\n                instance_as_index = helpers.class_instance_as_index(index)\n                if instance_as_index:\n                    index_value = instance_as_index\n            else:\n                index_value = index\n\n            if index_value is _SUBSCRIPT_SENTINEL:\n                raise InferenceError(node=self, context=context)\n\n            try:\n                assigned = value.getitem(index_value, context)\n            except (\n                AstroidTypeError,\n                AstroidIndexError,\n                AttributeInferenceError,\n                AttributeError,\n            ) as exc:\n                raise InferenceError(node=self, context=context) from exc\n\n            # Prevent inferring if the inferred subscript\n            # is the same as the original subscripted object.\n            if self is assigned or assigned is util.Uninferable:\n                yield util.Uninferable\n                return None\n            yield from assigned.infer(context)\n            found_one = True\n\n    if found_one:\n        return dict(node=self, context=context)\n    return None",
        "file_path": "astroid/inference.py",
        "chunk_index": 13,
        "metadata": {
          "filepath": "astroid/inference.py",
          "function_name": "infer_subscript",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 54,
          "start_line_no": 348,
          "end_line_no": 401
        }
      },
      {
        "rank": 7,
        "score": 0.6495540738105774,
        "content": "def _infer_map(node, context):\n    \"\"\"Infer all values based on Dict.items\"\"\"\n    values = {}\n    for name, value in node.items:\n        if isinstance(name, nodes.DictUnpack):\n            double_starred = helpers.safe_infer(value, context)\n            if not double_starred:\n                raise InferenceError\n            if not isinstance(double_starred, nodes.Dict):\n                raise InferenceError(node=node, context=context)\n            unpack_items = _infer_map(double_starred, context)\n            values = _update_with_replacement(values, unpack_items)\n        else:\n            key = helpers.safe_infer(name, context=context)\n            value = helpers.safe_infer(value, context=context)\n            if any(not elem for elem in (key, value)):\n                raise InferenceError(node=node, context=context)\n            values = _update_with_replacement(values, {key: value})\n    return values",
        "file_path": "astroid/inference.py",
        "chunk_index": 5,
        "metadata": {
          "filepath": "astroid/inference.py",
          "function_name": "_infer_map",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 19,
          "start_line_no": 146,
          "end_line_no": 164
        }
      },
      {
        "rank": 8,
        "score": 0.648688793182373,
        "content": "def infer_dict(node, context=None):\n    \"\"\"Try to infer a dict call to a Dict node.\n\n    The function treats the following cases:\n\n        * dict()\n        * dict(mapping)\n        * dict(iterable)\n        * dict(iterable, **kwargs)\n        * dict(mapping, **kwargs)\n        * dict(**kwargs)\n\n    If a case can't be inferred, we'll fallback to default inference.\n    \"\"\"\n    call = arguments.CallSite.from_call(node, context=context)\n    if call.has_invalid_arguments() or call.has_invalid_keywords():\n        raise UseInferenceDefault\n\n    args = call.positional_arguments\n    kwargs = list(call.keyword_arguments.items())\n\n    if not args and not kwargs:\n        # dict()\n        return nodes.Dict()\n    if kwargs and not args:\n        # dict(a=1, b=2, c=4)\n        items = [(nodes.Const(key), value) for key, value in kwargs]\n    elif len(args) == 1 and kwargs:\n        # dict(some_iterable, b=2, c=4)\n        elts = _get_elts(args[0], context)\n        keys = [(nodes.Const(key), value) for key, value in kwargs]\n        items = elts + keys\n    elif len(args) == 1:\n        items = _get_elts(args[0], context)\n    else:\n        raise UseInferenceDefault()\n    value = nodes.Dict(\n        col_offset=node.col_offset, lineno=node.lineno, parent=node.parent\n    )\n    value.postinit(items)\n    return value",
        "file_path": "astroid/brain/brain_builtin_inference.py",
        "chunk_index": 10,
        "metadata": {
          "filepath": "astroid/brain/brain_builtin_inference.py",
          "function_name": "infer_dict",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 41,
          "start_line_no": 339,
          "end_line_no": 379
        }
      },
      {
        "rank": 9,
        "score": 0.6324405670166016,
        "content": "def infer_attribute(self, context=None):\n    \"\"\"infer an Attribute node by using getattr on the associated object\"\"\"\n    for owner in self.expr.infer(context):\n        if owner is util.Uninferable:\n            yield owner\n            continue\n\n        if not context:\n            context = InferenceContext()\n        else:\n            context = copy_context(context)\n\n        old_boundnode = context.boundnode\n        try:\n            context.boundnode = owner\n            yield from owner.igetattr(self.attrname, context)\n        except (\n            AttributeInferenceError,\n            InferenceError,\n            AttributeError,\n        ):\n            pass\n        finally:\n            context.boundnode = old_boundnode\n    return dict(node=self, context=context)",
        "file_path": "astroid/inference.py",
        "chunk_index": 11,
        "metadata": {
          "filepath": "astroid/inference.py",
          "function_name": "infer_attribute",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 25,
          "start_line_no": 295,
          "end_line_no": 319
        }
      },
      {
        "rank": 10,
        "score": 0.6305243372917175,
        "content": "def infer_dict_fromkeys(node, context=None):\n    \"\"\"Infer dict.fromkeys\n\n    :param nodes.Call node: dict.fromkeys() call to infer\n    :param context.InferenceContext context: node context\n    :rtype nodes.Dict:\n        a Dictionary containing the values that astroid was able to infer.\n        In case the inference failed for any reason, an empty dictionary\n        will be inferred instead.\n    \"\"\"\n\n    def _build_dict_with_elements(elements):\n        new_node = nodes.Dict(\n            col_offset=node.col_offset, lineno=node.lineno, parent=node.parent\n        )\n        new_node.postinit(elements)\n        return new_node\n\n    call = arguments.CallSite.from_call(node, context=context)\n    if call.keyword_arguments:\n        raise UseInferenceDefault(\"TypeError: int() must take no keyword arguments\")\n    if len(call.positional_arguments) not in {1, 2}:\n        raise UseInferenceDefault(\n            \"TypeError: Needs between 1 and 2 positional arguments\"\n        )\n\n    default = nodes.Const(None)\n    values = call.positional_arguments[0]\n    try:\n        inferred_values = next(values.infer(context=context))\n    except (InferenceError, StopIteration):\n        return _build_dict_with_elements([])\n    if inferred_values is util.Uninferable:\n        return _build_dict_with_elements([])\n\n    # Limit to a couple of potential values, as this can become pretty complicated\n    accepted_iterable_elements = (nodes.Const,)\n    if isinstance(inferred_values, (nodes.List, nodes.Set, nodes.Tuple)):\n        elements = inferred_values.elts\n        for element in elements:\n            if not isinstance(element, accepted_iterable_elements):\n                # Fallback to an empty dict\n                return _build_dict_with_elements([])\n\n        elements_with_value = [(element, default) for element in elements]\n        return _build_dict_with_elements(elements_with_value)\n    if isinstance(inferred_values, nodes.Const) and isinstance(\n        inferred_values.value, (str, bytes)\n    ):\n        elements = [\n            (nodes.Const(element), default) for element in inferred_values.value\n        ]\n        return _build_dict_with_elements(elements)\n    if isinstance(inferred_values, nodes.Dict):\n        keys = inferred_values.itered()\n        for key in keys:\n            if not isinstance(key, accepted_iterable_elements):\n                # Fallback to an empty dict\n                return _build_dict_with_elements([])\n\n        elements_with_value = [(element, default) for element in keys]\n        return _build_dict_with_elements(elements_with_value)\n\n    # Fallback to an empty dictionary\n    return _build_dict_with_elements([])",
        "file_path": "astroid/brain/brain_builtin_inference.py",
        "chunk_index": 28,
        "metadata": {
          "filepath": "astroid/brain/brain_builtin_inference.py",
          "function_name": "infer_dict_fromkeys",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 65,
          "start_line_no": 831,
          "end_line_no": 895
        }
      }
    ]
  },
  "pylint-dev__astroid-1866": {
    "query": "\"TypeError: unsupported format string passed to NoneType.__format__\" while running type inference in version 2.12.x\n### Steps to reproduce\r\n\r\nI have no concise reproducer. Exception happens every time I run pylint on some internal code, with astroid 2.12.10 and 2.12.12 (debian bookworm). It does _not_ happen with earlier versions of astroid (not with version 2.9). The pylinted code itself is \"valid\", it runs in production here.\r\n\r\n### Current behavior\r\n\r\nWhen running pylint on some code, I get this exception:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3/dist-packages/pylint/utils/ast_walker.py\", line 90, in walk\r\n    callback(astroid)\r\n  File \"/usr/lib/python3/dist-packages/pylint/checkers/classes/special_methods_checker.py\", line 183, in visit_functiondef\r\n    inferred = _safe_infer_call_result(node, node)\r\n  File \"/usr/lib/python3/dist-packages/pylint/checkers/classes/special_methods_checker.py\", line 42, in _safe_infer_call_result\r\n    value = next(inferit)\r\n  File \"/usr/lib/python3/dist-packages/astroid/nodes/scoped_nodes/scoped_nodes.py\", line 1749, in infer_call_result\r\n    yield from returnnode.value.infer(context)\r\n  File \"/usr/lib/python3/dist-packages/astroid/nodes/node_ng.py\", line 159, in infer\r\n    results = list(self._explicit_inference(self, context, **kwargs))\r\n  File \"/usr/lib/python3/dist-packages/astroid/inference_tip.py\", line 45, in _inference_tip_cached\r\n    result = _cache[func, node] = list(func(*args, **kwargs))\r\n  File \"/usr/lib/python3/dist-packages/astroid/brain/brain_builtin_inference.py\", line 956, in _infer_str_format_call\r\n    formatted_string = format_template.format(*pos_values, **keyword_values)\r\nTypeError: unsupported format string passed to NoneType.__format__\r\n```\r\n\r\n### Expected behavior\r\n\r\nTypeError exception should not happen\r\n\r\n### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\r\n\r\n2.12.10,\r\n2.12.12\n",
    "method": "function",
    "top_k": 10,
    "results": [
      {
        "rank": 1,
        "score": 0.6983519792556763,
        "content": "def _infer_str_format_call(\n    node: nodes.Call, context: InferenceContext | None = None\n) -> Iterator[nodes.Const | type[util.Uninferable]]:\n    \"\"\"Return a Const node based on the template and passed arguments.\"\"\"\n    call = arguments.CallSite.from_call(node, context=context)\n    if isinstance(node.func.expr, nodes.Name):\n        value: nodes.Const = helpers.safe_infer(node.func.expr)\n    else:\n        value = node.func.expr\n\n    format_template = value.value\n\n    # Get the positional arguments passed\n    inferred_positional = [\n        helpers.safe_infer(i, context) for i in call.positional_arguments\n    ]\n    if not all(isinstance(i, nodes.Const) for i in inferred_positional):\n        return iter([util.Uninferable])\n    pos_values: list[str] = [i.value for i in inferred_positional]\n\n    # Get the keyword arguments passed\n    inferred_keyword = {\n        k: helpers.safe_infer(v, context) for k, v in call.keyword_arguments.items()\n    }\n    if not all(isinstance(i, nodes.Const) for i in inferred_keyword.values()):\n        return iter([util.Uninferable])\n    keyword_values: dict[str, str] = {k: v.value for k, v in inferred_keyword.items()}\n\n    try:\n        formatted_string = format_template.format(*pos_values, **keyword_values)\n    except (IndexError, KeyError):\n        # If there is an IndexError there are too few arguments to interpolate\n        return iter([util.Uninferable])\n\n    return iter([nodes.const_factory(formatted_string)])",
        "file_path": "astroid/brain/brain_builtin_inference.py",
        "chunk_index": 32,
        "metadata": {
          "filepath": "astroid/brain/brain_builtin_inference.py",
          "function_name": "_infer_str_format_call",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 35,
          "start_line_no": 927,
          "end_line_no": 961
        }
      },
      {
        "rank": 2,
        "score": 0.69122713804245,
        "content": "def _infer_old_style_string_formatting(\n    instance: nodes.Const, other: nodes.NodeNG, context: InferenceContext\n) -> tuple[type[util.Uninferable] | nodes.Const]:\n    \"\"\"Infer the result of '\"string\" % ...'.\n\n    TODO: Instead of returning Uninferable we should rely\n    on the call to '%' to see if the result is actually uninferable.\n    \"\"\"\n    values = None\n    if isinstance(other, nodes.Tuple):\n        if util.Uninferable in other.elts:\n            return (util.Uninferable,)\n        inferred_positional = [helpers.safe_infer(i, context) for i in other.elts]\n        if all(isinstance(i, nodes.Const) for i in inferred_positional):\n            values = tuple(i.value for i in inferred_positional)\n    elif isinstance(other, nodes.Dict):\n        values: dict[Any, Any] = {}\n        for pair in other.items:\n            key = helpers.safe_infer(pair[0], context)\n            if not isinstance(key, nodes.Const):\n                return (util.Uninferable,)\n            value = helpers.safe_infer(pair[1], context)\n            if not isinstance(value, nodes.Const):\n                return (util.Uninferable,)\n            values[key.value] = value.value\n    elif isinstance(other, nodes.Const):\n        values = other.value\n    else:\n        return (util.Uninferable,)\n\n    try:\n        return (nodes.const_factory(instance.value % values),)\n    except (TypeError, KeyError, ValueError):\n        return (util.Uninferable,)",
        "file_path": "astroid/inference.py",
        "chunk_index": 19,
        "metadata": {
          "filepath": "astroid/inference.py",
          "function_name": "_infer_old_style_string_formatting",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 34,
          "start_line_no": 629,
          "end_line_no": 662
        }
      },
      {
        "rank": 3,
        "score": 0.6751132011413574,
        "content": "def _is_str_format_call(node: nodes.Call) -> bool:\n    \"\"\"Catch calls to str.format().\"\"\"\n    if not isinstance(node.func, nodes.Attribute) or not node.func.attrname == \"format\":\n        return False\n\n    if isinstance(node.func.expr, nodes.Name):\n        value = helpers.safe_infer(node.func.expr)\n    else:\n        value = node.func.expr\n\n    return isinstance(value, nodes.Const) and isinstance(value.value, str)",
        "file_path": "astroid/brain/brain_builtin_inference.py",
        "chunk_index": 31,
        "metadata": {
          "filepath": "astroid/brain/brain_builtin_inference.py",
          "function_name": "_is_str_format_call",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 11,
          "start_line_no": 914,
          "end_line_no": 924
        }
      },
      {
        "rank": 4,
        "score": 0.622888445854187,
        "content": "def infer_str(node, context=None):\n    \"\"\"Infer str() calls\n\n    :param nodes.Call node: str() call to infer\n    :param context.InferenceContext: node context\n    :rtype nodes.Const: a Const containing an empty string\n    \"\"\"\n    call = arguments.CallSite.from_call(node, context=context)\n    if call.keyword_arguments:\n        raise UseInferenceDefault(\"TypeError: str() must take no keyword arguments\")\n    try:\n        return nodes.Const(\"\")\n    except (AstroidTypeError, InferenceError) as exc:\n        raise UseInferenceDefault(str(exc)) from exc",
        "file_path": "astroid/brain/brain_builtin_inference.py",
        "chunk_index": 26,
        "metadata": {
          "filepath": "astroid/brain/brain_builtin_inference.py",
          "function_name": "infer_str",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 14,
          "start_line_no": 783,
          "end_line_no": 796
        }
      },
      {
        "rank": 5,
        "score": 0.6031763553619385,
        "content": "def infer_typing_typevar_or_newtype(node, context_itton=None):\n    \"\"\"Infer a typing.TypeVar(...) or typing.NewType(...) call\"\"\"\n    try:\n        func = next(node.func.infer(context=context_itton))\n    except (InferenceError, StopIteration) as exc:\n        raise UseInferenceDefault from exc\n\n    if func.qname() not in TYPING_TYPEVARS_QUALIFIED:\n        raise UseInferenceDefault\n    if not node.args:\n        raise UseInferenceDefault\n    # Cannot infer from a dynamic class name (f-string)\n    if isinstance(node.args[0], JoinedStr):\n        raise UseInferenceDefault\n\n    typename = node.args[0].as_string().strip(\"'\")\n    node = extract_node(TYPING_TYPE_TEMPLATE.format(typename))\n    return node.infer(context=context_itton)",
        "file_path": "astroid/brain/brain_typing.py",
        "chunk_index": 1,
        "metadata": {
          "filepath": "astroid/brain/brain_typing.py",
          "function_name": "infer_typing_typevar_or_newtype",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 18,
          "start_line_no": 115,
          "end_line_no": 132
        }
      },
      {
        "rank": 6,
        "score": 0.5932272672653198,
        "content": "def infer_typing_cast(\n    node: Call, ctx: context.InferenceContext | None = None\n) -> Iterator[NodeNG]:\n    \"\"\"Infer call to cast() returning same type as casted-from var\"\"\"\n    if not isinstance(node.func, (Name, Attribute)):\n        raise UseInferenceDefault\n\n    try:\n        func = next(node.func.infer(context=ctx))\n    except (InferenceError, StopIteration) as exc:\n        raise UseInferenceDefault from exc\n    if (\n        not isinstance(func, FunctionDef)\n        or func.qname() != \"typing.cast\"\n        or len(node.args) != 2\n    ):\n        raise UseInferenceDefault\n\n    return node.args[1].infer(context=ctx)",
        "file_path": "astroid/brain/brain_typing.py",
        "chunk_index": 14,
        "metadata": {
          "filepath": "astroid/brain/brain_typing.py",
          "function_name": "infer_typing_cast",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 19,
          "start_line_no": 384,
          "end_line_no": 402
        }
      },
      {
        "rank": 7,
        "score": 0.5903385877609253,
        "content": "def visit_uninferable(self, node):\n        return str(node)",
        "file_path": "astroid/nodes/as_string.py",
        "chunk_index": 86,
        "metadata": {
          "filepath": "astroid/nodes/as_string.py",
          "function_name": "visit_uninferable",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 2,
          "start_line_no": 628,
          "end_line_no": 629
        }
      },
      {
        "rank": 8,
        "score": 0.5867988467216492,
        "content": "def infer_type(node, context=None):\n    \"\"\"Understand the one-argument form of *type*.\"\"\"\n    if len(node.args) != 1:\n        raise UseInferenceDefault\n\n    return helpers.object_type(node.args[0], context)",
        "file_path": "astroid/brain/brain_builtin_inference.py",
        "chunk_index": 18,
        "metadata": {
          "filepath": "astroid/brain/brain_builtin_inference.py",
          "function_name": "infer_type",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 6,
          "start_line_no": 597,
          "end_line_no": 602
        }
      },
      {
        "rank": 9,
        "score": 0.5838453769683838,
        "content": "def type_errors(self, context=None):\n        \"\"\"Get a list of type errors which can occur during inference.\n\n        Each TypeError is represented by a :class:`BadBinaryOperationMessage` ,\n        which holds the original exception.\n\n        :returns: The list of possible type errors.\n        :rtype: list(BadBinaryOperationMessage)\n        \"\"\"\n        try:\n            results = self._infer_augassign(context=context)\n            return [\n                result\n                for result in results\n                if isinstance(result, util.BadBinaryOperationMessage)\n            ]\n        except InferenceError:\n            return []",
        "file_path": "astroid/nodes/node_classes.py",
        "chunk_index": 48,
        "metadata": {
          "filepath": "astroid/nodes/node_classes.py",
          "function_name": "type_errors",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 18,
          "start_line_no": 1442,
          "end_line_no": 1459
        }
      },
      {
        "rank": 10,
        "score": 0.571911633014679,
        "content": "def safe_infer(\n    node: nodes.NodeNG | bases.Proxy, context: InferenceContext | None = None\n) -> InferenceResult | None:\n    \"\"\"Return the inferred value for the given node.\n\n    Return None if inference failed or if there is some ambiguity (more than\n    one node has been inferred).\n    \"\"\"\n    try:\n        inferit = node.infer(context=context)\n        value = next(inferit)\n    except (InferenceError, StopIteration):\n        return None\n    try:\n        next(inferit)\n        return None  # None if there is ambiguity on the inferred node\n    except InferenceError:\n        return None  # there is some kind of ambiguity\n    except StopIteration:\n        return value",
        "file_path": "astroid/helpers.py",
        "chunk_index": 7,
        "metadata": {
          "filepath": "astroid/helpers.py",
          "function_name": "safe_infer",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 20,
          "start_line_no": 155,
          "end_line_no": 174
        }
      }
    ]
  },
  "pylint-dev__astroid-1268": {
    "query": "'AsStringVisitor' object has no attribute 'visit_unknown'\n```python\r\n>>> import astroid\r\n>>> astroid.nodes.Unknown().as_string()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/tusharsadhwani/code/marvin-python/venv/lib/python3.9/site-packages/astroid/nodes/node_ng.py\", line 609, in as_string\r\n    return AsStringVisitor()(self)\r\n  File \"/Users/tusharsadhwani/code/marvin-python/venv/lib/python3.9/site-packages/astroid/nodes/as_string.py\", line 56, in __call__\r\n    return node.accept(self).replace(DOC_NEWLINE, \"\\n\")\r\n  File \"/Users/tusharsadhwani/code/marvin-python/venv/lib/python3.9/site-packages/astroid/nodes/node_ng.py\", line 220, in accept\r\n    func = getattr(visitor, \"visit_\" + self.__class__.__name__.lower())\r\nAttributeError: 'AsStringVisitor' object has no attribute 'visit_unknown'\r\n>>> \r\n```\r\n### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\r\n\r\n2.8.6-dev0\n",
    "method": "function",
    "top_k": 10,
    "results": [
      {
        "rank": 1,
        "score": 0.6093612313270569,
        "content": " visit_assignattr(self, node):\n        \"\"\"return an astroid.AssAttr node as string\"\"\"\n        return self.visit_attribute(node)\n\n ",
        "file_path": "astroid/nodes/as_string.py",
        "chunk_index": 10,
        "metadata": {
          "filepath": "astroid/nodes/as_string.py",
          "function_name": "visit_assignattr",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 5,
          "start_line_no": 114,
          "end_line_no": 116
        }
      },
      {
        "rank": 2,
        "score": 0.5998989939689636,
        "content": "ef ast_from_string(self, data, modname=\"\", filepath=None):\n        \"\"\"Given some source code as a string, return its corresponding astroid object\"\"\"\n        # pylint: disable=import-outside-toplevel; circular import\n        from astroid.builder import AstroidBuilder\n\n        return AstroidBuilder(self).string_build(data, modname, filepath)\n",
        "file_path": "astroid/manager.py",
        "chunk_index": 7,
        "metadata": {
          "filepath": "astroid/manager.py",
          "function_name": "ast_from_string",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 6,
          "start_line_no": 129,
          "end_line_no": 134
        }
      },
      {
        "rank": 3,
        "score": 0.5821908712387085,
        "content": "ef ast_from_module(self, module: types.ModuleType, modname: str = None):\n        \"\"\"given an imported module, return the astroid object\"\"\"\n        modname = modname or module.__name__\n        if modname in self.astroid_cache:\n            return self.astroid_cache[modname]\n        try:\n            # some builtin modules don't have __file__ attribute\n            filepath = module.__file__\n            if is_python_source(filepath):\n                return self.ast_from_file(filepath, modname)\n        except AttributeError:\n            pass\n\n        # pylint: disable=import-outside-toplevel; circular import\n        from astroid.builder import AstroidBuilder\n\n        return AstroidBuilder(self).module_build(module, modname)\n",
        "file_path": "astroid/manager.py",
        "chunk_index": 14,
        "metadata": {
          "filepath": "astroid/manager.py",
          "function_name": "ast_from_module",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 17,
          "start_line_no": 271,
          "end_line_no": 287
        }
      },
      {
        "rank": 4,
        "score": 0.575061559677124,
        "content": " visit_dict(self, node):\n        \"\"\"return an astroid.Dict node as string\"\"\"\n        return \"{%s}\" % \", \".join(self._visit_dict(node))\n\n ",
        "file_path": "astroid/nodes/as_string.py",
        "chunk_index": 29,
        "metadata": {
          "filepath": "astroid/nodes/as_string.py",
          "function_name": "visit_dict",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 5,
          "start_line_no": 229,
          "end_line_no": 231
        }
      },
      {
        "rank": 5,
        "score": 0.5723768472671509,
        "content": "ef infer_ast_from_something(self, obj, context=None):\n        \"\"\"infer astroid for the given class\"\"\"\n        if hasattr(obj, \"__class__\") and not isinstance(obj, type):\n            klass = obj.__class__\n        else:\n            klass = obj\n        try:\n            modname = klass.__module__\n        except AttributeError as exc:\n            raise AstroidBuildingError(\n                \"Unable to get module for {class_repr}.\",\n                cls=klass,\n                class_repr=safe_repr(klass),\n            ) from exc\n        except Exception as exc:\n            raise AstroidImportError(\n                \"Unexpected error while retrieving module for {class_repr}:\\n\"\n                \"{error}\",\n                cls=klass,\n                class_repr=safe_repr(klass),\n            ) from exc\n        try:\n            name = klass.__name__\n        except AttributeError as exc:\n            raise AstroidBuildingError(\n                \"Unable to get name for {class_repr}:\\n\",\n                cls=klass,\n                class_repr=safe_repr(klass),\n            ) from exc\n        except Exception as exc:\n            raise AstroidImportError(\n                \"Unexpected error while retrieving name for {class_repr}:\\n\" \"{error}\",\n                cls=klass,\n                class_repr=safe_repr(klass),\n            ) from exc\n        # take care, on living object __module__ is regularly wrong :(\n        modastroid = self.ast_from_module_name(modname)\n        if klass is obj:\n            for inferred in modastroid.igetattr(name, context):\n                yield inferred\n        else:\n            for inferred in modastroid.igetattr(name, context):\n                yield inferred.instantiate_class()\n",
        "file_path": "astroid/manager.py",
        "chunk_index": 16,
        "metadata": {
          "filepath": "astroid/manager.py",
          "function_name": "infer_ast_from_something",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 43,
          "start_line_no": 304,
          "end_line_no": 346
        }
      },
      {
        "rank": 6,
        "score": 0.5713139772415161,
        "content": "def visit(self, module):\n        \"\"\"Walk the given astroid *tree* and transform each encountered node\n\n        Only the nodes which have transforms registered will actually\n        be replaced or changed.\n        \"\"\"\n        return self._visit(module)",
        "file_path": "astroid/transforms.py",
        "chunk_index": 6,
        "metadata": {
          "filepath": "astroid/transforms.py",
          "function_name": "visit",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 7,
          "start_line_no": 90,
          "end_line_no": 96
        }
      },
      {
        "rank": 7,
        "score": 0.5682752132415771,
        "content": "ef ast_from_file(self, filepath, modname=None, fallback=True, source=False):\n        \"\"\"given a module name, return the astroid object\"\"\"\n        try:\n            filepath = get_source_file(filepath, include_no_ext=True)\n            source = True\n        except NoSourceFile:\n            pass\n        if modname is None:\n            try:\n                modname = \".\".join(modpath_from_file(filepath))\n            except ImportError:\n                modname = filepath\n        if (\n            modname in self.astroid_cache\n            and self.astroid_cache[modname].file == filepath\n        ):\n            return self.astroid_cache[modname]\n        if source:\n            # pylint: disable=import-outside-toplevel; circular import\n            from astroid.builder import AstroidBuilder\n\n            return AstroidBuilder(self).file_build(filepath, modname)\n        if fallback and modname:\n            return self.ast_from_module_name(modname)\n        raise AstroidBuildingError(\"Unable to build an AST for {path}.\", path=filepath)\n",
        "file_path": "astroid/manager.py",
        "chunk_index": 6,
        "metadata": {
          "filepath": "astroid/manager.py",
          "function_name": "ast_from_file",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 25,
          "start_line_no": 103,
          "end_line_no": 127
        }
      },
      {
        "rank": 8,
        "score": 0.5674736499786377,
        "content": " visit_functiondef(self, node):\n        \"\"\"return an astroid.FunctionDef node as string\"\"\"\n        return self.handle_functiondef(node, \"def\")\n\n ",
        "file_path": "astroid/nodes/as_string.py",
        "chunk_index": 42,
        "metadata": {
          "filepath": "astroid/nodes/as_string.py",
          "function_name": "visit_functiondef",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 5,
          "start_line_no": 345,
          "end_line_no": 347
        }
      },
      {
        "rank": 9,
        "score": 0.5659883618354797,
        "content": " visit_expr(self, node):\n        \"\"\"return an astroid.Discard node as string\"\"\"\n        return node.value.accept(self)\n\n ",
        "file_path": "astroid/nodes/as_string.py",
        "chunk_index": 33,
        "metadata": {
          "filepath": "astroid/nodes/as_string.py",
          "function_name": "visit_expr",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 5,
          "start_line_no": 254,
          "end_line_no": 256
        }
      },
      {
        "rank": 10,
        "score": 0.5613567233085632,
        "content": " visit_name(self, node):\n        \"\"\"return an astroid.Name node as string\"\"\"\n        return node.name\n\n ",
        "file_path": "astroid/nodes/as_string.py",
        "chunk_index": 55,
        "metadata": {
          "filepath": "astroid/nodes/as_string.py",
          "function_name": "visit_name",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 5,
          "start_line_no": 421,
          "end_line_no": 423
        }
      }
    ]
  },
  "pyvista__pyvista-4315": {
    "query": "Rectilinear grid does not allow Sequences as inputs\n### Describe the bug, what's wrong, and what you expected.\r\n\r\nRectilinear grid gives an error when `Sequence`s are passed in, but `ndarray` are ok.\r\n\r\n### Steps to reproduce the bug.\r\n\r\nThis doesn't work\r\n```python\r\nimport pyvista as pv\r\npv.RectilinearGrid([0, 1], [0, 1], [0, 1])\r\n```\r\n\r\nThis works\r\n```py\r\nimport pyvista as pv\r\nimport numpy as np\r\npv.RectilinearGrid(np.ndarray([0, 1]), np.ndarray([0, 1]), np.ndarray([0, 1]))\r\n```\r\n### System Information\r\n\r\n```shell\r\n--------------------------------------------------------------------------------\r\n  Date: Wed Apr 19 20:15:10 2023 UTC\r\n\r\n                OS : Linux\r\n            CPU(s) : 2\r\n           Machine : x86_64\r\n      Architecture : 64bit\r\n       Environment : IPython\r\n        GPU Vendor : Mesa/X.org\r\n      GPU Renderer : llvmpipe (LLVM 11.0.1, 256 bits)\r\n       GPU Version : 4.5 (Core Profile) Mesa 20.3.5\r\n\r\n  Python 3.11.2 (main, Mar 23 2023, 17:12:29) [GCC 10.2.1 20210110]\r\n\r\n           pyvista : 0.38.5\r\n               vtk : 9.2.6\r\n             numpy : 1.24.2\r\n           imageio : 2.27.0\r\n            scooby : 0.7.1\r\n             pooch : v1.7.0\r\n        matplotlib : 3.7.1\r\n           IPython : 8.12.0\r\n--------------------------------------------------------------------------------\r\n```\r\n\r\n\r\n### Screenshots\r\n\r\n_No response_\n",
    "method": "function",
    "top_k": 10,
    "results": [
      {
        "rank": 1,
        "score": 0.6902864575386047,
        "content": "def __init__(self, *args, check_duplicates=False, deep=False, **kwargs):\n        \"\"\"Initialize the rectilinear grid.\"\"\"\n        super().__init__()\n\n        if len(args) == 1:\n            if isinstance(args[0], _vtk.vtkRectilinearGrid):\n                if deep:\n                    self.deep_copy(args[0])\n                else:\n                    self.shallow_copy(args[0])\n            elif isinstance(args[0], (str, pathlib.Path)):\n                self._from_file(args[0], **kwargs)\n            elif isinstance(args[0], np.ndarray):\n                self._from_arrays(args[0], None, None, check_duplicates)\n            else:\n                raise TypeError(f'Type ({type(args[0])}) not understood by `RectilinearGrid`')\n\n        elif len(args) == 3 or len(args) == 2:\n            arg0_is_arr = isinstance(args[0], np.ndarray)\n            arg1_is_arr = isinstance(args[1], np.ndarray)\n            if len(args) == 3:\n                arg2_is_arr = isinstance(args[2], np.ndarray)\n            else:\n                arg2_is_arr = False\n\n            if all([arg0_is_arr, arg1_is_arr, arg2_is_arr]):\n                self._from_arrays(args[0], args[1], args[2], check_duplicates)\n            elif all([arg0_is_arr, arg1_is_arr]):\n                self._from_arrays(args[0], args[1], None, check_duplicates)\n            else:\n                raise TypeError(\"Arguments not understood by `RectilinearGrid`.\")",
        "file_path": "pyvista/core/grid.py",
        "chunk_index": 4,
        "metadata": {
          "filepath": "pyvista/core/grid.py",
          "function_name": "__init__",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 31,
          "start_line_no": 126,
          "end_line_no": 156
        }
      },
      {
        "rank": 2,
        "score": 0.6816917061805725,
        "content": "def _from_arrays(self, x, y, z, force_float=True):\n        \"\"\"Create VTK structured grid directly from numpy arrays.\n\n        Parameters\n        ----------\n        x : numpy.ndarray\n            Position of the points in x direction.\n\n        y : numpy.ndarray\n            Position of the points in y direction.\n\n        z : numpy.ndarray\n            Position of the points in z direction.\n\n        force_float : bool, optional\n            Casts the datatype to ``float32`` if points datatype is\n            non-float.  Default ``True``. Set this to ``False`` to allow\n            non-float types, though this may lead to truncation of\n            intermediate floats when transforming datasets.\n\n        \"\"\"\n        if not (x.shape == y.shape == z.shape):\n            raise ValueError('Input point array shapes must match exactly')\n\n        # make the output points the same precision as the input arrays\n        points = np.empty((x.size, 3), x.dtype)\n        points[:, 0] = x.ravel('F')\n        points[:, 1] = y.ravel('F')\n        points[:, 2] = z.ravel('F')\n\n        # ensure that the inputs are 3D\n        dim = list(x.shape)\n        while len(dim) < 3:\n            dim.append(1)\n\n        # Create structured grid\n        self.SetDimensions(dim)\n        self.SetPoints(pyvista.vtk_points(points, force_float=force_float))",
        "file_path": "pyvista/core/pointset.py",
        "chunk_index": 85,
        "metadata": {
          "filepath": "pyvista/core/pointset.py",
          "function_name": "_from_arrays",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 38,
          "start_line_no": 2008,
          "end_line_no": 2045
        }
      },
      {
        "rank": 3,
        "score": 0.6751937866210938,
        "content": "def cast_to_structured_grid(self) -> 'pyvista.StructuredGrid':\n        \"\"\"Cast this rectilinear grid to a structured grid.\n\n        Returns\n        -------\n        pyvista.StructuredGrid\n            This grid as a structured grid.\n\n        \"\"\"\n        alg = _vtk.vtkRectilinearGridToPointSet()\n        alg.SetInputData(self)\n        alg.Update()\n        return _get_output(alg)",
        "file_path": "pyvista/core/grid.py",
        "chunk_index": 19,
        "metadata": {
          "filepath": "pyvista/core/grid.py",
          "function_name": "cast_to_structured_grid",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 13,
          "start_line_no": 381,
          "end_line_no": 393
        }
      },
      {
        "rank": 4,
        "score": 0.6711309552192688,
        "content": "def load_rectilinear():\n    \"\"\"Load a sample uniform grid.\n\n    Returns\n    -------\n    pyvista.RectilinearGrid\n        Dataset.\n\n    Examples\n    --------\n    >>> from pyvista import examples\n    >>> dataset = examples.load_rectilinear()\n    >>> dataset.plot()\n\n    \"\"\"\n    return pyvista.RectilinearGrid(rectfile)",
        "file_path": "pyvista/examples/examples.py",
        "chunk_index": 4,
        "metadata": {
          "filepath": "pyvista/examples/examples.py",
          "function_name": "load_rectilinear",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 16,
          "start_line_no": 102,
          "end_line_no": 117
        }
      },
      {
        "rank": 5,
        "score": 0.6596612930297852,
        "content": "def cast_to_rectilinear_grid(self) -> 'RectilinearGrid':\n        \"\"\"Cast this uniform grid to a rectilinear grid.\n\n        Returns\n        -------\n        pyvista.RectilinearGrid\n            This uniform grid as a rectilinear grid.\n\n        \"\"\"\n\n        def gen_coords(i):\n            coords = (\n                np.cumsum(np.insert(np.full(self.dimensions[i] - 1, self.spacing[i]), 0, 0))\n                + self.origin[i]\n            )\n            return coords\n\n        xcoords = gen_coords(0)\n        ycoords = gen_coords(1)\n        zcoords = gen_coords(2)\n        grid = pyvista.RectilinearGrid(xcoords, ycoords, zcoords)\n        grid.point_data.update(self.point_data)\n        grid.cell_data.update(self.cell_data)\n        grid.field_data.update(self.field_data)\n        grid.copy_meta_from(self, deep=True)\n        return grid",
        "file_path": "pyvista/core/grid.py",
        "chunk_index": 35,
        "metadata": {
          "filepath": "pyvista/core/grid.py",
          "function_name": "cast_to_rectilinear_grid",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 26,
          "start_line_no": 779,
          "end_line_no": 804
        }
      },
      {
        "rank": 6,
        "score": 0.6535745859146118,
        "content": "def _from_arrays(\n        self,\n        offset,\n        cells,\n        cell_type,\n        points,\n        deep=True,\n        force_float=True,\n    ):\n        \"\"\"Create VTK unstructured grid from numpy arrays.\n\n        Parameters\n        ----------\n        offset : any, default None\n            Ignored (this is a pre-VTK9 legacy).\n\n        cells : sequence[int]\n            Array of cells.  Each cell contains the number of points in the\n            cell and the node numbers of the cell.\n\n        cell_type : sequence[int]\n            Cell types of each cell.  Each cell type numbers can be found from\n            vtk documentation.  More efficient if using ``np.uint8``. See\n            example below.\n\n        points : sequence[float]\n            Numpy array containing point locations.\n\n        deep : bool, default: True\n            When ``True``, makes a copy of the points array.  Default\n            ``False``.  Cells and cell types are always copied.\n\n        force_float : bool, default: True\n            Casts the datatype to ``float32`` if points datatype is\n            non-float.  Set this to ``False`` to allow non-float types,\n            though this may lead to truncation of intermediate floats when\n            transforming datasets.\n\n        Examples\n        --------\n        >>> import numpy as np\n        >>> from pyvista import CellType\n        >>> import pyvista\n        >>> cell0_ids = [8, 0, 1, 2, 3, 4, 5, 6, 7]\n        >>> cell1_ids = [8, 8, 9, 10, 11, 12, 13, 14, 15]\n        >>> cells = np.hstack((cell0_ids, cell1_ids))\n        >>> cell_type = np.array(\n        ...     [CellType.HEXAHEDRON, CellType.HEXAHEDRON], np.int8\n        ... )\n\n        >>> cell1 = np.array(\n        ...     [\n        ...         [0, 0, 0],\n        ...         [1, 0, 0],\n        ...         [1, 1, 0],\n        ...         [0, 1, 0],\n        ...         [0, 0, 1],\n        ...         [1, 0, 1],\n        ...         [1, 1, 1],\n        ...         [0, 1, 1],\n        ...     ],\n        ...     dtype=np.float32,\n        ... )\n\n        >>> cell2 = np.array(\n        ...     [\n        ...         [0, 0, 2],\n        ...         [1, 0, 2],\n        ...         [1, 1, 2],\n        ...         [0, 1, 2],\n        ...         [0, 0, 3],\n        ...         [1, 0, 3],\n        ...         [1, 1, 3],\n        ...         [0, 1, 3],\n        ...     ],\n        ...     dtype=np.float32,\n        ... )\n\n        >>> points = np.vstack((cell1, cell2))\n\n        >>> grid = pyvista.UnstructuredGrid(cells, cell_type, points)\n\n        \"\"\"\n        if offset is not None:\n            warnings.warn('VTK 9 no longer accepts an offset array', stacklevel=3)\n        # convert to arrays upfront\n        cells = np.asarray(cells)\n        cell_type = np.asarray(cell_type)\n        points = np.asarray(points)\n\n        # Convert to vtk arrays\n        vtkcells = CellArray(cells, cell_type.size, deep)\n        if cell_type.dtype != np.uint8:\n            cell_type = cell_type.astype(np.uint8)\n        cell_type = _vtk.numpy_to_vtk(cell_type, deep=deep)\n\n        points = pyvista.vtk_points(points, deep, force_float)\n        self.SetPoints(points)\n\n        self.SetCells(cell_type, vtkcells)",
        "file_path": "pyvista/core/pointset.py",
        "chunk_index": 73,
        "metadata": {
          "filepath": "pyvista/core/pointset.py",
          "function_name": "_from_arrays",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 100,
          "start_line_no": 1457,
          "end_line_no": 1556
        }
      },
      {
        "rank": 7,
        "score": 0.6473239064216614,
        "content": "def _from_arrays(\n        self, x: np.ndarray, y: np.ndarray, z: np.ndarray, check_duplicates: bool = False\n    ):\n        \"\"\"Create VTK rectilinear grid directly from numpy arrays.\n\n        Each array gives the uniques coordinates of the mesh along each axial\n        direction. To help ensure you are using this correctly, we take the unique\n        values of each argument.\n\n        Parameters\n        ----------\n        x : numpy.ndarray\n            Coordinates of the points in x direction.\n\n        y : numpy.ndarray\n            Coordinates of the points in y direction.\n\n        z : numpy.ndarray\n            Coordinates of the points in z direction.\n\n        check_duplicates : bool, optional\n            Check for duplications in any arrays that are passed.\n\n        \"\"\"\n        # Set the coordinates along each axial direction\n        # Must at least be an x array\n        if check_duplicates:\n            raise_has_duplicates(x)\n\n        # edges are shown as triangles if x is not floating point\n        if not np.issubdtype(x.dtype, np.floating):\n            x = x.astype(float)\n        self.SetXCoordinates(helpers.convert_array(x.ravel()))\n        if y is not None:\n            if check_duplicates:\n                raise_has_duplicates(y)\n            if not np.issubdtype(y.dtype, np.floating):\n                y = y.astype(float)\n            self.SetYCoordinates(helpers.convert_array(y.ravel()))\n        if z is not None:\n            if check_duplicates:\n                raise_has_duplicates(z)\n            if not np.issubdtype(z.dtype, np.floating):\n                z = z.astype(float)\n            self.SetZCoordinates(helpers.convert_array(z.ravel()))\n        # Ensure dimensions are properly set\n        self._update_dimensions()",
        "file_path": "pyvista/core/grid.py",
        "chunk_index": 8,
        "metadata": {
          "filepath": "pyvista/core/grid.py",
          "function_name": "_from_arrays",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 47,
          "start_line_no": 170,
          "end_line_no": 216
        }
      },
      {
        "rank": 8,
        "score": 0.6456307172775269,
        "content": "def _from_arrays(self, dims: Sequence, corners: Sequence) -> None:\n        \"\"\"Create a VTK explicit structured grid from NumPy arrays.\n\n        Parameters\n        ----------\n        dims : sequence[int]\n            A sequence of integers with shape (3,) containing the\n            topological dimensions of the grid.\n\n        corners : array_like[floats]\n            A sequence of floats with shape (number of corners, 3)\n            containing the coordinates of the corner points.\n\n        \"\"\"\n        shape0 = np.asanyarray(dims) - 1\n        shape1 = 2 * shape0\n        ncells = np.prod(shape0)\n        cells = 8 * np.ones((ncells, 9), dtype=int)\n        points, indices = np.unique(corners, axis=0, return_inverse=True)\n        connectivity = np.asarray(\n            [[0, 1, 1, 0, 0, 1, 1, 0], [0, 0, 1, 1, 0, 0, 1, 1], [0, 0, 0, 0, 1, 1, 1, 1]]\n        )\n        for c in range(ncells):\n            i, j, k = np.unravel_index(c, shape0, order='F')\n            coord = (2 * i + connectivity[0], 2 * j + connectivity[1], 2 * k + connectivity[2])\n            cinds = np.ravel_multi_index(coord, shape1, order='F')  # type: ignore\n            cells[c, 1:] = indices[cinds]\n        cells = cells.flatten()\n        points = pyvista.vtk_points(points)\n        cells = CellArray(cells, ncells)\n        self.SetDimensions(dims)\n        self.SetPoints(points)\n        self.SetCells(cells)",
        "file_path": "pyvista/core/pointset.py",
        "chunk_index": 101,
        "metadata": {
          "filepath": "pyvista/core/pointset.py",
          "function_name": "_from_arrays",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 33,
          "start_line_no": 2347,
          "end_line_no": 2379
        }
      },
      {
        "rank": 9,
        "score": 0.6372283101081848,
        "content": "def cast_to_structured_grid(self) -> 'pyvista.StructuredGrid':\n        \"\"\"Cast this uniform grid to a structured grid.\n\n        Returns\n        -------\n        pyvista.StructuredGrid\n            This grid as a structured grid.\n\n        \"\"\"\n        alg = _vtk.vtkImageToStructuredGrid()\n        alg.SetInputData(self)\n        alg.Update()\n        return _get_output(alg)",
        "file_path": "pyvista/core/grid.py",
        "chunk_index": 34,
        "metadata": {
          "filepath": "pyvista/core/grid.py",
          "function_name": "cast_to_structured_grid",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 13,
          "start_line_no": 765,
          "end_line_no": 777
        }
      },
      {
        "rank": 10,
        "score": 0.6367930769920349,
        "content": "def wrap_image_array(arr):\n    \"\"\"Wrap a numpy array as a pyvista.UniformGrid.\n\n    Parameters\n    ----------\n    arr : np.ndarray\n        A ``np.uint8`` ``(X, Y, (3 or 4)`` array.  For example\n        ``(768, 1024, 3)``.\n\n    \"\"\"\n    if arr.ndim != 3:\n        raise ValueError('Expecting a X by Y by (3 or 4) array')\n    if arr.shape[2] not in [3, 4]:\n        raise ValueError('Expecting a X by Y by (3 or 4) array')\n    if arr.dtype != np.uint8:\n        raise ValueError('Expecting a np.uint8 array')\n\n    img = _vtk.vtkImageData()\n    img.SetDimensions(arr.shape[1], arr.shape[0], 1)\n    wrap_img = pyvista.wrap(img)\n    wrap_img.point_data['PNGImage'] = arr[::-1].reshape(-1, arr.shape[2])\n    return wrap_img",
        "file_path": "pyvista/utilities/regression.py",
        "chunk_index": 1,
        "metadata": {
          "filepath": "pyvista/utilities/regression.py",
          "function_name": "wrap_image_array",
          "type": "function_definition",
          "chunk_size": 500,
          "line_count": 22,
          "start_line_no": 17,
          "end_line_no": 38
        }
      }
    ]
  }
}