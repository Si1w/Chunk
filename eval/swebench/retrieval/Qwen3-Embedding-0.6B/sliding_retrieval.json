{
  "sqlfluff__sqlfluff-1625": {
    "query": "TSQL - L031 incorrectly triggers \"Avoid using aliases in join condition\" when no join present\n## Expected Behaviour\r\n\r\nBoth of these queries should pass, the only difference is the addition of a table alias 'a':\r\n\r\n1/ no alias\r\n\r\n```\r\nSELECT [hello]\r\nFROM\r\n    mytable\r\n```\r\n\r\n2/ same query with alias\r\n\r\n```\r\nSELECT a.[hello]\r\nFROM\r\n    mytable AS a\r\n```\r\n\r\n## Observed Behaviour\r\n\r\n1/ passes\r\n2/ fails with: L031: Avoid using aliases in join condition.\r\n\r\nBut there is no join condition :-)\r\n\r\n## Steps to Reproduce\r\n\r\nLint queries above\r\n\r\n## Dialect\r\n\r\nTSQL\r\n\r\n## Version\r\n\r\nsqlfluff 0.6.9\r\nPython 3.6.9\r\n\r\n## Configuration\r\n\r\nN/A\n",
    "method": "sliding",
    "top_k": 10,
    "results": [
      {
        "rank": 1,
        "score": 0.7667140364646912,
        "content": "\"\"\"Implementation of Rule L031.\"\"\"\nfrom collections import Counter, defaultdict\nfrom typing import Generator, NamedTuple\nfrom sqlfluff.core.parser import BaseSegment\nfrom sqlfluff.core.rules.base import BaseRule, LintFix, LintResult\nfrom sqlfluff.core.rules.doc_decorators import document_fix_compatible\n@document_fix_compatible\nclass Rule_L031(BaseRule):\n    \"\"\"Avoid table aliases in from clauses and join conditions.\n    | **Anti-pattern**\n    | In this example, alias 'o' is used for the orders table, and 'c' is used for 'customers' table.\n    .. code-block:: sql\n        SELECT\n            COUNT(o.customer_id) as order_amount,\n            c.name\n        FROM orders as o\n        JOIN customers as c on o.id = c.user_id\n    | **Best practice**\n    |  Avoid aliases.\n    .. code-block:: sql\n        SELECT\n            COUNT(orders.customer_id) as order_amount,\n            customers.name\n        FROM orders\n        JOIN customers on orders.id = customers.user_id\n        -- Self-join will not raise issue\n        SELECT\n            table.a,\n            table_alias.b,\n        FROM\n            table\n            LEFT JOIN table AS table_alias ON table.foreign_key = table_alias.foreign_key\n    \"\"\"\n    def _eval(self, segment, **kwargs):\n        \"\"\"Identify aliases in from clause and join conditions.\n        Find base table, table expressions in join, and other expressions in select clause\n        and decide if it's needed to report them.\n        \"\"\"\n        if segment.is_type(\"select_statement\"):\n            # A buffer for all table expressions in join conditions\n            from_expression_elements = []\n            column_reference_segments = []\n            from_clause_segment = segment.get_child(\"from_clause\")\n            if not from_clause_segment:\n                return None\n            from_expression = from_clause_segment.get_child(\"from_expression\")\n            from_expression_element = None\n            if from_expression:\n                from_expression_element = from_expression.get_child(\n                    \"from_expression_element\"\n                )\n            if not from_expression_element:\n                return None\n            from_expression_element = from_expression_element.get_child(\n                \"table_expression\"\n            )\n            # Find base table\n            base_table = None\n            if from_expression_element:\n                base_table = from_expression_element.get_child(\"object_reference\")\n            from_clause_index = segment.segments.index(from_clause_segment)\n            from_clause_and_after = segment.segments[from_clause_index:]\n            for clause in from_clause_and_after:\n                for from_expression_element in clause.recursive_crawl(\n                    \"from_expression_element\"\n                ):\n                    from_expression_elements.append(from_expression_element)\n                for column_reference in clause.recursive_crawl(\"column_reference\"):\n                    column_reference_segments.append(column_reference)\n            return (\n                self._lint_aliases_in_join(\n                    base_table,\n                    from_expression_elements,\n                    column_reference_segments,\n                    segment,\n                )\n                or None\n            )\n        return None\n    class TableAliasInfo(NamedTuple):\n        \"\"\"Structure yielded by_filter_table_expressions().\"\"\"\n        table_ref: BaseSegment\n        whitespace_ref: BaseSegment\n        alias_exp_ref: BaseSegment\n        alias_identifier_ref: BaseSegment\n    @classmethod\n    def _filter_table_expressions(\n        cls, base_table, from_expression_elements\n    ) -> Generator[TableAliasInfo, None, None]:\n        for from_expression in from_expression_elements:\n            table_expression = from_expression.get_child(\"table_expression\")\n            if not table_expression:\n                continue\n            table_ref = table_expression.get_child(\"object_reference\")\n            # If the from_expression_element has no object_references - skip it\n            # An example case is a lateral flatten, where we have a function segment\n            # instead of a table_reference segment.\n            if not table_ref:\n                continue\n            # If this is self-join - skip it\n            if (\n                base_table\n                and base_table.raw == table_ref.raw\n                and base_table != table_ref\n            ):\n                continue\n            whitespace_ref = from_expression.get_child(\"whitespace\")\n            # If there's no alias expression - skip it\n            alias_exp_ref = from_expression.get_child(\"alias_expression\")\n            if alias_exp_ref is None:\n                continue\n            alias_identifier_ref = alias_exp_ref.get_child(\"identifier\")\n            yield cls.TableAliasInfo(\n                table_ref, whitespace_ref, alias_exp_ref, alias_identifier_ref\n            )\n    def _lint_aliases_in_join(\n        self, base_table, from_expression_elements, column_reference_segments, segment\n    ):\n        \"\"\"Lint and fix all aliases in joins - except for self-joins.\"\"\"\n        # A buffer to keep any violations.\n        violation_buff = []\n        to_check = list(\n            self._filter_table_expressions(base_table, from_expression_elements)\n        )\n        # How many times does each table appear in the FROM clause?\n        table_counts = Counter(ai.table_ref.raw for ai in to_check)\n        # What is the set of aliases used for each table? (We are mainly\n        # interested in the NUMBER of different aliases used.)\n        table_aliases = defaultdict(set)\n        for ai in to_check:\n            table_aliases[ai.table_ref.raw].add(ai.alias_identifier_ref.raw)\n        # For each aliased table, check whether to keep or remove it.\n        for alias_info in to_check:\n            # If the same table appears more than once in the FROM clause with\n            # different alias names, do not consider removing its aliases.\n            # The aliases may have been introduced simply to make each\n            # occurrence of the table independent within the query.\n            if (\n                table_counts[alias_info.table_ref.raw] > 1\n                and len(table_aliases[alias_info.table_ref.raw]) > 1\n            ):\n                continue\n            select_clause = segment.get_child(\"select_clause\")\n            ids_refs = []\n            # Find all references to alias in select clause\n            alias_name = alias_info.alias_identifier_ref.raw\n            for alias_with_column in select_clause.recursive_crawl(\"object_reference\"):\n                used_alias_ref = alias_with_column.get_child(\"identifier\")\n                if used_alias_ref and used_alias_ref.raw == alias_name:\n                    ids_refs.append(used_alias_ref)\n            # Find all references to alias in column references\n            for exp_ref in column_reference_segments:\n                used_alias_ref = exp_ref.get_child(\"identifier\")\n                # exp_ref.get_child('dot') ensures that the column reference includes a table reference\n                if used_alias_ref.raw == alias_name and exp_ref.get_child(\"dot\"):\n                    ids_refs.append(used_alias_ref)\n            # Fixes for deleting ` as sth` and for editing references to aliased tables\n            fixes = [\n                *[\n                    LintFix(\"delete\", d)\n                    for d in [alias_info.alias_exp_ref, alias_info.whitespace_ref]\n                ],\n                *[\n                    LintFix(\"edit\", alias, alias.edit(alias_info.table_ref.raw))\n                    for alias in [alias_info.alias_identifier_ref, *ids_refs]\n                ],\n            ]\n            violation_buff.append(\n                LintResult(\n                    anchor=alias_info.alias_identifier_ref,\n                    description=\"Avoid using aliases in join condition\",\n                    fixes=fixes,\n                )\n            )\n        return violation_buff or None",
        "file_path": "src/sqlfluff/rules/L031.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 2,
        "score": 0.6748765110969543,
        "content": "\"\"\"Implementation of Rule L013.\"\"\"\nfrom sqlfluff.core.rules.base import BaseRule, LintResult\nfrom sqlfluff.core.rules.doc_decorators import document_configuration\n@document_configuration\nclass Rule_L013(BaseRule):\n    \"\"\"Column expression without alias. Use explicit `AS` clause.\n    | **Anti-pattern**\n    | In this example, there is no alias for both sums.\n    .. code-block:: sql\n        SELECT\n            sum(a),\n            sum(b)\n        FROM foo\n    | **Best practice**\n    | Add aliases.\n    .. code-block:: sql\n        SELECT\n            sum(a) AS a_sum,\n            sum(b) AS b_sum\n        FROM foo\n    \"\"\"\n    config_keywords = [\"allow_scalar\"]\n    def _eval(self, segment, parent_stack, **kwargs):\n        \"\"\"Column expression without alias. Use explicit `AS` clause.\n        We look for the select_clause_element segment, and then evaluate\n        whether it has an alias segment or not and whether the expression\n        is complicated enough. `parent_stack` is to assess how many other\n        elements there are.\n        \"\"\"\n        if segment.is_type(\"select_clause_element\"):\n            if not any(e.is_type(\"alias_expression\") for e in segment.segments):\n                types = {e.get_type() for e in segment.segments if e.name != \"star\"}\n                unallowed_types = types - {\n                    \"whitespace\",\n                    \"newline\",\n                    \"column_reference\",\n                    \"wildcard_expression\",\n                }\n                if len(unallowed_types) > 0:\n                    # No fixes, because we don't know what the alias should be,\n                    # the user should document it themselves.\n                    if self.allow_scalar:\n                        # Check *how many* elements there are in the select\n                        # statement. If this is the only one, then we won't\n                        # report an error.\n                        num_elements = sum(\n                            e.is_type(\"select_clause_element\")\n                            for e in parent_stack[-1].segments\n                        )\n                        if num_elements > 1:\n                            return LintResult(anchor=segment)\n                        else:\n                            return None\n                    else:\n                        # Just error if we don't care.\n                        return LintResult(anchor=segment)\n        return None",
        "file_path": "src/sqlfluff/rules/L013.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 3,
        "score": 0.6721032857894897,
        "content": "\"\"\"Implementation of Rule L025.\"\"\"\nfrom sqlfluff.core.rules.base import LintFix, LintResult\nfrom sqlfluff.core.rules.doc_decorators import document_fix_compatible\nfrom sqlfluff.rules.L020 import Rule_L020\nfrom sqlfluff.core.dialects.common import AliasInfo\n@document_fix_compatible\nclass Rule_L025(Rule_L020):\n    \"\"\"Tables should not be aliased if that alias is not used.\n    | **Anti-pattern**\n    .. code-block:: sql\n        SELECT\n            a\n        FROM foo AS zoo\n    | **Best practice**\n    | Use the alias or remove it. An unused alias makes code\n    | harder to read without changing any functionality.\n    .. code-block:: sql\n        SELECT\n            zoo.a\n        FROM foo AS zoo\n        -- Alternatively...\n        SELECT\n            a\n        FROM foo\n    \"\"\"\n    def _lint_references_and_aliases(\n        self,\n        table_aliases,\n        standalone_aliases,\n        references,\n        col_aliases,\n        using_cols,\n        parent_select,\n    ):\n        \"\"\"Check all aliased references against tables referenced in the query.\"\"\"\n        # A buffer to keep any violations.\n        violation_buff = []\n        # Check all the references that we have, keep track of which aliases we refer to.\n        tbl_refs = set()\n        for r in references:\n            tbl_refs.update(\n                tr.part\n                for tr in r.extract_possible_references(\n                    level=r.ObjectReferenceLevel.TABLE\n                )\n            )\n        alias: AliasInfo\n        for alias in table_aliases:\n            if alias.aliased and alias.ref_str not in tbl_refs:\n                fixes = [LintFix(\"delete\", alias.alias_expression)]\n                found_alias_segment = False\n                # Walk back to remove indents/whitespaces\n                for segment in reversed(alias.from_expression_element.segments):\n                    if not found_alias_segment:\n                        if segment is alias.alias_expression:\n                            found_alias_segment = True\n                    else:\n                        if (\n                            segment.name == \"whitespace\"\n                            or segment.name == \"newline\"\n                            or segment.is_meta\n                        ):\n                            fixes.append(LintFix(\"delete\", segment))\n                        else:\n                            # Stop once we reach an other, \"regular\" segment.\n                            break\n                violation_buff.append(\n                    LintResult(\n                        anchor=alias.segment,\n                        description=\"Alias {!r} is never used in SELECT statement.\".format(\n                            alias.ref_str\n                        ),\n                        fixes=fixes,\n                    )\n                )\n        return violation_buff or None",
        "file_path": "src/sqlfluff/rules/L025.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 4,
        "score": 0.6473289132118225,
        "content": "\"\"\"Implementation of Rule L012.\"\"\"\nfrom sqlfluff.rules.L011 import Rule_L011\nclass Rule_L012(Rule_L011):\n    \"\"\"Implicit/explicit aliasing of columns.\n    Aliasing of columns to follow preference\n    (explicit using an `AS` clause is default).\n    NB: This rule inherits its functionality from obj:`Rule_L011` but is\n    separate so that they can be enabled and disabled separately.\n    | **Anti-pattern**\n    | In this example, the alias for column 'a' is implicit.\n    .. code-block:: sql\n        SELECT\n            a\n        FROM foo\n    | **Best practice**\n    | Add `AS` to make it explicit.\n    .. code-block:: sql\n        SELECT\n            a AS alias_col\n        FROM foo\n    \"\"\"\n    config_keywords = [\"aliasing\"]\n    _target_elems = (\"select_clause_element\",)",
        "file_path": "src/sqlfluff/rules/L012.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 5,
        "score": 0.6437839269638062,
        "content": "\"\"\"Implementation of Rule L032.\"\"\"\nfrom sqlfluff.core.rules.base import BaseRule, LintResult\nclass Rule_L032(BaseRule):\n    \"\"\"Prefer specifying join keys instead of using \"USING\".\n    | **Anti-pattern**\n    .. code-block:: sql\n        SELECT\n            table_a.field_1,\n            table_b.field_2\n        FROM\n            table_a\n        INNER JOIN table_b USING (id)\n    | **Best practice**\n    |  Specify the keys directly\n    .. code-block:: sql\n        SELECT\n            table_a.field_1,\n            table_b.field_2\n        FROM\n            table_a\n        INNER JOIN table_b\n            ON table_a.id = table_b.id\n    \"\"\"\n    def _eval(self, segment, **kwargs):\n        \"\"\"Look for USING in a join clause.\"\"\"\n        if segment.is_type(\"join_clause\"):\n            for seg in segment.segments:\n                if seg.is_type(\"keyword\") and seg.name == \"using\":\n                    return [\n                        LintResult(\n                            # Reference the element, not the string.\n                            anchor=seg,\n                            description=(\n                                \"Found USING statement. Expected only ON statements.\"\n                            ),\n                        )\n                    ]\n        return None",
        "file_path": "src/sqlfluff/rules/L032.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 6,
        "score": 0.6417247653007507,
        "content": "\"\"\"Implementation of Rule L011.\"\"\"\nfrom sqlfluff.core.parser import WhitespaceSegment, KeywordSegment\nfrom sqlfluff.core.rules.base import BaseRule, LintResult, LintFix\nfrom sqlfluff.core.rules.doc_decorators import document_fix_compatible\n@document_fix_compatible\nclass Rule_L011(BaseRule):\n    \"\"\"Implicit/explicit aliasing of table.\n    Aliasing of table to follow preference\n    (explicit using an `AS` clause is default).\n    | **Anti-pattern**\n    | In this example, the alias 'voo' is implicit.\n    .. code-block:: sql\n        SELECT\n            voo.a\n        FROM foo voo\n    | **Best practice**\n    | Add `AS` to make it explicit.\n    .. code-block:: sql\n        SELECT\n            voo.a\n        FROM foo AS voo\n    \"\"\"\n    config_keywords = [\"aliasing\"]\n    _target_elems = (\"from_expression_element\",)\n    def _eval(self, segment, parent_stack, raw_stack, **kwargs):\n        \"\"\"Implicit aliasing of table/column not allowed. Use explicit `AS` clause.\n        We look for the alias segment, and then evaluate its parent and whether\n        it contains an AS keyword. This is the _eval function for both L011 and L012.\n        The use of `raw_stack` is just for working out how much whitespace to add.\n        \"\"\"\n        fixes = []\n        if segment.is_type(\"alias_expression\"):\n            if parent_stack[-1].is_type(*self._target_elems):\n                if any(e.name.lower() == \"as\" for e in segment.segments):\n                    if self.aliasing == \"implicit\":\n                        if segment.segments[0].name.lower() == \"as\":\n                            # Remove the AS as we're using implict aliasing\n                            fixes.append(LintFix(\"delete\", segment.segments[0]))\n                            anchor = raw_stack[-1]\n                            # Remove whitespace before (if exists) or after (if not)\n                            if (\n                                len(raw_stack) > 0\n                                and raw_stack[-1].type == \"whitespace\"\n                            ):\n                                fixes.append(LintFix(\"delete\", raw_stack[-1]))\n                            elif (\n                                len(segment.segments) > 0\n                                and segment.segments[1].type == \"whitespace\"\n                            ):\n                                fixes.append(LintFix(\"delete\", segment.segments[1]))\n                            return LintResult(anchor=anchor, fixes=fixes)\n                else:\n                    insert_buff = []\n                    # Add initial whitespace if we need to...\n                    if raw_stack[-1].name not in [\"whitespace\", \"newline\"]:\n                        insert_buff.append(WhitespaceSegment())\n                    # Add an AS (Uppercase for now, but could be corrected later)\n                    insert_buff.append(KeywordSegment(\"AS\"))\n                    # Add a trailing whitespace if we need to\n                    if segment.segments[0].name not in [\"whitespace\", \"newline\"]:\n                        insert_buff.append(WhitespaceSegment())\n                    return LintResult(\n                        anchor=segment,\n                        fixes=[LintFix(\"create\", segment.segments[0], insert_buff)],\n                    )\n        return None",
        "file_path": "src/sqlfluff/rules/L011.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 7,
        "score": 0.6342813372612,
        "content": "\"\"\"Implementation of Rule L026.\"\"\"\nfrom sqlfluff.core.rules.analysis.select import get_aliases_from_select\nfrom sqlfluff.core.rules.base import LintResult\nfrom sqlfluff.core.rules.doc_decorators import document_configuration\nfrom sqlfluff.rules.L025 import Rule_L020\n@document_configuration\nclass Rule_L026(Rule_L020):\n    \"\"\"References cannot reference objects not present in FROM clause.\n    NB: This rule is disabled by default for BigQuery due to its use of\n    structs which trigger false positives. It can be enabled with the\n    `force_enable = True` flag.\n    | **Anti-pattern**\n    | In this example, the reference 'vee' has not been declared.\n    .. code-block:: sql\n        SELECT\n            vee.a\n        FROM foo\n    | **Best practice**\n    |  Remove the reference.\n    .. code-block:: sql\n        SELECT\n            a\n        FROM foo\n    \"\"\"\n    config_keywords = [\"force_enable\"]\n    @staticmethod\n    def _is_bad_tbl_ref(table_aliases, parent_select, tbl_ref):\n        \"\"\"Given a table reference, try to find what it's referring to.\"\"\"\n        # Is it referring to one of the table aliases?\n        if tbl_ref[0] in [a.ref_str for a in table_aliases]:\n            # Yes. Therefore okay.\n            return False\n        # Not a table alias. It it referring to a correlated subquery?\n        if parent_select:\n            parent_aliases, _ = get_aliases_from_select(parent_select)\n            if parent_aliases and tbl_ref[0] in [a[0] for a in parent_aliases]:\n                # Yes. Therefore okay.\n                return False\n        # It's not referring to an alias or a correlated subquery. Looks like a\n        # bad reference (i.e. referring to something unknown.)\n        return True\n    def _lint_references_and_aliases(\n        self,\n        table_aliases,\n        standalone_aliases,\n        references,\n        col_aliases,\n        using_cols,\n        parent_select,\n    ):\n        # A buffer to keep any violations.\n        violation_buff = []\n        # Check all the references that we have, do they reference present aliases?\n        for r in references:\n            tbl_refs = r.extract_possible_references(level=r.ObjectReferenceLevel.TABLE)\n            if tbl_refs and all(\n                self._is_bad_tbl_ref(table_aliases, parent_select, tbl_ref)\n                for tbl_ref in tbl_refs\n            ):\n                violation_buff.append(\n                    LintResult(\n                        # Return the first segment rather than the string\n                        anchor=tbl_refs[0].segments[0],\n                        description=f\"Reference {r.raw!r} refers to table/view \"\n                        \"not found in the FROM clause or found in parent \"\n                        \"subquery.\",\n                    )\n                )\n        return violation_buff or None\n    def _eval(self, segment, parent_stack, dialect, **kwargs):\n        \"\"\"Override Rule L020 for dialects that use structs.\n        Some dialects use structs (e.g. column.field) which look like\n        table references and so incorrectly trigger this rule.\n        \"\"\"\n        if dialect.name in [\"bigquery\"] and not self.force_enable:\n            return LintResult()\n        return super()._eval(segment, parent_stack, dialect, **kwargs)",
        "file_path": "src/sqlfluff/rules/L026.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 8,
        "score": 0.6271159052848816,
        "content": "\"\"\"Implementation of Rule L020.\"\"\"\nimport itertools\nfrom sqlfluff.core.rules.base import BaseRule, LintResult\nfrom sqlfluff.core.rules.analysis.select import get_select_statement_info\nclass Rule_L020(BaseRule):\n    \"\"\"Table aliases should be unique within each clause.\n    | **Anti-pattern**\n    | In this example, the alias 't' is reused for two different ables:\n    .. code-block:: sql\n        SELECT\n            t.a,\n            t.b\n        FROM foo AS t, bar AS t\n        -- this can also happen when using schemas where the implicit alias is the table name:\n        SELECT\n            a,\n            b\n        FROM\n            2020.foo,\n            2021.foo\n    | **Best practice**\n    | Make all tables have a unique alias\n    .. code-block:: sql\n        SELECT\n            f.a,\n            b.b\n        FROM foo AS f, bar AS b\n        -- Also use explicit alias's when referencing two tables with same name from two different schemas\n        SELECT\n            f1.a,\n            f2.b\n        FROM\n            2020.foo AS f1,\n            2021.foo AS f2\n    \"\"\"\n    def _lint_references_and_aliases(\n        self,\n        table_aliases,\n        standalone_aliases,\n        references,\n        col_aliases,\n        using_cols,\n        parent_select,\n    ):\n        \"\"\"Check whether any aliases are duplicates.\n        NB: Subclasses of this error should override this function.\n        \"\"\"\n        # Are any of the aliases the same?\n        duplicate = set()\n        for a1, a2 in itertools.combinations(table_aliases, 2):\n            # Compare the strings\n            if a1.ref_str == a2.ref_str and a1.ref_str:\n                duplicate.add(a2)\n        if duplicate:\n            return [\n                LintResult(\n                    # Reference the element, not the string.\n                    anchor=aliases.segment,\n                    description=(\n                        \"Duplicate table alias {!r}. Table \" \"aliases should be unique.\"\n                    ).format(aliases.ref_str),\n                )\n                for aliases in duplicate\n            ]\n        else:\n            return None\n    def _eval(self, segment, parent_stack, dialect, **kwargs):\n        \"\"\"Get References and Aliases and allow linting.\n        This rule covers a lot of potential cases of odd usages of\n        references, see the code for each of the potential cases.\n        Subclasses of this rule should override the\n        `_lint_references_and_aliases` method.\n        \"\"\"\n        if segment.is_type(\"select_statement\"):\n            select_info = get_select_statement_info(segment, dialect)\n            if not select_info:\n                return None\n            # Work out if we have a parent select function\n            parent_select = None\n            for seg in reversed(parent_stack):\n                if seg.is_type(\"select_statement\"):\n                    parent_select = seg\n                    break\n            # Pass them all to the function that does all the work.\n            # NB: Subclasses of this rules should override the function below\n            return self._lint_references_and_aliases(\n                select_info.table_aliases,\n                select_info.standalone_aliases,\n                select_info.reference_buffer,\n                select_info.col_aliases,\n                select_info.using_cols,\n                parent_select,\n            )\n        return None",
        "file_path": "src/sqlfluff/rules/L020.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 9,
        "score": 0.6152881979942322,
        "content": "\"\"\"Implementation of Rule L028.\"\"\"\nfrom sqlfluff.core.rules.base import LintResult\nfrom sqlfluff.core.rules.doc_decorators import document_configuration\nfrom sqlfluff.rules.L025 import Rule_L025\n@document_configuration\nclass Rule_L028(Rule_L025):\n    \"\"\"References should be consistent in statements with a single table.\n    NB: This rule is disabled by default for BigQuery due to its use of\n    structs which trigger false positives. It can be enabled with the\n    `force_enable = True` flag.\n    | **Anti-pattern**\n    | In this example, only the field `b` is referenced.\n    .. code-block:: sql\n        SELECT\n            a,\n            foo.b\n        FROM foo\n    | **Best practice**\n    |  Remove all the reference or reference all the fields.\n    .. code-block:: sql\n        SELECT\n            a,\n            b\n        FROM foo\n        -- Also good\n        SELECT\n            foo.a,\n            foo.b\n        FROM foo\n    \"\"\"\n    config_keywords = [\"single_table_references\", \"force_enable\"]\n    def _lint_references_and_aliases(\n        self,\n        table_aliases,\n        standalone_aliases,\n        references,\n        col_aliases,\n        using_cols,\n        parent_select,\n    ):\n        \"\"\"Iterate through references and check consistency.\"\"\"\n        # How many aliases are there? If more than one then abort.\n        if len(table_aliases) > 1:\n            return None\n        # A buffer to keep any violations.\n        violation_buff = []\n        # Check all the references that we have.\n        seen_ref_types = set()\n        for ref in references:\n            # We skip any unqualified wildcard references (i.e. *). They shouldn't count.\n            if not ref.is_qualified() and ref.is_type(\"wildcard_identifier\"):\n                continue\n            # Oddball case: Column aliases provided via function calls in by\n            # FROM or JOIN. References to these don't need to be qualified.\n            # Note there could be a table with a column by the same name as\n            # this alias, so avoid bogus warnings by just skipping them\n            # entirely rather than trying to enforce anything.\n            if ref.raw in standalone_aliases:\n                continue\n            this_ref_type = ref.qualification()\n            if self.single_table_references == \"consistent\":\n                if seen_ref_types and this_ref_type not in seen_ref_types:\n                    violation_buff.append(\n                        LintResult(\n                            anchor=ref,\n                            description=f\"{this_ref_type.capitalize()} reference \"\n                            f\"{ref.raw!r} found in single table select which is \"\n                            \"inconsistent with previous references.\",\n                        )\n                    )\n            elif self.single_table_references != this_ref_type:\n                violation_buff.append(\n                    LintResult(\n                        anchor=ref,\n                        description=\"{} reference {!r} found in single table select.\".format(\n                            this_ref_type.capitalize(), ref.raw\n                        ),\n                    )\n                )\n            seen_ref_types.add(this_ref_type)\n        return violation_buff or None\n    def _eval(self, segment, parent_stack, dialect, **kwargs):\n        \"\"\"Override Rule L025 for dialects that use structs.\n        Some dialects use structs (e.g. column.field) which look like\n        table references and so incorrectly trigger this rule.\n        \"\"\"\n        if dialect.name in [\"bigquery\"] and not self.force_enable:\n            return LintResult()\n        return super()._eval(segment, parent_stack, dialect, **kwargs)",
        "file_path": "src/sqlfluff/rules/L028.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 10,
        "score": 0.60893714427948,
        "content": "\"\"\"Implementation of Rule L024.\"\"\"\nfrom sqlfluff.core.rules.doc_decorators import document_fix_compatible\nfrom sqlfluff.rules.L023 import Rule_L023\n@document_fix_compatible\nclass Rule_L024(Rule_L023):\n    \"\"\"Single whitespace expected after USING in JOIN clause.\n    | **Anti-pattern**\n    .. code-block:: sql\n        SELECT b\n        FROM foo\n        LEFT JOIN zoo USING(a)\n    | **Best practice**\n    | The • character represents a space.\n    | Add a space after USING, to avoid confusing it\n    | for a function.\n    .. code-block:: sql\n       :force:\n        SELECT b\n        FROM foo\n        LEFT JOIN zoo USING•(a)\n    \"\"\"\n    expected_mother_segment_type = \"join_clause\"\n    pre_segment_identifier = (\"name\", \"using\")\n    post_segment_identifier = (\"type\", \"bracketed\")\n    expand_children = None\n    allow_newline = True",
        "file_path": "src/sqlfluff/rules/L024.py",
        "chunk_index": 0,
        "metadata": {}
      }
    ]
  },
  "sqlfluff__sqlfluff-2419": {
    "query": "Rule L060 could give a specific error message\nAt the moment rule L060 flags something like this:\r\n\r\n```\r\nL:  21 | P:   9 | L060 | Use 'COALESCE' instead of 'IFNULL' or 'NVL'.\r\n```\r\n\r\nSince we likely know the wrong word, it might be nice to actually flag that instead of both `IFNULL` and `NVL` - like most of the other rules do.\r\n\r\nThat is it should flag this:\r\n\r\n```\r\nL:  21 | P:   9 | L060 | Use 'COALESCE' instead of 'IFNULL'.\r\n```\r\n Or this:\r\n\r\n```\r\nL:  21 | P:   9 | L060 | Use 'COALESCE' instead of 'NVL'.\r\n```\r\n\r\nAs appropriate.\r\n\r\nWhat do you think @jpy-git ?\r\n\n",
    "method": "sliding",
    "top_k": 10,
    "results": [
      {
        "rank": 1,
        "score": 0.8216562271118164,
        "content": "\"\"\"Implementation of Rule L060.\"\"\"\nfrom typing import Optional\nfrom sqlfluff.core.parser.segments.raw import CodeSegment\nfrom sqlfluff.core.rules.base import BaseRule, LintFix, LintResult, RuleContext\nfrom sqlfluff.core.rules.doc_decorators import document_fix_compatible\n@document_fix_compatible\nclass Rule_L060(BaseRule):\n    \"\"\"Use ``COALESCE`` instead of ``IFNULL`` or ``NVL``.\n    | **Anti-pattern**\n    | ``IFNULL`` or ``NVL`` are used to fill ``NULL`` values.\n    .. code-block:: sql\n        SELECT ifnull(foo, 0) AS bar,\n        FROM baz;\n        SELECT nvl(foo, 0) AS bar,\n        FROM baz;\n    | **Best practice**\n    | Use ``COALESCE`` instead.\n    | ``COALESCE`` is universally supported,\n    | whereas Redshift doesn't support ``IFNULL``\n    | and BigQuery doesn't support ``NVL``.\n    | Additionally ``COALESCE`` is more flexible\n    | and accepts an arbitrary number of arguments.\n    .. code-block:: sql\n        SELECT coalesce(foo, 0) AS bar,\n        FROM baz;\n    \"\"\"\n    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Use ``COALESCE`` instead of ``IFNULL`` or ``NVL``.\"\"\"\n        # We only care about function names.\n        if context.segment.name != \"function_name_identifier\":\n            return None\n        # Only care if the function is ``IFNULL`` or ``NVL``.\n        if context.segment.raw_upper not in {\"IFNULL\", \"NVL\"}:\n            return None\n        # Create fix to replace ``IFNULL`` or ``NVL`` with ``COALESCE``.\n        fix = LintFix.replace(\n            context.segment,\n            [\n                CodeSegment(\n                    raw=\"COALESCE\",\n                    name=\"function_name_identifier\",\n                    type=\"function_name_identifier\",\n                )\n            ],\n        )\n        return LintResult(context.segment, [fix])",
        "file_path": "src/sqlfluff/rules/L060.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 2,
        "score": 0.6371965408325195,
        "content": "\"\"\"Implementation of Rule L043.\"\"\"\nfrom typing import List, Optional\nfrom sqlfluff.core.parser import (\n    WhitespaceSegment,\n    SymbolSegment,\n    KeywordSegment,\n)\nfrom sqlfluff.core.parser.segments.base import BaseSegment\nfrom sqlfluff.core.rules.base import BaseRule, LintFix, LintResult, RuleContext\nfrom sqlfluff.core.rules.doc_decorators import document_fix_compatible\nfrom sqlfluff.core.rules.functional import Segments, sp\n@document_fix_compatible\nclass Rule_L043(BaseRule):\n    \"\"\"Unnecessary ``CASE`` statement.\n    | **Anti-pattern**\n    | ``CASE`` statement returns booleans.\n    .. code-block:: sql\n        :force:\n        select\n            case\n                when fab > 0 then true\n                else false\n            end as is_fab\n        from fancy_table\n        -- This rule can also simplify CASE statements\n        -- that aim to fill NULL values.\n        select\n            case\n                when fab is null then 0\n                else fab\n            end as fab_clean\n        from fancy_table\n        -- This also covers where the case statement\n        -- replaces NULL values with NULL values.\n        select\n            case\n                when fab is null then null\n                else fab\n            end as fab_clean\n        from fancy_table\n    | **Best practice**\n    | Reduce to ``WHEN`` condition within ``COALESCE`` function.\n    .. code-block:: sql\n        :force:\n        select\n            coalesce(fab > 0, false) as is_fab\n        from fancy_table\n        -- To fill NULL values.\n        select\n            coalesce(fab, 0) as fab_clean\n        from fancy_table\n        -- NULL filling NULL.\n        select fab as fab_clean\n        from fancy_table\n    \"\"\"\n    @staticmethod\n    def _coalesce_fix_list(\n        context: RuleContext,\n        coalesce_arg_1: BaseSegment,\n        coalesce_arg_2: BaseSegment,\n        preceding_not: bool = False,\n    ) -> List[LintFix]:\n        \"\"\"Generate list of fixes to convert CASE statement to COALESCE function.\"\"\"\n        # Add coalesce and opening parenthesis.\n        edits = [\n            KeywordSegment(\"coalesce\"),\n            SymbolSegment(\"(\", name=\"start_bracket\", type=\"start_bracket\"),\n            coalesce_arg_1,\n            SymbolSegment(\",\", name=\"comma\", type=\"comma\"),\n            WhitespaceSegment(),\n            coalesce_arg_2,\n            SymbolSegment(\")\", name=\"end_bracket\", type=\"end_bracket\"),\n        ]\n        if preceding_not:\n            not_edits: List[BaseSegment] = [\n                KeywordSegment(\"not\"),\n                WhitespaceSegment(),\n            ]\n            edits = not_edits + edits\n        fixes = [\n            LintFix.replace(\n                context.segment,\n                edits,\n            )\n        ]\n        return fixes\n    @staticmethod\n    def _column_only_fix_list(\n        context: RuleContext,\n        column_reference_segment: BaseSegment,\n    ) -> List[LintFix]:\n        \"\"\"Generate list of fixes to reduce CASE statement to a single column.\"\"\"\n        fixes = [\n            LintFix.replace(\n                context.segment,\n                [column_reference_segment],\n            )\n        ]\n        return fixes\n    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Unnecessary CASE statement.\"\"\"\n        # Look for CASE expression.\n        if (\n            context.segment.is_type(\"case_expression\")\n            and context.segment.segments[0].name == \"case\"\n        ):\n            # Find all 'WHEN' clauses and the optional 'ELSE' clause.\n            children = context.functional.segment.children()\n            when_clauses = children.select(sp.is_type(\"when_clause\"))\n            else_clauses = children.select(sp.is_type(\"else_clause\"))\n            # Can't fix if multiple WHEN clauses.\n            if len(when_clauses) > 1:\n                return None\n            # Find condition and then expressions.\n            condition_expression = when_clauses.children(sp.is_type(\"expression\"))[0]\n            then_expression = when_clauses.children(sp.is_type(\"expression\"))[1]\n            # Method 1: Check if THEN/ELSE expressions are both Boolean and can\n            # therefore be reduced.\n            if else_clauses:\n                else_expression = else_clauses.children(sp.is_type(\"expression\"))[0]\n                upper_bools = [\"TRUE\", \"FALSE\"]\n                if (\n                    (then_expression.raw_upper in upper_bools)\n                    and (else_expression.raw_upper in upper_bools)\n                    and (then_expression.raw_upper != else_expression.raw_upper)\n                ):\n                    coalesce_arg_1 = condition_expression\n                    coalesce_arg_2 = KeywordSegment(\"false\")\n                    preceding_not = then_expression.raw_upper == \"FALSE\"\n                    fixes = self._coalesce_fix_list(\n                        context,\n                        coalesce_arg_1,\n                        coalesce_arg_2,\n                        preceding_not,\n                    )\n                    return LintResult(\n                        anchor=condition_expression,\n                        fixes=fixes,\n                        description=\"Unnecessary CASE statement. \"\n                        \"Use COALESCE function instead.\",\n                    )\n            # Method 2: Check if the condition expression is comparing a column\n            # reference to NULL and whether that column reference is also in either the\n            # THEN/ELSE expression. We can only apply this method when there is only\n            # one condition in the condition expression.\n            condition_expression_segments_raw = {\n                segment.raw_upper for segment in condition_expression.segments\n            }\n            if {\"IS\", \"NULL\"}.issubset(condition_expression_segments_raw) and (\n                not condition_expression_segments_raw.intersection({\"AND\", \"OR\"})\n            ):\n                # Check if the comparison is to NULL or NOT NULL.\n                is_not_prefix = \"NOT\" in condition_expression_segments_raw\n                # Locate column reference in condition expression.\n                column_reference_segment = (\n                    Segments(condition_expression)\n                    .children(sp.is_type(\"column_reference\"))\n                    .get()\n                )\n                # Return None if none found (this condition does not apply to functions)\n                if not column_reference_segment:\n                    return None\n                if else_clauses:\n                    else_expression = else_clauses.children(sp.is_type(\"expression\"))[0]\n                    # Check if we can reduce the CASE expression to a single coalesce\n                    # function.\n                    if (\n                        not is_not_prefix\n                        and column_reference_segment.raw_upper\n                        == else_expression.raw_upper\n                    ):\n                        coalesce_arg_1 = else_expression\n                        coalesce_arg_2 = then_expression\n                    elif (\n                        is_not_prefix\n                        and column_reference_segment.raw_upper\n                        == then_expression.raw_upper\n                    ):\n                        coalesce_arg_1 = then_expression\n                        coalesce_arg_2 = else_expression\n                    else:\n                        return None\n                    if coalesce_arg_2.raw_upper == \"NULL\":\n                        # Can just specify the column on it's own\n                        # rather than using a COALESCE function.\n                        return LintResult(\n                            anchor=condition_expression,\n                            fixes=self._column_only_fix_list(\n                                context,\n                                column_reference_segment,\n                            ),\n                            description=\"Unnecessary CASE statement. \"\n                            f\"Just use column '{column_reference_segment.raw}'.\",\n                        )\n                    return LintResult(\n                        anchor=condition_expression,\n                        fixes=self._coalesce_fix_list(\n                            context,\n                            coalesce_arg_1,\n                            coalesce_arg_2,\n                        ),\n                        description=\"Unnecessary CASE statement. \"\n                        \"Use COALESCE function instead.\",\n                    )\n                elif (\n                    column_reference_segment.raw_segments_upper\n                    == then_expression.raw_segments_upper\n                ):\n                    # Can just specify the column on it's own\n                    # rather than using a COALESCE function.\n                    # In this case no ELSE statement is equivalent to ELSE NULL.\n                    return LintResult(\n                        anchor=condition_expression,\n                        fixes=self._column_only_fix_list(\n                            context,\n                            column_reference_segment,\n                        ),\n                        description=\"Unnecessary CASE statement. \"\n                        f\"Just use column '{column_reference_segment.raw}'.\",\n                    )\n        return None",
        "file_path": "src/sqlfluff/rules/L043.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 3,
        "score": 0.6233620047569275,
        "content": "\"\"\"Implementation of Rule L049.\"\"\"\nfrom typing import List, Union\nfrom sqlfluff.core.parser import KeywordSegment, WhitespaceSegment\nfrom sqlfluff.core.rules.base import LintResult, LintFix, RuleContext\nfrom sqlfluff.core.rules.doc_decorators import document_fix_compatible\nfrom sqlfluff.rules.L006 import Rule_L006\n@document_fix_compatible\nclass Rule_L049(Rule_L006):\n    \"\"\"Comparisons with NULL should use \"IS\" or \"IS NOT\".\n    | **Anti-pattern**\n    | In this example, the ``=`` operator is used to check for ``NULL`` values.\n    .. code-block:: sql\n        SELECT\n            a\n        FROM foo\n        WHERE a = NULL\n    | **Best practice**\n    | Use ``IS`` or ``IS NOT`` to check for ``NULL`` values.\n    .. code-block:: sql\n        SELECT\n            a\n        FROM foo\n        WHERE a IS NULL\n    \"\"\"\n    def _eval(self, context: RuleContext) -> LintResult:\n        \"\"\"Relational operators should not be used to check for NULL values.\"\"\"\n        # Context/motivation for this rule:\n        # https://news.ycombinator.com/item?id=28772289\n        # https://stackoverflow.com/questions/9581745/sql-is-null-and-null\n        if len(context.segment.segments) <= 2:\n            return LintResult()\n        # Allow assignments in SET clauses\n        if context.parent_stack and context.parent_stack[-1].is_type(\n            \"set_clause_list\", \"execute_script_statement\"\n        ):\n            return LintResult()\n        # Allow assignments in EXEC clauses\n        if context.segment.is_type(\"set_clause_list\", \"execute_script_statement\"):\n            return LintResult()\n        # Iterate through children of this segment looking for equals or \"not\n        # equals\". Once found, check if the next code segment is a NULL literal.\n        idx_operator = None\n        operator = None\n        for idx, sub_seg in enumerate(context.segment.segments):\n            # Skip anything which is whitespace or non-code.\n            if sub_seg.is_whitespace or not sub_seg.is_code:\n                continue\n            # Look for \"=\" or \"<>\".\n            if not operator and sub_seg.name in (\"equals\", \"not_equal_to\"):\n                self.logger.debug(\n                    \"Found equals/not equals @%s: %r\", sub_seg.pos_marker, sub_seg.raw\n                )\n                idx_operator = idx\n                operator = sub_seg\n            elif operator:\n                # Look for a \"NULL\" literal.\n                if sub_seg.name == \"null_literal\":\n                    self.logger.debug(\n                        \"Found NULL literal following equals/not equals @%s: %r\",\n                        sub_seg.pos_marker,\n                        sub_seg.raw,\n                    )\n                    if sub_seg.raw[0] == \"N\":\n                        is_seg = KeywordSegment(\"IS\")\n                        not_seg = KeywordSegment(\"NOT\")\n                    else:\n                        is_seg = KeywordSegment(\"is\")\n                        not_seg = KeywordSegment(\"not\")\n                    edit: List[Union[WhitespaceSegment, KeywordSegment]] = (\n                        [is_seg]\n                        if operator.name == \"equals\"\n                        else [\n                            is_seg,\n                            WhitespaceSegment(),\n                            not_seg,\n                        ]\n                    )\n                    prev_seg = self._find_segment(\n                        idx_operator, context.segment.segments, before=True\n                    )\n                    next_seg = self._find_segment(\n                        idx_operator, context.segment.segments, before=False\n                    )\n                    if self._missing_whitespace(prev_seg, before=True):\n                        whitespace_segment: List[\n                            Union[WhitespaceSegment, KeywordSegment]\n                        ] = [WhitespaceSegment()]\n                        edit = whitespace_segment + edit\n                    if self._missing_whitespace(next_seg, before=False):\n                        edit = edit + [WhitespaceSegment()]\n                    return LintResult(\n                        anchor=operator,\n                        fixes=[\n                            LintFix.replace(\n                                operator,\n                                edit,\n                            )\n                        ],\n                    )\n        # If we get to here, it's not a violation\n        return LintResult()",
        "file_path": "src/sqlfluff/rules/L049.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 4,
        "score": 0.5984591245651245,
        "content": "\"\"\"Implementation of Rule L035.\"\"\"\nfrom typing import Optional\nfrom sqlfluff.core.rules.base import BaseRule, LintFix, LintResult, RuleContext\nfrom sqlfluff.core.rules.doc_decorators import document_fix_compatible\nimport sqlfluff.core.rules.functional.segment_predicates as sp\n@document_fix_compatible\nclass Rule_L035(BaseRule):\n    \"\"\"Do not specify ``else null`` in a case when statement (redundant).\n    | **Anti-pattern**\n    .. code-block:: sql\n        select\n            case\n                when name like '%cat%' then 'meow'\n                when name like '%dog%' then 'woof'\n                else null\n            end\n        from x\n    | **Best practice**\n    |  Omit ``else null``\n    .. code-block:: sql\n        select\n            case\n                when name like '%cat%' then 'meow'\n                when name like '%dog%' then 'woof'\n            end\n        from x\n    \"\"\"\n    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Find rule violations and provide fixes.\n        0. Look for a case expression\n        1. Look for \"ELSE\"\n        2. Mark \"ELSE\" for deletion (populate \"fixes\")\n        3. Backtrack and mark all newlines/whitespaces for deletion\n        4. Look for a raw \"NULL\" segment\n        5.a. The raw \"NULL\" segment is found, we mark it for deletion and return\n        5.b. We reach the end of case when without matching \"NULL\": the rule passes\n        \"\"\"\n        if context.segment.is_type(\"case_expression\"):\n            children = context.functional.segment.children()\n            else_clause = children.first(sp.is_type(\"else_clause\"))\n            # Does the \"ELSE\" have a \"NULL\"? NOTE: Here, it's safe to look for\n            # \"NULL\", as an expression would *contain* NULL but not be == NULL.\n            if else_clause and else_clause.children(\n                lambda child: child.raw_upper == \"NULL\"\n            ):\n                # Found ELSE with NULL. Delete the whole else clause as well as\n                # indents/whitespaces/meta preceding the ELSE. :TRICKY: Note\n                # the use of reversed() to make select() effectively search in\n                # reverse.\n                before_else = children.reversed().select(\n                    start_seg=else_clause[0],\n                    loop_while=sp.or_(\n                        sp.is_name(\"whitespace\", \"newline\"), sp.is_meta()\n                    ),\n                )\n                return LintResult(\n                    anchor=context.segment,\n                    fixes=[LintFix.delete(else_clause[0])]\n                    + [LintFix.delete(seg) for seg in before_else],\n                )\n        return None",
        "file_path": "src/sqlfluff/rules/L035.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 5,
        "score": 0.566110372543335,
        "content": "\"\"\"Implementation of Rule L040.\"\"\"\nfrom typing import Tuple, List\nfrom sqlfluff.core.rules.doc_decorators import (\n    document_configuration,\n    document_fix_compatible,\n)\nfrom sqlfluff.rules.L010 import Rule_L010\n@document_configuration\n@document_fix_compatible\nclass Rule_L040(Rule_L010):\n    \"\"\"Inconsistent capitalisation of boolean/null literal.\n    The functionality for this rule is inherited from :obj:`Rule_L010`.\n    | **Anti-pattern**\n    | In this example, 'null' and 'false' are in lower-case whereas 'TRUE' is in\n    | upper-case.\n    .. code-block:: sql\n        select\n            a,\n            null,\n            TRUE,\n            false\n        from foo\n    | **Best practice**\n    | Ensure all literal null/true/false literals cases are used consistently\n    .. code-block:: sql\n        select\n            a,\n            NULL,\n            TRUE,\n            FALSE\n        from foo\n        -- Also good\n        select\n            a,\n            null,\n            true,\n            false\n        from foo\n    \"\"\"\n    _target_elems: List[Tuple[str, str]] = [\n        (\"name\", \"null_literal\"),\n        (\"name\", \"boolean_literal\"),\n    ]\n    _description_elem = \"Boolean/null literals\"",
        "file_path": "src/sqlfluff/rules/L040.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 6,
        "score": 0.5213739275932312,
        "content": "\"\"\"Implementation of Rule L013.\"\"\"\nfrom typing import Optional\nfrom sqlfluff.core.rules.base import BaseRule, LintResult, RuleContext\nfrom sqlfluff.core.rules.doc_decorators import document_configuration\nimport sqlfluff.core.rules.functional.segment_predicates as sp\n@document_configuration\nclass Rule_L013(BaseRule):\n    \"\"\"Column expression without alias. Use explicit `AS` clause.\n    | **Anti-pattern**\n    | In this example, there is no alias for both sums.\n    .. code-block:: sql\n        SELECT\n            sum(a),\n            sum(b)\n        FROM foo\n    | **Best practice**\n    | Add aliases.\n    .. code-block:: sql\n        SELECT\n            sum(a) AS a_sum,\n            sum(b) AS b_sum\n        FROM foo\n    \"\"\"\n    config_keywords = [\"allow_scalar\"]\n    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Column expression without alias. Use explicit `AS` clause.\n        We look for the select_clause_element segment, and then evaluate\n        whether it has an alias segment or not and whether the expression\n        is complicated enough. `parent_stack` is to assess how many other\n        elements there are.\n        \"\"\"\n        segment = context.functional.segment\n        children = segment.children()\n        if segment.all(sp.is_type(\"select_clause_element\")) and not children.any(\n            sp.is_type(\"alias_expression\")\n        ):\n            # Ignore if it's a function with EMITS clause as EMITS is equivalent to AS\n            if (\n                children.select(sp.is_type(\"function\"))\n                .children()\n                .select(sp.is_type(\"emits_segment\"))\n            ):\n                return None\n            types = set(\n                children.select(sp.not_(sp.is_name(\"star\"))).apply(sp.get_type())\n            )\n            unallowed_types = types - {\n                \"whitespace\",\n                \"newline\",\n                \"column_reference\",\n                \"wildcard_expression\",\n            }\n            if unallowed_types:\n                # No fixes, because we don't know what the alias should be,\n                # the user should document it themselves.\n                if self.allow_scalar:  # type: ignore\n                    # Check *how many* elements there are in the select\n                    # statement. If this is the only one, then we won't\n                    # report an error.\n                    immediate_parent = context.functional.parent_stack.last()\n                    num_elements = len(\n                        immediate_parent.children(sp.is_type(\"select_clause_element\"))\n                    )\n                    if num_elements > 1:\n                        return LintResult(anchor=context.segment)\n                    else:\n                        return None\n                else:\n                    # Just error if we don't care.\n                    return LintResult(anchor=context.segment)\n        return None",
        "file_path": "src/sqlfluff/rules/L013.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 7,
        "score": 0.5162694454193115,
        "content": "\"\"\"Implementation of Rule L012.\"\"\"\nfrom sqlfluff.rules.L011 import Rule_L011\nclass Rule_L012(Rule_L011):\n    \"\"\"Implicit/explicit aliasing of columns.\n    Aliasing of columns to follow preference\n    (explicit using an `AS` clause is default).\n    NB: This rule inherits its functionality from :obj:`Rule_L011` but is\n    separate so that they can be enabled and disabled separately.\n    | **Anti-pattern**\n    | In this example, the alias for column 'a' is implicit.\n    .. code-block:: sql\n        SELECT\n            a alias_col\n        FROM foo\n    | **Best practice**\n    | Add `AS` to make it explicit.\n    .. code-block:: sql\n        SELECT\n            a AS alias_col\n        FROM foo\n    \"\"\"\n    config_keywords = [\"aliasing\"]\n    _target_elems = (\"select_clause_element\",)",
        "file_path": "src/sqlfluff/rules/L012.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 8,
        "score": 0.5144476890563965,
        "content": "\"\"\"Implementation of Rule L011.\"\"\"\nfrom typing import List, Optional, Union\nfrom sqlfluff.core.parser import (\n    WhitespaceSegment,\n    KeywordSegment,\n)\nfrom sqlfluff.core.rules.base import BaseRule, LintResult, LintFix, RuleContext\nfrom sqlfluff.core.rules.doc_decorators import document_fix_compatible\n@document_fix_compatible\nclass Rule_L011(BaseRule):\n    \"\"\"Implicit/explicit aliasing of table.\n    Aliasing of table to follow preference\n    (explicit using an `AS` clause is default).\n    | **Anti-pattern**\n    | In this example, the alias 'voo' is implicit.\n    .. code-block:: sql\n        SELECT\n            voo.a\n        FROM foo voo\n    | **Best practice**\n    | Add `AS` to make it explicit.\n    .. code-block:: sql\n        SELECT\n            voo.a\n        FROM foo AS voo\n    \"\"\"\n    config_keywords = [\"aliasing\"]\n    _target_elems = (\"from_expression_element\",)\n    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Implicit aliasing of table/column not allowed. Use explicit `AS` clause.\n        We look for the alias segment, and then evaluate its parent and whether\n        it contains an AS keyword. This is the _eval function for both L011 and L012.\n        The use of `raw_stack` is just for working out how much whitespace to add.\n        \"\"\"\n        fixes = []\n        if context.segment.is_type(\"alias_expression\"):\n            if context.parent_stack[-1].is_type(*self._target_elems):\n                if any(e.name.lower() == \"as\" for e in context.segment.segments):\n                    if self.aliasing == \"implicit\":  # type: ignore\n                        if context.segment.segments[0].name.lower() == \"as\":\n                            # Remove the AS as we're using implict aliasing\n                            fixes.append(LintFix.delete(context.segment.segments[0]))\n                            anchor = context.raw_stack[-1]\n                            # Remove whitespace before (if exists) or after (if not)\n                            if (\n                                len(context.raw_stack) > 0\n                                and context.raw_stack[-1].type == \"whitespace\"\n                            ):\n                                fixes.append(LintFix.delete(context.raw_stack[-1]))\n                            elif (\n                                len(context.segment.segments) > 0\n                                and context.segment.segments[1].type == \"whitespace\"\n                            ):\n                                fixes.append(\n                                    LintFix.delete(context.segment.segments[1])\n                                )\n                            return LintResult(anchor=anchor, fixes=fixes)\n                else:\n                    insert_buff: List[Union[WhitespaceSegment, KeywordSegment]] = []\n                    # Add initial whitespace if we need to...\n                    if context.raw_stack[-1].name not in [\"whitespace\", \"newline\"]:\n                        insert_buff.append(WhitespaceSegment())\n                    # Add an AS (Uppercase for now, but could be corrected later)\n                    insert_buff.append(KeywordSegment(\"AS\"))\n                    # Add a trailing whitespace if we need to\n                    if context.segment.segments[0].name not in [\n                        \"whitespace\",\n                        \"newline\",\n                    ]:\n                        insert_buff.append(WhitespaceSegment())\n                    return LintResult(\n                        anchor=context.segment,\n                        fixes=[\n                            LintFix.create_before(\n                                context.segment.segments[0],\n                                insert_buff,\n                            )\n                        ],\n                    )\n        return None",
        "file_path": "src/sqlfluff/rules/L011.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 9,
        "score": 0.49946871399879456,
        "content": "\"\"\"Implementation of Rule L054.\"\"\"\nfrom typing import Optional\nfrom sqlfluff.core.rules.base import BaseRule, LintResult, RuleContext\nfrom sqlfluff.core.rules.doc_decorators import (\n    document_configuration,\n)\n@document_configuration\nclass Rule_L054(BaseRule):\n    \"\"\"Inconsistent column references in ``GROUP BY/ORDER BY`` clauses.\n    | **Anti-pattern**\n    | A mix of implicit and explicit column references are used in a ``GROUP BY``\n    | clause.\n    .. code-block:: sql\n       :force:\n        SELECT\n            foo,\n            bar,\n            sum(baz) AS sum_value\n        FROM fake_table\n        GROUP BY\n            foo, 2;\n        -- The same also applies to column\n        -- references in ORDER BY clauses.\n        SELECT\n            foo,\n            bar\n        FROM fake_table\n        ORDER BY\n            1, bar;\n    | **Best practice**\n    | Reference all ``GROUP BY/ORDER BY`` columns either by name or by position.\n    .. code-block:: sql\n       :force:\n        -- GROUP BY: Explicit\n        SELECT\n            foo,\n            bar,\n            sum(baz) AS sum_value\n        FROM fake_table\n        GROUP BY\n            foo, bar;\n        -- ORDER BY: Explicit\n        SELECT\n            foo,\n            bar\n        FROM fake_table\n        ORDER BY\n            foo, bar;\n        -- GROUP BY: Implicit\n        SELECT\n            foo,\n            bar,\n            sum(baz) AS sum_value\n        FROM fake_table\n        GROUP BY\n            1, 2;\n        -- ORDER BY: Implicit\n        SELECT\n            foo,\n            bar\n        FROM fake_table\n        ORDER BY\n            1, 2;\n    \"\"\"\n    config_keywords = [\"group_by_and_order_by_style\"]\n    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Inconsistent column references in GROUP BY/ORDER BY clauses.\"\"\"\n        # Config type hints\n        self.group_by_and_order_by_style: str\n        # We only care about GROUP BY/ORDER BY clauses.\n        if not context.segment.is_type(\"groupby_clause\", \"orderby_clause\"):\n            return None\n        # Look at child segments and map column references to either the implict or\n        # explicit category.\n        # N.B. segment names are used as the numeric literal type is 'raw', so best to\n        # be specific with the name.\n        column_reference_category_map = {\n            \"ColumnReferenceSegment\": \"explicit\",\n            \"ExpressionSegment\": \"explicit\",\n            \"numeric_literal\": \"implicit\",\n        }\n        column_reference_category_set = {\n            column_reference_category_map[segment.name]\n            for segment in context.segment.segments\n            if segment.name in column_reference_category_map\n        }\n        # If there are no column references then just return\n        if not column_reference_category_set:\n            return LintResult(memory=context.memory)\n        if self.group_by_and_order_by_style == \"consistent\":\n            # If consistent naming then raise lint error if either:\n            if len(column_reference_category_set) > 1:\n                # 1. Both implicit and explicit column references are found in the same\n                # clause.\n                return LintResult(\n                    anchor=context.segment,\n                    memory=context.memory,\n                )\n            else:\n                # 2. A clause is found to contain column name references that\n                #    contradict the precedent set in earlier clauses.\n                current_group_by_order_by_convention = (\n                    column_reference_category_set.pop()\n                )\n                prior_group_by_order_by_convention = context.memory.get(\n                    \"prior_group_by_order_by_convention\"\n                )\n                if prior_group_by_order_by_convention and (\n                    prior_group_by_order_by_convention\n                    != current_group_by_order_by_convention\n                ):\n                    return LintResult(\n                        anchor=context.segment,\n                        memory=context.memory,\n                    )\n                context.memory[\n                    \"prior_group_by_order_by_convention\"\n                ] = current_group_by_order_by_convention\n        else:\n            # If explicit or implicit naming then raise lint error\n            # if the opposite reference type is detected.\n            if any(\n                category != self.group_by_and_order_by_style\n                for category in column_reference_category_set\n            ):\n                return LintResult(\n                    anchor=context.segment,\n                    memory=context.memory,\n                )\n        # Return memory for later clauses.\n        return LintResult(memory=context.memory)",
        "file_path": "src/sqlfluff/rules/L054.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 10,
        "score": 0.4925435185432434,
        "content": "\"\"\"Implementation of Rule L029.\"\"\"\nfrom typing import Optional\nfrom sqlfluff.core.rules.base import BaseRule, LintResult, RuleContext\nfrom sqlfluff.core.rules.doc_decorators import document_configuration\nfrom sqlfluff.rules.L014 import identifiers_policy_applicable\n@document_configuration\nclass Rule_L029(BaseRule):\n    \"\"\"Keywords should not be used as identifiers.\n    | **Anti-pattern**\n    | In this example, ``SUM`` (built-in function) is used as an alias.\n    .. code-block:: sql\n        SELECT\n            sum.a\n        FROM foo AS sum\n    | **Best practice**\n    |  Avoid keywords as the name of an alias.\n    .. code-block:: sql\n        SELECT\n            vee.a\n        FROM foo AS vee\n    \"\"\"\n    config_keywords = [\"unquoted_identifiers_policy\", \"quoted_identifiers_policy\"]\n    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Keywords should not be used as identifiers.\"\"\"\n        if (\n            (\n                context.segment.name == \"naked_identifier\"\n                and identifiers_policy_applicable(\n                    self.unquoted_identifiers_policy,  # type: ignore\n                    context.parent_stack,\n                )\n                and (\n                    context.segment.raw.upper()\n                    in context.dialect.sets(\"unreserved_keywords\")\n                )\n            )\n        ) or (\n            (\n                context.segment.name == \"quoted_identifier\"\n                and identifiers_policy_applicable(\n                    self.quoted_identifiers_policy, context.parent_stack  # type: ignore\n                )\n                and (\n                    context.segment.raw.upper()[1:-1]\n                    in context.dialect.sets(\"unreserved_keywords\")\n                    or context.segment.raw.upper()[1:-1]\n                    in context.dialect.sets(\"reserved_keywords\")\n                )\n            )\n        ):\n            return LintResult(anchor=context.segment)\n        else:\n            return None",
        "file_path": "src/sqlfluff/rules/L029.py",
        "chunk_index": 0,
        "metadata": {}
      }
    ]
  },
  "sqlfluff__sqlfluff-1733": {
    "query": "Extra space when first field moved to new line in a WITH statement\nNote, the query below uses a `WITH` statement. If I just try to fix the SQL within the CTE, this works fine.\r\n\r\nGiven the following SQL:\r\n\r\n```sql\r\nWITH example AS (\r\n    SELECT my_id,\r\n        other_thing,\r\n        one_more\r\n    FROM\r\n        my_table\r\n)\r\n\r\nSELECT *\r\nFROM example\r\n```\r\n\r\n## Expected Behaviour\r\n\r\nafter running `sqlfluff fix` I'd expect (`my_id` gets moved down and indented properly):\r\n\r\n```sql\r\nWITH example AS (\r\n    SELECT\r\n        my_id,\r\n        other_thing,\r\n        one_more\r\n    FROM\r\n        my_table\r\n)\r\n\r\nSELECT *\r\nFROM example\r\n```\r\n\r\n## Observed Behaviour\r\n\r\nafter running `sqlfluff fix` we get (notice that `my_id` is indented one extra space)\r\n\r\n```sql\r\nWITH example AS (\r\n    SELECT\r\n         my_id,\r\n        other_thing,\r\n        one_more\r\n    FROM\r\n        my_table\r\n)\r\n\r\nSELECT *\r\nFROM example\r\n```\r\n\r\n## Steps to Reproduce\r\n\r\nNoted above. Create a file with the initial SQL and fun `sqfluff fix` on it.\r\n\r\n## Dialect\r\n\r\nRunning with default config.\r\n\r\n## Version\r\nInclude the output of `sqlfluff --version` along with your Python version\r\n\r\nsqlfluff, version 0.7.0\r\nPython 3.7.5\r\n\r\n## Configuration\r\n\r\nDefault config.\r\n\n",
    "method": "sliding",
    "top_k": 10,
    "results": [
      {
        "rank": 1,
        "score": 0.691688060760498,
        "content": "\"\"\"Implementation of Rule L018.\"\"\"\nfrom sqlfluff.core.parser import NewlineSegment, WhitespaceSegment\nfrom sqlfluff.core.rules.base import BaseRule, LintFix, LintResult, RuleContext\nfrom sqlfluff.core.rules.doc_decorators import document_fix_compatible\n@document_fix_compatible\nclass Rule_L018(BaseRule):\n    \"\"\"WITH clause closing bracket should be aligned with WITH keyword.\n    | **Anti-pattern**\n    | The • character represents a space.\n    | In this example, the closing bracket is not aligned with WITH keyword.\n    .. code-block:: sql\n       :force:\n        WITH zoo AS (\n            SELECT a FROM foo\n        ••••)\n        SELECT * FROM zoo\n    | **Best practice**\n    | Remove the spaces to align the WITH keyword with the closing bracket.\n    .. code-block:: sql\n        WITH zoo AS (\n            SELECT a FROM foo\n        )\n        SELECT * FROM zoo\n    \"\"\"\n    _works_on_unparsable = False\n    config_keywords = [\"tab_space_size\"]\n    def _eval(self, context: RuleContext) -> LintResult:\n        \"\"\"WITH clause closing bracket should be aligned with WITH keyword.\n        Look for a with clause and evaluate the position of closing brackets.\n        \"\"\"\n        # We only trigger on start_bracket (open parenthesis)\n        if context.segment.is_type(\"with_compound_statement\"):\n            raw_stack_buff = list(context.raw_stack)\n            # Look for the with keyword\n            for seg in context.segment.segments:\n                if seg.name.lower() == \"with\":\n                    seg_line_no = seg.pos_marker.line_no\n                    break\n            else:  # pragma: no cover\n                # This *could* happen if the with statement is unparsable,\n                # in which case then the user will have to fix that first.\n                if any(s.is_type(\"unparsable\") for s in context.segment.segments):\n                    return LintResult()\n                # If it's parsable but we still didn't find a with, then\n                # we should raise that.\n                raise RuntimeError(\"Didn't find WITH keyword!\")\n            def indent_size_up_to(segs):\n                seg_buff = []\n                # Get any segments running up to the WITH\n                for elem in reversed(segs):\n                    if elem.is_type(\"newline\"):\n                        break\n                    elif elem.is_meta:\n                        continue\n                    else:\n                        seg_buff.append(elem)\n                # reverse the indent if we have one\n                if seg_buff:\n                    seg_buff = list(reversed(seg_buff))\n                indent_str = \"\".join(seg.raw for seg in seg_buff).replace(\n                    \"\\t\", \" \" * self.tab_space_size\n                )\n                indent_size = len(indent_str)\n                return indent_size, indent_str\n            balance = 0\n            with_indent, with_indent_str = indent_size_up_to(raw_stack_buff)\n            for seg in context.segment.iter_segments(\n                expanding=[\"common_table_expression\", \"bracketed\"], pass_through=True\n            ):\n                if seg.name == \"start_bracket\":\n                    balance += 1\n                elif seg.name == \"end_bracket\":\n                    balance -= 1\n                    if balance == 0:\n                        closing_bracket_indent, _ = indent_size_up_to(raw_stack_buff)\n                        indent_diff = closing_bracket_indent - with_indent\n                        # Is indent of closing bracket not the same as\n                        # indent of WITH keyword.\n                        if seg.pos_marker.line_no == seg_line_no:\n                            # Skip if it's the one-line version. That's ok\n                            pass\n                        elif indent_diff < 0:\n                            return LintResult(\n                                anchor=seg,\n                                fixes=[\n                                    LintFix(\n                                        \"create\",\n                                        seg,\n                                        WhitespaceSegment(\" \" * (-indent_diff)),\n                                    )\n                                ],\n                            )\n                        elif indent_diff > 0:\n                            # Is it all whitespace before the bracket on this line?\n                            prev_segs_on_line = [\n                                elem\n                                for elem in context.segment.iter_segments(\n                                    expanding=[\"common_table_expression\", \"bracketed\"],\n                                    pass_through=True,\n                                )\n                                if elem.pos_marker.line_no == seg.pos_marker.line_no\n                                and elem.pos_marker.line_pos < seg.pos_marker.line_pos\n                            ]\n                            if all(\n                                elem.is_type(\"whitespace\") for elem in prev_segs_on_line\n                            ):\n                                # We can move it back, it's all whitespace\n                                fixes = [\n                                    LintFix(\n                                        \"create\",\n                                        seg,\n                                        [WhitespaceSegment(with_indent_str)],\n                                    )\n                                ] + [\n                                    LintFix(\"delete\", elem)\n                                    for elem in prev_segs_on_line\n                                ]\n                            else:\n                                # We have to move it to a newline\n                                fixes = [\n                                    LintFix(\n                                        \"create\",\n                                        seg,\n                                        [\n                                            NewlineSegment(),\n                                            WhitespaceSegment(with_indent_str),\n                                        ],\n                                    )\n                                ]\n                            return LintResult(anchor=seg, fixes=fixes)\n                else:\n                    raw_stack_buff.append(seg)\n        return LintResult()",
        "file_path": "src/sqlfluff/rules/L018.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 2,
        "score": 0.6628550291061401,
        "content": "\"\"\"Implementation of Rule L004.\"\"\"\nfrom sqlfluff.core.parser import WhitespaceSegment\nfrom sqlfluff.core.rules.base import BaseRule, LintResult, LintFix, RuleContext\nfrom sqlfluff.core.rules.doc_decorators import (\n    document_fix_compatible,\n    document_configuration,\n)\n@document_fix_compatible\n@document_configuration\nclass Rule_L004(BaseRule):\n    \"\"\"Incorrect indentation type.\n    Note 1: spaces are only fixed to tabs if the number of spaces in the\n    indent is an integer multiple of the tab_space_size config.\n    Note 2: fixes are only applied to indents at the start of a line. Indents\n    after other text on the same line are not fixed.\n    | **Anti-pattern**\n    | Using tabs instead of spaces when indent_unit config set to spaces (default).\n    .. code-block:: sql\n       :force:\n        select\n        ••••a,\n        →   b\n        from foo\n    | **Best practice**\n    | Change the line to use spaces only.\n    .. code-block:: sql\n       :force:\n        select\n        ••••a,\n        ••••b\n        from foo\n    \"\"\"\n    config_keywords = [\"indent_unit\", \"tab_space_size\"]\n    # TODO fix indents after text: https://github.com/sqlfluff/sqlfluff/pull/590#issuecomment-739484190\n    def _eval(self, context: RuleContext) -> LintResult:\n        \"\"\"Incorrect indentation found in file.\"\"\"\n        # Config type hints\n        self.tab_space_size: int\n        self.indent_unit: str\n        tab = \"\\t\"\n        space = \" \"\n        correct_indent = (\n            space * self.tab_space_size if self.indent_unit == \"space\" else tab\n        )\n        wrong_indent = (\n            tab if self.indent_unit == \"space\" else space * self.tab_space_size\n        )\n        if (\n            context.segment.is_type(\"whitespace\")\n            and wrong_indent in context.segment.raw\n        ):\n            fixes = []\n            description = \"Incorrect indentation type found in file.\"\n            edit_indent = context.segment.raw.replace(wrong_indent, correct_indent)\n            # Ensure that the number of space indents is a multiple of tab_space_size\n            # before attempting to convert spaces to tabs to avoid mixed indents\n            # unless we are converted tabs to spaces (indent_unit = space)\n            if (\n                (\n                    self.indent_unit == \"space\"\n                    or context.segment.raw.count(space) % self.tab_space_size == 0\n                )\n                # Only attempt a fix at the start of a newline for now\n                and (\n                    len(context.raw_stack) == 0\n                    or context.raw_stack[-1].is_type(\"newline\")\n                )\n            ):\n                fixes = [\n                    LintFix(\n                        \"edit\",\n                        context.segment,\n                        WhitespaceSegment(raw=edit_indent),\n                    )\n                ]\n            elif not (\n                len(context.raw_stack) == 0 or context.raw_stack[-1].is_type(\"newline\")\n            ):\n                # give a helpful message if the wrong indent has been found and is not at the start of a newline\n                description += (\n                    \" The indent occurs after other text, so a manual fix is needed.\"\n                )\n            else:\n                # If we get here, the indent_unit is tabs, and the number of spaces is not a multiple of tab_space_size\n                description += \" The number of spaces is not a multiple of tab_space_size, so a manual fix is needed.\"\n            return LintResult(\n                anchor=context.segment, fixes=fixes, description=description\n            )\n        return LintResult()",
        "file_path": "src/sqlfluff/rules/L004.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 3,
        "score": 0.6565090417861938,
        "content": "\"\"\"Implementation of Rule L003.\"\"\"\nfrom typing import List, Optional, Sequence, Tuple\nfrom sqlfluff.core.parser import WhitespaceSegment\nfrom sqlfluff.core.parser.segments import BaseSegment, RawSegment\nfrom sqlfluff.core.rules.base import BaseRule, LintResult, LintFix, RuleContext\nfrom sqlfluff.core.rules.doc_decorators import (\n    document_fix_compatible,\n    document_configuration,\n)\nfrom sqlfluff.core.templaters import TemplatedFile\n@document_fix_compatible\n@document_configuration\nclass Rule_L003(BaseRule):\n    \"\"\"Indentation not consistent with previous lines.\n    Note:\n        This rule used to be _\"Indentation length is not a multiple\n        of `tab_space_size`\"_, but was changed to be much smarter.\n    | **Anti-pattern**\n    | The • character represents a space.\n    | In this example, the third line contains five spaces instead of four.\n    .. code-block:: sql\n       :force:\n        SELECT\n        ••••a,\n        •••••b\n        FROM foo\n    | **Best practice**\n    | Change the indentation to use a multiple of four spaces.\n    .. code-block:: sql\n       :force:\n        SELECT\n        ••••a,\n        ••••b\n        FROM foo\n    \"\"\"\n    _works_on_unparsable = False\n    _ignore_types: List[str] = [\"script_content\"]\n    config_keywords = [\"tab_space_size\", \"indent_unit\"]\n    @staticmethod\n    def _make_indent(\n        num: int = 1, tab_space_size: int = 4, indent_unit: str = \"space\"\n    ) -> str:\n        if indent_unit == \"tab\":\n            base_unit = \"\\t\"\n        elif indent_unit == \"space\":\n            base_unit = \" \" * tab_space_size\n        else:\n            raise ValueError(\n                f\"Parameter indent_unit has unexpected value: `{indent_unit}`. Expected `tab` or `space`.\"\n            )\n        return base_unit * num\n    @staticmethod\n    def _indent_size(segments: Sequence[RawSegment], tab_space_size: int = 4) -> int:\n        indent_size = 0\n        for elem in segments:\n            raw = elem.raw\n            # convert to spaces for convenience (and hanging indents)\n            raw = raw.replace(\"\\t\", \" \" * tab_space_size)\n            indent_size += len(raw)\n        return indent_size\n    @classmethod\n    def _reorder_raw_stack(\n        cls,\n        raw_stack: Tuple[RawSegment, ...],\n        templated_file: Optional[TemplatedFile],\n    ) -> Tuple[RawSegment, ...]:\n        \"\"\"Reorder raw_stack to simplify indentation logic.\n        Context: The indentation logic was mostly designed to work with normal\n        segment types. Templating introduces additional segments into the parse\n        tree, often in the \"wrong\" place with respect to the indentation logic,\n        for example, where do indent/dedent segments appear with respect to the\n        segments that trigger indent/dedent behavior? This function reorders\n        nodes locally (i.e. only within L003) to get the desired behavior.\n        \"\"\"\n        def segment_info(idx: int) -> Tuple[str, Optional[str]]:\n            \"\"\"Helper function for sort_current_line().\"\"\"\n            seg = current_line[idx]\n            return seg.type, cls._get_element_template_info(seg, templated_file)\n        def move_indent_before_templated() -> None:\n            \"\"\"Swap position of template and indent segment if code follows.\n            This allows for correct indentation of templated table names in\n            \"FROM\", for example:\n            SELECT brand\n            FROM\n                {{ product }}\n            \"\"\"\n            for idx in range(2, len(current_line)):\n                if (\n                    segment_info(idx - 2)\n                    == (\n                        \"placeholder\",\n                        \"templated\",\n                    )\n                    and segment_info(idx - 1) == (\"indent\", None)\n                    and segment_info(idx) == (\"raw\", None)\n                ):\n                    current_line[idx - 2], current_line[idx - 1] = (\n                        current_line[idx - 1],\n                        current_line[idx - 2],\n                    )\n        # Break raw_stack into lines.\n        lines = []\n        current_line = []\n        for elem in raw_stack:\n            if not elem.is_type(\"newline\"):\n                current_line.append(elem)\n            else:\n                move_indent_before_templated()\n                current_line.append(elem)\n                lines.append(current_line)\n                current_line = []\n        if current_line:\n            move_indent_before_templated()\n            lines.append(current_line)\n        new_raw_stack = [s for line in lines for s in line]\n        return tuple(new_raw_stack)\n    @classmethod\n    def _process_raw_stack(\n        cls,\n        raw_stack: Tuple[RawSegment, ...],\n        memory: dict = None,\n        tab_space_size: int = 4,\n        templated_file: Optional[TemplatedFile] = None,\n    ) -> dict:\n        \"\"\"Take the raw stack, split into lines and evaluate some stats.\"\"\"\n        raw_stack = cls._reorder_raw_stack(raw_stack, templated_file)\n        indent_balance = 0\n        line_no = 1\n        in_indent = True\n        indent_buffer: List[RawSegment] = []\n        line_buffer: List[RawSegment] = []\n        result_buffer = {}\n        indent_size = 0\n        line_indent_stack: List[int] = []\n        this_indent_balance = 0\n        clean_indent = False\n        hanger_pos = None\n        for elem in raw_stack:\n            line_buffer.append(elem)\n            # Pin indent_balance to above zero\n            if indent_balance < 0:\n                indent_balance = 0\n            if elem.is_type(\"newline\"):\n                result_buffer[line_no] = {\n                    \"line_no\": line_no,\n                    # Using slicing to copy line_buffer here to be py2 compliant\n                    \"line_buffer\": line_buffer[:],\n                    \"indent_buffer\": indent_buffer,\n                    \"indent_size\": indent_size,\n                    # Indent balance is the indent at the start of the first content\n                    \"indent_balance\": this_indent_balance,\n                    \"hanging_indent\": hanger_pos if line_indent_stack else None,\n                    # Clean indent is true if the line *ends* with an indent\n                    # or has an indent in the initial whitespace.\n                    \"clean_indent\": clean_indent,\n                }\n                line_no += 1\n                indent_buffer = []\n                line_buffer = []\n                indent_size = 0\n                in_indent = True\n                line_indent_stack = []\n                hanger_pos = None\n                # Assume an unclean indent, but if the last line\n                # ended with an indent then we might be ok.\n                clean_indent = False\n                # Was there an indent after the last code element of the previous line?\n                for search_elem in reversed(result_buffer[line_no - 1][\"line_buffer\"]):  # type: ignore\n                    if not search_elem.is_code and not search_elem.is_meta:\n                        continue\n                    elif search_elem.is_meta and search_elem.indent_val > 0:\n                        clean_indent = True\n                    break\n            elif in_indent:\n                if elem.is_type(\"whitespace\"):\n                    indent_buffer.append(elem)\n                elif elem.is_meta and elem.indent_val != 0:  # type: ignore\n                    indent_balance += elem.indent_val  # type: ignore\n                    if elem.indent_val > 0:  # type: ignore\n                        # a \"clean\" indent is one where it contains\n                        # an increase in indentation? Can't quite\n                        # remember the logic here. Let's go with that.\n                        clean_indent = True\n                else:\n                    in_indent = False\n                    this_indent_balance = indent_balance\n                    indent_size = cls._indent_size(\n                        indent_buffer, tab_space_size=tab_space_size\n                    )\n            elif elem.is_meta and elem.indent_val != 0:  # type: ignore\n                indent_balance += elem.indent_val  # type: ignore\n                if elem.indent_val > 0:  # type: ignore\n                    # Keep track of the indent at the last ... indent\n                    line_indent_stack.append(\n                        cls._indent_size(line_buffer, tab_space_size=tab_space_size)\n                    )\n                    hanger_pos = None\n                else:\n                    # this is a dedent, we could still have a hanging indent,\n                    # but only if there's enough on the stack\n                    if line_indent_stack:\n                        line_indent_stack.pop()\n            elif elem.is_code:\n                if hanger_pos is None:\n                    hanger_pos = cls._indent_size(\n                        line_buffer[:-1], tab_space_size=tab_space_size\n                    )\n            # If we hit the trigger element, stop processing.\n            if memory and elem is memory[\"trigger\"]:\n                break\n        # If we get to the end, and still have a buffer, add it on\n        if line_buffer:\n            result_buffer[line_no] = {\n                \"line_no\": line_no,\n                \"line_buffer\": line_buffer,\n                \"indent_buffer\": indent_buffer,\n                \"indent_size\": indent_size,\n                \"indent_balance\": this_indent_balance,\n                \"hanging_indent\": line_indent_stack.pop()\n                if line_indent_stack\n                else None,\n                \"clean_indent\": clean_indent,\n            }\n        return result_buffer\n    def _coerce_indent_to(\n        self,\n        desired_indent: str,\n        current_indent_buffer: Tuple[RawSegment, ...],\n        current_anchor: BaseSegment,\n    ) -> List[LintFix]:\n        \"\"\"Generate fixes to make an indent a certain size.\"\"\"\n        # If there shouldn't be an indent at all, just delete.\n        if len(desired_indent) == 0:\n            fixes = [LintFix(\"delete\", elem) for elem in current_indent_buffer]\n        # If we don't have any indent and we should, then add a single\n        elif len(\"\".join(elem.raw for elem in current_indent_buffer)) == 0:\n            fixes = [\n                LintFix(\n                    \"create\",\n                    current_anchor,\n                    WhitespaceSegment(\n                        raw=desired_indent,\n                    ),\n                )\n            ]\n        # Otherwise edit the first element to be the right size\n        else:\n            # Edit the first element of this line's indent.\n            fixes = [\n                LintFix(\n                    \"edit\",\n                    current_indent_buffer[0],\n                    WhitespaceSegment(\n                        raw=desired_indent,\n                    ),\n                )\n            ]\n        return fixes\n    @staticmethod\n    def _strip_buffers(line_dict: dict) -> dict:\n        \"\"\"Strip a line dict of buffers for logging.\"\"\"\n        return {\n            key: line_dict[key]\n            for key in line_dict\n            if key not in (\"line_buffer\", \"indent_buffer\")\n        }\n    @classmethod\n    def _is_last_segment(\n        cls,\n        segment: BaseSegment,\n        memory: dict,\n        parent_stack: Tuple[BaseSegment, ...],\n        siblings_post: Tuple[BaseSegment, ...],\n    ) -> bool:\n        \"\"\"Returns True if 'segment' is the very last node in the parse tree.\"\"\"\n        if siblings_post:\n            # We have subsequent siblings. Not finished.\n            return False\n        elif parent_stack:\n            # No subsequent siblings. Our parent is finished.\n            memory[\"finished\"].add(parent_stack[-1])\n        if segment.segments:\n            # We have children. Not finished.\n            return False\n        # We have no subsequent siblings or children. If all our parents are\n        # finished, the whole parse tree is finished.\n        for parent in parent_stack:\n            if parent not in memory[\"finished\"]:\n                return False\n        return True\n    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Indentation not consistent with previous lines.\n        To set the default tab size, set the `tab_space_size` value\n        in the appropriate configuration.\n        We compare each line (first non-whitespace element of the\n        line), with the indentation of previous lines. The presence\n        (or lack) of indent or dedent meta-characters indicate whether\n        the indent is appropriate.\n        - Any line is assessed by the indent level at the first non\n          whitespace element.\n        - Any increase in indentation may be _up to_ the number of\n          indent characters.\n        - Any line must be in line with the previous line which had\n          the same indent balance at its start.\n        - Apart from \"whole\" indents, a \"hanging\" indent is possible\n          if the line starts in line with either the indent of the\n          previous line or if it starts at the same indent as the *last*\n          indent meta segment in the previous line.\n        \"\"\"\n        # Config type hints\n        self.tab_space_size: int\n        self.indent_unit: str\n        raw_stack = context.raw_stack\n        # We ignore certain types (e.g. non-SQL scripts in functions)\n        # so check if on ignore list\n        if context.segment.type in self._ignore_types:\n            return LintResult()\n        for parent in context.parent_stack:\n            if parent.type in self._ignore_types:\n                return LintResult()\n        # Memory keeps track of what we've seen\n        if not context.memory:\n            memory: dict = {\n                # in_indent keeps track of whether we're in an indent right now\n                \"in_indent\": True,\n                # problem_lines keeps track of lines with problems so that we\n                # don't compare to them.\n                \"problem_lines\": [],\n                # hanging_lines keeps track of hanging lines so that we don't\n                # compare to them when assessing indent.\n                \"hanging_lines\": [],\n                # comment_lines keeps track of lines which are all comment.\n                \"comment_lines\": [],\n                # segments we've seen the last child of\n                \"finished\": set(),\n                # First non-whitespace node on a line.\n                \"trigger\": None,\n            }\n        else:\n            memory = context.memory\n        if context.segment.is_type(\"newline\"):\n            memory[\"in_indent\"] = True\n        elif memory[\"in_indent\"]:\n            if context.segment.is_type(\"whitespace\"):\n                # it's whitespace, carry on\n                pass\n            elif context.segment.segments or (context.segment.is_meta and context.segment.indent_val != 0):  # type: ignore\n                # it's not a raw segment or placeholder. Carry on.\n                pass\n            else:\n                memory[\"in_indent\"] = False\n                # we're found a non-whitespace element. This is our trigger,\n                # which we'll handle after this if-statement\n                memory[\"trigger\"] = context.segment\n        else:\n            # Not in indent and not a newline, don't trigger here.\n            pass\n        # Is this the last segment? If so, need to \"flush\" any leftovers.\n        is_last = self._is_last_segment(\n            context.segment, memory, context.parent_stack, context.siblings_post\n        )\n        if not context.segment.is_type(\"newline\") and not is_last:\n            # We only process complete lines or on the very last segment\n            # (since there may not be a newline on the very last line)..\n            return LintResult(memory=memory)\n        if raw_stack and raw_stack[-1] is not context.segment:\n            raw_stack = raw_stack + (context.segment,)\n        res = self._process_raw_stack(\n            raw_stack,\n            memory,\n            tab_space_size=self.tab_space_size,\n            templated_file=context.templated_file,\n        )\n        if res:\n            # Saw a newline or end of parse tree. Is the current line empty?\n            trigger_segment = memory[\"trigger\"]\n            if trigger_segment:\n                # Not empty. Process it.\n                result = self._process_current_line(res, memory)\n                if context.segment.is_type(\"newline\"):\n                    memory[\"trigger\"] = None\n                return result\n        return LintResult(memory=memory)\n    def _process_current_line(self, res: dict, memory: dict) -> LintResult:\n        \"\"\"Checks indentation of one line of code, returning a LintResult.\n        The _eval() function calls it for the current line of code:\n        - When passed a newline segment (thus ending a line)\n        - When passed the *final* segment in the entire parse tree (which may\n          not be a newline)\n        \"\"\"\n        this_line_no = max(res.keys())\n        this_line = res.pop(this_line_no)\n        self.logger.debug(\n            \"Evaluating line #%s. %s\",\n            this_line_no,\n            # Don't log the line or indent buffer, it's too noisy.\n            self._strip_buffers(this_line),\n        )\n        trigger_segment = memory[\"trigger\"]\n        # Is this line just comments? (Disregard trailing newline if present.)\n        check_comment_line = this_line[\"line_buffer\"]\n        if check_comment_line and all(\n            seg.is_type(\n                \"whitespace\", \"comment\", \"indent\"  # dedent is a subtype of indent\n            )\n            for seg in check_comment_line\n        ):\n            # Comment line, deal with it later.\n            memory[\"comment_lines\"].append(this_line_no)\n            self.logger.debug(\"    Comment Line. #%s\", this_line_no)\n            return LintResult(memory=memory)\n        # Is it a hanging indent?\n        # Find last meaningful line indent.\n        last_code_line = None\n        for k in sorted(res.keys(), reverse=True):\n            if any(seg.is_code for seg in res[k][\"line_buffer\"]):\n                last_code_line = k\n                break\n        if len(res) > 0 and last_code_line:\n            last_line_hanger_indent = res[last_code_line][\"hanging_indent\"]\n            # Let's just deal with hanging indents here.\n            if (\n                # NB: Hangers are only allowed if there was content after the last\n                # indent on the previous line. Otherwise it's just an indent.\n                this_line[\"indent_size\"] == last_line_hanger_indent\n                # Or they're if the indent balance is the same and the indent is the\n                # same AND the previous line was a hanger\n                or (\n                    this_line[\"indent_size\"] == res[last_code_line][\"indent_size\"]\n                    and this_line[\"indent_balance\"]\n                    == res[last_code_line][\"indent_balance\"]\n                    and last_code_line in memory[\"hanging_lines\"]\n                )\n            ) and (\n                # There MUST also be a non-zero indent. Otherwise we're just on the baseline.\n                this_line[\"indent_size\"]\n                > 0\n            ):\n                # This is a HANGER\n                memory[\"hanging_lines\"].append(this_line_no)\n                self.logger.debug(\"    Hanger Line. #%s\", this_line_no)\n                self.logger.debug(\n                    \"    Last Line: %s\", self._strip_buffers(res[last_code_line])\n                )\n                return LintResult(memory=memory)\n        # Is this an indented first line?\n        elif len(res) == 0:\n            if this_line[\"indent_size\"] > 0:\n                self.logger.debug(\"    Indented First Line. #%s\", this_line_no)\n                return LintResult(\n                    anchor=trigger_segment,\n                    memory=memory,\n                    description=\"First line has unexpected indent\",\n                    fixes=[\n                        LintFix(\"delete\", elem) for elem in this_line[\"indent_buffer\"]\n                    ],\n                )\n        # Assuming it's not a hanger, let's compare it to the other previous\n        # lines. We do it in reverse so that closer lines are more relevant.\n        for k in sorted(res.keys(), reverse=True):\n            # Is this a problem line?\n            if k in memory[\"problem_lines\"] + memory[\"hanging_lines\"]:\n                # Skip it if it is\n                continue\n            # Is this an empty line?\n            if not any(elem.is_code for elem in res[k][\"line_buffer\"]):\n                # Skip if it is\n                continue\n            # Work out the difference in indent\n            indent_diff = this_line[\"indent_balance\"] - res[k][\"indent_balance\"]\n            # If we're comparing to a previous, more deeply indented line, then skip and keep looking.\n            if indent_diff < 0:\n                continue\n            # Is the indent balance the same?\n            elif indent_diff == 0:\n                self.logger.debug(\"    [same indent balance] Comparing to #%s\", k)\n                if this_line[\"indent_size\"] != res[k][\"indent_size\"]:\n                    # Indents don't match even though balance is the same...\n                    memory[\"problem_lines\"].append(this_line_no)\n                    # Work out desired indent\n                    if res[k][\"indent_size\"] == 0:\n                        desired_indent = \"\"\n                    elif this_line[\"indent_size\"] == 0:\n                        desired_indent = self._make_indent(\n                            indent_unit=self.indent_unit,\n                            tab_space_size=self.tab_space_size,\n                        )\n                    else:\n                        # The previous indent.\n                        desired_indent = \"\".join(\n                            elem.raw for elem in res[k][\"indent_buffer\"]\n                        )\n                    # Make fixes\n                    fixes = self._coerce_indent_to(\n                        desired_indent=desired_indent,\n                        current_indent_buffer=this_line[\"indent_buffer\"],\n                        current_anchor=trigger_segment,\n                    )\n                    self.logger.debug(\n                        \"    !! Indentation does not match #%s. Fixes: %s\", k, fixes",
        "file_path": "src/sqlfluff/rules/L003.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 4,
        "score": 0.6433080434799194,
        "content": "\"\"\"Implementation of Rule L023.\"\"\"\nfrom typing import Optional, List\nfrom sqlfluff.core.parser import BaseSegment, WhitespaceSegment\nfrom sqlfluff.core.rules.base import BaseRule, LintFix, LintResult, RuleContext\nfrom sqlfluff.core.rules.doc_decorators import document_fix_compatible\n@document_fix_compatible\nclass Rule_L023(BaseRule):\n    \"\"\"Single whitespace expected after AS in WITH clause.\n    | **Anti-pattern**\n    .. code-block:: sql\n        WITH plop AS(\n            SELECT * FROM foo\n        )\n        SELECT a FROM plop\n    | **Best practice**\n    | The • character represents a space.\n    | Add a space after AS, to avoid confusing\n    | it for a function.\n    .. code-block:: sql\n       :force:\n        WITH plop AS•(\n            SELECT * FROM foo\n        )\n        SELECT a FROM plop\n    \"\"\"\n    expected_mother_segment_type = \"with_compound_statement\"\n    pre_segment_identifier = (\"name\", \"as\")\n    post_segment_identifier = (\"type\", \"bracketed\")\n    allow_newline = False\n    expand_children: Optional[List[str]] = [\"common_table_expression\"]\n    def _eval(self, context: RuleContext) -> Optional[List[LintResult]]:\n        \"\"\"Single whitespace expected in mother segment between pre and post segments.\"\"\"\n        error_buffer: List[LintResult] = []\n        if context.segment.is_type(self.expected_mother_segment_type):\n            last_code = None\n            mid_segs: List[BaseSegment] = []\n            for seg in context.segment.iter_segments(expanding=self.expand_children):\n                if seg.is_code:\n                    if (\n                        last_code\n                        and self.matches_target_tuples(\n                            last_code, [self.pre_segment_identifier]\n                        )\n                        and self.matches_target_tuples(\n                            seg, [self.post_segment_identifier]\n                        )\n                    ):\n                        # Do we actually have the right amount of whitespace?\n                        raw_inner = \"\".join(s.raw for s in mid_segs)\n                        if raw_inner != \" \" and not (\n                            self.allow_newline\n                            and any(s.name == \"newline\" for s in mid_segs)\n                        ):\n                            if not raw_inner:\n                                # There's nothing between. Just add a whitespace\n                                fixes = [\n                                    LintFix(\n                                        \"create\",\n                                        seg,\n                                        [WhitespaceSegment()],\n                                    )\n                                ]\n                            else:\n                                # Don't otherwise suggest a fix for now.\n                                # TODO: Enable more complex fixing here.\n                                fixes = None  # pragma: no cover\n                            error_buffer.append(\n                                LintResult(anchor=last_code, fixes=fixes)\n                            )\n                    mid_segs = []\n                    if not seg.is_meta:\n                        last_code = seg\n                else:\n                    mid_segs.append(seg)\n        return error_buffer or None",
        "file_path": "src/sqlfluff/rules/L023.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 5,
        "score": 0.621798574924469,
        "content": "\"\"\"This is an example of how to use the simple sqlfluff api.\"\"\"\nimport sqlfluff\n#  -------- LINTING ----------\nmy_bad_query = \"SeLEct  *, 1, blah as  fOO  from myTable\"\n# Lint the given string and get a list of violations found.\nresult = sqlfluff.lint(my_bad_query, dialect=\"bigquery\")\n# result =\n# [\n#     {\"code\": \"L010\", \"line_no\": 1, \"line_pos\": 1, \"description\": \"Keywords must be consistently upper case.\"}\n#     ...\n# ]\n#  -------- FIXING ----------\n# Fix the given string and get a string back which has been fixed.\nresult = sqlfluff.fix(my_bad_query, dialect=\"bigquery\")\n# result = 'SELECT  *, 1, blah AS  foo  FROM mytable\\n'\n# We can also fix just specific rules.\nresult = sqlfluff.fix(my_bad_query, rules=\"L010\")\n# result = 'SELECT  *, 1, blah AS  fOO  FROM myTable'\n# Or a subset of rules...\nresult = sqlfluff.fix(my_bad_query, rules=[\"L010\", \"L014\"])\n# result = 'SELECT  *, 1, blah AS  fOO  FROM mytable'\n#  -------- PARSING ----------\n# NOTE: sqlfluff is still in a relatively early phase of its\n# development and so until version 1.0.0 will offer no guarantee\n# that the names and structure of the objects returned by these\n# parse commands won't change between releases. Use with care\n# and keep updated with the changelog for the project for any\n# changes in this space.\nparsed = sqlfluff.parse(my_bad_query)\n# Get the structure of the query\nstructure = parsed.tree.to_tuple(show_raw=True, code_only=True)\n# structure = ('file', (('statement', (('select_statement', (('select_clause', (('keyword', 'SeLEct'), ...\n# Extract certain elements\nkeywords = [keyword.raw for keyword in parsed.tree.recursive_crawl(\"keyword\")]\n# keywords = ['SeLEct', 'as', 'from']\ntbl_refs = [tbl_ref.raw for tbl_ref in parsed.tree.recursive_crawl(\"table_reference\")]\n# tbl_refs == [\"myTable\"]",
        "file_path": "examples/01_basic_api_usage.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 6,
        "score": 0.6186068058013916,
        "content": "\"\"\"Implementation of Rule L002.\"\"\"\nfrom typing import Optional\nfrom sqlfluff.core.rules.base import BaseRule, LintResult, LintFix, RuleContext\nfrom sqlfluff.core.rules.doc_decorators import (\n    document_configuration,\n    document_fix_compatible,\n)\n@document_configuration\n@document_fix_compatible\nclass Rule_L002(BaseRule):\n    \"\"\"Mixed Tabs and Spaces in single whitespace.\n    This rule will fail if a single section of whitespace\n    contains both tabs and spaces.\n    | **Anti-pattern**\n    | The • character represents a space and the → character represents a tab.\n    | In this example, the second line contains two spaces and one tab.\n    .. code-block:: sql\n       :force:\n        SELECT\n        ••→a\n        FROM foo\n    | **Best practice**\n    | Change the line to use spaces only.\n    .. code-block:: sql\n       :force:\n        SELECT\n        ••••a\n        FROM foo\n    \"\"\"\n    config_keywords = [\"tab_space_size\"]\n    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Mixed Tabs and Spaces in single whitespace.\n        Only trigger from whitespace segments if they contain\n        multiple kinds of whitespace.\n        \"\"\"\n        # Config type hints\n        self.tab_space_size: int\n        if context.segment.is_type(\"whitespace\"):\n            if \" \" in context.segment.raw and \"\\t\" in context.segment.raw:\n                if len(context.raw_stack) == 0 or context.raw_stack[-1].is_type(\n                    \"newline\"\n                ):\n                    # We've got a single whitespace at the beginning of a line.\n                    # It's got a mix of spaces and tabs. Replace each tab with\n                    # a multiple of spaces\n                    return LintResult(\n                        anchor=context.segment,\n                        fixes=[\n                            LintFix(\n                                \"edit\",\n                                context.segment,\n                                context.segment.edit(\n                                    context.segment.raw.replace(\n                                        \"\\t\", \" \" * self.tab_space_size\n                                    )\n                                ),\n                            )\n                        ],\n                    )\n        return None",
        "file_path": "src/sqlfluff/rules/L002.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 7,
        "score": 0.6133211851119995,
        "content": "\"\"\"Implementation of Rule L001.\"\"\"\nfrom sqlfluff.core.rules.base import BaseRule, LintResult, LintFix, RuleContext\nfrom sqlfluff.core.rules.doc_decorators import document_fix_compatible\n@document_fix_compatible\nclass Rule_L001(BaseRule):\n    \"\"\"Unnecessary trailing whitespace.\n    | **Anti-pattern**\n    | The • character represents a space.\n    .. code-block:: sql\n       :force:\n        SELECT\n            a\n        FROM foo••\n    | **Best practice**\n    | Remove trailing spaces.\n    .. code-block:: sql\n        SELECT\n            a\n        FROM foo\n    \"\"\"\n    def _eval(self, context: RuleContext) -> LintResult:\n        \"\"\"Unnecessary trailing whitespace.\n        Look for newline segments, and then evaluate what\n        it was preceded by.\n        \"\"\"\n        # We only trigger on newlines\n        if (\n            context.segment.is_type(\"newline\")\n            and len(context.raw_stack) > 0\n            and context.raw_stack[-1].is_type(\"whitespace\")\n        ):\n            # If we find a newline, which is preceded by whitespace, then bad\n            deletions = []\n            idx = -1\n            while abs(idx) <= len(context.raw_stack) and context.raw_stack[idx].is_type(\n                \"whitespace\"\n            ):\n                deletions.append(context.raw_stack[idx])\n                idx -= 1\n            last_deletion_slice = deletions[-1].pos_marker.source_slice\n            # Check the raw source (before template expansion) immediately\n            # following the whitespace we want to delete. Often, what looks\n            # like trailing whitespace in rendered SQL is actually a line like:\n            # \"    {% for elem in elements %}\\n\", in which case the code is\n            # fine -- it's not trailing whitespace from a source code\n            # perspective.\n            if context.templated_file:\n                next_raw_slice = (\n                    context.templated_file.raw_slices_spanning_source_slice(\n                        slice(last_deletion_slice.stop, last_deletion_slice.stop)\n                    )\n                )\n                # If the next slice is literal, that means it's regular code, so\n                # it's safe to delete the trailing whitespace. If it's anything\n                # else, it's template code, so don't delete the whitespace because\n                # it's not REALLY trailing whitespace in terms of the raw source\n                # code.\n                if next_raw_slice[0].slice_type != \"literal\":\n                    return LintResult()\n            return LintResult(\n                anchor=deletions[-1],\n                fixes=[LintFix(\"delete\", d) for d in deletions],\n            )\n        return LintResult()",
        "file_path": "src/sqlfluff/rules/L001.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 8,
        "score": 0.6126976013183594,
        "content": "\"\"\"Implementation of Rule L022.\"\"\"\nfrom typing import Optional, List\nfrom sqlfluff.core.parser import NewlineSegment\nfrom sqlfluff.core.rules.base import BaseRule, LintFix, LintResult, RuleContext\nfrom sqlfluff.core.rules.doc_decorators import document_fix_compatible\n@document_fix_compatible\nclass Rule_L022(BaseRule):\n    \"\"\"Blank line expected but not found after CTE closing bracket.\n    | **Anti-pattern**\n    | There is no blank line after the CTE closing bracket. In queries with many\n    | CTEs this hinders readability.\n    .. code-block:: sql\n        WITH plop AS (\n            SELECT * FROM foo\n        )\n        SELECT a FROM plop\n    | **Best practice**\n    | Add a blank line.\n    .. code-block:: sql\n        WITH plop AS (\n            SELECT * FROM foo\n        )\n        SELECT a FROM plop\n    \"\"\"\n    config_keywords = [\"comma_style\"]\n    def _eval(self, context: RuleContext) -> Optional[List[LintResult]]:\n        \"\"\"Blank line expected but not found after CTE definition.\"\"\"\n        # Config type hints\n        self.comma_style: str\n        error_buffer = []\n        if context.segment.is_type(\"with_compound_statement\"):\n            # First we need to find all the commas, the end brackets, the\n            # things that come after that and the blank lines in between.\n            # Find all the closing brackets. They are our anchor points.\n            bracket_indices = []\n            expanded_segments = list(\n                context.segment.iter_segments(expanding=[\"common_table_expression\"])\n            )\n            for idx, seg in enumerate(expanded_segments):\n                if seg.is_type(\"bracketed\"):\n                    bracket_indices.append(idx)\n            # Work through each point and deal with it individually\n            for bracket_idx in bracket_indices:\n                forward_slice = expanded_segments[bracket_idx:]\n                seg_idx = 1\n                line_idx = 0\n                comma_seg_idx = 0\n                blank_lines = 0\n                comma_line_idx = None\n                line_blank = False\n                comma_style = None\n                line_starts = {}\n                comment_lines = []\n                self.logger.info(\n                    \"## CTE closing bracket found at %s, idx: %s. Forward slice: %.20r\",\n                    forward_slice[0].pos_marker,\n                    bracket_idx,\n                    \"\".join(elem.raw for elem in forward_slice),\n                )\n                # Work forward to map out the following segments.\n                while (\n                    forward_slice[seg_idx].is_type(\"comma\")\n                    or not forward_slice[seg_idx].is_code\n                ):\n                    if forward_slice[seg_idx].is_type(\"newline\"):\n                        if line_blank:\n                            # It's a blank line!\n                            blank_lines += 1\n                        line_blank = True\n                        line_idx += 1\n                        line_starts[line_idx] = seg_idx + 1\n                    elif forward_slice[seg_idx].is_type(\"comment\"):\n                        # Lines with comments aren't blank\n                        line_blank = False\n                        comment_lines.append(line_idx)\n                    elif forward_slice[seg_idx].is_type(\"comma\"):\n                        # Keep track of where the comma is.\n                        # We'll evaluate it later.\n                        comma_line_idx = line_idx\n                        comma_seg_idx = seg_idx\n                    seg_idx += 1\n                # Infer the comma style (NB this could be different for each case!)\n                if comma_line_idx is None:\n                    comma_style = \"final\"\n                elif line_idx == 0:\n                    comma_style = \"oneline\"\n                elif comma_line_idx == 0:\n                    comma_style = \"trailing\"\n                elif comma_line_idx == line_idx:\n                    comma_style = \"leading\"\n                else:\n                    comma_style = \"floating\"\n                # Readout of findings\n                self.logger.info(\n                    \"blank_lines: %s, comma_line_idx: %s. final_line_idx: %s, final_seg_idx: %s\",\n                    blank_lines,\n                    comma_line_idx,\n                    line_idx,\n                    seg_idx,\n                )\n                self.logger.info(\n                    \"comma_style: %r, line_starts: %r, comment_lines: %r\",\n                    comma_style,\n                    line_starts,\n                    comment_lines,\n                )\n                if blank_lines < 1:\n                    # We've got an issue\n                    self.logger.info(\"!! Found CTE without enough blank lines.\")\n                    # Based on the current location of the comma we insert newlines\n                    # to correct the issue.\n                    fix_type = \"create\"  # In most cases we just insert newlines.\n                    if comma_style == \"oneline\":\n                        # Here we respect the target comma style to insert at the relevant point.\n                        if self.comma_style == \"trailing\":\n                            # Add a blank line after the comma\n                            fix_point = forward_slice[comma_seg_idx + 1]\n                            # Optionally here, if the segment we've landed on is\n                            # whitespace then we REPLACE it rather than inserting.\n                            if forward_slice[comma_seg_idx + 1].is_type(\"whitespace\"):\n                                fix_type = \"edit\"\n                        elif self.comma_style == \"leading\":\n                            # Add a blank line before the comma\n                            fix_point = forward_slice[comma_seg_idx]\n                        # In both cases it's a double newline.\n                        num_newlines = 2\n                    else:\n                        # In the following cases we only care which one we're in\n                        # when comments don't get in the way. If they *do*, then\n                        # we just work around them.\n                        if not comment_lines or line_idx - 1 not in comment_lines:\n                            self.logger.info(\"Comment routines not applicable\")\n                            if comma_style in (\"trailing\", \"final\", \"floating\"):\n                                # Detected an existing trailing comma or it's a final CTE,\n                                # OR the comma isn't leading or trailing.\n                                # If the preceding segment is whitespace, replace it\n                                if forward_slice[seg_idx - 1].is_type(\"whitespace\"):\n                                    fix_point = forward_slice[seg_idx - 1]\n                                    fix_type = \"edit\"\n                                else:\n                                    # Otherwise add a single newline before the end content.\n                                    fix_point = forward_slice[seg_idx]\n                            elif comma_style == \"leading\":\n                                # Detected an existing leading comma.\n                                fix_point = forward_slice[comma_seg_idx]\n                        else:\n                            self.logger.info(\"Handling preceding comments\")\n                            offset = 1\n                            while line_idx - offset in comment_lines:\n                                offset += 1\n                            fix_point = forward_slice[\n                                line_starts[line_idx - (offset - 1)]\n                            ]\n                        # Note: There is an edge case where this isn't enough, if\n                        # comments are in strange places, but we'll catch them on\n                        # the next iteration.\n                        num_newlines = 1\n                    fixes = [\n                        LintFix(\n                            fix_type,\n                            fix_point,\n                            [NewlineSegment()] * num_newlines,\n                        )\n                    ]\n                    # Create a result, anchored on the start of the next content.\n                    error_buffer.append(\n                        LintResult(anchor=forward_slice[seg_idx], fixes=fixes)\n                    )\n        # Return the buffer if we have one.\n        return error_buffer or None",
        "file_path": "src/sqlfluff/rules/L022.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 9,
        "score": 0.6108560562133789,
        "content": "\"\"\"Implementation of Rule L005.\"\"\"\nfrom typing import Optional\nfrom sqlfluff.core.rules.base import BaseRule, LintResult, LintFix, RuleContext\nfrom sqlfluff.core.rules.doc_decorators import document_fix_compatible\n@document_fix_compatible\nclass Rule_L005(BaseRule):\n    \"\"\"Commas should not have whitespace directly before them.\n    Unless it's an indent. Trailing/leading commas are dealt with\n    in a different rule.\n    | **Anti-pattern**\n    | The • character represents a space.\n    | There is an extra space in line two before the comma.\n    .. code-block:: sql\n       :force:\n        SELECT\n            a•,\n            b\n        FROM foo\n    | **Best practice**\n    | Remove the space before the comma.\n    .. code-block:: sql\n        SELECT\n            a,\n            b\n        FROM foo\n    \"\"\"\n    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Commas should not have whitespace directly before them.\n        We need at least one segment behind us for this to work.\n        \"\"\"\n        if len(context.raw_stack) >= 1:\n            cm1 = context.raw_stack[-1]\n            if (\n                context.segment.is_type(\"comma\")\n                and cm1.is_type(\"whitespace\")\n                and cm1.pos_marker.line_pos > 1\n            ):\n                anchor = cm1\n                return LintResult(anchor=anchor, fixes=[LintFix(\"delete\", cm1)])\n        # Otherwise fine\n        return None",
        "file_path": "src/sqlfluff/rules/L005.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 10,
        "score": 0.6073628664016724,
        "content": "\"\"\"Implementation of Rule L039.\"\"\"\nfrom typing import List, Optional\nfrom sqlfluff.core.parser import WhitespaceSegment\nfrom sqlfluff.core.rules.base import BaseRule, LintFix, LintResult, RuleContext\nfrom sqlfluff.core.rules.doc_decorators import document_fix_compatible\n@document_fix_compatible\nclass Rule_L039(BaseRule):\n    \"\"\"Unnecessary whitespace found.\n    | **Anti-pattern**\n    .. code-block:: sql\n        SELECT\n            a,        b\n        FROM foo\n    | **Best practice**\n    | Unless an indent or preceding a comment, whitespace should\n    | be a single space.\n    .. code-block:: sql\n        SELECT\n            a, b\n        FROM foo\n    \"\"\"\n    def _eval(self, context: RuleContext) -> Optional[List[LintResult]]:\n        \"\"\"Unnecessary whitespace.\"\"\"\n        # For the given segment, lint whitespace directly within it.\n        prev_newline = True\n        prev_whitespace = None\n        violations = []\n        for seg in context.segment.segments:\n            if seg.is_type(\"newline\"):\n                prev_newline = True\n                prev_whitespace = None\n            elif seg.is_type(\"whitespace\"):\n                # This is to avoid indents\n                if not prev_newline:\n                    prev_whitespace = seg\n                prev_newline = False\n            elif seg.is_type(\"comment\"):\n                prev_newline = False\n                prev_whitespace = None\n            else:\n                if prev_whitespace:\n                    if prev_whitespace.raw != \" \":\n                        violations.append(\n                            LintResult(\n                                anchor=prev_whitespace,\n                                fixes=[\n                                    LintFix(\n                                        \"edit\",\n                                        prev_whitespace,\n                                        WhitespaceSegment(),\n                                    )\n                                ],\n                            )\n                        )\n                prev_newline = False\n                prev_whitespace = None\n        return violations or None",
        "file_path": "src/sqlfluff/rules/L039.py",
        "chunk_index": 0,
        "metadata": {}
      }
    ]
  },
  "sqlfluff__sqlfluff-1517": {
    "query": "\"Dropped elements in sequence matching\" when doubled semicolon\n## Expected Behaviour\r\nFrankly, I'm not sure whether it (doubled `;`) should be just ignored or rather some specific rule should be triggered.\r\n## Observed Behaviour\r\n```console\r\n(.venv) ?master ~/prod/_inne/sqlfluff> echo \"select id from tbl;;\" | sqlfluff lint -\r\nTraceback (most recent call last):\r\n  File \"/home/adam/prod/_inne/sqlfluff/.venv/bin/sqlfluff\", line 11, in <module>\r\n    load_entry_point('sqlfluff', 'console_scripts', 'sqlfluff')()\r\n  File \"/home/adam/prod/_inne/sqlfluff/.venv/lib/python3.9/site-packages/click/core.py\", line 1137, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/home/adam/prod/_inne/sqlfluff/.venv/lib/python3.9/site-packages/click/core.py\", line 1062, in main\r\n    rv = self.invoke(ctx)\r\n  File \"/home/adam/prod/_inne/sqlfluff/.venv/lib/python3.9/site-packages/click/core.py\", line 1668, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"/home/adam/prod/_inne/sqlfluff/.venv/lib/python3.9/site-packages/click/core.py\", line 1404, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"/home/adam/prod/_inne/sqlfluff/.venv/lib/python3.9/site-packages/click/core.py\", line 763, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/cli/commands.py\", line 347, in lint\r\n    result = lnt.lint_string_wrapped(sys.stdin.read(), fname=\"stdin\")\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/linter/linter.py\", line 789, in lint_string_wrapped\r\n    linted_path.add(self.lint_string(string, fname=fname, fix=fix))\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/linter/linter.py\", line 668, in lint_string\r\n    parsed = self.parse_string(in_str=in_str, fname=fname, config=config)\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/linter/linter.py\", line 607, in parse_string\r\n    return self.parse_rendered(rendered, recurse=recurse)\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/linter/linter.py\", line 313, in parse_rendered\r\n    parsed, pvs = cls._parse_tokens(\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/linter/linter.py\", line 190, in _parse_tokens\r\n    parsed: Optional[BaseSegment] = parser.parse(\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/parser/parser.py\", line 32, in parse\r\n    parsed = root_segment.parse(parse_context=ctx)\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/parser/segments/base.py\", line 821, in parse\r\n    check_still_complete(segments, m.matched_segments, m.unmatched_segments)\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/parser/helpers.py\", line 30, in check_still_complete\r\n    raise RuntimeError(\r\nRuntimeError: Dropped elements in sequence matching! 'select id from tbl;;' != ';'\r\n\r\n```\r\n## Steps to Reproduce\r\nRun \r\n```console\r\necho \"select id from tbl;;\" | sqlfluff lint -\r\n```\r\n## Dialect\r\ndefault (ansi)\r\n## Version\r\n```\r\nsqlfluff, version 0.6.6\r\nPython 3.9.5\r\n```\r\n## Configuration\r\nNone\r\n\n",
    "method": "sliding",
    "top_k": 10,
    "results": [
      {
        "rank": 1,
        "score": 0.6314066648483276,
        "content": "\"\"\"Implementation of Rule L038.\"\"\"\nfrom sqlfluff.core.parser import SymbolSegment\nfrom sqlfluff.core.rules.base import BaseRule, LintFix, LintResult\nfrom sqlfluff.core.rules.doc_decorators import (\n    document_fix_compatible,\n    document_configuration,\n)\n@document_configuration\n@document_fix_compatible\nclass Rule_L038(BaseRule):\n    \"\"\"Trailing commas within select clause.\n    For some database backends this is allowed. For some users\n    this may be something they wish to enforce (in line with\n    python best practice). Many database backends regard this\n    as a syntax error, and as such the sqlfluff default is to\n    forbid trailing commas in the select clause.\n    | **Anti-pattern**\n    .. code-block:: sql\n        SELECT\n            a, b,\n        FROM foo\n    | **Best practice**\n    .. code-block:: sql\n        SELECT\n            a, b\n        FROM foo\n    \"\"\"\n    config_keywords = [\"select_clause_trailing_comma\"]\n    def _eval(self, segment, parent_stack, **kwargs):\n        \"\"\"Trailing commas within select clause.\"\"\"\n        if segment.is_type(\"select_clause\"):\n            # Iterate content to find last element\n            last_content = None\n            for seg in segment.segments:\n                if seg.is_code:\n                    last_content = seg\n            # What mode are we in?\n            if self.select_clause_trailing_comma == \"forbid\":\n                # Is it a comma?\n                if last_content.is_type(\"comma\"):\n                    return LintResult(\n                        anchor=last_content,\n                        fixes=[LintFix(\"delete\", last_content)],\n                        description=\"Trailing comma in select statement forbidden\",\n                    )\n            elif self.select_clause_trailing_comma == \"require\":\n                if not last_content.is_type(\"comma\"):\n                    new_comma = SymbolSegment(\",\", name=\"comma\", type=\"comma\")\n                    return LintResult(\n                        anchor=last_content,\n                        fixes=[\n                            LintFix(\"edit\", last_content, [last_content, new_comma])\n                        ],\n                        description=\"Trailing comma in select statement required\",\n                    )\n        return None",
        "file_path": "src/sqlfluff/rules/L038.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 2,
        "score": 0.6011937260627747,
        "content": "\"\"\"Sequence and Bracketed Grammars.\"\"\"\nfrom typing import Optional, List, Tuple, cast\nfrom sqlfluff.core.errors import SQLParseError\nfrom sqlfluff.core.parser.segments import (\n    BaseSegment,\n    Indent,\n    Dedent,\n    allow_ephemeral,\n    BracketedSegment,\n    MetaSegment,\n)\nfrom sqlfluff.core.parser.helpers import trim_non_code_segments, check_still_complete\nfrom sqlfluff.core.parser.match_result import MatchResult\nfrom sqlfluff.core.parser.match_wrapper import match_wrapper\nfrom sqlfluff.core.parser.context import ParseContext\nfrom sqlfluff.core.parser.grammar.base import (\n    BaseGrammar,\n    cached_method_for_parse_context,\n)\nfrom sqlfluff.core.parser.grammar.conditional import Conditional\nclass Sequence(BaseGrammar):\n    \"\"\"Match a specific sequence of elements.\"\"\"\n    @cached_method_for_parse_context\n    def simple(self, parse_context: ParseContext) -> Optional[List[str]]:\n        \"\"\"Does this matcher support a uppercase hash matching route?\n        Sequence does provide this, as long as the *first* non-optional\n        element does, *AND* and optional elements which preceded it also do.\n        \"\"\"\n        simple_buff = []\n        for opt in self._elements:\n            simple = opt.simple(parse_context=parse_context)\n            if not simple:\n                return None\n            simple_buff += simple\n            if not opt.is_optional():\n                # We found our first non-optional element!\n                return simple_buff\n        # If *all* elements are optional AND simple, I guess it's also simple.\n        return simple_buff\n    @match_wrapper()\n    @allow_ephemeral\n    def match(self, segments, parse_context):\n        \"\"\"Match a specific sequence of elements.\"\"\"\n        if isinstance(segments, BaseSegment):\n            segments = tuple(segments)  # pragma: no cover TODO?\n        matched_segments = MatchResult.from_empty()\n        unmatched_segments = segments\n        # Buffers of uninstantiated meta segments.\n        meta_pre_nc = ()\n        meta_post_nc = ()\n        early_break = False\n        for idx, elem in enumerate(self._elements):\n            # Check for an early break.\n            if early_break:\n                break\n            while True:\n                # Consume non-code if appropriate\n                if self.allow_gaps:\n                    pre_nc, mid_seg, post_nc = trim_non_code_segments(\n                        unmatched_segments\n                    )\n                else:\n                    pre_nc = ()\n                    mid_seg = unmatched_segments\n                    post_nc = ()\n                # Is it an indent or dedent?\n                if elem.is_meta:\n                    # Elements with a negative indent value come AFTER\n                    # the whitespace. Positive or neutral come BEFORE.\n                    if elem.indent_val < 0:\n                        meta_post_nc += (elem(),)\n                    else:\n                        meta_pre_nc += (elem(),)\n                    break\n                # Is it a conditional? If so is it active\n                if isinstance(elem, Conditional) and not elem.is_enabled(parse_context):\n                    # If it's not active, skip it.\n                    break\n                if len(pre_nc + mid_seg + post_nc) == 0:\n                    # We've run our of sequence without matching everything.\n                    # Do only optional or meta elements remain?\n                    if all(\n                        e.is_optional() or e.is_meta or isinstance(e, Conditional)\n                        for e in self._elements[idx:]\n                    ):\n                        # then it's ok, and we can return what we've got so far.\n                        # No need to deal with anything left over because we're at the end,\n                        # unless it's a meta segment.\n                        # We'll add those meta segments after any existing ones. So\n                        # the go on the meta_post_nc stack.\n                        for e in self._elements[idx:]:\n                            # If it's meta, instantiate it.\n                            if e.is_meta:\n                                meta_post_nc += (e(),)  # pragma: no cover TODO?\n                            # If it's conditional and it's enabled, match it.\n                            if isinstance(e, Conditional) and e.is_enabled(\n                                parse_context\n                            ):\n                                meta_match = e.match(tuple(), parse_context)\n                                if meta_match:\n                                    meta_post_nc += meta_match.matched_segments\n                        # Early break to exit via the happy match path.\n                        early_break = True\n                        break\n                    else:\n                        # we've got to the end of the sequence without matching all\n                        # required elements.\n                        return MatchResult.from_unmatched(segments)\n                else:\n                    # We've already dealt with potential whitespace above, so carry on to matching\n                    with parse_context.deeper_match() as ctx:\n                        elem_match = elem.match(mid_seg, parse_context=ctx)\n                    if elem_match.has_match():\n                        # We're expecting mostly partial matches here, but complete\n                        # matches are possible. Don't be greedy with whitespace!\n                        matched_segments += (\n                            meta_pre_nc\n                            + pre_nc\n                            + meta_post_nc\n                            + elem_match.matched_segments\n                        )\n                        meta_pre_nc = ()\n                        meta_post_nc = ()\n                        unmatched_segments = elem_match.unmatched_segments + post_nc\n                        # Each time we do this, we do a sense check to make sure we haven't\n                        # dropped anything. (Because it's happened before!).\n                        check_still_complete(\n                            segments,\n                            matched_segments.matched_segments,\n                            unmatched_segments,\n                        )\n                        # Break out of the while loop and move to the next element.\n                        break\n                    else:\n                        # If we can't match an element, we should ascertain whether it's\n                        # required. If so then fine, move on, but otherwise we should crash\n                        # out without a match. We have not matched the sequence.\n                        if elem.is_optional():\n                            # This will crash us out of the while loop and move us\n                            # onto the next matching element\n                            break\n                        else:\n                            return MatchResult.from_unmatched(segments)\n        # If we get to here, we've matched all of the elements (or skipped them)\n        # but still have some segments left (or perhaps have precisely zero left).\n        # In either case, we're golden. Return successfully, with any leftovers as\n        # the unmatched elements. Meta all go at the end regardless of wny trailing\n        # whitespace.\n        return MatchResult(\n            BaseSegment._position_segments(\n                matched_segments.matched_segments + meta_pre_nc + meta_post_nc,\n            ),\n            unmatched_segments,\n        )\nclass Bracketed(Sequence):\n    \"\"\"Match if this is a bracketed sequence, with content that matches one of the elements.\n    Note that the contents of the Bracketed Expression are treated as an expected sequence.\n    Changelog:\n    - Post 0.3.2: Bracketed inherits from Sequence and anything within\n      the the `Bracketed()` expression is treated as a sequence. For the\n      content of the Brackets, we call the `match()` method of the sequence\n      grammar.\n    - Post 0.1.0: Bracketed was separate from sequence, and the content\n      of the expression were treated as options (like OneOf).\n    - Pre 0.1.0: Bracketed inherited from Sequence and simply added\n      brackets to that sequence,\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        # Store the bracket type. NB: This is only\n        # hydrated into segments at runtime.\n        self.bracket_type = kwargs.pop(\"bracket_type\", \"round\")\n        self.bracket_pairs_set = kwargs.pop(\"bracket_pairs_set\", \"bracket_pairs\")\n        # Allow optional override for special bracket-like things\n        self.start_bracket = kwargs.pop(\"start_bracket\", None)\n        self.end_bracket = kwargs.pop(\"end_bracket\", None)\n        super().__init__(*args, **kwargs)\n    @cached_method_for_parse_context\n    def simple(self, parse_context: ParseContext) -> Optional[List[str]]:\n        \"\"\"Does this matcher support a uppercase hash matching route?\n        Bracketed does this easily, we just look for the bracket.\n        \"\"\"\n        start_bracket, _, _ = self.get_bracket_from_dialect(parse_context)\n        return start_bracket.simple(parse_context=parse_context)\n    def get_bracket_from_dialect(self, parse_context):\n        \"\"\"Rehydrate the bracket segments in question.\"\"\"\n        for bracket_type, start_ref, end_ref, persists in parse_context.dialect.sets(\n            self.bracket_pairs_set\n        ):\n            if bracket_type == self.bracket_type:\n                start_bracket = parse_context.dialect.ref(start_ref)\n                end_bracket = parse_context.dialect.ref(end_ref)\n                break\n        else:  # pragma: no cover\n            raise ValueError(\n                \"bracket_type {!r} not found in bracket_pairs of {!r} dialect.\".format(\n                    self.bracket_type, parse_context.dialect.name\n                )\n            )\n        return start_bracket, end_bracket, persists\n    @match_wrapper()\n    @allow_ephemeral\n    def match(\n        self, segments: Tuple[\"BaseSegment\", ...], parse_context: ParseContext\n    ) -> MatchResult:\n        \"\"\"Match if this is a bracketed sequence, with content that matches one of the elements.\n        1. work forwards to find the first bracket.\n           If we find something other that whitespace, then fail out.\n        2. Once we have the first bracket, we need to bracket count forward to find its partner.\n        3. Assuming we find its partner then we try and match what goes between them\n           using the match method of Sequence.\n           If we match, great. If not, then we return an empty match.\n           If we never find its partner then we return an empty match but should probably\n           log a parsing warning, or error?\n        \"\"\"\n        # Trim ends if allowed.\n        if self.allow_gaps:\n            pre_nc, seg_buff, post_nc = trim_non_code_segments(segments)\n        else:\n            seg_buff = segments  # pragma: no cover TODO?\n        # Rehydrate the bracket segments in question.\n        # bracket_persits controls whether we make a BracketedSegment or not.\n        start_bracket, end_bracket, bracket_persists = self.get_bracket_from_dialect(\n            parse_context\n        )\n        # Allow optional override for special bracket-like things\n        start_bracket = self.start_bracket or start_bracket\n        end_bracket = self.end_bracket or end_bracket\n        # Are we dealing with a pre-existing BracketSegment?\n        if seg_buff[0].is_type(\"bracketed\"):\n            seg: BracketedSegment = cast(BracketedSegment, seg_buff[0])\n            content_segs = seg.segments[len(seg.start_bracket) : -len(seg.end_bracket)]\n            bracket_segment = seg\n            trailing_segments = seg_buff[1:]\n        # Otherwise try and match the segments directly.\n        else:\n            # Look for the first bracket\n            with parse_context.deeper_match() as ctx:\n                start_match = start_bracket.match(seg_buff, parse_context=ctx)\n            if start_match:\n                seg_buff = start_match.unmatched_segments\n            else:\n                # Can't find the opening bracket. No Match.\n                return MatchResult.from_unmatched(segments)\n            # Look for the closing bracket\n            content_segs, end_match, _ = self._bracket_sensitive_look_ahead_match(\n                segments=seg_buff,\n                matchers=[end_bracket],\n                parse_context=parse_context,\n                start_bracket=start_bracket,\n                end_bracket=end_bracket,\n                bracket_pairs_set=self.bracket_pairs_set,\n            )\n            if not end_match:  # pragma: no cover\n                raise SQLParseError(\n                    \"Couldn't find closing bracket for opening bracket.\",\n                    segment=start_match.matched_segments[0],\n                )\n            # Construct a bracket segment\n            bracket_segment = BracketedSegment(\n                segments=(\n                    start_match.matched_segments\n                    + content_segs\n                    + end_match.matched_segments\n                ),\n                start_bracket=start_match.matched_segments,\n                end_bracket=end_match.matched_segments,\n            )\n            trailing_segments = end_match.unmatched_segments\n        # Then trim whitespace and deal with the case of non-code content e.g. \"(   )\"\n        if self.allow_gaps:\n            pre_segs, content_segs, post_segs = trim_non_code_segments(content_segs)\n        else:  # pragma: no cover TODO?\n            pre_segs = ()\n            post_segs = ()\n        # If we've got a case of empty brackets check whether that is allowed.\n        if not content_segs:\n            if not self._elements or (\n                all(e.is_optional() for e in self._elements)\n                and (self.allow_gaps or (not pre_segs and not post_segs))\n            ):\n                return MatchResult(\n                    (bracket_segment,)\n                    if bracket_persists\n                    else bracket_segment.segments,\n                    trailing_segments,\n                )\n            else:\n                return MatchResult.from_unmatched(segments)\n        # Match the content using super. Sequence will interpret the content of the elements.\n        with parse_context.deeper_match() as ctx:\n            content_match = super().match(content_segs, parse_context=ctx)\n        # We require a complete match for the content (hopefully for obvious reasons)\n        if content_match.is_complete():\n            # Reconstruct the bracket segment post match.\n            # We need to realign the meta segments so the pos markers are correct.\n            # Have we already got indents?\n            meta_idx = None\n            for idx, seg in enumerate(bracket_segment.segments):\n                if seg.is_meta and cast(MetaSegment, seg).indent_val > 0:\n                    meta_idx = idx\n                    break\n            # If we've already got indents, don't add more.\n            if meta_idx:\n                bracket_segment.segments = BaseSegment._position_segments(\n                    bracket_segment.start_bracket\n                    + pre_segs\n                    + content_match.all_segments()\n                    + post_segs\n                    + bracket_segment.end_bracket\n                )\n            # Append some indent and dedent tokens at the start and the end.\n            else:\n                bracket_segment.segments = BaseSegment._position_segments(\n                    # NB: The nc segments go *outside* the indents.\n                    bracket_segment.start_bracket\n                    + (Indent(),)  # Add a meta indent here\n                    + pre_segs\n                    + content_match.all_segments()\n                    + post_segs\n                    + (Dedent(),)  # Add a meta indent here\n                    + bracket_segment.end_bracket\n                )\n            return MatchResult(\n                (bracket_segment,) if bracket_persists else bracket_segment.segments,\n                trailing_segments,\n            )\n        # No complete match. Fail.\n        else:\n            return MatchResult.from_unmatched(segments)",
        "file_path": "src/sqlfluff/core/parser/grammar/sequence.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 3,
        "score": 0.5898969769477844,
        "content": "\"\"\"Implementation of Rule L040.\"\"\"\nfrom sqlfluff.core.parser import NewlineSegment, WhitespaceSegment\nfrom sqlfluff.core.rules.base import BaseRule, LintFix, LintResult\nfrom sqlfluff.core.rules.doc_decorators import document_fix_compatible\n@document_fix_compatible\nclass Rule_L041(BaseRule):\n    \"\"\"SELECT clause modifiers such as DISTINCT must be on the same line as SELECT.\n    | **Anti-pattern**\n    .. code-block:: sql\n        select\n            distinct a,\n            b\n        from x\n    | **Best practice**\n    .. code-block:: sql\n        select distinct\n            a,\n            b\n        from x\n    \"\"\"\n    def _eval(self, segment, **kwargs):\n        \"\"\"Select clause modifiers must appear on same line as SELECT.\"\"\"\n        if segment.is_type(\"select_clause\"):\n            # Does the select clause have modifiers?\n            select_modifier = segment.get_child(\"select_clause_modifier\")\n            if not select_modifier:\n                return None  # No. We're done.\n            select_modifier_idx = segment.segments.index(select_modifier)\n            # Does the select clause contain a newline?\n            newline = segment.get_child(\"newline\")\n            if not newline:\n                return None  # No. We're done.\n            newline_idx = segment.segments.index(newline)\n            # Is there a newline before the select modifier?\n            if newline_idx > select_modifier_idx:\n                return None  # No, we're done.\n            # Yes to all the above. We found an issue.\n            # E.g.: \" DISTINCT\\n\"\n            replace_newline_with = [\n                WhitespaceSegment(),\n                select_modifier,\n                NewlineSegment(),\n            ]\n            fixes = [\n                # E.g. \"\\n\" -> \" DISTINCT\\n.\n                LintFix(\"edit\", newline, replace_newline_with),\n                # E.g. \"DISTINCT\" -> X\n                LintFix(\"delete\", select_modifier),\n            ]\n            # E.g. \" \" after \"DISTINCT\"\n            ws_to_delete = segment.select_children(\n                start_seg=select_modifier,\n                select_if=lambda s: s.is_type(\"whitespace\"),\n                loop_while=lambda s: s.is_type(\"whitespace\") or s.is_meta,\n            )\n            # E.g. \" \" -> X\n            fixes += [LintFix(\"delete\", ws) for ws in ws_to_delete]\n            return LintResult(\n                anchor=segment,\n                fixes=fixes,\n            )",
        "file_path": "src/sqlfluff/rules/L041.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 4,
        "score": 0.5895127058029175,
        "content": "\"\"\"Implementation of Rule L036.\"\"\"\nfrom typing import List, NamedTuple\nfrom sqlfluff.core.parser import WhitespaceSegment\nfrom sqlfluff.core.parser import BaseSegment, NewlineSegment\nfrom sqlfluff.core.rules.base import BaseRule, LintFix, LintResult\nfrom sqlfluff.core.rules.doc_decorators import document_fix_compatible\nclass SelectTargetsInfo(NamedTuple):\n    \"\"\"Info about select targets and nearby whitespace.\"\"\"\n    select_idx: int\n    first_new_line_idx: int\n    first_select_target_idx: int\n    first_whitespace_idx: int\n    select_targets: List[BaseSegment]\n@document_fix_compatible\nclass Rule_L036(BaseRule):\n    \"\"\"Select targets should be on a new line unless there is only one select target.\n    | **Anti-pattern**\n    .. code-block:: sql\n        select\n            *\n        from x\n    | **Best practice**\n    .. code-block:: sql\n        select\n            a,\n            b,\n            c\n        from x\n    \"\"\"\n    def _eval(self, segment, raw_stack, **kwargs):\n        if segment.is_type(\"select_clause\"):\n            select_targets_info = self._get_indexes(segment)\n            if len(select_targets_info.select_targets) == 1:\n                parent_stack = kwargs.get(\"parent_stack\")\n                return self._eval_single_select_target_element(\n                    select_targets_info, segment, parent_stack\n                )\n            elif len(select_targets_info.select_targets) > 1:\n                return self._eval_multiple_select_target_elements(\n                    select_targets_info, segment\n                )\n    @staticmethod\n    def _get_indexes(segment):\n        select_idx = -1\n        first_new_line_idx = -1\n        first_select_target_idx = -1\n        first_whitespace_idx = -1\n        select_targets = []\n        for fname_idx, seg in enumerate(segment.segments):\n            if seg.is_type(\"select_clause_element\"):\n                select_targets.append(seg)\n                if first_select_target_idx == -1:\n                    first_select_target_idx = fname_idx\n            if seg.is_type(\"keyword\") and seg.name == \"select\" and select_idx == -1:\n                select_idx = fname_idx\n            if seg.is_type(\"newline\") and first_new_line_idx == -1:\n                first_new_line_idx = fname_idx\n            # TRICKY: Ignore whitespace prior to the first newline, e.g. if\n            # the line with \"SELECT\" (before any select targets) has trailing\n            # whitespace.\n            if (\n                seg.is_type(\"whitespace\")\n                and first_new_line_idx != -1\n                and first_whitespace_idx == -1\n            ):\n                first_whitespace_idx = fname_idx\n        return SelectTargetsInfo(\n            select_idx,\n            first_new_line_idx,\n            first_select_target_idx,\n            first_whitespace_idx,\n            select_targets,\n        )\n    def _eval_multiple_select_target_elements(self, select_targets_info, segment):\n        \"\"\"Multiple select targets. Ensure each is on a separate line.\"\"\"\n        # Insert newline before every select target.\n        fixes = []\n        for i, select_target in enumerate(select_targets_info.select_targets):\n            base_segment = (\n                segment if not i else select_targets_info.select_targets[i - 1]\n            )\n            if (\n                base_segment.pos_marker.working_line_no\n                == select_target.pos_marker.working_line_no\n            ):\n                # Find and delete any whitespace before the select target.\n                start_seg = select_targets_info.select_idx\n                # If any select modifier (e.g. distinct ) is present, start\n                # there rather than at the beginning.\n                modifier = segment.get_child(\"select_clause_modifier\")\n                if modifier:\n                    start_seg = segment.segments.index(modifier)\n                ws_to_delete = segment.select_children(\n                    start_seg=segment.segments[start_seg]\n                    if not i\n                    else select_targets_info.select_targets[i - 1],\n                    select_if=lambda s: s.is_type(\"whitespace\"),\n                    loop_while=lambda s: s.is_type(\"whitespace\", \"comma\") or s.is_meta,\n                )\n                fixes += [LintFix(\"delete\", ws) for ws in ws_to_delete]\n                fixes.append(LintFix(\"create\", select_target, NewlineSegment()))\n        if fixes:\n            return LintResult(anchor=segment, fixes=fixes)\n    def _eval_single_select_target_element(\n        self, select_targets_info, select_clause, parent_stack\n    ):\n        is_wildcard = False\n        for segment in select_clause.segments:\n            if segment.is_type(\"select_clause_element\"):\n                for sub_segment in segment.segments:\n                    if sub_segment.is_type(\"wildcard_expression\"):\n                        is_wildcard = True\n        if is_wildcard:\n            return None\n        elif (\n            select_targets_info.select_idx\n            < select_targets_info.first_new_line_idx\n            < select_targets_info.first_select_target_idx\n        ):\n            # Do we have a modifier?\n            modifier = select_clause.get_child(\"select_clause_modifier\")\n            # Prepare the select clause which will be inserted\n            # In most (but not all) case we'll want to replace the newline with\n            # the statement and a newline, but in some cases however (see #1424)\n            # we don't need the final newline.\n            copy_with_newline = True\n            insert_buff = [\n                WhitespaceSegment(),\n                select_clause.segments[select_targets_info.first_select_target_idx],\n            ]\n            # Check if the modifier is one we care about\n            if modifier:\n                # If it's already on the first line, ignore it.\n                if (\n                    select_clause.segments.index(modifier)\n                    < select_targets_info.first_new_line_idx\n                ):\n                    modifier = None\n            fixes = [\n                # Delete the first select target from its original location.\n                # We'll add it to the right section at the end, once we know\n                # what to add.\n                LintFix(\n                    \"delete\",\n                    select_clause.segments[select_targets_info.first_select_target_idx],\n                ),\n            ]\n            start_idx = 0\n            # If we have a modifier to move:\n            if modifier:\n                # Add it to the insert\n                insert_buff = [WhitespaceSegment(), modifier] + insert_buff\n                modifier_idx = select_clause.segments.index(modifier)\n                # Delete the whitespace after it (which is two after, thanks to indent)\n                if (\n                    len(select_clause.segments) > modifier_idx + 1\n                    and select_clause.segments[modifier_idx + 2].is_whitespace\n                ):\n                    fixes += [\n                        LintFix(\n                            \"delete\",\n                            select_clause.segments[modifier_idx + 2],\n                        ),\n                    ]\n                # Delete the modifier itself\n                fixes += [\n                    LintFix(\n                        \"delete\",\n                        modifier,\n                    ),\n                ]\n                # Set the position marker for removing the preceding\n                # whitespace and newline, which we'll use below.\n                start_idx = modifier_idx\n            else:\n                # Set the position marker for removing the preceding\n                # whitespace and newline, which we'll use below.\n                start_idx = select_targets_info.first_select_target_idx\n            if parent_stack and parent_stack[-1].is_type(\"select_statement\"):\n                select_stmt = parent_stack[-1]\n                select_clause_idx = select_stmt.segments.index(select_clause)\n                after_select_clause_idx = select_clause_idx + 1\n                if len(select_stmt.segments) > after_select_clause_idx:\n                    if select_stmt.segments[after_select_clause_idx].is_type(\"newline\"):\n                        # The select_clause is immediately followed by a\n                        # newline. Delete the newline in order to avoid leaving\n                        # behind an empty line after fix.\n                        delete_last_newline = True\n                        # Since, we're deleting the newline, we should also delete all\n                        # whitespace before it or it will add random whitespace to\n                        # following statements. So walk back through the segment\n                        # deleting whitespace until you get the previous newline, or\n                        # something else.\n                        idx = 1\n                        while start_idx - idx < len(select_clause.segments):\n                            # Delete any whitespace\n                            if select_clause.segments[start_idx - idx].is_type(\n                                \"whitespace\"\n                            ):\n                                fixes += [\n                                    LintFix(\n                                        \"delete\",\n                                        select_clause.segments[start_idx - idx],\n                                    ),\n                                ]\n                            # Once we see a newline, then we're done\n                            if select_clause.segments[start_idx - idx].is_type(\n                                \"newline\",\n                            ):\n                                break\n                            # If we see anything other than whitespace,\n                            # then we're done, but in this case we want to\n                            # keep the final newline.\n                            if not select_clause.segments[start_idx - idx].is_type(\n                                \"whitespace\", \"newline\"\n                            ):\n                                delete_last_newline = False\n                                break\n                            idx += 1\n                        # Finally delete the newline, unless we've decided not to\n                        if delete_last_newline:\n                            fixes.append(\n                                LintFix(\n                                    \"delete\",\n                                    select_stmt.segments[after_select_clause_idx],\n                                )\n                            )\n                    elif select_stmt.segments[after_select_clause_idx].is_type(\n                        \"whitespace\"\n                    ):\n                        # The select_clause has stuff after (most likely a comment)\n                        # Delete the whitespace immeadiately after the select clause\n                        # so the other stuff aligns nicely based on where the select\n                        # clause started\n                        fixes += [\n                            LintFix(\n                                \"delete\",\n                                select_stmt.segments[after_select_clause_idx],\n                            ),\n                        ]\n                    elif select_stmt.segments[after_select_clause_idx].is_type(\n                        \"dedent\"\n                    ):\n                        # The end of the select statement, so this is the one\n                        # case we don't want the newline added to end of\n                        # select_clause (see #1424)\n                        copy_with_newline = False\n                        # Again let's strip back the whitespace, bnut simpler\n                        # as don't need to worry about new line so just break\n                        # if see non-whitespace\n                        idx = 1\n                        start_idx = select_clause_idx - 1\n                        while start_idx - idx < len(select_clause.segments):\n                            # Delete any whitespace\n                            if select_clause.segments[start_idx - idx].is_type(\n                                \"whitespace\"\n                            ):\n                                fixes += [\n                                    LintFix(\n                                        \"delete\",\n                                        select_clause.segments[start_idx - idx],\n                                    ),\n                                ]\n                            # Once we see a newline, then we're done\n                            if select_clause.segments[start_idx - idx].is_type(\n                                \"newline\"\n                            ):\n                                break\n                            # If we see anything other than whitespace,\n                            # then we're done, but in this case we want to\n                            # keep the final newline.\n                            if not select_clause.segments[start_idx - idx].is_type(\n                                \"whitespace\", \"newline\"\n                            ):\n                                copy_with_newline = True\n                                break\n                            idx += 1\n            if copy_with_newline:\n                insert_buff = insert_buff + [NewlineSegment()]\n            fixes += [\n                # Insert the select_clause in place of the first newlin in the\n                # Select statement\n                LintFix(\n                    \"edit\",\n                    select_clause.segments[select_targets_info.first_new_line_idx],\n                    insert_buff,\n                ),\n            ]\n            return LintResult(\n                anchor=select_clause,\n                fixes=fixes,\n            )\n        else:\n            return None",
        "file_path": "src/sqlfluff/rules/L036.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 5,
        "score": 0.582917332649231,
        "content": "\"\"\"Implementation of Rule L048.\"\"\"\nfrom typing import Tuple, List\nfrom sqlfluff.core.rules.doc_decorators import document_fix_compatible\nfrom sqlfluff.rules.L006 import Rule_L006\n@document_fix_compatible\nclass Rule_L048(Rule_L006):\n    \"\"\"Quoted literals should be surrounded by a single whitespace.\n    | **Anti-pattern**\n    | In this example, there is a space missing space between the string 'foo'\n    | and the keyword AS.\n    .. code-block:: sql\n        SELECT\n            'foo'AS bar\n        FROM foo\n    | **Best practice**\n    | Keep a single space.\n    .. code-block:: sql\n        SELECT\n            'foo' AS bar\n        FROM foo\n    \"\"\"\n    _target_elems: List[Tuple[str, str]] = [\n        (\"name\", \"quoted_literal\"),\n    ]\n    @staticmethod\n    def _missing_whitespace(seg, before=True):\n        \"\"\"Check whether we're missing whitespace given an adjoining segment.\n        This avoids flagging for commas after quoted strings.\n        https://github.com/sqlfluff/sqlfluff/issues/943\n        \"\"\"\n        simple_res = Rule_L006._missing_whitespace(seg, before=before)\n        if not before and seg and seg.is_type(\"comma\"):\n            return False\n        return simple_res",
        "file_path": "src/sqlfluff/rules/L048.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 6,
        "score": 0.5802721977233887,
        "content": "\"\"\"Implementation of Rule L019.\"\"\"\nfrom typing import Dict, Any\nfrom sqlfluff.core.parser import WhitespaceSegment\nfrom sqlfluff.core.rules.base import BaseRule, LintFix, LintResult\nfrom sqlfluff.core.rules.doc_decorators import (\n    document_fix_compatible,\n    document_configuration,\n)\n@document_fix_compatible\n@document_configuration\nclass Rule_L019(BaseRule):\n    \"\"\"Leading/Trailing comma enforcement.\n    | **Anti-pattern**\n    | There is a mixture of leading and trailing commas.\n    .. code-block:: sql\n        SELECT\n            a\n            , b,\n            c\n        FROM foo\n    | **Best practice**\n    | By default sqlfluff prefers trailing commas, however it\n    | is configurable for leading commas. Whichever option you chose\n    | it does expect you to be consistent.\n    .. code-block:: sql\n        SELECT\n            a,\n            b,\n            c\n        FROM foo\n        -- Alternatively, set the configuration file to 'leading'\n        -- and then the following would be acceptable:\n        SELECT\n            a\n            , b\n            , c\n        FROM foo\n    \"\"\"\n    _works_on_unparsable = False\n    config_keywords = [\"comma_style\"]\n    @staticmethod\n    def _last_comment_seg(raw_stack):\n        \"\"\"Trace the raw stack back to the most recent comment segment.\n        A return value of `None` indicates no code segments preceding the current position.\n        \"\"\"\n        for segment in raw_stack[::-1]:\n            if segment.is_comment:\n                return segment\n        return None\n    @staticmethod\n    def _last_code_seg(raw_stack):\n        \"\"\"Trace the raw stack back to the most recent code segment.\n        A return value of `None` indicates no code segments preceding the current position.\n        \"\"\"\n        for segment in raw_stack[::-1]:\n            if segment.is_code or segment.is_type(\"newline\"):\n                return segment\n        return None\n    def _eval(self, segment, raw_stack, memory, **kwargs):\n        \"\"\"Enforce comma placement.\n        For leading commas we're looking for trailing commas, so\n        we look for newline segments. For trailing commas we're\n        looking for leading commas, so we look for the comma itself.\n        We also want to handle proper whitespace removal/addition. We remove\n        any trailing whitespace after the leading comma, when converting a\n        leading comma to a trailing comma. We add whitespace after the leading\n        comma when converting a trailing comma to a leading comma.\n        \"\"\"\n        if not memory:\n            memory: Dict[str, Any] = {\n                # Trailing comma keys\n                #\n                # Do we have a fix in place for removing a leading\n                # comma violation, and inserting a new trailing comma?\n                \"insert_trailing_comma\": False,\n                # A list of whitespace segments that come after a\n                # leading comma violation, to be removed during fixing.\n                \"whitespace_deletions\": None,\n                # The leading comma violation segment to be removed during fixing\n                \"last_leading_comma_seg\": None,\n                # The newline segment where we're going to insert our new trailing\n                # comma during fixing\n                \"anchor_for_new_trailing_comma_seg\": None,\n                #\n                # Leading comma keys\n                #\n                # Do we have a fix in place for removing a trailing\n                # comma violation, and inserting a new leading comma?\n                \"insert_leading_comma\": False,\n                # The trailing comma violation segment to be removed during fixing\n                \"last_trailing_comma_segment\": None,\n            }\n        if self.comma_style == \"trailing\":\n            # A comma preceded by a new line == a leading comma\n            if segment.is_type(\"comma\"):\n                last_seg = self._last_code_seg(raw_stack)\n                if last_seg.is_type(\"newline\"):\n                    # Recorded where the fix should be applied\n                    memory[\"last_leading_comma_seg\"] = segment\n                    last_comment_seg = self._last_comment_seg(raw_stack)\n                    inline_comment = (\n                        last_comment_seg.pos_marker.line_no\n                        == last_seg.pos_marker.line_no\n                        if last_comment_seg\n                        else False\n                    )\n                    # If we have a comment right before the newline, then anchor\n                    # the fix at the comment instead\n                    memory[\"anchor_for_new_trailing_comma_seg\"] = (\n                        last_seg if not inline_comment else last_comment_seg\n                    )\n                    # Trigger fix routine\n                    memory[\"insert_trailing_comma\"] = True\n                    memory[\"whitespace_deletions\"] = []\n                    return LintResult(memory=memory)\n            # Have we found a leading comma violation?\n            if memory[\"insert_trailing_comma\"]:\n                # Search for trailing whitespace to delete after the leading\n                # comma violation\n                if segment.is_type(\"whitespace\"):\n                    memory[\"whitespace_deletions\"] += [segment]\n                    return LintResult(memory=memory)\n                else:\n                    # We've run out of whitespace to delete, time to fix\n                    last_leading_comma_seg = memory[\"last_leading_comma_seg\"]\n                    # Scan backwards to find the last code segment, skipping\n                    # over lines that are either entirely blank or just a\n                    # comment. We want to place the comma immediately after it.\n                    last_code_seg = None\n                    while last_code_seg is None or last_code_seg.is_type(\"newline\"):\n                        last_code_seg = self._last_code_seg(\n                            raw_stack[\n                                : raw_stack.index(\n                                    last_code_seg\n                                    if last_code_seg\n                                    else memory[\"last_leading_comma_seg\"]\n                                )\n                            ]\n                        )\n                    return LintResult(\n                        anchor=last_leading_comma_seg,\n                        description=\"Found leading comma. Expected only trailing.\",\n                        fixes=[\n                            LintFix(\"delete\", last_leading_comma_seg),\n                            *[\n                                LintFix(\"delete\", d)\n                                for d in memory[\"whitespace_deletions\"]\n                            ],\n                            LintFix(\n                                \"edit\",\n                                last_code_seg,\n                                # Reuse the previous leading comma violation to\n                                # create a new trailing comma\n                                [last_code_seg, last_leading_comma_seg],\n                            ),\n                        ],\n                    )\n        elif self.comma_style == \"leading\":\n            # A new line preceded by a comma == a trailing comma\n            if segment.is_type(\"newline\"):\n                last_seg = self._last_code_seg(raw_stack)\n                # no code precedes the current position: no issue\n                if last_seg is None:\n                    return None\n                if last_seg.is_type(\"comma\"):\n                    # Trigger fix routine\n                    memory[\"insert_leading_comma\"] = True\n                    # Record where the fix should be applied\n                    memory[\"last_trailing_comma_segment\"] = last_seg\n                    return LintResult(memory=memory)\n            # Have we found a trailing comma violation?\n            if memory[\"insert_leading_comma\"]:\n                # Only insert the comma here if this isn't a comment/whitespace segment\n                if segment.is_code:\n                    last_comma_seg = memory[\"last_trailing_comma_segment\"]\n                    # Create whitespace to insert after the new leading comma\n                    new_whitespace_seg = WhitespaceSegment()\n                    return LintResult(\n                        anchor=last_comma_seg,\n                        description=\"Found trailing comma. Expected only leading.\",\n                        fixes=[\n                            LintFix(\"delete\", anchor=last_comma_seg),\n                            LintFix(\n                                \"create\",\n                                anchor=segment,\n                                edit=[last_comma_seg, new_whitespace_seg],\n                            ),\n                        ],\n                    )\n        # Otherwise, no issue\n        return None",
        "file_path": "src/sqlfluff/rules/L019.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 7,
        "score": 0.5794320702552795,
        "content": "\"\"\"This is an example of how to use the simple sqlfluff api.\"\"\"\nimport sqlfluff\n#  -------- LINTING ----------\nmy_bad_query = \"SeLEct  *, 1, blah as  fOO  from myTable\"\n# Lint the given string and get a list of violations found.\nresult = sqlfluff.lint(my_bad_query, dialect=\"bigquery\")\n# result =\n# [\n#     {\"code\": \"L010\", \"line_no\": 1, \"line_pos\": 1, \"description\": \"Keywords must be consistently upper case.\"}\n#     ...\n# ]\n#  -------- FIXING ----------\n# Fix the given string and get a string back which has been fixed.\nresult = sqlfluff.fix(my_bad_query, dialect=\"bigquery\")\n# result = 'SELECT  *, 1, blah AS  foo  FROM mytable\\n'\n# We can also fix just specific rules.\nresult = sqlfluff.fix(my_bad_query, rules=\"L010\")\n# result = 'SELECT  *, 1, blah AS  fOO  FROM myTable'\n# Or a subset of rules...\nresult = sqlfluff.fix(my_bad_query, rules=[\"L010\", \"L014\"])\n# result = 'SELECT  *, 1, blah AS  fOO  FROM mytable'\n#  -------- PARSING ----------\n# NOTE: sqlfluff is still in a relatively early phase of its\n# development and so until version 1.0.0 will offer no guarantee\n# that the names and structure of the objects returned by these\n# parse commands won't change between releases. Use with care\n# and keep updated with the changelog for the project for any\n# changes in this space.\nparsed = sqlfluff.parse(my_bad_query)\n# Get the structure of the query\nstructure = parsed.tree.to_tuple(show_raw=True, code_only=True)\n# structure = ('file', (('statement', (('select_statement', (('select_clause', (('keyword', 'SeLEct'), ...\n# Extract certain elements\nkeywords = [keyword.raw for keyword in parsed.tree.recursive_crawl(\"keyword\")]\n# keywords = ['SeLEct', 'as', 'from']\ntbl_refs = [tbl_ref.raw for tbl_ref in parsed.tree.recursive_crawl(\"table_reference\")]\n# tbl_refs == [\"myTable\"]",
        "file_path": "examples/01_basic_api_usage.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 8,
        "score": 0.5725761651992798,
        "content": "\"\"\"Implementation of Rule L034.\"\"\"\nfrom sqlfluff.core.rules.base import BaseRule, LintFix, LintResult\nfrom sqlfluff.core.rules.doc_decorators import document_fix_compatible\n@document_fix_compatible\nclass Rule_L034(BaseRule):\n    \"\"\"Use wildcards then simple targets before calculations and aggregates in select statements.\n    | **Anti-pattern**\n    .. code-block:: sql\n        select\n            a,\n            *,\n            row_number() over (partition by id order by date) as y,\n            b\n        from x\n    | **Best practice**\n    |  Order \"select\" targets in ascending complexity\n    .. code-block:: sql\n        select\n            *,\n            a,\n            b,\n            row_number() over (partition by id order by date) as y\n        from x\n    \"\"\"\n    def _validate(self, i, segment):\n        # Check if we've seen a more complex select target element already\n        if self.seen_band_elements[i + 1 : :] != [[]] * len(\n            self.seen_band_elements[i + 1 : :]\n        ):\n            # Found a violation (i.e. a simpler element that *follows* a more\n            # complex element.\n            self.violation_exists = True\n        self.current_element_band = i\n        self.seen_band_elements[i].append(segment)\n    def _eval(self, segment, parent_stack, **kwargs):\n        self.violation_buff = []\n        self.violation_exists = False\n        # Bands of select targets in order to be enforced\n        select_element_order_preference = (\n            (\"wildcard_expression\",),\n            (\n                \"object_reference\",\n                \"literal\",\n                \"cast_expression\",\n                (\"function\", \"cast\"),\n                (\"expression\", \"cast_expression\"),\n            ),\n        )\n        # Track which bands have been seen, with additional empty list for the non-matching elements\n        # If we find a matching target element, we append the element to the corresponding index\n        self.seen_band_elements = [[] for i in select_element_order_preference] + [[]]\n        if segment.is_type(\"select_clause\"):\n            # Ignore select clauses which belong to:\n            # - set expression, which is most commonly a union\n            # - insert_statement\n            # - create table statement\n            #\n            # In each of these contexts, the order of columns in a select should\n            # be preserved.\n            if len(parent_stack) >= 2 and parent_stack[-2].is_type(\n                \"insert_statement\", \"set_expression\"\n            ):\n                return None\n            if len(parent_stack) >= 3 and parent_stack[-3].is_type(\n                \"create_table_statement\"\n            ):\n                return None\n            select_clause_segment = segment\n            select_target_elements = segment.get_children(\"select_clause_element\")\n            if not select_target_elements:\n                return None\n            # Iterate through all the select targets to find any order violations\n            for segment in select_target_elements:\n                # The band index of the current segment in select_element_order_preference\n                self.current_element_band = None\n                # Compare the segment to the bands in select_element_order_preference\n                for i, band in enumerate(select_element_order_preference):\n                    for e in band:\n                        # Identify simple select target\n                        if segment.get_child(e):\n                            self._validate(i, segment)\n                        # Identify function\n                        elif type(e) == tuple and e[0] == \"function\":\n                            try:\n                                if (\n                                    segment.get_child(\"function\")\n                                    .get_child(\"function_name\")\n                                    .raw\n                                    == e[1]\n                                ):\n                                    self._validate(i, segment)\n                            except AttributeError:\n                                # If the segment doesn't match\n                                pass\n                        # Identify simple expression\n                        elif type(e) == tuple and e[0] == \"expression\":\n                            try:\n                                if (\n                                    segment.get_child(\"expression\").get_child(e[1])\n                                    and segment.get_child(\"expression\").segments[0].type\n                                    in (\n                                        \"column_reference\",\n                                        \"object_reference\",\n                                        \"literal\",\n                                    )\n                                    # len == 2 to ensure the expression is 'simple'\n                                    and len(segment.get_child(\"expression\").segments)\n                                    == 2\n                                ):\n                                    self._validate(i, segment)\n                            except AttributeError:\n                                # If the segment doesn't match\n                                pass\n                # If the target doesn't exist in select_element_order_preference then it is 'complex' and must go last\n                if self.current_element_band is None:\n                    self.seen_band_elements[-1].append(segment)\n            if self.violation_exists:\n                # Create a list of all the edit fixes\n                # We have to do this at the end of iterating through all the select_target_elements to get the order correct\n                # This means we can't add a lint fix to each individual LintResult as we go\n                ordered_select_target_elements = [\n                    segment for band in self.seen_band_elements for segment in band\n                ]\n                # TODO: The \"if\" in the loop below compares corresponding items\n                # to avoid creating \"do-nothing\" edits. A potentially better\n                # approach would leverage difflib.SequenceMatcher.get_opcodes(),\n                # which generates a list of edit actions (similar to the\n                # command-line \"diff\" tool in Linux). This is more complex to\n                # implement, but minimizing the number of LintFixes makes the\n                # final application of patches (in \"sqlfluff fix\") more robust.\n                fixes = [\n                    LintFix(\n                        \"edit\",\n                        initial_select_target_element,\n                        replace_select_target_element,\n                    )\n                    for initial_select_target_element, replace_select_target_element in zip(\n                        select_target_elements, ordered_select_target_elements\n                    )\n                    if initial_select_target_element\n                    is not replace_select_target_element\n                ]\n                # Anchoring on the select statement segment ensures that\n                # select statements which include macro targets are ignored\n                # when ignore_templated_areas is set\n                lint_result = LintResult(anchor=select_clause_segment, fixes=fixes)\n                self.violation_buff = [lint_result]\n        return self.violation_buff or None",
        "file_path": "src/sqlfluff/rules/L034.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 9,
        "score": 0.5706962943077087,
        "content": "\"\"\"The simple public API methods.\"\"\"\nfrom sqlfluff.core import Linter\nclass APIParsingError(ValueError):\n    \"\"\"An exception which holds a set of violations.\"\"\"\n    def __init__(self, violations, **kwargs):\n        self.violations = violations\n        self.msg = f\"Found {len(violations)} issues while parsing string.\"\n        for viol in violations:\n            self.msg += f\"\\n{viol!s}\"\n        super().__init__(self.msg, **kwargs)\ndef _unify_str_or_file(sql):\n    \"\"\"Unify string and files in the same format.\"\"\"\n    if not isinstance(sql, str):\n        try:\n            sql = sql.read()\n        except AttributeError:  # pragma: no cover\n            raise TypeError(\"Value passed as sql is not a string or a readable object.\")\n    return sql\ndef lint(sql, dialect=\"ansi\", rules=None):\n    \"\"\"Lint a sql string or file.\n    Args:\n        sql (:obj:`str` or file-like object): The sql to be linted\n            either as a string or a subclass of :obj:`TextIOBase`.\n        dialect (:obj:`str`, optional): A reference to the dialect of the sql\n            to be linted. Defaults to `ansi`.\n        rules (:obj:`str` or iterable of :obj:`str`, optional): A subset of rule\n            reference to lint for.\n    Returns:\n        :obj:`list` of :obj:`dict` for each violation found.\n    \"\"\"\n    sql = _unify_str_or_file(sql)\n    linter = Linter(dialect=dialect, rules=rules)\n    result = linter.lint_string_wrapped(sql)\n    result_records = result.as_records()\n    # Return just the violations for this file\n    return [] if not result_records else result_records[0][\"violations\"]\ndef fix(sql, dialect=\"ansi\", rules=None):\n    \"\"\"Fix a sql string or file.\n    Args:\n        sql (:obj:`str` or file-like object): The sql to be linted\n            either as a string or a subclass of :obj:`TextIOBase`.\n        dialect (:obj:`str`, optional): A reference to the dialect of the sql\n            to be linted. Defaults to `ansi`.\n        rules (:obj:`str` or iterable of :obj:`str`, optional): A subset of rule\n            reference to lint for.\n    Returns:\n        :obj:`str` for the fixed sql if possible.\n    \"\"\"\n    sql = _unify_str_or_file(sql)\n    linter = Linter(dialect=dialect, rules=rules)\n    result = linter.lint_string_wrapped(sql, fix=True)\n    fixed_string = result.paths[0].files[0].fix_string()[0]\n    return fixed_string\ndef parse(sql, dialect=\"ansi\"):\n    \"\"\"Parse a sql string or file.\n    Args:\n        sql (:obj:`str` or file-like object): The sql to be linted\n            either as a string or a subclass of :obj:`TextIOBase`.\n        dialect (:obj:`str`, optional): A reference to the dialect of the sql\n            to be linted. Defaults to `ansi`.\n    Returns:\n        :obj:`ParsedString` containing the parsed structure.\n    \"\"\"\n    sql = _unify_str_or_file(sql)\n    linter = Linter(dialect=dialect)\n    parsed = linter.parse_string(sql)\n    # If we encounter any parsing errors, raise them in a combined issue.\n    if parsed.violations:\n        raise APIParsingError(parsed.violations)\n    return parsed",
        "file_path": "src/sqlfluff/api/simple.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 10,
        "score": 0.5704967975616455,
        "content": "\"\"\"The MSSQL T-SQL dialect.\nhttps://docs.microsoft.com/en-us/sql/t-sql/language-elements/language-elements-transact-sql\n\"\"\"\nfrom sqlfluff.core.parser import (\n    BaseSegment,\n    Sequence,\n    OneOf,\n    Bracketed,\n    Ref,\n    Anything,\n    Nothing,\n    RegexLexer,\n    CodeSegment,\n    RegexParser,\n    Delimited,\n    Matchable,\n    NamedParser,\n    StartsWith,\n    OptionallyBracketed,\n    Dedent,\n    AnyNumberOf,\n)\nfrom sqlfluff.core.dialects import load_raw_dialect\nfrom sqlfluff.dialects.tsql_keywords import RESERVED_KEYWORDS, UNRESERVED_KEYWORDS\nansi_dialect = load_raw_dialect(\"ansi\")\ntsql_dialect = ansi_dialect.copy_as(\"tsql\")\n# Should really clear down the old keywords but some are needed by certain segments\n# tsql_dialect.sets(\"reserved_keywords\").clear()\n# tsql_dialect.sets(\"unreserved_keywords\").clear()\ntsql_dialect.sets(\"reserved_keywords\").update(RESERVED_KEYWORDS)\ntsql_dialect.sets(\"unreserved_keywords\").update(UNRESERVED_KEYWORDS)\ntsql_dialect.insert_lexer_matchers(\n    [\n        RegexLexer(\n            \"atsign\",\n            r\"[@][a-zA-Z0-9_]+\",\n            CodeSegment,\n        ),\n        RegexLexer(\n            \"square_quote\",\n            r\"\\[([a-zA-Z0-9][^\\[\\]]*)*\\]\",\n            CodeSegment,\n        ),\n        # T-SQL unicode strings\n        RegexLexer(\"single_quote_with_n\", r\"N'([^'\\\\]|\\\\.)*'\", CodeSegment),\n    ],\n    before=\"back_quote\",\n)\ntsql_dialect.add(\n    BracketedIdentifierSegment=NamedParser(\n        \"square_quote\", CodeSegment, name=\"quoted_identifier\", type=\"identifier\"\n    ),\n    QuotedLiteralSegmentWithN=NamedParser(\n        \"single_quote_with_n\", CodeSegment, name=\"quoted_literal\", type=\"literal\"\n    ),\n)\ntsql_dialect.replace(\n    # Below delimiterstatement might need to be removed in the future as delimiting\n    # is optional with semicolon and GO is a end of statement indicator.\n    DelimiterSegment=OneOf(\n        Sequence(Ref(\"SemicolonSegment\"), Ref(\"GoStatementSegment\")),\n        Ref(\"SemicolonSegment\"),\n        Ref(\"GoStatementSegment\"),\n    ),\n    SingleIdentifierGrammar=OneOf(\n        Ref(\"NakedIdentifierSegment\"),\n        Ref(\"QuotedIdentifierSegment\"),\n        Ref(\"BracketedIdentifierSegment\"),\n    ),\n    LiteralGrammar=OneOf(\n        Ref(\"QuotedLiteralSegment\"),\n        Ref(\"QuotedLiteralSegmentWithN\"),\n        Ref(\"NumericLiteralSegment\"),\n        Ref(\"BooleanLiteralGrammar\"),\n        Ref(\"QualifiedNumericLiteralSegment\"),\n        # NB: Null is included in the literals, because it is a keyword which\n        # can otherwise be easily mistaken for an identifier.\n        Ref(\"NullLiteralSegment\"),\n        Ref(\"DateTimeLiteralGrammar\"),\n    ),\n    ParameterNameSegment=RegexParser(\n        r\"[@][A-Za-z0-9_]+\", CodeSegment, name=\"parameter\", type=\"parameter\"\n    ),\n    FunctionNameIdentifierSegment=RegexParser(\n        r\"[A-Z][A-Z0-9_]*|\\[[A-Z][A-Z0-9_]*\\]\",\n        CodeSegment,\n        name=\"function_name_identifier\",\n        type=\"function_name_identifier\",\n    ),\n    DatatypeIdentifierSegment=Ref(\"SingleIdentifierGrammar\"),\n    PrimaryKeyGrammar=Sequence(\n        \"PRIMARY\", \"KEY\", OneOf(\"CLUSTERED\", \"NONCLUSTERED\", optional=True)\n    ),\n    FromClauseTerminatorGrammar=OneOf(\n        \"WHERE\",\n        \"LIMIT\",\n        Sequence(\"GROUP\", \"BY\"),\n        Sequence(\"ORDER\", \"BY\"),\n        \"HAVING\",\n        \"PIVOT\",\n        \"UNPIVOT\",\n        Ref(\"SetOperatorSegment\"),\n        Ref(\"WithNoSchemaBindingClauseSegment\"),\n    ),\n    JoinKeywords=OneOf(\"JOIN\", \"APPLY\", Sequence(\"OUTER\", \"APPLY\")),\n)\n@tsql_dialect.segment(replace=True)\nclass StatementSegment(ansi_dialect.get_segment(\"StatementSegment\")):  # type: ignore\n    \"\"\"Overriding StatementSegment to allow for additional segment parsing.\"\"\"\n    parse_grammar = ansi_dialect.get_segment(\"StatementSegment\").parse_grammar.copy(\n        insert=[\n            Ref(\"CreateProcedureStatementSegment\"),\n            Ref(\"IfExpressionStatement\"),\n            Ref(\"DeclareStatementSegment\"),\n            Ref(\"SetStatementSegment\"),\n            Ref(\"AlterTableSwitchStatementSegment\"),\n            Ref(\n                \"CreateTableAsSelectStatementSegment\"\n            ),  # Azure Synapse Analytics specific\n        ],\n    )\n@tsql_dialect.segment(replace=True)\nclass SelectClauseModifierSegment(BaseSegment):\n    \"\"\"Things that come after SELECT but before the columns.\"\"\"\n    type = \"select_clause_modifier\"\n    match_grammar = OneOf(\n        \"DISTINCT\",\n        \"ALL\",\n        Sequence(\n            \"TOP\",\n            OptionallyBracketed(Ref(\"ExpressionSegment\")),\n            Sequence(\"PERCENT\", optional=True),\n            Sequence(\"WITH\", \"TIES\", optional=True),\n        ),\n    )\n@tsql_dialect.segment(replace=True)\nclass UnorderedSelectStatementSegment(BaseSegment):\n    \"\"\"A `SELECT` statement without any ORDER clauses or later.\n    We need to change ANSI slightly to remove LimitClauseSegment\n    and NamedWindowSegment which don't exist in T-SQL.\n    \"\"\"\n    type = \"select_statement\"\n    # match grammar. This one makes sense in the context of knowing that it's\n    # definitely a statement, we just don't know what type yet.\n    match_grammar = StartsWith(\n        # NB: In bigquery, the select clause may include an EXCEPT, which\n        # will also match the set operator, but by starting with the whole\n        # select clause rather than just the SELECT keyword, we mitigate that\n        # here.\n        Ref(\"SelectClauseSegment\"),\n        terminator=OneOf(\n            Ref(\"SetOperatorSegment\"),\n            Ref(\"WithNoSchemaBindingClauseSegment\"),\n            Ref(\"OrderByClauseSegment\"),\n        ),\n        enforce_whitespace_preceding_terminator=True,\n    )\n    parse_grammar = Sequence(\n        Ref(\"SelectClauseSegment\"),\n        # Dedent for the indent in the select clause.\n        # It's here so that it can come AFTER any whitespace.\n        Dedent,\n        Ref(\"FromClauseSegment\", optional=True),\n        Ref(\"PivotUnpivotStatementSegment\", optional=True),\n        Ref(\"WhereClauseSegment\", optional=True),\n        Ref(\"GroupByClauseSegment\", optional=True),\n        Ref(\"HavingClauseSegment\", optional=True),\n    )\n@tsql_dialect.segment(replace=True)\nclass SelectStatementSegment(BaseSegment):\n    \"\"\"A `SELECT` statement.\n    We need to change ANSI slightly to remove LimitClauseSegment\n    and NamedWindowSegment which don't exist in T-SQL.\n    \"\"\"\n    type = \"select_statement\"\n    match_grammar = ansi_dialect.get_segment(\n        \"SelectStatementSegment\"\n    ).match_grammar.copy()\n    # Remove the Limit and Window statements from ANSI\n    parse_grammar = UnorderedSelectStatementSegment.parse_grammar.copy(\n        insert=[\n            Ref(\"OrderByClauseSegment\", optional=True),\n        ]\n    )\n@tsql_dialect.segment(replace=True)\nclass CreateIndexStatementSegment(BaseSegment):\n    \"\"\"A `CREATE INDEX` statement.\n    https://docs.microsoft.com/en-us/sql/t-sql/statements/create-index-transact-sql?view=sql-server-ver15\n    \"\"\"\n    type = \"create_index_statement\"\n    match_grammar = Sequence(\n        \"CREATE\",\n        Ref(\"OrReplaceGrammar\", optional=True),\n        Sequence(\"UNIQUE\", optional=True),\n        OneOf(\"CLUSTERED\", \"NONCLUSTERED\", optional=True),\n        \"INDEX\",\n        Ref(\"IfNotExistsGrammar\", optional=True),\n        Ref(\"IndexReferenceSegment\"),\n        \"ON\",\n        Ref(\"TableReferenceSegment\"),\n        Sequence(\n            Bracketed(\n                Delimited(\n                    Ref(\"IndexColumnDefinitionSegment\"),\n                ),\n            )\n        ),\n        Sequence(\n            \"INCLUDE\",\n            Sequence(\n                Bracketed(\n                    Delimited(\n                        Ref(\"IndexColumnDefinitionSegment\"),\n                    ),\n                )\n            ),\n            optional=True,\n        ),\n    )\n@tsql_dialect.segment()\nclass PivotUnpivotStatementSegment(BaseSegment):\n    \"\"\"Declaration of a variable.\n    https://docs.microsoft.com/en-us/sql/t-sql/queries/from-using-pivot-and-unpivot?view=sql-server-ver15\n    \"\"\"\n    type = \"pivotunpivot_segment\"\n    match_grammar = StartsWith(\n        OneOf(\"PIVOT\", \"UNPIVOT\"),\n        terminator=Ref(\"FromClauseTerminatorGrammar\"),\n        enforce_whitespace_preceding_terminator=True,\n    )\n    parse_grammar = Sequence(\n        OneOf(\n            Sequence(\n                \"PIVOT\",\n                OptionallyBracketed(\n                    Sequence(\n                        OptionallyBracketed(Ref(\"FunctionSegment\")),\n                        \"FOR\",\n                        Ref(\"ColumnReferenceSegment\"),\n                        \"IN\",\n                        Bracketed(Delimited(Ref(\"ColumnReferenceSegment\"))),\n                    )\n                ),\n            ),\n            Sequence(\n                \"UNPIVOT\",\n                OptionallyBracketed(\n                    Sequence(\n                        OptionallyBracketed(Ref(\"ColumnReferenceSegment\")),\n                        \"FOR\",\n                        Ref(\"ColumnReferenceSegment\"),\n                        \"IN\",\n                        Bracketed(Delimited(Ref(\"ColumnReferenceSegment\"))),\n                    )\n                ),\n            ),\n        ),\n        \"AS\",\n        Ref(\"TableReferenceSegment\"),\n    )\n@tsql_dialect.segment()\nclass DeclareStatementSegment(BaseSegment):\n    \"\"\"Declaration of a variable.\n    https://docs.microsoft.com/en-us/sql/t-sql/language-elements/declare-local-variable-transact-sql?view=sql-server-ver15\n    \"\"\"\n    type = \"declare_segment\"\n    match_grammar = StartsWith(\"DECLARE\")\n    parse_grammar = Sequence(\n        \"DECLARE\",\n        Delimited(Ref(\"ParameterNameSegment\")),\n        Ref(\"DatatypeSegment\"),\n        Sequence(\n            Ref(\"EqualsSegment\"),\n            OneOf(\n                Ref(\"LiteralGrammar\"),\n                Bracketed(Ref(\"SelectStatementSegment\")),\n                Ref(\"BareFunctionSegment\"),\n                Ref(\"FunctionSegment\"),\n            ),\n            optional=True,\n        ),\n    )\n@tsql_dialect.segment(replace=True)\nclass ObjectReferenceSegment(BaseSegment):\n    \"\"\"A reference to an object.\n    Update ObjectReferenceSegment to only allow dot separated SingleIdentifierGrammar\n    So Square Bracketed identifiers can be matched.\n    \"\"\"\n    type = \"object_reference\"\n    # match grammar (don't allow whitespace)\n    match_grammar: Matchable = Delimited(\n        Ref(\"SingleIdentifierGrammar\"),\n        delimiter=OneOf(\n            Ref(\"DotSegment\"), Sequence(Ref(\"DotSegment\"), Ref(\"DotSegment\"))\n        ),\n        allow_gaps=False,\n    )\n@tsql_dialect.segment()\nclass GoStatementSegment(BaseSegment):\n    \"\"\"GO signals the end of a batch of Transact-SQL statements to the SQL Server utilities.\n    GO statements are not part of the TSQL language. They are used to signal batch statements\n    so that clients know in how batches of statements can be executed.\n    \"\"\"\n    type = \"go_statement\"\n    match_grammar = Sequence(\"GO\")\n@tsql_dialect.segment(replace=True)\nclass DatatypeSegment(BaseSegment):\n    \"\"\"A data type segment.\n    Updated for Transact-SQL to allow bracketed data types with bracketed schemas.\n    \"\"\"\n    type = \"data_type\"\n    match_grammar = Sequence(\n        # Some dialects allow optional qualification of data types with schemas\n        Sequence(\n            Ref(\"SingleIdentifierGrammar\"),\n            Ref(\"DotSegment\"),\n            allow_gaps=False,\n            optional=True,\n        ),\n        OneOf(\n            Ref(\"DatatypeIdentifierSegment\"),\n            Bracketed(Ref(\"DatatypeIdentifierSegment\"), bracket_type=\"square\"),\n        ),\n        Bracketed(\n            OneOf(\n                Delimited(Ref(\"ExpressionSegment\")),\n                # The brackets might be empty for some cases...\n                optional=True,\n            ),\n            # There may be no brackets for some data types\n            optional=True,\n        ),\n        Ref(\"CharCharacterSetSegment\", optional=True),\n    )\n@tsql_dialect.segment()\nclass NextValueSequenceSegment(BaseSegment):\n    \"\"\"Segment to get next value from a sequence.\"\"\"\n    type = \"sequence_next_value\"\n    match_grammar = Sequence(\n        \"NEXT\",\n        \"VALUE\",\n        \"FOR\",\n        Ref(\"ObjectReferenceSegment\"),\n    )\n@tsql_dialect.segment()\nclass IfExpressionStatement(BaseSegment):\n    \"\"\"IF-ELSE-END IF statement.\n    https://docs.microsoft.com/en-us/sql/t-sql/language-elements/if-else-transact-sql?view=sql-server-ver15\n    \"\"\"\n    type = \"if_then_statement\"\n    match_grammar = Sequence(\n        OneOf(\n            Sequence(Ref(\"IfNotExistsGrammar\"), Ref(\"SelectStatementSegment\")),\n            Sequence(Ref(\"IfExistsGrammar\"), Ref(\"SelectStatementSegment\")),\n            \"IF\",\n            Ref(\"ExpressionSegment\"),\n        ),\n        Ref(\"StatementSegment\"),\n        Sequence(\"ELSE\", Ref(\"StatementSegment\"), optional=True),\n    )\n@tsql_dialect.segment(replace=True)\nclass ColumnConstraintSegment(BaseSegment):\n    \"\"\"A column option; each CREATE TABLE column can have 0 or more.\"\"\"\n    type = \"column_constraint_segment\"\n    # Column constraint from\n    # https://www.postgresql.org/docs/12/sql-createtable.html\n    match_grammar = Sequence(\n        Sequence(\n            \"CONSTRAINT\",\n            Ref(\"ObjectReferenceSegment\"),  # Constraint name\n            optional=True,\n        ),\n        OneOf(\n            Sequence(Ref.keyword(\"NOT\", optional=True), \"NULL\"),  # NOT NULL or NULL\n            Sequence(  # DEFAULT <value>\n                \"DEFAULT\",\n                OneOf(\n                    Ref(\"LiteralGrammar\"),\n                    Ref(\"FunctionSegment\"),\n                    # ?? Ref('IntervalExpressionSegment')\n                    OptionallyBracketed(Ref(\"NextValueSequenceSegment\")),\n                ),\n            ),\n            Ref(\"PrimaryKeyGrammar\"),\n            \"UNIQUE\",  # UNIQUE\n            \"AUTO_INCREMENT\",  # AUTO_INCREMENT (MySQL)\n            \"UNSIGNED\",  # UNSIGNED (MySQL)\n            Sequence(  # REFERENCES reftable [ ( refcolumn) ]\n                \"REFERENCES\",\n                Ref(\"ColumnReferenceSegment\"),\n                # Foreign columns making up FOREIGN KEY constraint\n                Ref(\"BracketedColumnReferenceListGrammar\", optional=True),\n            ),\n            Ref(\"CommentClauseSegment\"),\n        ),\n    )\n@tsql_dialect.segment(replace=True)\nclass CreateFunctionStatementSegment(BaseSegment):\n    \"\"\"A `CREATE FUNCTION` statement.\n    This version in the TSQL dialect should be a \"common subset\" of the\n    structure of the code for those dialects.\n    Updated to include AS after declaration of RETURNS. Might be integrated in ANSI though.\n    postgres: https://www.postgresql.org/docs/9.1/sql-createfunction.html\n    snowflake: https://docs.snowflake.com/en/sql-reference/sql/create-function.html\n    bigquery: https://cloud.google.com/bigquery/docs/reference/standard-sql/user-defined-functions\n    tsql/mssql : https://docs.microsoft.com/en-us/sql/t-sql/statements/create-function-transact-sql?view=sql-server-ver15\n    \"\"\"\n    type = \"create_function_statement\"\n    match_grammar = Sequence(\n        \"CREATE\",\n        Sequence(\"OR\", \"ALTER\", optional=True),\n        \"FUNCTION\",\n        Anything(),\n    )\n    parse_grammar = Sequence(\n        \"CREATE\",\n        Sequence(\"OR\", \"ALTER\", optional=True),\n        \"FUNCTION\",\n        Ref(\"ObjectReferenceSegment\"),\n        Ref(\"FunctionParameterListGrammar\"),\n        Sequence(  # Optional function return type\n            \"RETURNS\",\n            Ref(\"DatatypeSegment\"),\n            optional=True,\n        ),\n        Ref(\"FunctionDefinitionGrammar\"),\n    )\n@tsql_dialect.segment()\nclass SetStatementSegment(BaseSegment):\n    \"\"\"A Set statement.\n    Setting an already declared variable or global variable.\n    https://docs.microsoft.com/en-us/sql/t-sql/statements/set-statements-transact-sql?view=sql-server-ver15\n    \"\"\"\n    type = \"set_segment\"\n    match_grammar = StartsWith(\"SET\")\n    parse_grammar = Sequence(\n        \"SET\",\n        OneOf(\n            Ref(\"ParameterNameSegment\"),\n            \"DATEFIRST\",\n            \"DATEFORMAT\",\n            \"DEADLOCK_PRIORITY\",\n            \"LOCK_TIMEOUT\",\n            \"CONCAT_NULL_YIELDS_NULL\",\n            \"CURSOR_CLOSE_ON_COMMIT\",\n            \"FIPS_FLAGGER\",\n            \"IDENTITY_INSERT\",\n            \"LANGUAGE\",\n            \"OFFSETS\",\n            \"QUOTED_IDENTIFIER\",\n            \"ARITHABORT\",\n            \"ARITHIGNORE\",\n            \"FMTONLY\",\n            \"NOCOUNT\",\n            \"NOEXEC\",\n            \"NUMERIC_ROUNDABORT\",\n            \"PARSEONLY\",\n            \"QUERY_GOVERNOR_COST_LIMIT\",\n            \"RESULT CACHING (Preview)\",\n            \"ROWCOUNT\",\n            \"TEXTSIZE\",\n            \"ANSI_DEFAULTS\",\n            \"ANSI_NULL_DFLT_OFF\",\n            \"ANSI_NULL_DFLT_ON\",\n            \"ANSI_NULLS\",\n            \"ANSI_PADDING\",\n            \"ANSI_WARNINGS\",\n            \"FORCEPLAN\",\n            \"SHOWPLAN_ALL\",\n            \"SHOWPLAN_TEXT\",\n            \"SHOWPLAN_XML\",\n            \"STATISTICS IO\",\n            \"STATISTICS XML\",\n            \"STATISTICS PROFILE\",\n            \"STATISTICS TIME\",\n            \"IMPLICIT_TRANSACTIONS\",\n            \"REMOTE_PROC_TRANSACTIONS\",\n            \"TRANSACTION ISOLATION LEVEL\",\n            \"XACT_ABORT\",\n        ),\n        OneOf(\n            \"ON\",\n            \"OFF\",\n            Sequence(\n                Ref(\"EqualsSegment\"),\n                OneOf(\n                    Delimited(\n                        OneOf(\n                            Ref(\"LiteralGrammar\"),\n                            Bracketed(Ref(\"SelectStatementSegment\")),\n                            Ref(\"FunctionSegment\"),\n                            Bracketed(\n                                Delimited(\n                                    OneOf(\n                                        Ref(\"LiteralGrammar\"),\n                                        Bracketed(Ref(\"SelectStatementSegment\")),\n                                        Ref(\"BareFunctionSegment\"),\n                                        Ref(\"FunctionSegment\"),\n                                    )\n                                )",
        "file_path": "src/sqlfluff/dialects/dialect_tsql.py",
        "chunk_index": 0,
        "metadata": {}
      }
    ]
  },
  "sqlfluff__sqlfluff-1763": {
    "query": "dbt postgres fix command errors with UnicodeEncodeError and also wipes the .sql file\n_If this is a parsing or linting issue, please include a minimal SQL example which reproduces the issue, along with the `sqlfluff parse` output, `sqlfluff lint` output and `sqlfluff fix` output when relevant._\r\n\r\n## Expected Behaviour\r\nViolation failure notice at a minimum, without wiping the file. Would like a way to ignore the known error at a minimum as --noqa is not getting past this. Actually would expect --noqa to totally ignore this.\r\n\r\n## Observed Behaviour\r\nReported error: `UnicodeEncodeError: 'charmap' codec can't encode character '\\u2192' in position 120: character maps to <undefined>`\r\n\r\n## Steps to Reproduce\r\nSQL file:\r\n```sql\r\nSELECT\r\n    reacted_table_name_right.descendant_id AS category_id,\r\n    string_agg(redacted_table_name_left.name, ' → ' ORDER BY reacted_table_name_right.generations DESC) AS breadcrumbs -- noqa\r\nFROM {{ ref2('redacted_schema_name', 'redacted_table_name_left') }} AS redacted_table_name_left\r\nINNER JOIN {{ ref2('redacted_schema_name', 'reacted_table_name_right') }} AS reacted_table_name_right\r\n    ON redacted_table_name_left.id = order_issue_category_hierarchies.ancestor_id\r\nGROUP BY reacted_table_name_right.descendant_id\r\n```\r\nRunning `sqlfluff fix --ignore templating,parsing,lexing -vvvv` and accepting proposed fixes for linting violations.\r\n\r\n## Dialect\r\n`postgres`, with `dbt` templater\r\n\r\n## Version\r\n`python 3.7.12`\r\n`sqlfluff 0.7.0`\r\n`sqlfluff-templater-dbt 0.7.0`\r\n\r\n## Configuration\r\nI've tried a few, here's one:\r\n```\r\n[sqlfluff]\r\nverbose = 2\r\ndialect = postgres\r\ntemplater = dbt\r\nexclude_rules = None\r\noutput_line_length = 80\r\nrunaway_limit = 10\r\nignore_templated_areas = True\r\nprocesses = 3\r\n# Comma separated list of file extensions to lint.\r\n\r\n# NB: This config will only apply in the root folder.\r\nsql_file_exts = .sql\r\n\r\n[sqlfluff:indentation]\r\nindented_joins = False\r\nindented_using_on = True\r\ntemplate_blocks_indent = True\r\n\r\n[sqlfluff:templater]\r\nunwrap_wrapped_queries = True\r\n\r\n[sqlfluff:templater:jinja]\r\napply_dbt_builtins = True\r\n\r\n[sqlfluff:templater:jinja:macros]\r\n# Macros provided as builtins for dbt projects\r\ndbt_ref = {% macro ref(model_ref) %}{{model_ref}}{% endmacro %}\r\ndbt_source = {% macro source(source_name, table) %}{{source_name}}_{{table}}{% endmacro %}\r\ndbt_config = {% macro config() %}{% for k in kwargs %}{% endfor %}{% endmacro %}\r\ndbt_var = {% macro var(variable, default='') %}item{% endmacro %}\r\ndbt_is_incremental = {% macro is_incremental() %}True{% endmacro %}\r\n\r\n# Common config across rules\r\n[sqlfluff:rules]\r\ntab_space_size = 4\r\nindent_unit = space\r\nsingle_table_references = consistent\r\nunquoted_identifiers_policy = all\r\n\r\n# L001 - Remove trailing whitespace (fix)\r\n# L002 - Single section of whitespace should not contain both tabs and spaces (fix)\r\n# L003 - Keep consistent indentation (fix)\r\n# L004 - We use 4 spaces for indentation just for completeness (fix)\r\n# L005 - Remove space before commas (fix)\r\n# L006 - Operators (+, -, *, /) will be wrapped by a single space each side (fix)\r\n\r\n# L007 - Operators should not be at the end of a line\r\n[sqlfluff:rules:L007]  # Keywords\r\noperator_new_lines = after\r\n\r\n# L008 - Always use a single whitespace after a comma (fix)\r\n# L009 - Files will always end with a trailing newline\r\n\r\n# L010 - All keywords will use full upper case (fix)\r\n[sqlfluff:rules:L010]  # Keywords\r\ncapitalisation_policy = upper\r\n\r\n# L011 - Always explicitly alias tables (fix)\r\n[sqlfluff:rules:L011]  # Aliasing\r\naliasing = explicit\r\n\r\n# L012 - Do not have to explicitly alias all columns\r\n[sqlfluff:rules:L012]  # Aliasing\r\naliasing = explicit\r\n\r\n# L013 - Always explicitly alias a column with an expression in it (fix)\r\n[sqlfluff:rules:L013]  # Aliasing\r\nallow_scalar = False\r\n\r\n# L014 - Always user full lower case for 'quoted identifiers' -> column refs. without an alias (fix)\r\n[sqlfluff:rules:L014]  # Unquoted identifiers\r\nextended_capitalisation_policy = lower\r\n\r\n# L015 - Always remove parenthesis when using DISTINCT to be clear that DISTINCT applies to all columns (fix)\r\n\r\n# L016 - Lines should be 120 characters of less. Comment lines should not be ignored (fix)\r\n[sqlfluff:rules:L016]\r\nignore_comment_lines = False\r\nmax_line_length = 120\r\n\r\n# L017 - There should not be whitespace between function name and brackets (fix)\r\n# L018 - Always align closing bracket of WITH to the WITH keyword (fix)\r\n\r\n# L019 - Always use trailing commas / commas at the end of the line (fix)\r\n[sqlfluff:rules:L019]\r\ncomma_style = trailing\r\n\r\n# L020 - Table aliases will always be unique per statement\r\n# L021 - Remove any use of ambiguous DISTINCT and GROUP BY combinations. Lean on removing the GROUP BY.\r\n# L022 - Add blank lines after common table expressions (CTE) / WITH.\r\n# L023 - Always add a single whitespace after AS in a WITH clause (fix)\r\n\r\n[sqlfluff:rules:L026]\r\nforce_enable = False\r\n\r\n# L027 - Always add references if more than one referenced table or view is used\r\n\r\n[sqlfluff:rules:L028]\r\nforce_enable = False\r\n\r\n[sqlfluff:rules:L029]  # Keyword identifiers\r\nunquoted_identifiers_policy = aliases\r\n\r\n[sqlfluff:rules:L030]  # Function names\r\ncapitalisation_policy = upper\r\n\r\n# L032 - We prefer use of join keys rather than USING\r\n# L034 - We prefer ordering of columns in select statements as (fix):\r\n# 1. wildcards\r\n# 2. single identifiers\r\n# 3. calculations and aggregates\r\n\r\n# L035 - Omit 'else NULL'; it is redundant (fix)\r\n# L036 - Move select targets / identifiers onto new lines each (fix)\r\n# L037 - When using ORDER BY, make the direction explicit (fix)\r\n\r\n# L038 - Never use trailing commas at the end of the SELECT clause\r\n[sqlfluff:rules:L038]\r\nselect_clause_trailing_comma = forbid\r\n\r\n# L039 - Remove unnecessary whitespace (fix)\r\n\r\n[sqlfluff:rules:L040]  # Null & Boolean Literals\r\ncapitalisation_policy = upper\r\n\r\n# L042 - Join clauses should not contain subqueries. Use common tables expressions (CTE) instead.\r\n[sqlfluff:rules:L042]\r\n# By default, allow subqueries in from clauses, but not join clauses.\r\nforbid_subquery_in = join\r\n\r\n# L043 - Reduce CASE WHEN conditions to COALESCE (fix)\r\n# L044 - Prefer a known number of columns along the path to the source data\r\n# L045 - Remove unused common tables expressions (CTE) / WITH statements (fix)\r\n# L046 - Jinja tags should have a single whitespace on both sides\r\n\r\n# L047 - Use COUNT(*) instead of COUNT(0) or COUNT(1) alternatives (fix)\r\n[sqlfluff:rules:L047]  # Consistent syntax to count all rows\r\nprefer_count_1 = False\r\nprefer_count_0 = False\r\n\r\n# L048 - Quoted literals should be surrounded by a single whitespace (fix)\r\n# L049 - Always use IS or IS NOT for comparisons with NULL (fix)\r\n```\r\n\n",
    "method": "sliding",
    "top_k": 10,
    "results": [
      {
        "rank": 1,
        "score": 0.668727695941925,
        "content": "\"\"\"This is an example of how to use the simple sqlfluff api.\"\"\"\nimport sqlfluff\n#  -------- LINTING ----------\nmy_bad_query = \"SeLEct  *, 1, blah as  fOO  from myTable\"\n# Lint the given string and get a list of violations found.\nresult = sqlfluff.lint(my_bad_query, dialect=\"bigquery\")\n# result =\n# [\n#     {\"code\": \"L010\", \"line_no\": 1, \"line_pos\": 1, \"description\": \"Keywords must be consistently upper case.\"}\n#     ...\n# ]\n#  -------- FIXING ----------\n# Fix the given string and get a string back which has been fixed.\nresult = sqlfluff.fix(my_bad_query, dialect=\"bigquery\")\n# result = 'SELECT  *, 1, blah AS  foo  FROM mytable\\n'\n# We can also fix just specific rules.\nresult = sqlfluff.fix(my_bad_query, rules=\"L010\")\n# result = 'SELECT  *, 1, blah AS  fOO  FROM myTable'\n# Or a subset of rules...\nresult = sqlfluff.fix(my_bad_query, rules=[\"L010\", \"L014\"])\n# result = 'SELECT  *, 1, blah AS  fOO  FROM mytable'\n#  -------- PARSING ----------\n# NOTE: sqlfluff is still in a relatively early phase of its\n# development and so until version 1.0.0 will offer no guarantee\n# that the names and structure of the objects returned by these\n# parse commands won't change between releases. Use with care\n# and keep updated with the changelog for the project for any\n# changes in this space.\nparsed = sqlfluff.parse(my_bad_query)\n# Get the structure of the query\nstructure = parsed.tree.to_tuple(show_raw=True, code_only=True)\n# structure = ('file', (('statement', (('select_statement', (('select_clause', (('keyword', 'SeLEct'), ...\n# Extract certain elements\nkeywords = [keyword.raw for keyword in parsed.tree.recursive_crawl(\"keyword\")]\n# keywords = ['SeLEct', 'as', 'from']\ntbl_refs = [tbl_ref.raw for tbl_ref in parsed.tree.recursive_crawl(\"table_reference\")]\n# tbl_refs == [\"myTable\"]",
        "file_path": "examples/01_basic_api_usage.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 2,
        "score": 0.6604406833648682,
        "content": "        click.echo(\"Done. Please check your files to confirm.\")\n        return True\n    # If some failed then return false\n    click.echo(\n        \"Done. Some operations failed. Please check your files to confirm.\"\n    )  # pragma: no cover\n    click.echo(\n        \"Some errors cannot be fixed or there is another error blocking it.\"\n    )  # pragma: no cover\n    return False  # pragma: no cover\n@cli.command()\n@common_options\n@core_options\n@click.option(\n    \"-f\",\n    \"--force\",\n    is_flag=True,\n    help=(\n        \"skip the confirmation prompt and go straight to applying \"\n        \"fixes. **Use this with caution.**\"\n    ),\n)\n@click.option(\n    \"--fixed-suffix\", default=None, help=\"An optional suffix to add to fixed files.\"\n)\n@click.option(\n    \"-p\",\n    \"--processes\",\n    type=int,\n    default=1,\n    help=\"The number of parallel processes to run.\",\n)\n@click.argument(\"paths\", nargs=-1)\ndef fix(\n    force: bool,\n    paths: Tuple[str],\n    processes: int,\n    bench: bool = False,\n    fixed_suffix: str = \"\",\n    logger: Optional[logging.Logger] = None,\n    **kwargs,\n) -> NoReturn:\n    \"\"\"Fix SQL files.\n    PATH is the path to a sql file or directory to lint. This can be either a\n    file ('path/to/file.sql'), a path ('directory/of/sql/files'), a single ('-')\n    character to indicate reading from *stdin* or a dot/blank ('.'/' ') which will\n    be interpreted like passing the current working directory as a path argument.\n    \"\"\"\n    # some quick checks\n    fixing_stdin = (\"-\",) == paths\n    config = get_config(**kwargs)\n    lnt, formatter = get_linter_and_formatter(config, silent=fixing_stdin)\n    verbose = config.get(\"verbose\")\n    exit_code = 0\n    formatter.dispatch_config(lnt)\n    # Set up logging.\n    set_logging_level(verbosity=verbose, logger=logger, stderr_output=fixing_stdin)\n    # handle stdin case. should output formatted sql to stdout and nothing else.\n    if fixing_stdin:\n        stdin = sys.stdin.read()\n        result = lnt.lint_string_wrapped(stdin, fname=\"stdin\", fix=True)\n        templater_error = result.num_violations(types=SQLTemplaterError) > 0\n        unfixable_error = result.num_violations(types=SQLLintError, fixable=False) > 0\n        if result.num_violations(types=SQLLintError, fixable=True) > 0:\n            stdout = result.paths[0].files[0].fix_string()[0]\n        else:\n            stdout = stdin\n        if templater_error:\n            click.echo(\n                colorize(\n                    \"Fix aborted due to unparseable template variables.\",\n                    Color.red,\n                ),\n                err=True,\n            )\n            click.echo(\n                colorize(\n                    \"Use '--ignore templating' to attempt to fix anyway.\",\n                    Color.red,\n                ),\n                err=True,\n            )\n        if unfixable_error:\n            click.echo(colorize(\"Unfixable violations detected.\", Color.red), err=True)\n        click.echo(stdout, nl=False)\n        sys.exit(1 if templater_error or unfixable_error else 0)\n    # Lint the paths (not with the fix argument at this stage), outputting as we go.\n    click.echo(\"==== finding fixable violations ====\")\n    try:\n        result = lnt.lint_paths(\n            paths, fix=True, ignore_non_existent_files=False, processes=processes\n        )\n    except OSError:\n        click.echo(\n            colorize(\n                f\"The path(s) '{paths}' could not be accessed. Check it/they exist(s).\",\n                Color.red,\n            ),\n            err=True,\n        )\n        sys.exit(1)\n    # NB: We filter to linting violations here, because they're\n    # the only ones which can be potentially fixed.\n    if result.num_violations(types=SQLLintError, fixable=True) > 0:\n        click.echo(\"==== fixing violations ====\")\n        click.echo(\n            f\"{result.num_violations(types=SQLLintError, fixable=True)} fixable linting violations found\"\n        )\n        if force:\n            click.echo(f\"{colorize('FORCE MODE', Color.red)}: Attempting fixes...\")\n            success = do_fixes(\n                lnt,\n                result,\n                formatter,\n                types=SQLLintError,\n                fixed_file_suffix=fixed_suffix,\n            )\n            if not success:\n                sys.exit(1)  # pragma: no cover\n        else:\n            click.echo(\n                \"Are you sure you wish to attempt to fix these? [Y/n] \", nl=False\n            )\n            c = click.getchar().lower()\n            click.echo(\"...\")\n            if c in (\"y\", \"\\r\", \"\\n\"):\n                click.echo(\"Attempting fixes...\")\n                success = do_fixes(\n                    lnt,\n                    result,\n                    formatter,\n                    types=SQLLintError,\n                    fixed_file_suffix=fixed_suffix,\n                )\n                if not success:\n                    sys.exit(1)  # pragma: no cover\n                else:\n                    _completion_message(config)\n            elif c == \"n\":\n                click.echo(\"Aborting...\")\n                exit_code = 1\n            else:  # pragma: no cover\n                click.echo(\"Invalid input, please enter 'Y' or 'N'\")\n                click.echo(\"Aborting...\")\n                exit_code = 1\n    else:\n        click.echo(\"==== no fixable linting violations found ====\")\n        _completion_message(config)\n    if result.num_violations(types=SQLLintError, fixable=False) > 0:\n        click.echo(\n            f\"  [{result.num_violations(types=SQLLintError, fixable=False)} unfixable linting violations found]\"\n        )\n        exit_code = 1\n    if result.num_violations(types=SQLTemplaterError) > 0:\n        click.echo(\n            f\"  [{result.num_violations(types=SQLTemplaterError)} templating errors found]\"\n        )\n        exit_code = 1\n    if bench:\n        click.echo(\"==== overall timings ====\")\n        click.echo(cli_table([(\"Clock time\", result.total_time)]))\n        timing_summary = result.timing_summary()\n        for step in timing_summary:\n            click.echo(f\"=== {step} ===\")\n            click.echo(cli_table(timing_summary[step].items()))\n    sys.exit(exit_code)\ndef _completion_message(config: FluffConfig) -> None:\n    click.echo(f\"All Finished{'' if config.get('nocolor') else ' 📜 🎉'}!\")\ndef quoted_presenter(dumper, data):\n    \"\"\"Re-presenter which always double quotes string values needing escapes.\"\"\"\n    if \"\\n\" in data or \"\\t\" in data or \"'\" in data:\n        return dumper.represent_scalar(\"tag:yaml.org,2002:str\", data, style='\"')\n    else:\n        return dumper.represent_scalar(\"tag:yaml.org,2002:str\", data, style=\"\")\n@cli.command()\n@common_options\n@core_options\n@click.argument(\"path\", nargs=1)\n@click.option(\n    \"--recurse\", default=0, help=\"The depth to recursively parse to (0 for unlimited)\"\n)\n@click.option(\n    \"-c\",\n    \"--code-only\",\n    is_flag=True,\n    help=\"Output only the code elements of the parse tree.\",\n)\n@click.option(\n    \"-m\",\n    \"--include-meta\",\n    is_flag=True,\n    help=(\n        \"Include meta segments (indents, dedents and placeholders) in the output. \"\n        \"This only applies when outputting json or yaml.\"\n    ),\n)\n@click.option(\n    \"-f\",\n    \"--format\",\n    default=FormatType.human.value,\n    type=click.Choice(\n        [\n            FormatType.human.value,\n            FormatType.json.value,\n            FormatType.yaml.value,\n        ],\n        case_sensitive=False,\n    ),\n    help=\"What format to return the parse result in.\",\n)\n@click.option(\n    \"--profiler\", is_flag=True, help=\"Set this flag to engage the python profiler.\"\n)\n@click.option(\n    \"--nofail\",\n    is_flag=True,\n    help=(\n        \"If set, the exit code will always be zero, regardless of violations \"\n        \"found. This is potentially useful during rollout.\"\n    ),\n)\ndef parse(\n    path: str,\n    code_only: bool,\n    include_meta: bool,\n    format: str,\n    profiler: bool,\n    bench: bool,\n    nofail: bool,\n    logger: Optional[logging.Logger] = None,\n    **kwargs,\n) -> NoReturn:\n    \"\"\"Parse SQL files and just spit out the result.\n    PATH is the path to a sql file or directory to lint. This can be either a\n    file ('path/to/file.sql'), a path ('directory/of/sql/files'), a single ('-')\n    character to indicate reading from *stdin* or a dot/blank ('.'/' ') which will\n    be interpreted like passing the current working directory as a path argument.\n    \"\"\"\n    c = get_config(**kwargs)\n    # We don't want anything else to be logged if we want json or yaml output\n    non_human_output = format in (FormatType.json.value, FormatType.yaml.value)\n    lnt, formatter = get_linter_and_formatter(c, silent=non_human_output)\n    verbose = c.get(\"verbose\")\n    recurse = c.get(\"recurse\")\n    formatter.dispatch_config(lnt)\n    # Set up logging.\n    set_logging_level(verbosity=verbose, logger=logger, stderr_output=non_human_output)\n    # TODO: do this better\n    if profiler:\n        # Set up the profiler if required\n        try:\n            import cProfile\n        except ImportError:  # pragma: no cover\n            click.echo(\"The cProfiler is not available on your platform.\")\n            sys.exit(1)\n        pr = cProfile.Profile()\n        pr.enable()\n    try:\n        t0 = time.monotonic()\n        # handle stdin if specified via lone '-'\n        if \"-\" == path:\n            parsed_strings = [\n                lnt.parse_string(\n                    sys.stdin.read(), \"stdin\", recurse=recurse, config=lnt.config\n                ),\n            ]\n        else:\n            # A single path must be specified for this command\n            parsed_strings = list(lnt.parse_path(path, recurse=recurse))\n        total_time = time.monotonic() - t0\n        violations_count = 0\n        # iterative print for human readout\n        if format == FormatType.human.value:\n            violations_count = _print_out_violations_and_timing(\n                bench, code_only, total_time, verbose, parsed_strings\n            )\n        else:\n            parsed_strings_dict = [\n                dict(\n                    filepath=linted_result.fname,\n                    segments=linted_result.tree.as_record(\n                        code_only=code_only, show_raw=True, include_meta=include_meta\n                    )\n                    if linted_result.tree\n                    else None,\n                )\n                for linted_result in parsed_strings\n            ]\n            if format == FormatType.yaml.value:\n                # For yaml dumping always dump double quoted strings if they contain tabs or newlines.\n                yaml.add_representer(str, quoted_presenter)\n                click.echo(yaml.dump(parsed_strings_dict))\n            elif format == FormatType.json.value:\n                click.echo(json.dumps(parsed_strings_dict))\n    except OSError:  # pragma: no cover\n        click.echo(\n            colorize(\n                f\"The path '{path}' could not be accessed. Check it exists.\",\n                Color.red,\n            ),\n            err=True,\n        )\n        sys.exit(1)\n    if profiler:\n        pr.disable()\n        profiler_buffer = StringIO()\n        ps = pstats.Stats(pr, stream=profiler_buffer).sort_stats(\"cumulative\")\n        ps.print_stats()\n        click.echo(\"==== profiler stats ====\")\n        # Only print the first 50 lines of it\n        click.echo(\"\\n\".join(profiler_buffer.getvalue().split(\"\\n\")[:50]))\n    if violations_count > 0 and not nofail:\n        sys.exit(66)  # pragma: no cover\n    else:\n        sys.exit(0)\ndef _print_out_violations_and_timing(\n    bench: bool,\n    code_only: bool,\n    total_time: float,\n    verbose: int,\n    parsed_strings: List[ParsedString],\n) -> int:\n    \"\"\"Used by human formatting during the parse.\"\"\"\n    violations_count = 0\n    timing = TimingSummary()\n    for parsed_string in parsed_strings:\n        timing.add(parsed_string.time_dict)\n        if parsed_string.tree:\n            click.echo(parsed_string.tree.stringify(code_only=code_only))\n        else:\n            # TODO: Make this prettier\n            click.echo(\"...Failed to Parse...\")  # pragma: no cover\n        violations_count += len(parsed_string.violations)\n        if parsed_string.violations:\n            click.echo(\"==== parsing violations ====\")  # pragma: no cover\n        for v in parsed_string.violations:\n            click.echo(format_violation(v))  # pragma: no cover\n        if parsed_string.violations and parsed_string.config.get(\"dialect\") == \"ansi\":\n            click.echo(format_dialect_warning())  # pragma: no cover\n        if verbose >= 2:\n            click.echo(\"==== timings ====\")\n            click.echo(cli_table(parsed_string.time_dict.items()))\n    if verbose >= 2 or bench:\n        click.echo(\"==== overall timings ====\")\n        click.echo(cli_table([(\"Clock time\", total_time)]))\n        timing_summary = timing.summary()\n        for step in timing_summary:\n            click.echo(f\"=== {step} ===\")\n            click.echo(cli_table(timing_summary[step].items()))\n    return violations_count\n# This \"__main__\" handler allows invoking SQLFluff using \"python -m\", which\n# simplifies the use of cProfile, e.g.:\n# python -m cProfile -s cumtime -m sqlfluff.cli.commands lint slow_file.sql\nif __name__ == \"__main__\":\n    cli.main(sys.argv[1:])  # pragma: no cover",
        "file_path": "src/sqlfluff/cli/commands.py",
        "chunk_index": 1,
        "metadata": {}
      },
      {
        "rank": 3,
        "score": 0.6518934965133667,
        "content": "\"\"\"The simple public API methods.\"\"\"\nfrom sqlfluff.core import Linter\nclass APIParsingError(ValueError):\n    \"\"\"An exception which holds a set of violations.\"\"\"\n    def __init__(self, violations, **kwargs):\n        self.violations = violations\n        self.msg = f\"Found {len(violations)} issues while parsing string.\"\n        for viol in violations:\n            self.msg += f\"\\n{viol!s}\"\n        super().__init__(self.msg, **kwargs)\ndef _unify_str_or_file(sql):\n    \"\"\"Unify string and files in the same format.\"\"\"\n    if not isinstance(sql, str):\n        try:\n            sql = sql.read()\n        except AttributeError:  # pragma: no cover\n            raise TypeError(\"Value passed as sql is not a string or a readable object.\")\n    return sql\ndef lint(sql, dialect=\"ansi\", rules=None):\n    \"\"\"Lint a sql string or file.\n    Args:\n        sql (:obj:`str` or file-like object): The sql to be linted\n            either as a string or a subclass of :obj:`TextIOBase`.\n        dialect (:obj:`str`, optional): A reference to the dialect of the sql\n            to be linted. Defaults to `ansi`.\n        rules (:obj:`str` or iterable of :obj:`str`, optional): A subset of rule\n            reference to lint for.\n    Returns:\n        :obj:`list` of :obj:`dict` for each violation found.\n    \"\"\"\n    sql = _unify_str_or_file(sql)\n    linter = Linter(dialect=dialect, rules=rules)\n    result = linter.lint_string_wrapped(sql)\n    result_records = result.as_records()\n    # Return just the violations for this file\n    return [] if not result_records else result_records[0][\"violations\"]\ndef fix(sql, dialect=\"ansi\", rules=None):\n    \"\"\"Fix a sql string or file.\n    Args:\n        sql (:obj:`str` or file-like object): The sql to be linted\n            either as a string or a subclass of :obj:`TextIOBase`.\n        dialect (:obj:`str`, optional): A reference to the dialect of the sql\n            to be linted. Defaults to `ansi`.\n        rules (:obj:`str` or iterable of :obj:`str`, optional): A subset of rule\n            reference to lint for.\n    Returns:\n        :obj:`str` for the fixed sql if possible.\n    \"\"\"\n    sql = _unify_str_or_file(sql)\n    linter = Linter(dialect=dialect, rules=rules)\n    result = linter.lint_string_wrapped(sql, fix=True)\n    fixed_string = result.paths[0].files[0].fix_string()[0]\n    return fixed_string\ndef parse(sql, dialect=\"ansi\"):\n    \"\"\"Parse a sql string or file.\n    Args:\n        sql (:obj:`str` or file-like object): The sql to be linted\n            either as a string or a subclass of :obj:`TextIOBase`.\n        dialect (:obj:`str`, optional): A reference to the dialect of the sql\n            to be linted. Defaults to `ansi`.\n    Returns:\n        :obj:`ParsedString` containing the parsed structure.\n    \"\"\"\n    sql = _unify_str_or_file(sql)\n    linter = Linter(dialect=dialect)\n    parsed = linter.parse_string(sql)\n    # If we encounter any parsing errors, raise them in a combined issue.\n    if parsed.violations:\n        raise APIParsingError(parsed.violations)\n    return parsed",
        "file_path": "src/sqlfluff/api/simple.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 4,
        "score": 0.6467386484146118,
        "content": "\"\"\"Errors - these are closely linked to what used to be called violations.\"\"\"\nfrom typing import Optional, Tuple\nCheckTuple = Tuple[str, int, int]\nclass SQLBaseError(ValueError):\n    \"\"\"Base Error Class for all violations.\"\"\"\n    _code: Optional[str] = None\n    _identifier = \"base\"\n    def __init__(\n        self,\n        *args,\n        pos=None,\n        line_no=0,\n        line_pos=0,\n        ignore=False,\n        fatal=False,\n        **kwargs\n    ):\n        self.fatal = fatal\n        self.ignore = ignore\n        if pos:\n            self.line_no, self.line_pos = pos.source_position()\n        else:\n            self.line_no = line_no\n            self.line_pos = line_pos\n        super().__init__(*args, **kwargs)\n    @property\n    def fixable(self):\n        \"\"\"Should this error be considered fixable?\"\"\"\n        return False\n    def rule_code(self):\n        \"\"\"Fetch the code of the rule which cause this error.\n        NB: This only returns a real code for some subclasses of\n        error, (the ones with a `rule` attribute), but otherwise\n        returns a placeholder value which can be used instead.\n        \"\"\"\n        if hasattr(self, \"rule\"):\n            return self.rule.code\n        else:\n            return self._code or \"????\"\n    def desc(self):\n        \"\"\"Fetch a description of this violation.\n        NB: For violations which don't directly implement a rule\n        this attempts to return the error message linked to whatever\n        caused the violation. Optionally some errors may have their\n        description set directly.\n        \"\"\"\n        if hasattr(self, \"description\") and self.description:\n            # This can only override if it's present AND\n            # if it's non-null.\n            return self.description\n        elif hasattr(self, \"rule\"):\n            return self.rule.description\n        else:\n            # Return the first element - probably a string message\n            if len(self.args) > 1:  # pragma: no cover TODO?\n                return self.args\n            elif len(self.args) == 1:\n                return self.args[0]\n            else:  # pragma: no cover TODO?\n                return self.__class__.__name__\n    def get_info_dict(self):\n        \"\"\"Return a dict of properties.\n        This is useful in the API for outputting violations.\n        \"\"\"\n        return {\n            \"line_no\": self.line_no,\n            \"line_pos\": self.line_pos,\n            \"code\": self.rule_code(),\n            \"description\": self.desc(),\n        }\n    def ignore_if_in(self, ignore_iterable):\n        \"\"\"Ignore this violation if it matches the iterable.\"\"\"\n        # Type conversion\n        if isinstance(ignore_iterable, str):  # pragma: no cover TODO?\n            ignore_iterable = []\n        # Ignoring\n        if self._identifier in ignore_iterable:\n            self.ignore = True\nclass SQLTemplaterError(SQLBaseError):\n    \"\"\"An error which occurred during templating.\n    Args:\n        pos (:obj:`PosMarker`, optional): The position which the error\n            occured at.\n    \"\"\"\n    _code = \"TMP\"\n    _identifier = \"templating\"\nclass SQLTemplaterSkipFile(RuntimeError):\n    \"\"\"An error returned from a templater to skip a file.\"\"\"\n    pass\nclass SQLLexError(SQLBaseError):\n    \"\"\"An error which occurred during lexing.\n    Args:\n        pos (:obj:`PosMarker`, optional): The position which the error\n            occured at.\n    \"\"\"\n    _code = \"LXR\"\n    _identifier = \"lexing\"\nclass SQLParseError(SQLBaseError):\n    \"\"\"An error which occurred during parsing.\n    Args:\n        segment (:obj:`BaseSegment`, optional): The segment which is relevant\n            for the failure in parsing. This is likely to be a subclass of\n            `BaseSegment` rather than the parent class itself. This is mostly\n            used for logging and for referencing position.\n    \"\"\"\n    _code = \"PRS\"\n    _identifier = \"parsing\"\n    def __init__(self, *args, segment=None, **kwargs):\n        # Store the segment on creation - we might need it later\n        self.segment = segment\n        if self.segment:\n            kwargs[\"pos\"] = self.segment.pos_marker\n        super().__init__(*args, **kwargs)\nclass SQLLintError(SQLBaseError):\n    \"\"\"An error which occurred during linting.\n    In particular we reference the rule here to do extended logging based on\n    the rule in question which caused the fail.\n    Args:\n        segment (:obj:`BaseSegment`, optional): The segment which is relevant\n            for the failure in parsing. This is likely to be a subclass of\n            `BaseSegment` rather than the parent class itself. This is mostly\n            used for logging and for referencing position.\n    \"\"\"\n    _identifier = \"linting\"\n    def __init__(\n        self, *args, segment=None, rule=None, fixes=None, description=None, **kwargs\n    ):\n        # Something about position, message and fix?\n        self.segment = segment\n        if self.segment:\n            kwargs[\"pos\"] = self.segment.pos_marker\n        self.rule = rule\n        self.fixes = fixes or []\n        self.description = description\n        super().__init__(*args, **kwargs)\n    @property\n    def fixable(self):\n        \"\"\"Should this error be considered fixable?\"\"\"\n        if self.fixes:\n            return True\n        return False\n    def check_tuple(self) -> CheckTuple:\n        \"\"\"Get a tuple representing this error. Mostly for testing.\"\"\"\n        return (\n            self.rule.code,\n            self.line_no,\n            self.line_pos,\n        )\n    def __repr__(self):\n        return \"<SQLLintError: rule {} pos:{!r}, #fixes: {}, description: {}>\".format(\n            self.rule_code(),\n            (self.line_no, self.line_pos),\n            len(self.fixes),\n            self.description,\n        )\nclass SQLFluffUserError(ValueError):\n    \"\"\"An error which should be fed back to the user.\"\"\"",
        "file_path": "src/sqlfluff/core/errors.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 5,
        "score": 0.6140047907829285,
        "content": "\"\"\"Defines the LintedFile class.\nThis holds linting results for a single file, and also\ncontains all of the routines to apply fixes to that file\npost linting.\n\"\"\"\nimport os\nimport logging\nfrom typing import (\n    Any,\n    Iterable,\n    List,\n    NamedTuple,\n    Optional,\n    Tuple,\n    Union,\n    cast,\n    Type,\n)\nfrom sqlfluff.core.errors import (\n    SQLBaseError,\n    SQLLintError,\n    CheckTuple,\n)\nfrom sqlfluff.core.string_helpers import findall\nfrom sqlfluff.core.templaters import TemplatedFile\n# Classes needed only for type checking\nfrom sqlfluff.core.parser.segments.base import BaseSegment, FixPatch\nfrom sqlfluff.core.linter.common import NoQaDirective, EnrichedFixPatch\n# Instantiate the linter logger\nlinter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\nclass LintedFile(NamedTuple):\n    \"\"\"A class to store the idea of a linted file.\"\"\"\n    path: str\n    violations: List[SQLBaseError]\n    time_dict: dict\n    tree: Optional[BaseSegment]\n    ignore_mask: List[NoQaDirective]\n    templated_file: TemplatedFile\n    encoding: str\n    def check_tuples(self, raise_on_non_linting_violations=True) -> List[CheckTuple]:\n        \"\"\"Make a list of check_tuples.\n        This assumes that all the violations found are\n        linting violations (and therefore implement `check_tuple()`).\n        If they don't then this function raises that error.\n        \"\"\"\n        vs: List[CheckTuple] = []\n        v: SQLLintError\n        for v in self.get_violations():\n            if hasattr(v, \"check_tuple\"):\n                vs.append(v.check_tuple())\n            elif raise_on_non_linting_violations:\n                raise v\n        return vs\n    def get_violations(\n        self,\n        rules: Optional[Union[str, Tuple[str, ...]]] = None,\n        types: Optional[Union[Type[SQLBaseError], Iterable[Type[SQLBaseError]]]] = None,\n        filter_ignore: bool = True,\n        fixable: bool = None,\n    ) -> list:\n        \"\"\"Get a list of violations, respecting filters and ignore options.\n        Optionally now with filters.\n        \"\"\"\n        violations = self.violations\n        # Filter types\n        if types:\n            # If it's a singular type, make it a single item in a tuple\n            # otherwise coerce to tuple normally so that we can use it with\n            # isinstance.\n            if isinstance(types, type) and issubclass(types, SQLBaseError):\n                types = (types,)\n            else:\n                types = tuple(types)  # pragma: no cover TODO?\n            violations = [v for v in violations if isinstance(v, types)]\n        # Filter rules\n        if rules:\n            if isinstance(rules, str):\n                rules = (rules,)\n            else:\n                rules = tuple(rules)\n            violations = [v for v in violations if v.rule_code() in rules]\n        # Filter fixable\n        if fixable is not None:\n            # Assume that fixable is true or false if not None\n            violations = [v for v in violations if v.fixable is fixable]\n        # Filter ignorable violations\n        if filter_ignore:\n            violations = [v for v in violations if not v.ignore]\n            # Ignore any rules in the ignore mask\n            if self.ignore_mask:\n                violations = self.ignore_masked_violations(violations, self.ignore_mask)\n        return violations\n    @staticmethod\n    def _ignore_masked_violations_single_line(\n        violations: List[SQLBaseError], ignore_mask: List[NoQaDirective]\n    ):\n        \"\"\"Returns whether to ignore error for line-specific directives.\n        The \"ignore\" list is assumed to ONLY contain NoQaDirectives with\n        action=None.\n        \"\"\"\n        for ignore in ignore_mask:\n            violations = [\n                v\n                for v in violations\n                if not (\n                    v.line_no == ignore.line_no\n                    and (ignore.rules is None or v.rule_code() in ignore.rules)\n                )\n            ]\n        return violations\n    @staticmethod\n    def _should_ignore_violation_line_range(\n        line_no: int, ignore_rule: List[NoQaDirective]\n    ):\n        \"\"\"Returns whether to ignore a violation at line_no.\"\"\"\n        # Loop through the NoQaDirectives to find the state of things at\n        # line_no. Assumptions about \"ignore_rule\":\n        # - Contains directives for only ONE RULE, i.e. the rule that was\n        #   violated at line_no\n        # - Sorted in ascending order by line number\n        disable = False\n        for ignore in ignore_rule:\n            if ignore.line_no > line_no:\n                break\n            disable = ignore.action == \"disable\"\n        return disable\n    @classmethod\n    def _ignore_masked_violations_line_range(\n        cls, violations: List[SQLBaseError], ignore_mask: List[NoQaDirective]\n    ):\n        \"\"\"Returns whether to ignore error for line-range directives.\n        The \"ignore\" list is assumed to ONLY contain NoQaDirectives where\n        action is \"enable\" or \"disable\".\n        \"\"\"\n        result = []\n        for v in violations:\n            # Find the directives that affect the violated rule \"v\", either\n            # because they specifically reference it or because they don't\n            # specify a list of rules, thus affecting ALL rules.\n            ignore_rule = sorted(\n                (\n                    ignore\n                    for ignore in ignore_mask\n                    if not ignore.rules\n                    or (v.rule_code() in cast(Tuple[str, ...], ignore.rules))\n                ),\n                key=lambda ignore: ignore.line_no,\n            )\n            # Determine whether to ignore the violation, based on the relevant\n            # enable/disable directives.\n            if not cls._should_ignore_violation_line_range(v.line_no, ignore_rule):\n                result.append(v)\n        return result\n    @classmethod\n    def ignore_masked_violations(\n        cls, violations: List[SQLBaseError], ignore_mask: List[NoQaDirective]\n    ) -> List[SQLBaseError]:\n        \"\"\"Remove any violations specified by ignore_mask.\n        This involves two steps:\n        1. Filter out violations affected by single-line \"noqa\" directives.\n        2. Filter out violations affected by disable/enable \"noqa\" directives.\n        \"\"\"\n        ignore_specific = [ignore for ignore in ignore_mask if not ignore.action]\n        ignore_range = [ignore for ignore in ignore_mask if ignore.action]\n        violations = cls._ignore_masked_violations_single_line(\n            violations, ignore_specific\n        )\n        violations = cls._ignore_masked_violations_line_range(violations, ignore_range)\n        return violations\n    def num_violations(self, **kwargs) -> int:\n        \"\"\"Count the number of violations.\n        Optionally now with filters.\n        \"\"\"\n        violations = self.get_violations(**kwargs)\n        return len(violations)\n    def is_clean(self) -> bool:\n        \"\"\"Return True if there are no ignorable violations.\"\"\"\n        return not any(self.get_violations(filter_ignore=True))\n    @staticmethod\n    def _log_hints(\n        patch: Union[EnrichedFixPatch, FixPatch], templated_file: TemplatedFile\n    ):\n        \"\"\"Log hints for debugging during patch generation.\"\"\"\n        # This next bit is ALL FOR LOGGING AND DEBUGGING\n        max_log_length = 10\n        if patch.templated_slice.start >= max_log_length:\n            pre_hint = templated_file.templated_str[\n                patch.templated_slice.start\n                - max_log_length : patch.templated_slice.start\n            ]\n        else:\n            pre_hint = templated_file.templated_str[: patch.templated_slice.start]\n        if patch.templated_slice.stop + max_log_length < len(\n            templated_file.templated_str\n        ):\n            post_hint = templated_file.templated_str[\n                patch.templated_slice.stop : patch.templated_slice.stop + max_log_length\n            ]\n        else:\n            post_hint = templated_file.templated_str[patch.templated_slice.stop :]\n        linter_logger.debug(\n            \"        Templated Hint: ...%r <> %r...\", pre_hint, post_hint\n        )\n    def fix_string(self) -> Tuple[Any, bool]:\n        \"\"\"Obtain the changes to a path as a string.\n        We use the source mapping features of TemplatedFile\n        to generate a list of \"patches\" which cover the non\n        templated parts of the file and refer back to the locations\n        in the original file.\n        NB: This is MUCH FASTER than the original approach\n        using difflib in pre 0.4.0.\n        There is an important distinction here between Slices and\n        Segments. A Slice is a portion of a file which is determined\n        by the templater based on which portions of the source file\n        are templated or not, and therefore before Lexing and so is\n        completely dialect agnostic. A Segment is determined by the\n        Lexer from portions of strings after templating.\n        \"\"\"\n        linter_logger.debug(\"Original Tree: %r\", self.templated_file.templated_str)\n        assert self.tree\n        linter_logger.debug(\"Fixed Tree: %r\", self.tree.raw)\n        # The sliced file is contiguous in the TEMPLATED space.\n        # NB: It has gaps and repeats in the source space.\n        # It's also not the FIXED file either.\n        linter_logger.debug(\"### Templated File.\")\n        for idx, file_slice in enumerate(self.templated_file.sliced_file):\n            t_str = self.templated_file.templated_str[file_slice.templated_slice]\n            s_str = self.templated_file.source_str[file_slice.source_slice]\n            if t_str == s_str:\n                linter_logger.debug(\n                    \"    File slice: %s %r [invariant]\", idx, file_slice\n                )\n            else:\n                linter_logger.debug(\"    File slice: %s %r\", idx, file_slice)\n                linter_logger.debug(\"    \\t\\t\\ttemplated: %r\\tsource: %r\", t_str, s_str)\n        original_source = self.templated_file.source_str\n        # Make sure no patches overlap and divide up the source file into slices.\n        # Any Template tags in the source file are off limits.\n        source_only_slices = self.templated_file.source_only_slices()\n        linter_logger.debug(\"Source-only slices: %s\", source_only_slices)\n        # Iterate patches, filtering and translating as we go:\n        linter_logger.debug(\"### Beginning Patch Iteration.\")\n        filtered_source_patches = []\n        dedupe_buffer = []\n        # We use enumerate so that we get an index for each patch. This is entirely\n        # so when debugging logs we can find a given patch again!\n        patch: Union[EnrichedFixPatch, FixPatch]\n        for idx, patch in enumerate(\n            self.tree.iter_patches(templated_str=self.templated_file.templated_str)\n        ):\n            linter_logger.debug(\"  %s Yielded patch: %s\", idx, patch)\n            self._log_hints(patch, self.templated_file)\n            # Attempt to convert to source space.\n            try:\n                source_slice = self.templated_file.templated_slice_to_source_slice(\n                    patch.templated_slice,\n                )\n            except ValueError:\n                linter_logger.info(\n                    \"      - Skipping. Source space Value Error. i.e. attempted insertion within templated section.\"\n                )\n                # If we try and slice within a templated section, then we may fail\n                # in which case, we should skip this patch.\n                continue\n            # Check for duplicates\n            dedupe_tuple = (source_slice, patch.fixed_raw)\n            if dedupe_tuple in dedupe_buffer:\n                linter_logger.info(\n                    \"      - Skipping. Source space Duplicate: %s\", dedupe_tuple\n                )\n                continue\n            # We now evaluate patches in the source-space for whether they overlap\n            # or disrupt any templated sections.\n            # The intent here is that unless explicitly stated, a fix should never\n            # disrupt a templated section.\n            # NOTE: We rely here on the patches being sorted.\n            # TODO: Implement a mechanism for doing templated section fixes. For\n            # now it's just not allowed.\n            # Get the affected raw slices.\n            local_raw_slices = self.templated_file.raw_slices_spanning_source_slice(\n                source_slice\n            )\n            local_type_list = [slc.slice_type for slc in local_raw_slices]\n            enriched_patch = EnrichedFixPatch(\n                source_slice=source_slice,\n                templated_slice=patch.templated_slice,\n                patch_category=patch.patch_category,\n                fixed_raw=patch.fixed_raw,\n                templated_str=self.templated_file.templated_str[patch.templated_slice],\n                source_str=self.templated_file.source_str[source_slice],\n            )\n            # Deal with the easy case of only literals\n            if set(local_type_list) == {\"literal\"}:\n                linter_logger.info(\n                    \"      * Keeping patch on literal-only section: %s\", enriched_patch\n                )\n                filtered_source_patches.append(enriched_patch)\n                dedupe_buffer.append(enriched_patch.dedupe_tuple())\n            # Is it a zero length patch.\n            elif (\n                enriched_patch.source_slice.start == enriched_patch.source_slice.stop\n                and enriched_patch.source_slice.start == local_raw_slices[0].source_idx\n            ):\n                linter_logger.info(\n                    \"      * Keeping insertion patch on slice boundary: %s\",\n                    enriched_patch,\n                )\n                filtered_source_patches.append(enriched_patch)\n                dedupe_buffer.append(enriched_patch.dedupe_tuple())\n            # If it's ONLY templated then we should skip it.\n            elif \"literal\" not in local_type_list:\n                linter_logger.info(\n                    \"      - Skipping patch over templated section: %s\", enriched_patch\n                )\n            # If we span more than two slices then we should just skip it. Too Hard.\n            elif len(local_raw_slices) > 2:\n                linter_logger.info(\n                    \"      - Skipping patch over more than two raw slices: %s\",\n                    enriched_patch,\n                )\n            # If it's an insertion (i.e. the string in the pre-fix template is '') then we\n            # won't be able to place it, so skip.\n            elif not enriched_patch.templated_str:  # pragma: no cover TODO?\n                linter_logger.info(\n                    \"      - Skipping insertion patch in templated section: %s\",\n                    enriched_patch,\n                )\n            # If the string from the templated version isn't in the source, then we can't fix it.\n            elif (\n                enriched_patch.templated_str not in enriched_patch.source_str\n            ):  # pragma: no cover TODO?\n                linter_logger.info(\n                    \"      - Skipping edit patch on templated content: %s\",\n                    enriched_patch,\n                )\n            else:\n                # Identify all the places the string appears in the source content.\n                positions = list(\n                    findall(enriched_patch.templated_str, enriched_patch.source_str)\n                )\n                if len(positions) != 1:\n                    linter_logger.debug(\n                        \"        - Skipping edit patch on non-unique templated content: %s\",\n                        enriched_patch,\n                    )\n                    continue\n                # We have a single occurrence of the thing we want to patch. This\n                # means we can use its position to place our patch.\n                new_source_slice = slice(  # pragma: no cover\n                    enriched_patch.source_slice.start + positions[0],\n                    enriched_patch.source_slice.start\n                    + positions[0]\n                    + len(enriched_patch.templated_str),\n                )\n                enriched_patch = EnrichedFixPatch(  # pragma: no cover\n                    source_slice=new_source_slice,\n                    templated_slice=enriched_patch.templated_slice,\n                    patch_category=enriched_patch.patch_category,\n                    fixed_raw=enriched_patch.fixed_raw,\n                    templated_str=enriched_patch.templated_str,\n                    source_str=enriched_patch.source_str,\n                )\n                linter_logger.debug(  # pragma: no cover\n                    \"      * Keeping Tricky Case. Positions: %s, New Slice: %s, Patch: %s\",\n                    positions,\n                    new_source_slice,\n                    enriched_patch,\n                )\n                filtered_source_patches.append(enriched_patch)  # pragma: no cover\n                dedupe_buffer.append(enriched_patch.dedupe_tuple())  # pragma: no cover\n                continue  # pragma: no cover\n        # Sort the patches before building up the file.\n        filtered_source_patches = sorted(\n            filtered_source_patches, key=lambda x: x.source_slice.start\n        )\n        # We now slice up the file using the patches and any source only slices.\n        # This gives us regions to apply changes to.\n        slice_buff = []\n        source_idx = 0\n        for patch in filtered_source_patches:\n            # Are there templated slices at or before the start of this patch?\n            while (\n                source_only_slices\n                and source_only_slices[0].source_idx < patch.source_slice.start\n            ):\n                next_so_slice = source_only_slices.pop(0).source_slice()\n                # Add a pre-slice before the next templated slices if needed.\n                if next_so_slice.start > source_idx:\n                    slice_buff.append(slice(source_idx, next_so_slice.start))\n                # Add the templated slice.\n                slice_buff.append(next_so_slice)\n                source_idx = next_so_slice.stop\n            # Is there a gap between current position and this patch?\n            if patch.source_slice.start > source_idx:\n                # Add a slice up to this patch.\n                slice_buff.append(slice(source_idx, patch.source_slice.start))\n            # Is this patch covering an area we've already covered?\n            if patch.source_slice.start < source_idx:\n                linter_logger.info(\n                    \"Skipping overlapping patch at Index %s, Patch: %s\",\n                    source_idx,\n                    patch,\n                )\n                # Ignore the patch for now...\n                continue\n            # Add this patch.\n            slice_buff.append(patch.source_slice)\n            source_idx = patch.source_slice.stop\n        # Add a tail slice.\n        if source_idx < len(self.templated_file.source_str):\n            slice_buff.append(slice(source_idx, len(self.templated_file.source_str)))\n        linter_logger.debug(\"Final slice buffer: %s\", slice_buff)\n        # Iterate through the patches, building up the new string.\n        str_buff = \"\"\n        for source_slice in slice_buff:\n            # Is it one in the patch buffer:\n            for patch in filtered_source_patches:\n                if patch.source_slice == source_slice:\n                    # Use the patched version\n                    linter_logger.debug(\n                        \"%-30s    %s    %r > %r\",\n                        f\"Appending {patch.patch_category} Patch:\",\n                        patch.source_slice,\n                        patch.source_str,\n                        patch.fixed_raw,\n                    )\n                    str_buff += patch.fixed_raw\n                    break\n            else:\n                # Use the raw string\n                linter_logger.debug(\n                    \"Appending Raw:                    %s     %r\",\n                    source_slice,\n                    self.templated_file.source_str[source_slice],\n                )\n                str_buff += self.templated_file.source_str[source_slice]\n        # The success metric here is whether anything ACTUALLY changed.\n        return str_buff, str_buff != original_source\n    def persist_tree(self, suffix: str = \"\") -> bool:\n        \"\"\"Persist changes to the given path.\"\"\"\n        write_buff, success = self.fix_string()\n        if success:\n            fname = self.path\n            # If there is a suffix specified, then use it.s\n            if suffix:\n                root, ext = os.path.splitext(fname)\n                fname = root + suffix + ext\n            # Actually write the file.\n            with open(fname, \"w\", encoding=self.encoding) as f:\n                f.write(write_buff)\n        return success",
        "file_path": "src/sqlfluff/core/linter/linted_file.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 6,
        "score": 0.6029958724975586,
        "content": "\"\"\"Sqlfluff is a SQL linter for humans.\"\"\"\nimport sys\nimport pytest\n# Expose the public API.\nfrom sqlfluff.api import lint, fix, parse, list_rules, list_dialects  # noqa: F401\n# Set the version attribute of the library\nimport pkg_resources\nimport configparser\n# Get the current version\nconfig = configparser.ConfigParser()\nconfig.read([pkg_resources.resource_filename(\"sqlfluff\", \"config.ini\")])\n__version__ = config.get(\"sqlfluff\", \"version\")\n# Check major python version\nif sys.version_info[0] < 3:\n    raise Exception(\"Sqlfluff does not support Python 2. Please upgrade to Python 3.\")\n# Check minor python version\nelif sys.version_info[1] < 6:\n    raise Exception(\n        \"Sqlfluff %s only supports Python 3.6 and beyond. \"\n        \"Use an earlier version of sqlfluff or a later version of Python\" % __version__\n    )\n# Register helper functions to support variable introspection on failure.\npytest.register_assert_rewrite(\"sqlfluff.testing\")",
        "file_path": "src/sqlfluff/__init__.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 7,
        "score": 0.5912927389144897,
        "content": "\"\"\"Elements which wrap the sqlfluff core library for public use.\"\"\"\n# flake8: noqa: F401\n# Expose the simple api\nfrom sqlfluff.api.simple import lint, fix, parse, APIParsingError\nfrom sqlfluff.api.info import list_rules, list_dialects",
        "file_path": "src/sqlfluff/api/__init__.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 8,
        "score": 0.5908507108688354,
        "content": "\"\"\"Defines the linter class.\"\"\"\nimport os\nimport time\nimport logging\nfrom typing import (\n    Any,\n    List,\n    Sequence,\n    Optional,\n    Tuple,\n    Union,\n    cast,\n    Iterable,\n    Iterator,\n)\nimport pathspec\nfrom sqlfluff.core.errors import (\n    SQLBaseError,\n    SQLLexError,\n    SQLLintError,\n    SQLParseError,\n    SQLTemplaterSkipFile,\n)\nfrom sqlfluff.core.parser import Lexer, Parser\nfrom sqlfluff.core.file_helpers import get_encoding\nfrom sqlfluff.core.templaters import TemplatedFile\nfrom sqlfluff.core.rules import get_ruleset\nfrom sqlfluff.core.config import FluffConfig, ConfigLoader\n# Classes needed only for type checking\nfrom sqlfluff.core.linter.runner import get_runner\nfrom sqlfluff.core.parser.segments.base import BaseSegment\nfrom sqlfluff.core.parser.segments.meta import MetaSegment\nfrom sqlfluff.core.parser.segments.raw import RawSegment\nfrom sqlfluff.core.rules.base import BaseRule\nfrom sqlfluff.core.linter.common import (\n    RuleTuple,\n    ParsedString,\n    NoQaDirective,\n    RenderedFile,\n)\nfrom sqlfluff.core.linter.linted_file import LintedFile\nfrom sqlfluff.core.linter.linted_dir import LintedDir\nfrom sqlfluff.core.linter.linting_result import LintingResult\nWalkableType = Iterable[Tuple[str, Optional[List[str]], List[str]]]\n# Instantiate the linter logger\nlinter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\nclass Linter:\n    \"\"\"The interface class to interact with the linter.\"\"\"\n    # Default to allowing process parallelism\n    allow_process_parallelism = True\n    def __init__(\n        self,\n        config: Optional[FluffConfig] = None,\n        formatter: Any = None,\n        dialect: Optional[str] = None,\n        rules: Optional[Union[str, List[str]]] = None,\n        user_rules: Optional[Union[str, List[str]]] = None,\n    ) -> None:\n        # Store the config object\n        self.config = FluffConfig.from_kwargs(\n            config=config, dialect=dialect, rules=rules\n        )\n        # Get the dialect and templater\n        self.dialect = self.config.get(\"dialect_obj\")\n        self.templater = self.config.get(\"templater_obj\")\n        # Store the formatter for output\n        self.formatter = formatter\n        # Store references to user rule classes\n        self.user_rules = user_rules or []\n    def get_ruleset(self, config: Optional[FluffConfig] = None) -> List[BaseRule]:\n        \"\"\"Get hold of a set of rules.\"\"\"\n        rs = get_ruleset()\n        # Register any user rules\n        for rule in self.user_rules:\n            rs.register(rule)\n        cfg = config or self.config\n        return rs.get_rulelist(config=cfg)\n    def rule_tuples(self) -> List[RuleTuple]:\n        \"\"\"A simple pass through to access the rule tuples of the rule set.\"\"\"\n        rs = self.get_ruleset()\n        return [RuleTuple(rule.code, rule.description) for rule in rs]\n    # #### Static methods\n    # These are the building blocks of the linting process.\n    @staticmethod\n    def _load_raw_file_and_config(fname, root_config):\n        \"\"\"Load a raw file and the associated config.\"\"\"\n        file_config = root_config.make_child_from_path(fname)\n        encoding = get_encoding(fname=fname, config=file_config)\n        with open(fname, encoding=encoding, errors=\"backslashreplace\") as target_file:\n            raw_file = target_file.read()\n        # Scan the raw file for config commands.\n        file_config.process_raw_file_for_config(raw_file)\n        # Return the raw file and config\n        return raw_file, file_config, encoding\n    @staticmethod\n    def _lex_templated_file(\n        templated_file: TemplatedFile, config: FluffConfig\n    ) -> Tuple[Optional[Sequence[BaseSegment]], List[SQLLexError], FluffConfig]:\n        \"\"\"Lex a templated file.\n        NOTE: This potentially mutates the config, so make sure to\n        use the returned one.\n        \"\"\"\n        violations = []\n        linter_logger.info(\"LEXING RAW (%s)\", templated_file.fname)\n        # Get the lexer\n        lexer = Lexer(config=config)\n        # Lex the file and log any problems\n        try:\n            tokens, lex_vs = lexer.lex(templated_file)\n            # We might just get the violations as a list\n            violations += lex_vs\n            linter_logger.info(\n                \"Lexed tokens: %s\", [seg.raw for seg in tokens] if tokens else None\n            )\n        except SQLLexError as err:\n            linter_logger.info(\"LEXING FAILED! (%s): %s\", templated_file.fname, err)\n            violations.append(err)\n            return None, violations, config\n        if not tokens:  # pragma: no cover TODO?\n            return None, violations, config\n        # Check that we've got sensible indentation from the lexer.\n        # We might need to suppress if it's a complicated file.\n        templating_blocks_indent = config.get(\"template_blocks_indent\", \"indentation\")\n        if isinstance(templating_blocks_indent, str):\n            force_block_indent = templating_blocks_indent.lower().strip() == \"force\"\n        else:\n            force_block_indent = False\n        templating_blocks_indent = bool(templating_blocks_indent)\n        # If we're forcing it through we don't check.\n        if templating_blocks_indent and not force_block_indent:\n            indent_balance = sum(\n                getattr(elem, \"indent_val\", 0)\n                for elem in cast(Tuple[BaseSegment, ...], tokens)\n            )\n            if indent_balance != 0:\n                linter_logger.debug(\n                    \"Indent balance test failed for %r. Template indents will not be linted for this file.\",\n                    templated_file.fname,\n                )\n                # Don't enable the templating blocks.\n                templating_blocks_indent = False\n        # The file will have been lexed without config, so check all indents\n        # are enabled.\n        new_tokens = []\n        for token in cast(Tuple[BaseSegment, ...], tokens):\n            if token.is_meta:\n                token = cast(MetaSegment, token)\n                if token.indent_val != 0:\n                    # Don't allow it if we're not linting templating block indents.\n                    if not templating_blocks_indent:\n                        continue\n            new_tokens.append(token)\n        # Return new buffer\n        return new_tokens, violations, config\n    @staticmethod\n    def _parse_tokens(\n        tokens: Sequence[BaseSegment],\n        config: FluffConfig,\n        recurse: bool = True,\n        fname: Optional[str] = None,\n    ) -> Tuple[Optional[BaseSegment], List[SQLParseError]]:\n        parser = Parser(config=config)\n        violations = []\n        # Parse the file and log any problems\n        try:\n            parsed: Optional[BaseSegment] = parser.parse(\n                tokens, recurse=recurse, fname=fname\n            )\n        except SQLParseError as err:\n            linter_logger.info(\"PARSING FAILED! : %s\", err)\n            violations.append(err)\n            return None, violations\n        if parsed:\n            linter_logger.info(\"\\n###\\n#\\n# {}\\n#\\n###\".format(\"Parsed Tree:\"))\n            linter_logger.info(\"\\n\" + parsed.stringify())\n            # We may succeed parsing, but still have unparsable segments. Extract them here.\n            for unparsable in parsed.iter_unparsables():\n                # No exception has been raised explicitly, but we still create one here\n                # so that we can use the common interface\n                violations.append(\n                    SQLParseError(\n                        \"Line {0[0]}, Position {0[1]}: Found unparsable section: {1!r}\".format(\n                            unparsable.pos_marker.working_loc,\n                            unparsable.raw\n                            if len(unparsable.raw) < 40\n                            else unparsable.raw[:40] + \"...\",\n                        ),\n                        segment=unparsable,\n                    )\n                )\n                linter_logger.info(\"Found unparsable segment...\")\n                linter_logger.info(unparsable.stringify())\n        return parsed, violations\n    @staticmethod\n    def parse_noqa(comment: str, line_no: int):\n        \"\"\"Extract ignore mask entries from a comment string.\"\"\"\n        # Also trim any whitespace afterward\n        if comment.startswith(\"noqa\"):\n            # This is an ignore identifier\n            comment_remainder = comment[4:]\n            if comment_remainder:\n                if not comment_remainder.startswith(\":\"):\n                    return SQLParseError(\n                        \"Malformed 'noqa' section. Expected 'noqa: <rule>[,...]\",\n                        line_no=line_no,\n                    )\n                comment_remainder = comment_remainder[1:].strip()\n                if comment_remainder:\n                    action: Optional[str]\n                    if \"=\" in comment_remainder:\n                        action, rule_part = comment_remainder.split(\"=\", 1)\n                        if action not in {\"disable\", \"enable\"}:  # pragma: no cover\n                            return SQLParseError(\n                                \"Malformed 'noqa' section. \"\n                                \"Expected 'noqa: enable=<rule>[,...] | all' \"\n                                \"or 'noqa: disable=<rule>[,...] | all\",\n                                line_no=line_no,\n                            )\n                    else:\n                        action = None\n                        rule_part = comment_remainder\n                        if rule_part in {\"disable\", \"enable\"}:\n                            return SQLParseError(\n                                \"Malformed 'noqa' section. \"\n                                \"Expected 'noqa: enable=<rule>[,...] | all' \"\n                                \"or 'noqa: disable=<rule>[,...] | all\",\n                                line_no=line_no,\n                            )\n                    rules: Optional[Tuple[str, ...]]\n                    if rule_part != \"all\":\n                        rules = tuple(r.strip() for r in rule_part.split(\",\"))\n                    else:\n                        rules = None\n                    return NoQaDirective(line_no, rules, action)\n            return NoQaDirective(line_no, None, None)\n        return None\n    @staticmethod\n    def remove_templated_errors(\n        linting_errors: List[SQLBaseError],\n    ) -> List[SQLBaseError]:\n        \"\"\"Filter a list of lint errors, removing those which only occur in templated slices.\"\"\"\n        # Filter out any linting errors in templated sections if relevant.\n        result: List[SQLBaseError] = []\n        for e in linting_errors:\n            if isinstance(e, SQLLintError):\n                if (\n                    # Is it in a literal section?\n                    e.segment.pos_marker.is_literal()\n                    # Is it a rule that is designed to work on templated sections?\n                    or e.rule.targets_templated\n                ):\n                    result.append(e)\n            else:\n                # If it's another type, just keep it. (E.g. SQLParseError from\n                # malformed \"noqa\" comment).\n                result.append(e)\n        return result\n    @staticmethod\n    def _warn_unfixable(code: str):\n        linter_logger.warning(\n            f\"One fix for {code} not applied, it would re-cause the same error.\"\n        )\n    # ### Class Methods\n    # These compose the base static methods into useful recipes.\n    @classmethod\n    def parse_rendered(cls, rendered: RenderedFile, recurse: bool = True):\n        \"\"\"Parse a rendered file.\"\"\"\n        t0 = time.monotonic()\n        violations = cast(List[SQLBaseError], rendered.templater_violations)\n        tokens: Optional[Sequence[BaseSegment]]\n        if rendered.templated_file:\n            tokens, lvs, config = cls._lex_templated_file(\n                rendered.templated_file, rendered.config\n            )\n            violations += lvs\n        else:\n            tokens = None\n        t1 = time.monotonic()\n        linter_logger.info(\"PARSING (%s)\", rendered.fname)\n        if tokens:\n            parsed, pvs = cls._parse_tokens(\n                tokens, rendered.config, recurse=recurse, fname=rendered.fname\n            )\n            violations += pvs\n        else:\n            parsed = None\n        time_dict = {\n            **rendered.time_dict,\n            \"lexing\": t1 - t0,\n            \"parsing\": time.monotonic() - t1,\n        }\n        return ParsedString(\n            parsed,\n            violations,\n            time_dict,\n            rendered.templated_file,\n            rendered.config,\n            rendered.fname,\n        )\n    @classmethod\n    def extract_ignore_from_comment(cls, comment: RawSegment):\n        \"\"\"Extract ignore mask entries from a comment segment.\"\"\"\n        # Also trim any whitespace afterward\n        comment_content = comment.raw_trimmed().strip()\n        comment_line, _ = comment.pos_marker.source_position()\n        result = cls.parse_noqa(comment_content, comment_line)\n        if isinstance(result, SQLParseError):\n            result.segment = comment\n        return result\n    @classmethod\n    def extract_ignore_mask(\n        cls, tree: BaseSegment\n    ) -> Tuple[List[NoQaDirective], List[SQLBaseError]]:\n        \"\"\"Look for inline ignore comments and return NoQaDirectives.\"\"\"\n        ignore_buff: List[NoQaDirective] = []\n        violations: List[SQLBaseError] = []\n        for comment in tree.recursive_crawl(\"comment\"):\n            if comment.name == \"inline_comment\":\n                ignore_entry = cls.extract_ignore_from_comment(comment)\n                if isinstance(ignore_entry, SQLParseError):\n                    violations.append(ignore_entry)\n                elif ignore_entry:\n                    ignore_buff.append(ignore_entry)\n        if ignore_buff:\n            linter_logger.info(\"Parsed noqa directives from file: %r\", ignore_buff)\n        return ignore_buff, violations\n    @classmethod\n    def lint_fix_parsed(\n        cls,\n        tree: BaseSegment,\n        config: FluffConfig,\n        rule_set: List[BaseRule],\n        fix: bool = False,\n        fname: Optional[str] = None,\n        templated_file: Optional[TemplatedFile] = None,\n        formatter: Any = None,\n    ) -> Tuple[BaseSegment, List[SQLBaseError], List[NoQaDirective]]:\n        \"\"\"Lint and optionally fix a tree object.\"\"\"\n        # Keep track of the linting errors\n        all_linting_errors = []\n        # A placeholder for the fixes we had on the previous loop\n        last_fixes = None\n        # Keep a set of previous versions to catch infinite loops.\n        previous_versions = {tree.raw}\n        # If we are fixing then we want to loop up to the runaway_limit, otherwise just once for linting.\n        loop_limit = config.get(\"runaway_limit\") if fix else 1\n        # Dispatch the output for the lint header\n        if formatter:\n            formatter.dispatch_lint_header(fname)\n        # Look for comment segments which might indicate lines to ignore.\n        ignore_buff, ivs = cls.extract_ignore_mask(tree)\n        all_linting_errors += ivs\n        for loop in range(loop_limit):\n            changed = False\n            for crawler in rule_set:\n                # fixes should be a dict {} with keys edit, delete, create\n                # delete is just a list of segments to delete\n                # edit and create are list of tuples. The first element is the\n                # \"anchor\", the segment to look for either to edit or to insert BEFORE.\n                # The second is the element to insert or create.\n                linting_errors, _, fixes, _ = crawler.crawl(\n                    tree,\n                    ignore_mask=ignore_buff,\n                    dialect=config.get(\"dialect_obj\"),\n                    fname=fname,\n                    templated_file=templated_file,\n                )\n                all_linting_errors += linting_errors\n                if fix and fixes:\n                    linter_logger.info(f\"Applying Fixes [{crawler.code}]: {fixes}\")\n                    # Do some sanity checks on the fixes before applying.\n                    if fixes == last_fixes:  # pragma: no cover\n                        cls._warn_unfixable(crawler.code)\n                    else:\n                        last_fixes = fixes\n                        new_tree, _ = tree.apply_fixes(fixes)\n                        # Check for infinite loops\n                        if new_tree.raw not in previous_versions:\n                            # We've not seen this version of the file so far. Continue.\n                            tree = new_tree\n                            previous_versions.add(tree.raw)\n                            changed = True\n                            continue\n                        else:\n                            # Applying these fixes took us back to a state which we've\n                            # seen before. Abort.\n                            cls._warn_unfixable(crawler.code)\n            if loop == 0:\n                # Keep track of initial errors for reporting.\n                initial_linting_errors = all_linting_errors.copy()\n            if fix and not changed:\n                # We did not change the file. Either the file is clean (no fixes), or\n                # any fixes which are present will take us back to a previous state.\n                linter_logger.info(\n                    f\"Fix loop complete. Stability achieved after {loop}/{loop_limit} loops.\"\n                )\n                break\n        if fix and loop + 1 == loop_limit:\n            linter_logger.warning(f\"Loop limit on fixes reached [{loop_limit}].\")\n        if config.get(\"ignore_templated_areas\", default=True):\n            initial_linting_errors = cls.remove_templated_errors(initial_linting_errors)\n        return tree, initial_linting_errors, ignore_buff\n    @classmethod\n    def lint_parsed(\n        cls,\n        parsed: ParsedString,\n        rule_set: List[BaseRule],\n        fix: bool = False,\n        formatter: Any = None,\n        encoding: str = \"utf8\",\n    ):\n        \"\"\"Lint a ParsedString and return a LintedFile.\"\"\"\n        violations = parsed.violations\n        time_dict = parsed.time_dict\n        tree: Optional[BaseSegment]\n        if parsed.tree:\n            t0 = time.monotonic()\n            linter_logger.info(\"LINTING (%s)\", parsed.fname)\n            tree, initial_linting_errors, ignore_buff = cls.lint_fix_parsed(\n                parsed.tree,\n                config=parsed.config,\n                rule_set=rule_set,\n                fix=fix,\n                fname=parsed.fname,\n                templated_file=parsed.templated_file,\n                formatter=formatter,\n            )\n            # Update the timing dict\n            time_dict[\"linting\"] = time.monotonic() - t0\n            # We're only going to return the *initial* errors, rather\n            # than any generated during the fixing cycle.\n            violations += initial_linting_errors\n        else:\n            # If no parsed tree, set to None\n            tree = None\n            ignore_buff = []\n        # We process the ignore config here if appropriate\n        for violation in violations:\n            violation.ignore_if_in(parsed.config.get(\"ignore\"))\n        linted_file = LintedFile(\n            parsed.fname,\n            violations,\n            time_dict,\n            tree,\n            ignore_mask=ignore_buff,\n            templated_file=parsed.templated_file,\n            encoding=encoding,\n        )\n        # This is the main command line output from linting.\n        if formatter:\n            formatter.dispatch_file_violations(\n                parsed.fname, linted_file, only_fixable=fix\n            )\n        # Safety flag for unset dialects\n        if parsed.config.get(\"dialect\") == \"ansi\" and linted_file.get_violations(\n            fixable=True if fix else None, types=SQLParseError\n        ):\n            if formatter:  # pragma: no cover TODO?\n                formatter.dispatch_dialect_warning()\n        return linted_file\n    @classmethod\n    def lint_rendered(\n        cls,\n        rendered: RenderedFile,\n        rule_set: List[BaseRule],\n        fix: bool = False,\n        formatter: Any = None,\n    ) -> LintedFile:\n        \"\"\"Take a RenderedFile and return a LintedFile.\"\"\"\n        parsed = cls.parse_rendered(rendered)\n        return cls.lint_parsed(\n            parsed,\n            rule_set=rule_set,\n            fix=fix,\n            formatter=formatter,\n            encoding=rendered.encoding,\n        )\n    # ### Instance Methods\n    # These are tied to a specific instance and so are not necessarily\n    # safe to use in parallel operations.\n    def render_string(\n        self, in_str: str, fname: str, config: FluffConfig, encoding: str\n    ) -> RenderedFile:\n        \"\"\"Template the file.\"\"\"\n        linter_logger.info(\"TEMPLATING RAW [%s] (%s)\", self.templater.name, fname)\n        # Start the templating timer\n        t0 = time.monotonic()\n        if not config.get(\"templater_obj\") == self.templater:\n            linter_logger.warning(\n                (\n                    f\"Attempt to set templater to {config.get('templater_obj').name} failed. Using {self.templater.name} \"\n                    \"templater. Templater cannot be set in a .sqlfluff file in a subdirectory of the current working \"\n                    \"directory. It can be set in a .sqlfluff in the current working directory. See Nesting section of the \"\n                    \"docs for more details.\"\n                )\n            )\n        try:\n            templated_file, templater_violations = self.templater.process(\n                in_str=in_str, fname=fname, config=config, formatter=self.formatter\n            )",
        "file_path": "src/sqlfluff/core/linter/linter.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 9,
        "score": 0.5905313491821289,
        "content": "\"\"\"Setup file for example plugin.\"\"\"\nfrom setuptools import setup\nfrom os.path import dirname, join\ndef read(*names, **kwargs):\n    \"\"\"Read a file and return the contents as a string.\"\"\"\n    return open(\n        join(dirname(__file__), *names), encoding=kwargs.get(\"encoding\", \"utf8\")\n    ).read()\nsetup(\n    name=\"sqlfluff-templater-dbt\",\n    version=\"0.7.1\",\n    include_package_data=False,\n    license=\"MIT License\",\n    description=\"Lint your dbt project SQL.\",\n    long_description=read(\"README.md\"),\n    # Make sure pypi is expecting markdown!\n    long_description_content_type=\"text/markdown\",\n    author=\"Alan Cruickshank\",\n    author_email=\"alan@designingoverload.com\",\n    url=\"https://github.com/sqlfluff/sqlfluff\",\n    python_requires=\">=3.6\",\n    keywords=[\n        \"sqlfluff\",\n        \"sql\",\n        \"linter\",\n        \"formatter\",\n        \"dbt\",\n    ],\n    project_urls={\n        \"Homepage\": \"https://www.sqlfluff.com\",\n        \"Documentation\": \"https://docs.sqlfluff.com\",\n        \"Changes\": \"https://github.com/sqlfluff/sqlfluff/blob/main/CHANGELOG.md\",\n        \"Source\": \"https://github.com/sqlfluff/sqlfluff\",\n        \"Issue Tracker\": \"https://github.com/sqlfluff/sqlfluff/issues\",\n        \"Twitter\": \"https://twitter.com/SQLFluff\",\n        \"Chat\": \"https://github.com/sqlfluff/sqlfluff#sqlfluff-on-slack\",\n    },\n    packages=[\"sqlfluff_templater_dbt\"],\n    classifiers=[\n        # complete classifier list: http://pypi.python.org/pypi?%3Aaction=list_classifiers\n        \"Development Status :: 4 - Beta\",\n        # 'Development Status :: 5 - Production/Stable',\n        \"Environment :: Console\",\n        \"Intended Audience :: Developers\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: Unix\",\n        \"Operating System :: POSIX\",\n        \"Operating System :: MacOS\",\n        \"Operating System :: Microsoft :: Windows\",\n        \"Programming Language :: Python\",\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3.6\",\n        \"Programming Language :: Python :: 3.7\",\n        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: Implementation :: CPython\",\n        \"Programming Language :: Python :: Implementation :: PyPy\",\n        \"Topic :: Utilities\",\n        \"Topic :: Software Development :: Quality Assurance\",\n    ],\n    install_requires=[\"sqlfluff>=0.7.0\", \"dbt>=0.17\"],\n    entry_points={\"sqlfluff\": [\"sqlfluff_templater_dbt = sqlfluff_templater_dbt\"]},\n)",
        "file_path": "plugins/sqlfluff-templater-dbt/setup.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 10,
        "score": 0.5904144048690796,
        "content": "\"\"\"This module integrates SQLFluff with diff_cover's \"diff-quality\" tool.\"\"\"\nfrom typing import List\nfrom diff_cover.hook import hookimpl as diff_cover_hookimpl\nfrom diff_cover.violationsreporters.base import BaseViolationReporter, Violation\nfrom sqlfluff.core import FluffConfig, Linter\nclass SQLFluffViolationReporter(BaseViolationReporter):\n    \"\"\"Class that implements diff-quality integration.\"\"\"\n    supported_extensions = [\"sql\"]\n    def __init__(self):\n        \"\"\"Calls the base class constructor to set the object's name.\"\"\"\n        super().__init__(\"sqlfluff\")\n    @staticmethod\n    def violations(src_path: str) -> List[Violation]:\n        \"\"\"Return list of violations.\n        Given the path to a .sql file, analyze it and return a list of\n        violations (i.e. formatting or style issues).\n        \"\"\"\n        linter = Linter(config=FluffConfig.from_root())\n        linted_path = linter.lint_path(src_path, ignore_non_existent_files=True)\n        result = []\n        for violation in linted_path.get_violations():\n            try:\n                # Normal SQLFluff warnings\n                message = f\"{violation.rule_code()}: {violation.description}\"\n            except AttributeError:\n                # Parse errors\n                message = str(violation)\n            result.append(Violation(violation.line_no, message))\n        return result\n    def measured_lines(self, src_path: str) -> None:  # pragma: no cover\n        \"\"\"Return list of the lines in src_path that were measured.\"\"\"\n@diff_cover_hookimpl\ndef diff_cover_report_quality() -> SQLFluffViolationReporter:\n    \"\"\"Returns the SQLFluff plugin.\n    This function is registered as a diff_cover entry point. diff-quality calls\n    it in order to \"discover\" the SQLFluff plugin.\n    :return: Object that implements the BaseViolationReporter ABC\n    \"\"\"\n    return SQLFluffViolationReporter()",
        "file_path": "src/sqlfluff/diff_quality_plugin.py",
        "chunk_index": 0,
        "metadata": {}
      }
    ]
  },
  "marshmallow-code__marshmallow-1359": {
    "query": "3.0: DateTime fields cannot be used as inner field for List or Tuple fields\nBetween releases 3.0.0rc8 and 3.0.0rc9, `DateTime` fields have started throwing an error when being instantiated as inner fields of container fields like `List` or `Tuple`. The snippet below works in <=3.0.0rc8 and throws the error below in >=3.0.0rc9 (and, worryingly, 3.0.0):\r\n\r\n```python\r\nfrom marshmallow import fields, Schema\r\n\r\nclass MySchema(Schema):\r\n    times = fields.List(fields.DateTime())\r\n\r\ns = MySchema()\r\n```\r\n\r\nTraceback:\r\n```\r\nTraceback (most recent call last):\r\n  File \"test-mm.py\", line 8, in <module>\r\n    s = MySchema()\r\n  File \"/Users/victor/.pyenv/versions/marshmallow/lib/python3.6/site-packages/marshmallow/schema.py\", line 383, in __init__\r\n    self.fields = self._init_fields()\r\n  File \"/Users/victor/.pyenv/versions/marshmallow/lib/python3.6/site-packages/marshmallow/schema.py\", line 913, in _init_fields\r\n    self._bind_field(field_name, field_obj)\r\n  File \"/Users/victor/.pyenv/versions/marshmallow/lib/python3.6/site-packages/marshmallow/schema.py\", line 969, in _bind_field\r\n    field_obj._bind_to_schema(field_name, self)\r\n  File \"/Users/victor/.pyenv/versions/marshmallow/lib/python3.6/site-packages/marshmallow/fields.py\", line 636, in _bind_to_schema\r\n    self.inner._bind_to_schema(field_name, self)\r\n  File \"/Users/victor/.pyenv/versions/marshmallow/lib/python3.6/site-packages/marshmallow/fields.py\", line 1117, in _bind_to_schema\r\n    or getattr(schema.opts, self.SCHEMA_OPTS_VAR_NAME)\r\nAttributeError: 'List' object has no attribute 'opts'\r\n```\r\n\r\nIt seems like it's treating the parent field as a Schema without checking that it is indeed a schema, so the `schema.opts` statement fails as fields don't have an `opts` attribute.\n",
    "method": "sliding",
    "top_k": 10,
    "results": [
      {
        "rank": 1,
        "score": 0.6510454416275024,
        "content": "        self._schema = None  # Cached Schema instance\n        super().__init__(default=default, **kwargs)\n    @property\n    def schema(self):\n        \"\"\"The nested Schema object.\n        .. versionchanged:: 1.0.0\n            Renamed from `serializer` to `schema`.\n        \"\"\"\n        if not self._schema:\n            # Inherit context from parent.\n            context = getattr(self.parent, \"context\", {})\n            if isinstance(self.nested, SchemaABC):\n                self._schema = self.nested\n                self._schema.context.update(context)\n            else:\n                if isinstance(self.nested, type) and issubclass(self.nested, SchemaABC):\n                    schema_class = self.nested\n                elif not isinstance(self.nested, (str, bytes)):\n                    raise ValueError(\n                        \"Nested fields must be passed a \"\n                        \"Schema, not {}.\".format(self.nested.__class__)\n                    )\n                elif self.nested == \"self\":\n                    ret = self\n                    while not isinstance(ret, SchemaABC):\n                        ret = ret.parent\n                    schema_class = ret.__class__\n                else:\n                    schema_class = class_registry.get_class(self.nested)\n                self._schema = schema_class(\n                    many=self.many,\n                    only=self.only,\n                    exclude=self.exclude,\n                    context=context,\n                    load_only=self._nested_normalized_option(\"load_only\"),\n                    dump_only=self._nested_normalized_option(\"dump_only\"),\n                )\n        return self._schema\n    def _nested_normalized_option(self, option_name):\n        nested_field = \"%s.\" % self.name\n        return [\n            field.split(nested_field, 1)[1]\n            for field in getattr(self.root, option_name, set())\n            if field.startswith(nested_field)\n        ]\n    def _serialize(self, nested_obj, attr, obj, many=False, **kwargs):\n        # Load up the schema first. This allows a RegistryError to be raised\n        # if an invalid schema name was passed\n        schema = self.schema\n        if nested_obj is None:\n            return None\n        return schema.dump(nested_obj, many=self.many or many)\n    def _test_collection(self, value, many=False):\n        many = self.many or many\n        if many and not utils.is_collection(value):\n            raise self.make_error(\"type\", input=value, type=value.__class__.__name__)\n    def _load(self, value, data, partial=None, many=False):\n        try:\n            valid_data = self.schema.load(\n                value, unknown=self.unknown, partial=partial, many=self.many or many\n            )\n        except ValidationError as error:\n            raise ValidationError(\n                error.messages, valid_data=error.valid_data\n            ) from error\n        return valid_data\n    def _deserialize(self, value, attr, data, partial=None, many=False, **kwargs):\n        \"\"\"Same as :meth:`Field._deserialize` with additional ``partial`` argument.\n        :param bool|tuple partial: For nested schemas, the ``partial``\n            parameter passed to `Schema.load`.\n        .. versionchanged:: 3.0.0\n            Add ``partial`` parameter.\n        \"\"\"\n        self._test_collection(value, many=many)\n        return self._load(value, data, partial=partial, many=many)\nclass Pluck(Nested):\n    \"\"\"Allows you to replace nested data with one of the data's fields.\n    Example: ::\n        from marshmallow import Schema, fields\n        class ArtistSchema(Schema):\n            id = fields.Int()\n            name = fields.Str()\n        class AlbumSchema(Schema):\n            artist = fields.Pluck(ArtistSchema, 'id')\n        in_data = {'artist': 42}\n        loaded = AlbumSchema().load(in_data) # => {'artist': {'id': 42}}\n        dumped = AlbumSchema().dump(loaded)  # => {'artist': 42}\n    :param Schema nested: The Schema class or class name (string)\n        to nest, or ``\"self\"`` to nest the :class:`Schema` within itself.\n    :param str field_name: The key to pluck a value from.\n    :param kwargs: The same keyword arguments that :class:`Nested` receives.\n    \"\"\"\n    def __init__(self, nested, field_name, **kwargs):\n        super().__init__(nested, only=(field_name,), **kwargs)\n        self.field_name = field_name\n    @property\n    def _field_data_key(self):\n        only_field = self.schema.fields[self.field_name]\n        return only_field.data_key or self.field_name\n    def _serialize(self, nested_obj, attr, obj, **kwargs):\n        ret = super()._serialize(nested_obj, attr, obj, **kwargs)\n        if ret is None:\n            return None\n        if self.many:\n            return utils.pluck(ret, key=self._field_data_key)\n        return ret[self._field_data_key]\n    def _deserialize(self, value, attr, data, partial=None, **kwargs):\n        self._test_collection(value)\n        if self.many:\n            value = [{self._field_data_key: v} for v in value]\n        else:\n            value = {self._field_data_key: value}\n        return self._load(value, data, partial=partial)\nclass List(Field):\n    \"\"\"A list field, composed with another `Field` class or\n    instance.\n    Example: ::\n        numbers = fields.List(fields.Float())\n    :param Field cls_or_instance: A field class or instance.\n    :param bool default: Default value for serialization.\n    :param kwargs: The same keyword arguments that :class:`Field` receives.\n    .. versionchanged:: 2.0.0\n        The ``allow_none`` parameter now applies to deserialization and\n        has the same semantics as the other fields.\n    .. versionchanged:: 3.0.0rc9\n        Does not serialize scalar values to single-item lists.\n    \"\"\"\n    default_error_messages = {\"invalid\": \"Not a valid list.\"}\n    def __init__(self, cls_or_instance, **kwargs):\n        super().__init__(**kwargs)\n        try:\n            self.inner = resolve_field_instance(cls_or_instance)\n        except FieldInstanceResolutionError as error:\n            raise ValueError(\n                \"The list elements must be a subclass or instance of \"\n                \"marshmallow.base.FieldABC.\"\n            ) from error\n        if isinstance(self.inner, Nested):\n            self.only = self.inner.only\n            self.exclude = self.inner.exclude\n    def _bind_to_schema(self, field_name, schema):\n        super()._bind_to_schema(field_name, schema)\n        self.inner = copy.deepcopy(self.inner)\n        self.inner._bind_to_schema(field_name, self)\n        if isinstance(self.inner, Nested):\n            self.inner.only = self.only\n            self.inner.exclude = self.exclude\n    def _serialize(self, value, attr, obj, **kwargs):\n        if value is None:\n            return None\n        # Optimize dumping a list of Nested objects by calling dump(many=True)\n        if isinstance(self.inner, Nested) and not self.inner.many:\n            return self.inner._serialize(value, attr, obj, many=True, **kwargs)\n        return [self.inner._serialize(each, attr, obj, **kwargs) for each in value]\n    def _deserialize(self, value, attr, data, **kwargs):\n        if not utils.is_collection(value):\n            raise self.make_error(\"invalid\")\n        # Optimize loading a list of Nested objects by calling load(many=True)\n        if isinstance(self.inner, Nested) and not self.inner.many:\n            return self.inner.deserialize(value, many=True, **kwargs)\n        result = []\n        errors = {}\n        for idx, each in enumerate(value):\n            try:\n                result.append(self.inner.deserialize(each, **kwargs))\n            except ValidationError as error:\n                if error.valid_data is not None:\n                    result.append(error.valid_data)\n                errors.update({idx: error.messages})\n        if errors:\n            raise ValidationError(errors, valid_data=result)\n        return result\nclass Tuple(Field):\n    \"\"\"A tuple field, composed of a fixed number of other `Field` classes or\n    instances\n    Example: ::\n        row = Tuple((fields.String(), fields.Integer(), fields.Float()))\n    .. note::\n        Because of the structured nature of `collections.namedtuple` and\n        `typing.NamedTuple`, using a Schema within a Nested field for them is\n        more appropriate than using a `Tuple` field.\n    :param Iterable[Field] tuple_fields: An iterable of field classes or\n        instances.\n    :param kwargs: The same keyword arguments that :class:`Field` receives.\n    .. versionadded:: 3.0.0rc4\n    \"\"\"\n    default_error_messages = {\"invalid\": \"Not a valid tuple.\"}\n    def __init__(self, tuple_fields, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        if not utils.is_collection(tuple_fields):\n            raise ValueError(\n                \"tuple_fields must be an iterable of Field classes or \" \"instances.\"\n            )\n        try:\n            self.tuple_fields = [\n                resolve_field_instance(cls_or_instance)\n                for cls_or_instance in tuple_fields\n            ]\n        except FieldInstanceResolutionError as error:\n            raise ValueError(\n                'Elements of \"tuple_fields\" must be subclasses or '\n                \"instances of marshmallow.base.FieldABC.\"\n            ) from error\n        self.validate_length = Length(equal=len(self.tuple_fields))\n    def _bind_to_schema(self, field_name, schema):\n        super()._bind_to_schema(field_name, schema)\n        new_tuple_fields = []\n        for field in self.tuple_fields:\n            field = copy.deepcopy(field)\n            field._bind_to_schema(field_name, self)\n            new_tuple_fields.append(field)\n        self.tuple_fields = new_tuple_fields\n    def _serialize(self, value, attr, obj, **kwargs):\n        if value is None:\n            return None\n        return tuple(\n            field._serialize(each, attr, obj, **kwargs)\n            for field, each in zip(self.tuple_fields, value)\n        )\n    def _deserialize(self, value, attr, data, **kwargs):\n        if not utils.is_collection(value):\n            raise self.make_error(\"invalid\")\n        self.validate_length(value)\n        result = []\n        errors = {}\n        for idx, (field, each) in enumerate(zip(self.tuple_fields, value)):\n            try:\n                result.append(field.deserialize(each, **kwargs))\n            except ValidationError as error:\n                if error.valid_data is not None:\n                    result.append(error.valid_data)\n                errors.update({idx: error.messages})\n        if errors:\n            raise ValidationError(errors, valid_data=result)\n        return tuple(result)\nclass String(Field):\n    \"\"\"A string field.\n    :param kwargs: The same keyword arguments that :class:`Field` receives.\n    \"\"\"\n    default_error_messages = {\n        \"invalid\": \"Not a valid string.\",\n        \"invalid_utf8\": \"Not a valid utf-8 string.\",\n    }\n    def _serialize(self, value, attr, obj, **kwargs):\n        if value is None:\n            return None\n        return utils.ensure_text_type(value)\n    def _deserialize(self, value, attr, data, **kwargs):\n        if not isinstance(value, (str, bytes)):\n            raise self.make_error(\"invalid\")\n        try:\n            return utils.ensure_text_type(value)\n        except UnicodeDecodeError as error:\n            raise self.make_error(\"invalid_utf8\") from error\nclass UUID(String):\n    \"\"\"A UUID field.\"\"\"\n    default_error_messages = {\"invalid_uuid\": \"Not a valid UUID.\"}\n    def _validated(self, value):\n        \"\"\"Format the value or raise a :exc:`ValidationError` if an error occurs.\"\"\"\n        if value is None:\n            return None\n        if isinstance(value, uuid.UUID):\n            return value\n        try:\n            if isinstance(value, bytes) and len(value) == 16:\n                return uuid.UUID(bytes=value)\n            else:\n                return uuid.UUID(value)\n        except (ValueError, AttributeError, TypeError) as error:\n            raise self.make_error(\"invalid_uuid\") from error\n    def _serialize(self, value, attr, obj, **kwargs):\n        val = str(value) if value is not None else None\n        return super()._serialize(val, attr, obj, **kwargs)\n    def _deserialize(self, value, attr, data, **kwargs):\n        return self._validated(value)\nclass Number(Field):\n    \"\"\"Base class for number fields.\n    :param bool as_string: If True, format the serialized value as a string.\n    :param kwargs: The same keyword arguments that :class:`Field` receives.\n    \"\"\"\n    num_type = float\n    default_error_messages = {\n        \"invalid\": \"Not a valid number.\",\n        \"too_large\": \"Number too large.\",\n    }\n    def __init__(self, *, as_string=False, **kwargs):\n        self.as_string = as_string\n        super().__init__(**kwargs)\n    def _format_num(self, value):\n        \"\"\"Return the number value for value, given this field's `num_type`.\"\"\"\n        return self.num_type(value)\n    def _validated(self, value):\n        \"\"\"Format the value or raise a :exc:`ValidationError` if an error occurs.\"\"\"\n        if value is None:\n            return None\n        # (value is True or value is False) is ~5x faster than isinstance(value, bool)\n        if value is True or value is False:\n            raise self.make_error(\"invalid\", input=value)\n        try:\n            return self._format_num(value)\n        except (TypeError, ValueError) as error:\n            raise self.make_error(\"invalid\", input=value) from error\n        except OverflowError as error:\n            raise self.make_error(\"too_large\", input=value) from error\n    def _to_string(self, value):\n        return str(value)\n    def _serialize(self, value, attr, obj, **kwargs):\n        \"\"\"Return a string if `self.as_string=True`, otherwise return this field's `num_type`.\"\"\"\n        if value is None:\n            return None\n        ret = self._format_num(value)\n        return self._to_string(ret) if self.as_string else ret\n    def _deserialize(self, value, attr, data, **kwargs):\n        return self._validated(value)\nclass Integer(Number):\n    \"\"\"An integer field.\n    :param kwargs: The same keyword arguments that :class:`Number` receives.\n    \"\"\"\n    num_type = int\n    default_error_messages = {\"invalid\": \"Not a valid integer.\"}\n    def __init__(self, *, strict=False, **kwargs):\n        self.strict = strict\n        super().__init__(**kwargs)\n    # override Number\n    def _validated(self, value):\n        if self.strict:\n            if isinstance(value, numbers.Number) and isinstance(\n                value, numbers.Integral\n            ):\n                return super()._validated(value)\n            raise self.make_error(\"invalid\", input=value)\n        return super()._validated(value)\nclass Float(Number):\n    \"\"\"A double as an IEEE-754 double precision string.\n    :param bool allow_nan: If `True`, `NaN`, `Infinity` and `-Infinity` are allowed,\n        even though they are illegal according to the JSON specification.\n    :param bool as_string: If True, format the value as a string.\n    :param kwargs: The same keyword arguments that :class:`Number` receives.\n    \"\"\"\n    num_type = float\n    default_error_messages = {\n        \"special\": \"Special numeric values (nan or infinity) are not permitted.\"\n    }\n    def __init__(self, *, allow_nan=False, as_string=False, **kwargs):\n        self.allow_nan = allow_nan\n        super().__init__(as_string=as_string, **kwargs)\n    def _validated(self, value):\n        num = super()._validated(value)\n        if self.allow_nan is False:\n            if math.isnan(num) or num == float(\"inf\") or num == float(\"-inf\"):\n                raise self.make_error(\"special\")\n        return num\nclass Decimal(Number):\n    \"\"\"A field that (de)serializes to the Python ``decimal.Decimal`` type.\n    It's safe to use when dealing with money values, percentages, ratios\n    or other numbers where precision is critical.\n    .. warning::\n        This field serializes to a `decimal.Decimal` object by default. If you need\n        to render your data as JSON, keep in mind that the `json` module from the\n        standard library does not encode `decimal.Decimal`. Therefore, you must use\n        a JSON library that can handle decimals, such as `simplejson`, or serialize\n        to a string by passing ``as_string=True``.\n    .. warning::\n        If a JSON `float` value is passed to this field for deserialization it will\n        first be cast to its corresponding `string` value before being deserialized\n        to a `decimal.Decimal` object. The default `__str__` implementation of the\n        built-in Python `float` type may apply a destructive transformation upon\n        its input data and therefore cannot be relied upon to preserve precision.\n        To avoid this, you can instead pass a JSON `string` to be deserialized\n        directly.\n    :param int places: How many decimal places to quantize the value. If `None`, does\n        not quantize the value.\n    :param rounding: How to round the value during quantize, for example\n        `decimal.ROUND_UP`. If None, uses the rounding value from\n        the current thread's context.\n    :param bool allow_nan: If `True`, `NaN`, `Infinity` and `-Infinity` are allowed,\n        even though they are illegal according to the JSON specification.\n    :param bool as_string: If True, serialize to a string instead of a Python\n        `decimal.Decimal` type.\n    :param kwargs: The same keyword arguments that :class:`Number` receives.\n    .. versionadded:: 1.2.0\n    \"\"\"\n    num_type = decimal.Decimal\n    default_error_messages = {\n        \"special\": \"Special numeric values (nan or infinity) are not permitted.\"\n    }\n    def __init__(\n        self, places=None, rounding=None, *, allow_nan=False, as_string=False, **kwargs\n    ):\n        self.places = (\n            decimal.Decimal((0, (1,), -places)) if places is not None else None\n        )\n        self.rounding = rounding\n        self.allow_nan = allow_nan\n        super().__init__(as_string=as_string, **kwargs)\n    # override Number\n    def _format_num(self, value):\n        num = decimal.Decimal(str(value))\n        if self.allow_nan:\n            if num.is_nan():\n                return decimal.Decimal(\"NaN\")  # avoid sNaN, -sNaN and -NaN\n        if self.places is not None and num.is_finite():\n            num = num.quantize(self.places, rounding=self.rounding)\n        return num\n    # override Number\n    def _validated(self, value):\n        try:\n            num = super()._validated(value)\n        except decimal.InvalidOperation as error:\n            raise self.make_error(\"invalid\") from error\n        if not self.allow_nan and (num.is_nan() or num.is_infinite()):\n            raise self.make_error(\"special\")\n        return num\n    # override Number\n    def _to_string(self, value):\n        return format(value, \"f\")\nclass Boolean(Field):\n    \"\"\"A boolean field.\n    :param set truthy: Values that will (de)serialize to `True`. If an empty\n        set, any non-falsy value will deserialize to `True`. If `None`,\n        `marshmallow.fields.Boolean.truthy` will be used.\n    :param set falsy: Values that will (de)serialize to `False`. If `None`,\n        `marshmallow.fields.Boolean.falsy` will be used.\n    :param kwargs: The same keyword arguments that :class:`Field` receives.\n    \"\"\"\n    #: Default truthy values.\n    truthy = {\n        \"t\",\n        \"T\",\n        \"true\",\n        \"True\",\n        \"TRUE\",\n        \"on\",\n        \"On\",\n        \"ON\",\n        \"y\",\n        \"Y\",\n        \"yes\",\n        \"Yes\",\n        \"YES\",\n        \"1\",\n        1,\n        True,\n    }\n    #: Default falsy values.\n    falsy = {\n        \"f\",\n        \"F\",\n        \"false\",\n        \"False\",\n        \"FALSE\",\n        \"off\",\n        \"Off\",\n        \"OFF\",\n        \"n\",\n        \"N\",\n        \"no\",\n        \"No\",\n        \"NO\",\n        \"0\",\n        0,\n        0.0,\n        False,\n    }\n    default_error_messages = {\"invalid\": \"Not a valid boolean.\"}\n    def __init__(self, *, truthy=None, falsy=None, **kwargs):\n        super().__init__(**kwargs)\n        if truthy is not None:\n            self.truthy = set(truthy)\n        if falsy is not None:\n            self.falsy = set(falsy)\n    def _serialize(self, value, attr, obj, **kwargs):\n        if value is None:\n            return None\n        elif value in self.truthy:\n            return True\n        elif value in self.falsy:\n            return False\n        return bool(value)\n    def _deserialize(self, value, attr, data, **kwargs):\n        if not self.truthy:\n            return bool(value)\n        else:\n            try:\n                if value in self.truthy:\n                    return True\n                elif value in self.falsy:\n                    return False\n            except TypeError as error:\n                raise self.make_error(\"invalid\", input=value) from error\n        raise self.make_error(\"invalid\", input=value)\nclass DateTime(Field):\n    \"\"\"A formatted datetime string.\n    Example: ``'2014-12-22T03:12:58.019077+00:00'``\n    :param str format: Either ``\"rfc\"`` (for RFC822), ``\"iso\"`` (for ISO8601),\n        or a date format string. If `None`, defaults to \"iso\".\n    :param kwargs: The same keyword arguments that :class:`Field` receives.\n    .. versionchanged:: 3.0.0rc9\n        Does not modify timezone information on (de)serialization.\n    \"\"\"",
        "file_path": "src/marshmallow/fields.py",
        "chunk_index": 1,
        "metadata": {}
      },
      {
        "rank": 2,
        "score": 0.6116684675216675,
        "content": "                return decimal.Decimal(\"NaN\")  # avoid sNaN, -sNaN and -NaN\n        if self.places is not None and num.is_finite():\n            num = num.quantize(self.places, rounding=self.rounding)\n        return num\n    # override Number\n    def _validated(self, value):\n        try:\n            num = super()._validated(value)\n        except decimal.InvalidOperation as error:\n            raise self.make_error(\"invalid\") from error\n        if not self.allow_nan and (num.is_nan() or num.is_infinite()):\n            raise self.make_error(\"special\")\n        return num\n    # override Number\n    def _to_string(self, value):\n        return format(value, \"f\")\nclass Boolean(Field):\n    \"\"\"A boolean field.\n    :param set truthy: Values that will (de)serialize to `True`. If an empty\n        set, any non-falsy value will deserialize to `True`. If `None`,\n        `marshmallow.fields.Boolean.truthy` will be used.\n    :param set falsy: Values that will (de)serialize to `False`. If `None`,\n        `marshmallow.fields.Boolean.falsy` will be used.\n    :param kwargs: The same keyword arguments that :class:`Field` receives.\n    \"\"\"\n    #: Default truthy values.\n    truthy = {\n        \"t\",\n        \"T\",\n        \"true\",\n        \"True\",\n        \"TRUE\",\n        \"on\",\n        \"On\",\n        \"ON\",\n        \"y\",\n        \"Y\",\n        \"yes\",\n        \"Yes\",\n        \"YES\",\n        \"1\",\n        1,\n        True,\n    }\n    #: Default falsy values.\n    falsy = {\n        \"f\",\n        \"F\",\n        \"false\",\n        \"False\",\n        \"FALSE\",\n        \"off\",\n        \"Off\",\n        \"OFF\",\n        \"n\",\n        \"N\",\n        \"no\",\n        \"No\",\n        \"NO\",\n        \"0\",\n        0,\n        0.0,\n        False,\n    }\n    default_error_messages = {\"invalid\": \"Not a valid boolean.\"}\n    def __init__(self, *, truthy=None, falsy=None, **kwargs):\n        super().__init__(**kwargs)\n        if truthy is not None:\n            self.truthy = set(truthy)\n        if falsy is not None:\n            self.falsy = set(falsy)\n    def _serialize(self, value, attr, obj, **kwargs):\n        if value is None:\n            return None\n        elif value in self.truthy:\n            return True\n        elif value in self.falsy:\n            return False\n        return bool(value)\n    def _deserialize(self, value, attr, data, **kwargs):\n        if not self.truthy:\n            return bool(value)\n        else:\n            try:\n                if value in self.truthy:\n                    return True\n                elif value in self.falsy:\n                    return False\n            except TypeError as error:\n                raise self.make_error(\"invalid\", input=value) from error\n        raise self.make_error(\"invalid\", input=value)\nclass DateTime(Field):\n    \"\"\"A formatted datetime string.\n    Example: ``'2014-12-22T03:12:58.019077+00:00'``\n    :param str format: Either ``\"rfc\"`` (for RFC822), ``\"iso\"`` (for ISO8601),\n        or a date format string. If `None`, defaults to \"iso\".\n    :param kwargs: The same keyword arguments that :class:`Field` receives.\n    .. versionchanged:: 3.0.0rc9\n        Does not modify timezone information on (de)serialization.\n    \"\"\"\n    SERIALIZATION_FUNCS = {\n        \"iso\": utils.isoformat,\n        \"iso8601\": utils.isoformat,\n        \"rfc\": utils.rfcformat,\n        \"rfc822\": utils.rfcformat,\n    }\n    DESERIALIZATION_FUNCS = {\n        \"iso\": utils.from_iso_datetime,\n        \"iso8601\": utils.from_iso_datetime,\n        \"rfc\": utils.from_rfc,\n        \"rfc822\": utils.from_rfc,\n    }\n    DEFAULT_FORMAT = \"iso\"\n    OBJ_TYPE = \"datetime\"\n    SCHEMA_OPTS_VAR_NAME = \"datetimeformat\"\n    default_error_messages = {\n        \"invalid\": \"Not a valid {obj_type}.\",\n        \"invalid_awareness\": \"Not a valid {awareness} {obj_type}.\",\n        \"format\": '\"{input}\" cannot be formatted as a {obj_type}.',\n    }\n    def __init__(self, format=None, **kwargs):\n        super().__init__(**kwargs)\n        # Allow this to be None. It may be set later in the ``_serialize``\n        # or ``_deserialize`` methods. This allows a Schema to dynamically set the\n        # format, e.g. from a Meta option\n        self.format = format\n    def _bind_to_schema(self, field_name, schema):\n        super()._bind_to_schema(field_name, schema)\n        self.format = (\n            self.format\n            or getattr(schema.opts, self.SCHEMA_OPTS_VAR_NAME)\n            or self.DEFAULT_FORMAT\n        )\n    def _serialize(self, value, attr, obj, **kwargs):\n        if value is None:\n            return None\n        data_format = self.format or self.DEFAULT_FORMAT\n        format_func = self.SERIALIZATION_FUNCS.get(data_format)\n        if format_func:\n            return format_func(value)\n        else:\n            return value.strftime(data_format)\n    def _deserialize(self, value, attr, data, **kwargs):\n        if not value:  # Falsy values, e.g. '', None, [] are not valid\n            raise self.make_error(\"invalid\", input=value, obj_type=self.OBJ_TYPE)\n        data_format = self.format or self.DEFAULT_FORMAT\n        func = self.DESERIALIZATION_FUNCS.get(data_format)\n        if func:\n            try:\n                return func(value)\n            except (TypeError, AttributeError, ValueError) as error:\n                raise self.make_error(\n                    \"invalid\", input=value, obj_type=self.OBJ_TYPE\n                ) from error\n        else:\n            try:\n                return self._make_object_from_format(value, data_format)\n            except (TypeError, AttributeError, ValueError) as error:\n                raise self.make_error(\n                    \"invalid\", input=value, obj_type=self.OBJ_TYPE\n                ) from error\n    @staticmethod\n    def _make_object_from_format(value, data_format):\n        return dt.datetime.strptime(value, data_format)\nclass NaiveDateTime(DateTime):\n    \"\"\"A formatted naive datetime string.\n    :param str format: See :class:`DateTime`.\n    :param timezone timezone: Used on deserialization. If `None`,\n        aware datetimes are rejected. If not `None`, aware datetimes are\n        converted to this timezone before their timezone information is\n        removed.\n    :param kwargs: The same keyword arguments that :class:`Field` receives.\n    .. versionadded:: 3.0.0rc9\n    \"\"\"\n    AWARENESS = \"naive\"\n    def __init__(self, format=None, *, timezone=None, **kwargs):\n        super().__init__(format=format, **kwargs)\n        self.timezone = timezone\n    def _deserialize(self, value, attr, data, **kwargs):\n        ret = super()._deserialize(value, attr, data, **kwargs)\n        if is_aware(ret):\n            if self.timezone is None:\n                raise self.make_error(\n                    \"invalid_awareness\",\n                    awareness=self.AWARENESS,\n                    obj_type=self.OBJ_TYPE,\n                )\n            ret = ret.astimezone(self.timezone).replace(tzinfo=None)\n        return ret\nclass AwareDateTime(DateTime):\n    \"\"\"A formatted aware datetime string.\n    :param str format: See :class:`DateTime`.\n    :param timezone default_timezone: Used on deserialization. If `None`, naive\n        datetimes are rejected. If not `None`, naive datetimes are set this\n        timezone.\n    :param kwargs: The same keyword arguments that :class:`Field` receives.\n    .. versionadded:: 3.0.0rc9\n    \"\"\"\n    AWARENESS = \"aware\"\n    def __init__(self, format=None, *, default_timezone=None, **kwargs):\n        super().__init__(format=format, **kwargs)\n        self.default_timezone = default_timezone\n    def _deserialize(self, value, attr, data, **kwargs):\n        ret = super()._deserialize(value, attr, data, **kwargs)\n        if not is_aware(ret):\n            if self.default_timezone is None:\n                raise self.make_error(\n                    \"invalid_awareness\",\n                    awareness=self.AWARENESS,\n                    obj_type=self.OBJ_TYPE,\n                )\n            ret = ret.replace(tzinfo=self.default_timezone)\n        return ret\nclass Time(Field):\n    \"\"\"ISO8601-formatted time string.\n    :param kwargs: The same keyword arguments that :class:`Field` receives.\n    \"\"\"\n    default_error_messages = {\n        \"invalid\": \"Not a valid time.\",\n        \"format\": '\"{input}\" cannot be formatted as a time.',\n    }\n    def _serialize(self, value, attr, obj, **kwargs):\n        if value is None:\n            return None\n        ret = value.isoformat()\n        if value.microsecond:\n            return ret[:15]\n        return ret\n    def _deserialize(self, value, attr, data, **kwargs):\n        \"\"\"Deserialize an ISO8601-formatted time to a :class:`datetime.time` object.\"\"\"\n        if not value:  # falsy values are invalid\n            raise self.make_error(\"invalid\")\n        try:\n            return utils.from_iso_time(value)\n        except (AttributeError, TypeError, ValueError) as error:\n            raise self.make_error(\"invalid\") from error\nclass Date(DateTime):\n    \"\"\"ISO8601-formatted date string.\n    :param format: Either ``\"iso\"`` (for ISO8601) or a date format string.\n        If `None`, defaults to \"iso\".\n    :param kwargs: The same keyword arguments that :class:`Field` receives.\n    \"\"\"\n    default_error_messages = {\n        \"invalid\": \"Not a valid date.\",\n        \"format\": '\"{input}\" cannot be formatted as a date.',\n    }\n    SERIALIZATION_FUNCS = {\"iso\": utils.to_iso_date, \"iso8601\": utils.to_iso_date}\n    DESERIALIZATION_FUNCS = {\"iso\": utils.from_iso_date, \"iso8601\": utils.from_iso_date}\n    DEFAULT_FORMAT = \"iso\"\n    OBJ_TYPE = \"date\"\n    SCHEMA_OPTS_VAR_NAME = \"dateformat\"\n    @staticmethod\n    def _make_object_from_format(value, data_format):\n        return dt.datetime.strptime(value, data_format).date()\nclass TimeDelta(Field):\n    \"\"\"A field that (de)serializes a :class:`datetime.timedelta` object to an\n    integer and vice versa. The integer can represent the number of days,\n    seconds or microseconds.\n    :param str precision: Influences how the integer is interpreted during\n        (de)serialization. Must be 'days', 'seconds', 'microseconds',\n        'milliseconds', 'minutes', 'hours' or 'weeks'.\n    :param kwargs: The same keyword arguments that :class:`Field` receives.\n    .. versionchanged:: 2.0.0\n        Always serializes to an integer value to avoid rounding errors.\n        Add `precision` parameter.\n    \"\"\"\n    DAYS = \"days\"\n    SECONDS = \"seconds\"\n    MICROSECONDS = \"microseconds\"\n    MILLISECONDS = \"milliseconds\"\n    MINUTES = \"minutes\"\n    HOURS = \"hours\"\n    WEEKS = \"weeks\"\n    default_error_messages = {\n        \"invalid\": \"Not a valid period of time.\",\n        \"format\": \"{input!r} cannot be formatted as a timedelta.\",\n    }\n    def __init__(self, precision=SECONDS, **kwargs):\n        precision = precision.lower()\n        units = (\n            self.DAYS,\n            self.SECONDS,\n            self.MICROSECONDS,\n            self.MILLISECONDS,\n            self.MINUTES,\n            self.HOURS,\n            self.WEEKS,\n        )\n        if precision not in units:\n            msg = 'The precision must be {} or \"{}\".'.format(\n                \", \".join(['\"{}\"'.format(each) for each in units[:-1]]), units[-1]\n            )\n            raise ValueError(msg)\n        self.precision = precision\n        super().__init__(**kwargs)\n    def _serialize(self, value, attr, obj, **kwargs):\n        if value is None:\n            return None\n        base_unit = dt.timedelta(**{self.precision: 1})\n        return int(value.total_seconds() / base_unit.total_seconds())\n    def _deserialize(self, value, attr, data, **kwargs):\n        try:\n            value = int(value)\n        except (TypeError, ValueError) as error:\n            raise self.make_error(\"invalid\") from error\n        kwargs = {self.precision: value}\n        try:\n            return dt.timedelta(**kwargs)\n        except OverflowError as error:\n            raise self.make_error(\"invalid\") from error\nclass Mapping(Field):\n    \"\"\"An abstract class for objects with key-value pairs.\n    :param Field keys: A field class or instance for dict keys.\n    :param Field values: A field class or instance for dict values.\n    :param kwargs: The same keyword arguments that :class:`Field` receives.\n    .. note::\n        When the structure of nested data is not known, you may omit the\n        `keys` and `values` arguments to prevent content validation.\n    .. versionadded:: 3.0.0rc4\n    \"\"\"\n    mapping_type = dict\n    default_error_messages = {\"invalid\": \"Not a valid mapping type.\"}\n    def __init__(self, keys=None, values=None, **kwargs):\n        super().__init__(**kwargs)\n        if keys is None:\n            self.key_field = None\n        else:\n            try:\n                self.key_field = resolve_field_instance(keys)\n            except FieldInstanceResolutionError as error:\n                raise ValueError(\n                    '\"keys\" must be a subclass or instance of '\n                    \"marshmallow.base.FieldABC.\"\n                ) from error\n        if values is None:\n            self.value_field = None\n        else:\n            try:\n                self.value_field = resolve_field_instance(values)\n            except FieldInstanceResolutionError as error:\n                raise ValueError(\n                    '\"values\" must be a subclass or instance of '\n                    \"marshmallow.base.FieldABC.\"\n                ) from error\n            if isinstance(self.value_field, Nested):\n                self.only = self.value_field.only\n                self.exclude = self.value_field.exclude\n    def _bind_to_schema(self, field_name, schema):\n        super()._bind_to_schema(field_name, schema)\n        if self.value_field:\n            self.value_field = copy.deepcopy(self.value_field)\n            self.value_field._bind_to_schema(field_name, self)\n        if isinstance(self.value_field, Nested):\n            self.value_field.only = self.only\n            self.value_field.exclude = self.exclude\n        if self.key_field:\n            self.key_field = copy.deepcopy(self.key_field)\n            self.key_field._bind_to_schema(field_name, self)\n    def _serialize(self, value, attr, obj, **kwargs):\n        if value is None:\n            return None\n        if not self.value_field and not self.key_field:\n            return value\n        #  Serialize keys\n        if self.key_field is None:\n            keys = {k: k for k in value.keys()}\n        else:\n            keys = {\n                k: self.key_field._serialize(k, None, None, **kwargs)\n                for k in value.keys()\n            }\n        #  Serialize values\n        result = self.mapping_type()\n        if self.value_field is None:\n            for k, v in value.items():\n                if k in keys:\n                    result[keys[k]] = v\n        else:\n            for k, v in value.items():\n                result[keys[k]] = self.value_field._serialize(v, None, None, **kwargs)\n        return result\n    def _deserialize(self, value, attr, data, **kwargs):\n        if not isinstance(value, _Mapping):\n            raise self.make_error(\"invalid\")\n        if not self.value_field and not self.key_field:\n            return value\n        errors = collections.defaultdict(dict)\n        #  Deserialize keys\n        if self.key_field is None:\n            keys = {k: k for k in value.keys()}\n        else:\n            keys = {}\n            for key in value.keys():\n                try:\n                    keys[key] = self.key_field.deserialize(key, **kwargs)\n                except ValidationError as error:\n                    errors[key][\"key\"] = error.messages\n        #  Deserialize values\n        result = self.mapping_type()\n        if self.value_field is None:\n            for k, v in value.items():\n                if k in keys:\n                    result[keys[k]] = v\n        else:\n            for key, val in value.items():\n                try:\n                    deser_val = self.value_field.deserialize(val, **kwargs)\n                except ValidationError as error:\n                    errors[key][\"value\"] = error.messages\n                    if error.valid_data is not None and key in keys:\n                        result[keys[key]] = error.valid_data\n                else:\n                    if key in keys:\n                        result[keys[key]] = deser_val\n        if errors:\n            raise ValidationError(errors, valid_data=result)\n        return result\nclass Dict(Mapping):\n    \"\"\"A dict field. Supports dicts and dict-like objects. Extends\n    Mapping with dict as the mapping_type.\n    Example: ::\n        numbers = fields.Dict(keys=fields.Str(), values=fields.Float())\n    :param kwargs: The same keyword arguments that :class:`Mapping` receives.\n    .. versionadded:: 2.1.0\n    \"\"\"\n    mapping_type = dict\nclass Url(String):\n    \"\"\"A validated URL field. Validation occurs during both serialization and\n    deserialization.\n    :param default: Default value for the field if the attribute is not set.\n    :param str attribute: The name of the attribute to get the value from. If\n        `None`, assumes the attribute has the same name as the field.\n    :param bool relative: Whether to allow relative URLs.\n    :param bool require_tld: Whether to reject non-FQDN hostnames.\n    :param kwargs: The same keyword arguments that :class:`String` receives.\n    \"\"\"\n    default_error_messages = {\"invalid\": \"Not a valid URL.\"}\n    def __init__(self, *, relative=False, schemes=None, require_tld=True, **kwargs):\n        super().__init__(**kwargs)\n        self.relative = relative\n        self.require_tld = require_tld\n        # Insert validation into self.validators so that multiple errors can be\n        # stored.\n        self.validators.insert(\n            0,\n            validate.URL(\n                relative=self.relative,\n                schemes=schemes,\n                require_tld=self.require_tld,\n                error=self.error_messages[\"invalid\"],\n            ),\n        )\nclass Email(String):\n    \"\"\"A validated email field. Validation occurs during both serialization and\n    deserialization.\n    :param args: The same positional arguments that :class:`String` receives.\n    :param kwargs: The same keyword arguments that :class:`String` receives.\n    \"\"\"\n    default_error_messages = {\"invalid\": \"Not a valid email address.\"}\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        # Insert validation into self.validators so that multiple errors can be\n        # stored.\n        self.validators.insert(0, validate.Email(error=self.error_messages[\"invalid\"]))\nclass Method(Field):\n    \"\"\"A field that takes the value returned by a `Schema` method.\n    :param str serialize: The name of the Schema method from which\n        to retrieve the value. The method must take an argument ``obj``\n        (in addition to self) that is the object to be serialized.\n    :param str deserialize: Optional name of the Schema method for deserializing\n        a value The method must take a single argument ``value``, which is the\n        value to deserialize.\n    .. versionchanged:: 2.0.0\n        Removed optional ``context`` parameter on methods. Use ``self.context`` instead.\n    .. versionchanged:: 2.3.0\n        Deprecated ``method_name`` parameter in favor of ``serialize`` and allow\n        ``serialize`` to not be passed at all.\n    .. versionchanged:: 3.0.0\n        Removed ``method_name`` parameter.\n    \"\"\"\n    _CHECK_ATTRIBUTE = False\n    def __init__(self, serialize=None, deserialize=None, **kwargs):\n        # Set dump_only and load_only based on arguments\n        kwargs[\"dump_only\"] = bool(serialize) and not bool(deserialize)\n        kwargs[\"load_only\"] = bool(deserialize) and not bool(serialize)\n        super().__init__(**kwargs)\n        self.serialize_method_name = serialize\n        self.deserialize_method_name = deserialize\n    def _serialize(self, value, attr, obj, **kwargs):\n        if not self.serialize_method_name:\n            return missing_\n        method = utils.callable_or_raise(\n            getattr(self.parent, self.serialize_method_name, None)\n        )\n        return method(obj)\n    def _deserialize(self, value, attr, data, **kwargs):\n        if self.deserialize_method_name:\n            method = utils.callable_or_raise(\n                getattr(self.parent, self.deserialize_method_name, None)\n            )",
        "file_path": "src/marshmallow/fields.py",
        "chunk_index": 2,
        "metadata": {}
      },
      {
        "rank": 3,
        "score": 0.5967512130737305,
        "content": "\"\"\"Exception classes for marshmallow-related errors.\"\"\"\n# Key used for schema-level validation errors\nSCHEMA = \"_schema\"\nclass MarshmallowError(Exception):\n    \"\"\"Base class for all marshmallow-related errors.\"\"\"\nclass ValidationError(MarshmallowError):\n    \"\"\"Raised when validation fails on a field or schema.\n    Validators and custom fields should raise this exception.\n    :param str|list|dict message: An error message, list of error messages, or dict of\n        error messages. If a dict, the keys are subitems and the values are error messages.\n    :param str field_name: Field name to store the error on.\n        If `None`, the error is stored as schema-level error.\n    :param dict data: Raw input data.\n    :param dict valid_data: Valid (de)serialized data.\n    \"\"\"\n    def __init__(\n        self, message, field_name=SCHEMA, data=None, valid_data=None, **kwargs\n    ):\n        self.messages = [message] if isinstance(message, (str, bytes)) else message\n        self.field_name = field_name\n        self.data = data\n        self.valid_data = valid_data\n        self.kwargs = kwargs\n        super().__init__(message)\n    def normalized_messages(self):\n        if self.field_name == SCHEMA and isinstance(self.messages, dict):\n            return self.messages\n        return {self.field_name: self.messages}\nclass RegistryError(NameError):\n    \"\"\"Raised when an invalid operation is performed on the serializer\n    class registry.\n    \"\"\"\nclass StringNotCollectionError(MarshmallowError, TypeError):\n    \"\"\"Raised when a string is passed when a list of strings is expected.\"\"\"\nclass FieldInstanceResolutionError(MarshmallowError, TypeError):\n    \"\"\"Raised when schema to instantiate is neither a Schema class nor an instance.\"\"\"",
        "file_path": "src/marshmallow/exceptions.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 4,
        "score": 0.5957945585250854,
        "content": "\"\"\"Field classes for various types of data.\"\"\"\nimport collections\nimport copy\nimport datetime as dt\nimport numbers\nimport uuid\nimport decimal\nimport math\nimport warnings\nfrom collections.abc import Mapping as _Mapping\nfrom marshmallow import validate, utils, class_registry\nfrom marshmallow.base import FieldABC, SchemaABC\nfrom marshmallow.utils import (\n    is_collection,\n    missing as missing_,\n    resolve_field_instance,\n    is_aware,\n)\nfrom marshmallow.exceptions import (\n    ValidationError,\n    StringNotCollectionError,\n    FieldInstanceResolutionError,\n)\nfrom marshmallow.validate import Validator, Length\n__all__ = [\n    \"Field\",\n    \"Raw\",\n    \"Nested\",\n    \"Mapping\",\n    \"Dict\",\n    \"List\",\n    \"Tuple\",\n    \"String\",\n    \"UUID\",\n    \"Number\",\n    \"Integer\",\n    \"Decimal\",\n    \"Boolean\",\n    \"Float\",\n    \"DateTime\",\n    \"NaiveDateTime\",\n    \"AwareDateTime\",\n    \"Time\",\n    \"Date\",\n    \"TimeDelta\",\n    \"Url\",\n    \"URL\",\n    \"Email\",\n    \"Method\",\n    \"Function\",\n    \"Str\",\n    \"Bool\",\n    \"Int\",\n    \"Constant\",\n    \"Pluck\",\n]\nMISSING_ERROR_MESSAGE = (\n    \"ValidationError raised by `{class_name}`, but error key `{key}` does \"\n    \"not exist in the `error_messages` dictionary.\"\n)\nclass Field(FieldABC):\n    \"\"\"Basic field from which other fields should extend. It applies no\n    formatting by default, and should only be used in cases where\n    data does not need to be formatted before being serialized or deserialized.\n    On error, the name of the field will be returned.\n    :param default: If set, this value will be used during serialization if the input value\n        is missing. If not set, the field will be excluded from the serialized output if the\n        input value is missing. May be a value or a callable.\n    :param missing: Default deserialization value for the field if the field is not\n        found in the input data. May be a value or a callable.\n    :param str data_key: The name of the dict key in the external representation, i.e.\n        the input of `load` and the output of `dump`.\n        If `None`, the key will match the name of the field.\n    :param str attribute: The name of the attribute to get the value from when serializing.\n        If `None`, assumes the attribute has the same name as the field.\n        Note: This should only be used for very specific use cases such as\n        outputting multiple fields for a single attribute. In most cases,\n        you should use ``data_key`` instead.\n    :param callable validate: Validator or collection of validators that are called\n        during deserialization. Validator takes a field's input value as\n        its only parameter and returns a boolean.\n        If it returns `False`, an :exc:`ValidationError` is raised.\n    :param required: Raise a :exc:`ValidationError` if the field value\n        is not supplied during deserialization.\n    :param allow_none: Set this to `True` if `None` should be considered a valid value during\n        validation/deserialization. If ``missing=None`` and ``allow_none`` is unset,\n        will default to ``True``. Otherwise, the default is ``False``.\n    :param bool load_only: If `True` skip this field during serialization, otherwise\n        its value will be present in the serialized data.\n    :param bool dump_only: If `True` skip this field during deserialization, otherwise\n        its value will be present in the deserialized object. In the context of an\n        HTTP API, this effectively marks the field as \"read-only\".\n    :param dict error_messages: Overrides for `Field.default_error_messages`.\n    :param metadata: Extra arguments to be stored as metadata.\n    .. versionchanged:: 2.0.0\n        Removed `error` parameter. Use ``error_messages`` instead.\n    .. versionchanged:: 2.0.0\n        Added `allow_none` parameter, which makes validation/deserialization of `None`\n        consistent across fields.\n    .. versionchanged:: 2.0.0\n        Added `load_only` and `dump_only` parameters, which allow field skipping\n        during the (de)serialization process.\n    .. versionchanged:: 2.0.0\n        Added `missing` parameter, which indicates the value for a field if the field\n        is not found during deserialization.\n    .. versionchanged:: 2.0.0\n        ``default`` value is only used if explicitly set. Otherwise, missing values\n        inputs are excluded from serialized output.\n    .. versionchanged:: 3.0.0b8\n        Add ``data_key`` parameter for the specifying the key in the input and\n        output data. This parameter replaced both ``load_from`` and ``dump_to``.\n    \"\"\"\n    # Some fields, such as Method fields and Function fields, are not expected\n    #  to exist as attributes on the objects to serialize. Set this to False\n    #  for those fields\n    _CHECK_ATTRIBUTE = True\n    _creation_index = 0  # Used for sorting\n    #: Default error messages for various kinds of errors. The keys in this dictionary\n    #: are passed to `Field.fail`. The values are error messages passed to\n    #: :exc:`marshmallow.exceptions.ValidationError`.\n    default_error_messages = {\n        \"required\": \"Missing data for required field.\",\n        \"null\": \"Field may not be null.\",\n        \"validator_failed\": \"Invalid value.\",\n    }\n    def __init__(\n        self,\n        *,\n        default=missing_,\n        missing=missing_,\n        data_key=None,\n        attribute=None,\n        validate=None,\n        required=False,\n        allow_none=None,\n        load_only=False,\n        dump_only=False,\n        error_messages=None,\n        **metadata\n    ):\n        self.default = default\n        self.attribute = attribute\n        self.data_key = data_key\n        self.validate = validate\n        if utils.is_iterable_but_not_string(validate):\n            if not utils.is_generator(validate):\n                self.validators = validate\n            else:\n                self.validators = list(validate)\n        elif callable(validate):\n            self.validators = [validate]\n        elif validate is None:\n            self.validators = []\n        else:\n            raise ValueError(\n                \"The 'validate' parameter must be a callable \"\n                \"or a collection of callables.\"\n            )\n        # If missing=None, None should be considered valid by default\n        if allow_none is None:\n            if missing is None:\n                self.allow_none = True\n            else:\n                self.allow_none = False\n        else:\n            self.allow_none = allow_none\n        self.load_only = load_only\n        self.dump_only = dump_only\n        if required is True and missing is not missing_:\n            raise ValueError(\"'missing' must not be set for required fields.\")\n        self.required = required\n        self.missing = missing\n        self.metadata = metadata\n        self._creation_index = Field._creation_index\n        Field._creation_index += 1\n        # Collect default error message from self and parent classes\n        messages = {}\n        for cls in reversed(self.__class__.__mro__):\n            messages.update(getattr(cls, \"default_error_messages\", {}))\n        messages.update(error_messages or {})\n        self.error_messages = messages\n    def __repr__(self):\n        return (\n            \"<fields.{ClassName}(default={self.default!r}, \"\n            \"attribute={self.attribute!r}, \"\n            \"validate={self.validate}, required={self.required}, \"\n            \"load_only={self.load_only}, dump_only={self.dump_only}, \"\n            \"missing={self.missing}, allow_none={self.allow_none}, \"\n            \"error_messages={self.error_messages})>\".format(\n                ClassName=self.__class__.__name__, self=self\n            )\n        )\n    def __deepcopy__(self, memo):\n        return copy.copy(self)\n    def get_value(self, obj, attr, accessor=None, default=missing_):\n        \"\"\"Return the value for a given key from an object.\n        :param object obj: The object to get the value from.\n        :param str attr: The attribute/key in `obj` to get the value from.\n        :param callable accessor: A callable used to retrieve the value of `attr` from\n            the object `obj`. Defaults to `marshmallow.utils.get_value`.\n        \"\"\"\n        # NOTE: Use getattr instead of direct attribute access here so that\n        # subclasses aren't required to define `attribute` member\n        attribute = getattr(self, \"attribute\", None)\n        accessor_func = accessor or utils.get_value\n        check_key = attr if attribute is None else attribute\n        return accessor_func(obj, check_key, default)\n    def _validate(self, value):\n        \"\"\"Perform validation on ``value``. Raise a :exc:`ValidationError` if validation\n        does not succeed.\n        \"\"\"\n        errors = []\n        kwargs = {}\n        for validator in self.validators:\n            try:\n                r = validator(value)\n                if not isinstance(validator, Validator) and r is False:\n                    raise self.make_error(\"validator_failed\")\n            except ValidationError as err:\n                kwargs.update(err.kwargs)\n                if isinstance(err.messages, dict):\n                    errors.append(err.messages)\n                else:\n                    errors.extend(err.messages)\n        if errors:\n            raise ValidationError(errors, **kwargs)\n    def make_error(self, key: str, **kwargs) -> ValidationError:\n        \"\"\"Helper method to make a `ValidationError` with an error message\n        from ``self.error_messages``.\n        \"\"\"\n        try:\n            msg = self.error_messages[key]\n        except KeyError as error:\n            class_name = self.__class__.__name__\n            msg = MISSING_ERROR_MESSAGE.format(class_name=class_name, key=key)\n            raise AssertionError(msg) from error\n        if isinstance(msg, (str, bytes)):\n            msg = msg.format(**kwargs)\n        return ValidationError(msg)\n    def fail(self, key: str, **kwargs):\n        \"\"\"Helper method that raises a `ValidationError` with an error message\n        from ``self.error_messages``.\n        .. deprecated:: 3.0.0\n            Use `make_error <marshmallow.fields.Field.make_error>` instead.\n        \"\"\"\n        warnings.warn(\n            '`Field.fail` is deprecated. Use `raise self.make_error(\"{}\", ...)` instead.'.format(\n                key\n            ),\n            DeprecationWarning,\n        )\n        raise self.make_error(key=key, **kwargs)\n    def _validate_missing(self, value):\n        \"\"\"Validate missing values. Raise a :exc:`ValidationError` if\n        `value` should be considered missing.\n        \"\"\"\n        if value is missing_:\n            if hasattr(self, \"required\") and self.required:\n                raise self.make_error(\"required\")\n        if value is None:\n            if hasattr(self, \"allow_none\") and self.allow_none is not True:\n                raise self.make_error(\"null\")\n    def serialize(self, attr, obj, accessor=None, **kwargs):\n        \"\"\"Pulls the value for the given key from the object, applies the\n        field's formatting and returns the result.\n        :param str attr: The attribute/key to get from the object.\n        :param str obj: The object to access the attribute/key from.\n        :param callable accessor: Function used to access values from ``obj``.\n        :param dict kwargs: Field-specific keyword arguments.\n        \"\"\"\n        if self._CHECK_ATTRIBUTE:\n            value = self.get_value(obj, attr, accessor=accessor)\n            if value is missing_ and hasattr(self, \"default\"):\n                default = self.default\n                value = default() if callable(default) else default\n            if value is missing_:\n                return value\n        else:\n            value = None\n        return self._serialize(value, attr, obj, **kwargs)\n    def deserialize(self, value, attr=None, data=None, **kwargs):\n        \"\"\"Deserialize ``value``.\n        :param value: The value to deserialize.\n        :param str attr: The attribute/key in `data` to deserialize.\n        :param dict data: The raw input data passed to `Schema.load`.\n        :param dict kwargs: Field-specific keyword arguments.\n        :raise ValidationError: If an invalid value is passed or if a required value\n            is missing.\n        \"\"\"\n        # Validate required fields, deserialize, then validate\n        # deserialized value\n        self._validate_missing(value)\n        if value is missing_:\n            _miss = self.missing\n            return _miss() if callable(_miss) else _miss\n        if getattr(self, \"allow_none\", False) is True and value is None:\n            return None\n        output = self._deserialize(value, attr, data, **kwargs)\n        self._validate(output)\n        return output\n    # Methods for concrete classes to override.\n    def _bind_to_schema(self, field_name, schema):\n        \"\"\"Update field with values from its parent schema. Called by\n        :meth:`Schema._bind_field <marshmallow.Schema._bind_field>`.\n        :param str field_name: Field name set in schema.\n        :param Schema schema: Parent schema.\n        \"\"\"\n        self.parent = self.parent or schema\n        self.name = self.name or field_name\n    def _serialize(self, value, attr, obj, **kwargs):\n        \"\"\"Serializes ``value`` to a basic Python datatype. Noop by default.\n        Concrete :class:`Field` classes should implement this method.\n        Example: ::\n            class TitleCase(Field):\n                def _serialize(self, value, attr, obj, **kwargs):\n                    if not value:\n                        return ''\n                    return str(value).title()\n        :param value: The value to be serialized.\n        :param str attr: The attribute or key on the object to be serialized.\n        :param object obj: The object the value was pulled from.\n        :param dict kwargs: Field-specific keyword arguments.\n        :return: The serialized value\n        \"\"\"\n        return value\n    def _deserialize(self, value, attr, data, **kwargs):\n        \"\"\"Deserialize value. Concrete :class:`Field` classes should implement this method.\n        :param value: The value to be deserialized.\n        :param str attr: The attribute/key in `data` to be deserialized.\n        :param dict data: The raw input data passed to the `Schema.load`.\n        :param dict kwargs: Field-specific keyword arguments.\n        :raise ValidationError: In case of formatting or validation failure.\n        :return: The deserialized value.\n        .. versionchanged:: 2.0.0\n            Added ``attr`` and ``data`` parameters.\n        .. versionchanged:: 3.0.0\n            Added ``**kwargs`` to signature.\n        \"\"\"\n        return value\n    # Properties\n    @property\n    def context(self):\n        \"\"\"The context dictionary for the parent :class:`Schema`.\"\"\"\n        return self.parent.context\n    @property\n    def root(self):\n        \"\"\"Reference to the `Schema` that this field belongs to even if it is buried in a\n        container field (e.g. `List`).\n        Return `None` for unbound fields.\n        \"\"\"\n        ret = self\n        while hasattr(ret, \"parent\"):\n            ret = ret.parent\n        return ret if isinstance(ret, SchemaABC) else None\nclass Raw(Field):\n    \"\"\"Field that applies no formatting or validation.\"\"\"\n    pass\nclass Nested(Field):\n    \"\"\"Allows you to nest a :class:`Schema <marshmallow.Schema>`\n    inside a field.\n    Examples: ::\n        user = fields.Nested(UserSchema)\n        user2 = fields.Nested('UserSchema')  # Equivalent to above\n        collaborators = fields.Nested(UserSchema, many=True, only=('id',))\n        parent = fields.Nested('self')\n    When passing a `Schema <marshmallow.Schema>` instance as the first argument,\n    the instance's ``exclude``, ``only``, and ``many`` attributes will be respected.\n    Therefore, when passing the ``exclude``, ``only``, or ``many`` arguments to `fields.Nested`,\n    you should pass a `Schema <marshmallow.Schema>` class (not an instance) as the first argument.\n    ::\n        # Yes\n        author = fields.Nested(UserSchema, only=('id', 'name'))\n        # No\n        author = fields.Nested(UserSchema(), only=('id', 'name'))\n    :param Schema nested: The Schema class or class name (string)\n        to nest, or ``\"self\"`` to nest the :class:`Schema` within itself.\n    :param tuple exclude: A list or tuple of fields to exclude.\n    :param only: A list or tuple of fields to marshal. If `None`, all fields are marshalled.\n        This parameter takes precedence over ``exclude``.\n    :param bool many: Whether the field is a collection of objects.\n    :param unknown: Whether to exclude, include, or raise an error for unknown\n        fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`.\n    :param kwargs: The same keyword arguments that :class:`Field` receives.\n    \"\"\"\n    default_error_messages = {\"type\": \"Invalid type.\"}\n    def __init__(\n        self, nested, *, default=missing_, exclude=tuple(), only=None, **kwargs\n    ):\n        # Raise error if only or exclude is passed as string, not list of strings\n        if only is not None and not is_collection(only):\n            raise StringNotCollectionError('\"only\" should be a collection of strings.')\n        if exclude is not None and not is_collection(exclude):\n            raise StringNotCollectionError(\n                '\"exclude\" should be a collection of strings.'\n            )\n        self.nested = nested\n        self.only = only\n        self.exclude = exclude\n        self.many = kwargs.get(\"many\", False)\n        self.unknown = kwargs.get(\"unknown\")\n        self._schema = None  # Cached Schema instance\n        super().__init__(default=default, **kwargs)\n    @property\n    def schema(self):\n        \"\"\"The nested Schema object.\n        .. versionchanged:: 1.0.0\n            Renamed from `serializer` to `schema`.\n        \"\"\"\n        if not self._schema:\n            # Inherit context from parent.\n            context = getattr(self.parent, \"context\", {})\n            if isinstance(self.nested, SchemaABC):\n                self._schema = self.nested\n                self._schema.context.update(context)\n            else:\n                if isinstance(self.nested, type) and issubclass(self.nested, SchemaABC):\n                    schema_class = self.nested\n                elif not isinstance(self.nested, (str, bytes)):\n                    raise ValueError(\n                        \"Nested fields must be passed a \"\n                        \"Schema, not {}.\".format(self.nested.__class__)\n                    )\n                elif self.nested == \"self\":\n                    ret = self\n                    while not isinstance(ret, SchemaABC):\n                        ret = ret.parent\n                    schema_class = ret.__class__\n                else:\n                    schema_class = class_registry.get_class(self.nested)\n                self._schema = schema_class(\n                    many=self.many,\n                    only=self.only,\n                    exclude=self.exclude,\n                    context=context,\n                    load_only=self._nested_normalized_option(\"load_only\"),\n                    dump_only=self._nested_normalized_option(\"dump_only\"),\n                )\n        return self._schema\n    def _nested_normalized_option(self, option_name):\n        nested_field = \"%s.\" % self.name\n        return [\n            field.split(nested_field, 1)[1]\n            for field in getattr(self.root, option_name, set())\n            if field.startswith(nested_field)\n        ]\n    def _serialize(self, nested_obj, attr, obj, many=False, **kwargs):\n        # Load up the schema first. This allows a RegistryError to be raised\n        # if an invalid schema name was passed\n        schema = self.schema\n        if nested_obj is None:\n            return None\n        return schema.dump(nested_obj, many=self.many or many)\n    def _test_collection(self, value, many=False):\n        many = self.many or many\n        if many and not utils.is_collection(value):\n            raise self.make_error(\"type\", input=value, type=value.__class__.__name__)\n    def _load(self, value, data, partial=None, many=False):\n        try:\n            valid_data = self.schema.load(\n                value, unknown=self.unknown, partial=partial, many=self.many or many\n            )\n        except ValidationError as error:\n            raise ValidationError(\n                error.messages, valid_data=error.valid_data\n            ) from error\n        return valid_data\n    def _deserialize(self, value, attr, data, partial=None, many=False, **kwargs):\n        \"\"\"Same as :meth:`Field._deserialize` with additional ``partial`` argument.\n        :param bool|tuple partial: For nested schemas, the ``partial``\n            parameter passed to `Schema.load`.\n        .. versionchanged:: 3.0.0\n            Add ``partial`` parameter.\n        \"\"\"\n        self._test_collection(value, many=many)\n        return self._load(value, data, partial=partial, many=many)\nclass Pluck(Nested):\n    \"\"\"Allows you to replace nested data with one of the data's fields.\n    Example: ::\n        from marshmallow import Schema, fields\n        class ArtistSchema(Schema):\n            id = fields.Int()\n            name = fields.Str()\n        class AlbumSchema(Schema):\n            artist = fields.Pluck(ArtistSchema, 'id')\n        in_data = {'artist': 42}\n        loaded = AlbumSchema().load(in_data) # => {'artist': {'id': 42}}\n        dumped = AlbumSchema().dump(loaded)  # => {'artist': 42}\n    :param Schema nested: The Schema class or class name (string)\n        to nest, or ``\"self\"`` to nest the :class:`Schema` within itself.\n    :param str field_name: The key to pluck a value from.\n    :param kwargs: The same keyword arguments that :class:`Nested` receives.\n    \"\"\"\n    def __init__(self, nested, field_name, **kwargs):\n        super().__init__(nested, only=(field_name,), **kwargs)\n        self.field_name = field_name\n    @property\n    def _field_data_key(self):\n        only_field = self.schema.fields[self.field_name]\n        return only_field.data_key or self.field_name\n    def _serialize(self, nested_obj, attr, obj, **kwargs):",
        "file_path": "src/marshmallow/fields.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 5,
        "score": 0.5834320783615112,
        "content": "\"\"\"The :class:`Schema` class, including its metaclass and options (class Meta).\"\"\"\nfrom collections import defaultdict, OrderedDict\nfrom collections.abc import Mapping\nfrom functools import lru_cache\nimport datetime as dt\nimport uuid\nimport decimal\nimport copy\nimport inspect\nimport json\nimport typing\nimport warnings\nfrom marshmallow import base, fields as ma_fields, class_registry\nfrom marshmallow.error_store import ErrorStore\nfrom marshmallow.exceptions import ValidationError, StringNotCollectionError\nfrom marshmallow.orderedset import OrderedSet\nfrom marshmallow.decorators import (\n    POST_DUMP,\n    POST_LOAD,\n    PRE_DUMP,\n    PRE_LOAD,\n    VALIDATES,\n    VALIDATES_SCHEMA,\n)\nfrom marshmallow.utils import (\n    RAISE,\n    EXCLUDE,\n    INCLUDE,\n    missing,\n    set_value,\n    get_value,\n    is_collection,\n    is_instance_or_subclass,\n    is_iterable_but_not_string,\n)\ndef _get_fields(attrs, field_class, pop=False, ordered=False):\n    \"\"\"Get fields from a class. If ordered=True, fields will sorted by creation index.\n    :param attrs: Mapping of class attributes\n    :param type field_class: Base field class\n    :param bool pop: Remove matching fields\n    \"\"\"\n    fields = [\n        (field_name, field_value)\n        for field_name, field_value in attrs.items()\n        if is_instance_or_subclass(field_value, field_class)\n    ]\n    if pop:\n        for field_name, _ in fields:\n            del attrs[field_name]\n    if ordered:\n        fields.sort(key=lambda pair: pair[1]._creation_index)\n    return fields\n# This function allows Schemas to inherit from non-Schema classes and ensures\n#   inheritance according to the MRO\ndef _get_fields_by_mro(klass, field_class, ordered=False):\n    \"\"\"Collect fields from a class, following its method resolution order. The\n    class itself is excluded from the search; only its parents are checked. Get\n    fields from ``_declared_fields`` if available, else use ``__dict__``.\n    :param type klass: Class whose fields to retrieve\n    :param type field_class: Base field class\n    \"\"\"\n    mro = inspect.getmro(klass)\n    # Loop over mro in reverse to maintain correct order of fields\n    return sum(\n        (\n            _get_fields(\n                getattr(base, \"_declared_fields\", base.__dict__),\n                field_class,\n                ordered=ordered,\n            )\n            for base in mro[:0:-1]\n        ),\n        [],\n    )\nclass SchemaMeta(type):\n    \"\"\"Metaclass for the Schema class. Binds the declared fields to\n    a ``_declared_fields`` attribute, which is a dictionary mapping attribute\n    names to field objects. Also sets the ``opts`` class attribute, which is\n    the Schema class's ``class Meta`` options.\n    \"\"\"\n    def __new__(mcs, name, bases, attrs):\n        meta = attrs.get(\"Meta\")\n        ordered = getattr(meta, \"ordered\", False)\n        if not ordered:\n            # Inherit 'ordered' option\n            # Warning: We loop through bases instead of MRO because we don't\n            # yet have access to the class object\n            # (i.e. can't call super before we have fields)\n            for base_ in bases:\n                if hasattr(base_, \"Meta\") and hasattr(base_.Meta, \"ordered\"):\n                    ordered = base_.Meta.ordered\n                    break\n            else:\n                ordered = False\n        cls_fields = _get_fields(attrs, base.FieldABC, pop=True, ordered=ordered)\n        klass = super().__new__(mcs, name, bases, attrs)\n        inherited_fields = _get_fields_by_mro(klass, base.FieldABC, ordered=ordered)\n        meta = klass.Meta\n        # Set klass.opts in __new__ rather than __init__ so that it is accessible in\n        # get_declared_fields\n        klass.opts = klass.OPTIONS_CLASS(meta, ordered=ordered)\n        # Add fields specified in the `include` class Meta option\n        cls_fields += list(klass.opts.include.items())\n        dict_cls = OrderedDict if ordered else dict\n        # Assign _declared_fields on class\n        klass._declared_fields = mcs.get_declared_fields(\n            klass=klass,\n            cls_fields=cls_fields,\n            inherited_fields=inherited_fields,\n            dict_cls=dict_cls,\n        )\n        return klass\n    @classmethod\n    def get_declared_fields(mcs, klass, cls_fields, inherited_fields, dict_cls):\n        \"\"\"Returns a dictionary of field_name => `Field` pairs declard on the class.\n        This is exposed mainly so that plugins can add additional fields, e.g. fields\n        computed from class Meta options.\n        :param type klass: The class object.\n        :param list cls_fields: The fields declared on the class, including those added\n            by the ``include`` class Meta option.\n        :param list inherited_fields: Inherited fields.\n        :param type dict_class: Either `dict` or `OrderedDict`, depending on the whether\n            the user specified `ordered=True`.\n        \"\"\"\n        return dict_cls(inherited_fields + cls_fields)\n    def __init__(cls, name, bases, attrs):\n        super().__init__(cls, bases, attrs)\n        if name and cls.opts.register:\n            class_registry.register(name, cls)\n        cls._hooks = cls.resolve_hooks()\n    def resolve_hooks(cls):\n        \"\"\"Add in the decorated processors\n        By doing this after constructing the class, we let standard inheritance\n        do all the hard work.\n        \"\"\"\n        mro = inspect.getmro(cls)\n        hooks = defaultdict(list)\n        for attr_name in dir(cls):\n            # Need to look up the actual descriptor, not whatever might be\n            # bound to the class. This needs to come from the __dict__ of the\n            # declaring class.\n            for parent in mro:\n                try:\n                    attr = parent.__dict__[attr_name]\n                except KeyError:\n                    continue\n                else:\n                    break\n            else:\n                # In case we didn't find the attribute and didn't break above.\n                # We should never hit this - it's just here for completeness\n                # to exclude the possibility of attr being undefined.\n                continue\n            try:\n                hook_config = attr.__marshmallow_hook__\n            except AttributeError:\n                pass\n            else:\n                for key in hook_config.keys():\n                    # Use name here so we can get the bound method later, in\n                    # case the processor was a descriptor or something.\n                    hooks[key].append(attr_name)\n        return hooks\nclass SchemaOpts:\n    \"\"\"class Meta options for the :class:`Schema`. Defines defaults.\"\"\"\n    def __init__(self, meta, ordered=False):\n        self.fields = getattr(meta, \"fields\", ())\n        if not isinstance(self.fields, (list, tuple)):\n            raise ValueError(\"`fields` option must be a list or tuple.\")\n        self.additional = getattr(meta, \"additional\", ())\n        if not isinstance(self.additional, (list, tuple)):\n            raise ValueError(\"`additional` option must be a list or tuple.\")\n        if self.fields and self.additional:\n            raise ValueError(\n                \"Cannot set both `fields` and `additional` options\"\n                \" for the same Schema.\"\n            )\n        self.exclude = getattr(meta, \"exclude\", ())\n        if not isinstance(self.exclude, (list, tuple)):\n            raise ValueError(\"`exclude` must be a list or tuple.\")\n        self.dateformat = getattr(meta, \"dateformat\", None)\n        self.datetimeformat = getattr(meta, \"datetimeformat\", None)\n        if hasattr(meta, \"json_module\"):\n            warnings.warn(\n                \"The json_module class Meta option is deprecated. Use render_module instead.\",\n                DeprecationWarning,\n            )\n            render_module = getattr(meta, \"json_module\", json)\n        else:\n            render_module = json\n        self.render_module = getattr(meta, \"render_module\", render_module)\n        self.ordered = getattr(meta, \"ordered\", ordered)\n        self.index_errors = getattr(meta, \"index_errors\", True)\n        self.include = getattr(meta, \"include\", {})\n        self.load_only = getattr(meta, \"load_only\", ())\n        self.dump_only = getattr(meta, \"dump_only\", ())\n        self.unknown = getattr(meta, \"unknown\", RAISE)\n        self.register = getattr(meta, \"register\", True)\nclass BaseSchema(base.SchemaABC):\n    \"\"\"Base schema class with which to define custom schemas.\n    Example usage:\n    .. code-block:: python\n        import datetime as dt\n        from marshmallow import Schema, fields\n        class Album:\n            def __init__(self, title, release_date):\n                self.title = title\n                self.release_date = release_date\n        class AlbumSchema(Schema):\n            title = fields.Str()\n            release_date = fields.Date()\n        # Or, equivalently\n        class AlbumSchema2(Schema):\n            class Meta:\n                fields = (\"title\", \"release_date\")\n        album = Album(\"Beggars Banquet\", dt.date(1968, 12, 6))\n        schema = AlbumSchema()\n        data = schema.dump(album)\n        data  # {'release_date': '1968-12-06', 'title': 'Beggars Banquet'}\n    :param tuple|list only: Whitelist of the declared fields to select when\n        instantiating the Schema. If None, all fields are used. Nested fields\n        can be represented with dot delimiters.\n    :param tuple|list exclude: Blacklist of the declared fields to exclude\n        when instantiating the Schema. If a field appears in both `only` and\n        `exclude`, it is not used. Nested fields can be represented with dot\n        delimiters.\n    :param bool many: Should be set to `True` if ``obj`` is a collection\n        so that the object will be serialized to a list.\n    :param dict context: Optional context passed to :class:`fields.Method` and\n        :class:`fields.Function` fields.\n    :param tuple|list load_only: Fields to skip during serialization (write-only fields)\n    :param tuple|list dump_only: Fields to skip during deserialization (read-only fields)\n    :param bool|tuple partial: Whether to ignore missing fields and not require\n        any fields declared. Propagates down to ``Nested`` fields as well. If\n        its value is an iterable, only missing fields listed in that iterable\n        will be ignored. Use dot delimiters to specify nested fields.\n    :param unknown: Whether to exclude, include, or raise an error for unknown\n        fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`.\n    .. versionchanged:: 3.0.0\n        `prefix` parameter removed.\n    .. versionchanged:: 2.0.0\n        `__validators__`, `__preprocessors__`, and `__data_handlers__` are removed in favor of\n        `marshmallow.decorators.validates_schema`,\n        `marshmallow.decorators.pre_load` and `marshmallow.decorators.post_dump`.\n        `__accessor__` and `__error_handler__` are deprecated. Implement the\n        `handle_error` and `get_attribute` methods instead.\n    \"\"\"\n    TYPE_MAPPING = {\n        str: ma_fields.String,\n        bytes: ma_fields.String,\n        dt.datetime: ma_fields.DateTime,\n        float: ma_fields.Float,\n        bool: ma_fields.Boolean,\n        tuple: ma_fields.Raw,\n        list: ma_fields.Raw,\n        set: ma_fields.Raw,\n        int: ma_fields.Integer,\n        uuid.UUID: ma_fields.UUID,\n        dt.time: ma_fields.Time,\n        dt.date: ma_fields.Date,\n        dt.timedelta: ma_fields.TimeDelta,\n        decimal.Decimal: ma_fields.Decimal,\n    }\n    #: Overrides for default schema-level error messages\n    error_messages = {}\n    _default_error_messages = {\n        \"type\": \"Invalid input type.\",\n        \"unknown\": \"Unknown field.\",\n    }\n    OPTIONS_CLASS = SchemaOpts\n    class Meta:\n        \"\"\"Options object for a Schema.\n        Example usage: ::\n            class Meta:\n                fields = (\"id\", \"email\", \"date_created\")\n                exclude = (\"password\", \"secret_attribute\")\n        Available options:\n        - ``fields``: Tuple or list of fields to include in the serialized result.\n        - ``additional``: Tuple or list of fields to include *in addition* to the\n            explicitly declared fields. ``additional`` and ``fields`` are\n            mutually-exclusive options.\n        - ``include``: Dictionary of additional fields to include in the schema. It is\n            usually better to define fields as class variables, but you may need to\n            use this option, e.g., if your fields are Python keywords. May be an\n            `OrderedDict`.\n        - ``exclude``: Tuple or list of fields to exclude in the serialized result.\n            Nested fields can be represented with dot delimiters.\n        - ``dateformat``: Default format for `Date <fields.Date>` fields.\n        - ``datetimeformat``: Default format for `DateTime <fields.DateTime>` fields.\n        - ``render_module``: Module to use for `loads <Schema.loads>` and `dumps <Schema.dumps>`.\n            Defaults to `json` from the standard library.\n        - ``ordered``: If `True`, order serialization output according to the\n            order in which fields were declared. Output of `Schema.dump` will be a\n            `collections.OrderedDict`.\n        - ``index_errors``: If `True`, errors dictionaries will include the index\n            of invalid items in a collection.\n        - ``load_only``: Tuple or list of fields to exclude from serialized results.\n        - ``dump_only``: Tuple or list of fields to exclude from deserialization\n        - ``unknown``: Whether to exclude, include, or raise an error for unknown\n            fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`.\n        - ``register``: Whether to register the `Schema` with marshmallow's internal\n            class registry. Must be `True` if you intend to refer to this `Schema`\n            by class name in `Nested` fields. Only set this to `False` when memory\n            usage is critical. Defaults to `True`.\n        \"\"\"\n        pass\n    def __init__(\n        self,\n        *,\n        only=None,\n        exclude=(),\n        many=False,\n        context=None,\n        load_only=(),\n        dump_only=(),\n        partial=False,\n        unknown=None\n    ):\n        # Raise error if only or exclude is passed as string, not list of strings\n        if only is not None and not is_collection(only):\n            raise StringNotCollectionError('\"only\" should be a list of strings')\n        if exclude is not None and not is_collection(exclude):\n            raise StringNotCollectionError('\"exclude\" should be a list of strings')\n        # copy declared fields from metaclass\n        self.declared_fields = copy.deepcopy(self._declared_fields)\n        self.many = many\n        self.only = only\n        self.exclude = set(self.opts.exclude) | set(exclude)\n        self.ordered = self.opts.ordered\n        self.load_only = set(load_only) or set(self.opts.load_only)\n        self.dump_only = set(dump_only) or set(self.opts.dump_only)\n        self.partial = partial\n        self.unknown = unknown or self.opts.unknown\n        self.context = context or {}\n        self._normalize_nested_options()\n        #: Dictionary mapping field_names -> :class:`Field` objects\n        self.fields = self._init_fields()\n        self.dump_fields, self.load_fields = self.dict_class(), self.dict_class()\n        for field_name, field_obj in self.fields.items():\n            if field_obj.load_only:\n                self.load_fields[field_name] = field_obj\n            elif field_obj.dump_only:\n                self.dump_fields[field_name] = field_obj\n            else:\n                self.load_fields[field_name] = field_obj\n                self.dump_fields[field_name] = field_obj\n        messages = {}\n        messages.update(self._default_error_messages)\n        for cls in reversed(self.__class__.__mro__):\n            messages.update(getattr(cls, \"error_messages\", {}))\n        messages.update(self.error_messages or {})\n        self.error_messages = messages\n    def __repr__(self):\n        return \"<{ClassName}(many={self.many})>\".format(\n            ClassName=self.__class__.__name__, self=self\n        )\n    @property\n    def dict_class(self):\n        return OrderedDict if self.ordered else dict\n    @property\n    def set_class(self):\n        return OrderedSet if self.ordered else set\n    @classmethod\n    def from_dict(\n        cls, fields: typing.Dict[str, ma_fields.Field], *, name: str = \"GeneratedSchema\"\n    ) -> typing.Type[\"Schema\"]:\n        \"\"\"Generate a `Schema` class given a dictionary of fields.\n        .. code-block:: python\n            from marshmallow import Schema, fields\n            PersonSchema = Schema.from_dict({\"name\": fields.Str()})\n            print(PersonSchema().load({\"name\": \"David\"}))  # => {'name': 'David'}\n        Generated schemas are not added to the class registry and therefore cannot\n        be referred to by name in `Nested` fields.\n        :param dict fields: Dictionary mapping field names to field instances.\n        :param str name: Optional name for the class, which will appear in\n            the ``repr`` for the class.\n        .. versionadded:: 3.0.0\n        \"\"\"\n        attrs = fields.copy()\n        attrs[\"Meta\"] = type(\n            \"GeneratedMeta\", (getattr(cls, \"Meta\", object),), {\"register\": False}\n        )\n        schema_cls = type(name, (cls,), attrs)\n        return schema_cls\n    ##### Override-able methods #####\n    def handle_error(self, error, data, *, many, **kwargs):\n        \"\"\"Custom error handler function for the schema.\n        :param ValidationError error: The `ValidationError` raised during (de)serialization.\n        :param data: The original input data.\n        :param bool many: Value of ``many`` on dump or load.\n        :param bool partial: Value of ``partial`` on load.\n        .. versionadded:: 2.0.0\n        .. versionchanged:: 3.0.0rc9\n            Receives `many` and `partial` (on deserialization) as keyword arguments.\n        \"\"\"\n        pass\n    def get_attribute(self, obj, attr, default):\n        \"\"\"Defines how to pull values from an object to serialize.\n        .. versionadded:: 2.0.0\n        .. versionchanged:: 3.0.0a1\n            Changed position of ``obj`` and ``attr``.\n        \"\"\"\n        return get_value(obj, attr, default)\n    ##### Serialization/Deserialization API #####\n    @staticmethod\n    def _call_and_store(getter_func, data, *, field_name, error_store, index=None):\n        \"\"\"Call ``getter_func`` with ``data`` as its argument, and store any `ValidationErrors`.\n        :param callable getter_func: Function for getting the serialized/deserialized\n            value from ``data``.\n        :param data: The data passed to ``getter_func``.\n        :param str field_name: Field name.\n        :param int index: Index of the item being validated, if validating a collection,\n            otherwise `None`.\n        \"\"\"\n        try:\n            value = getter_func(data)\n        except ValidationError as error:\n            error_store.store_error(error.messages, field_name, index=index)\n            # When a Nested field fails validation, the marshalled data is stored\n            # on the ValidationError's valid_data attribute\n            return error.valid_data or missing\n        return value\n    def _serialize(self, obj, *, many=False):\n        \"\"\"Serialize ``obj``.\n        :param obj: The object(s) to serialize.\n        :param bool many: `True` if ``data`` should be serialized as a collection.\n        :return: A dictionary of the serialized data\n        .. versionchanged:: 1.0.0\n            Renamed from ``marshal``.\n        \"\"\"\n        if many and obj is not None:\n            return [self._serialize(d, many=False) for d in obj]\n        ret = self.dict_class()\n        for attr_name, field_obj in self.dump_fields.items():\n            value = field_obj.serialize(attr_name, obj, accessor=self.get_attribute)\n            if value is missing:\n                continue\n            key = field_obj.data_key or attr_name\n            ret[key] = value\n        return ret\n    def dump(self, obj, *, many=None):\n        \"\"\"Serialize an object to native Python data types according to this\n        Schema's fields.\n        :param obj: The object to serialize.\n        :param bool many: Whether to serialize `obj` as a collection. If `None`, the value\n            for `self.many` is used.\n        :return: A dict of serialized data\n        :rtype: dict\n        .. versionadded:: 1.0.0\n        .. versionchanged:: 3.0.0b7\n            This method returns the serialized data rather than a ``(data, errors)`` duple.\n            A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised\n            if ``obj`` is invalid.\n        .. versionchanged:: 3.0.0rc9\n            Validation no longer occurs upon serialization.\n        \"\"\"\n        many = self.many if many is None else bool(many)\n        if many and is_iterable_but_not_string(obj):\n            obj = list(obj)\n        if self._has_processors(PRE_DUMP):\n            processed_obj = self._invoke_dump_processors(\n                PRE_DUMP, obj, many=many, original_data=obj\n            )\n        else:\n            processed_obj = obj\n        result = self._serialize(processed_obj, many=many)\n        if self._has_processors(POST_DUMP):\n            result = self._invoke_dump_processors(\n                POST_DUMP, result, many=many, original_data=obj\n            )\n        return result\n    def dumps(self, obj, *args, many=None, **kwargs):\n        \"\"\"Same as :meth:`dump`, except return a JSON-encoded string.\n        :param obj: The object to serialize.\n        :param bool many: Whether to serialize `obj` as a collection. If `None`, the value\n            for `self.many` is used.\n        :return: A ``json`` string\n        :rtype: str\n        .. versionadded:: 1.0.0\n        .. versionchanged:: 3.0.0b7\n            This method returns the serialized data rather than a ``(data, errors)`` duple.\n            A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised\n            if ``obj`` is invalid.\n        \"\"\"\n        serialized = self.dump(obj, many=many)\n        return self.opts.render_module.dumps(serialized, *args, **kwargs)\n    def _deserialize(\n        self, data, *, error_store, many=False, partial=False, unknown=RAISE, index=None\n    ):\n        \"\"\"Deserialize ``data``.\n        :param dict data: The data to deserialize.\n        :param ErrorStore error_store: Structure to store errors.\n        :param bool many: `True` if ``data`` should be deserialized as a collection.\n        :param bool|tuple partial: Whether to ignore missing fields and not require\n            any fields declared. Propagates down to ``Nested`` fields as well. If\n            its value is an iterable, only missing fields listed in that iterable\n            will be ignored. Use dot delimiters to specify nested fields.\n        :param unknown: Whether to exclude, include, or raise an error for unknown\n            fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`.\n        :param int index: Index of the item being serialized (for storing errors) if",
        "file_path": "src/marshmallow/schema.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 6,
        "score": 0.5771660208702087,
        "content": "\"\"\"Utility methods for marshmallow.\"\"\"\nimport collections\nimport functools\nimport datetime as dt\nimport inspect\nimport json\nimport re\nimport typing\nfrom collections.abc import Mapping\nfrom email.utils import format_datetime, parsedate_to_datetime\nfrom pprint import pprint as py_pprint\nfrom marshmallow.base import FieldABC\nfrom marshmallow.exceptions import FieldInstanceResolutionError\nEXCLUDE = \"exclude\"\nINCLUDE = \"include\"\nRAISE = \"raise\"\nclass _Missing:\n    def __bool__(self):\n        return False\n    def __copy__(self):\n        return self\n    def __deepcopy__(self, _):\n        return self\n    def __repr__(self):\n        return \"<marshmallow.missing>\"\n# Singleton value that indicates that a field's value is missing from input\n# dict passed to :meth:`Schema.load`. If the field's value is not required,\n# it's ``default`` value is used.\nmissing = _Missing()\ndef is_generator(obj):\n    \"\"\"Return True if ``obj`` is a generator\n    \"\"\"\n    return inspect.isgeneratorfunction(obj) or inspect.isgenerator(obj)\ndef is_iterable_but_not_string(obj):\n    \"\"\"Return True if ``obj`` is an iterable object that isn't a string.\"\"\"\n    return (hasattr(obj, \"__iter__\") and not hasattr(obj, \"strip\")) or is_generator(obj)\ndef is_collection(obj):\n    \"\"\"Return True if ``obj`` is a collection type, e.g list, tuple, queryset.\"\"\"\n    return is_iterable_but_not_string(obj) and not isinstance(obj, Mapping)\ndef is_instance_or_subclass(val, class_):\n    \"\"\"Return True if ``val`` is either a subclass or instance of ``class_``.\"\"\"\n    try:\n        return issubclass(val, class_)\n    except TypeError:\n        return isinstance(val, class_)\ndef is_keyed_tuple(obj):\n    \"\"\"Return True if ``obj`` has keyed tuple behavior, such as\n    namedtuples or SQLAlchemy's KeyedTuples.\n    \"\"\"\n    return isinstance(obj, tuple) and hasattr(obj, \"_fields\")\ndef pprint(obj, *args, **kwargs):\n    \"\"\"Pretty-printing function that can pretty-print OrderedDicts\n    like regular dictionaries. Useful for printing the output of\n    :meth:`marshmallow.Schema.dump`.\n    \"\"\"\n    if isinstance(obj, collections.OrderedDict):\n        print(json.dumps(obj, *args, **kwargs))\n    else:\n        py_pprint(obj, *args, **kwargs)\n# https://stackoverflow.com/a/27596917\ndef is_aware(datetime):\n    return (\n        datetime.tzinfo is not None and datetime.tzinfo.utcoffset(datetime) is not None\n    )\ndef from_rfc(datestring):\n    \"\"\"Parse a RFC822-formatted datetime string and return a datetime object.\n    https://stackoverflow.com/questions/885015/how-to-parse-a-rfc-2822-date-time-into-a-python-datetime  # noqa: B950\n    \"\"\"\n    return parsedate_to_datetime(datestring)\ndef rfcformat(datetime):\n    \"\"\"Return the RFC822-formatted representation of a datetime object.\n    :param datetime datetime: The datetime.\n    \"\"\"\n    return format_datetime(datetime)\n# Hat tip to Django for ISO8601 deserialization functions\n_iso8601_datetime_re = re.compile(\n    r\"(?P<year>\\d{4})-(?P<month>\\d{1,2})-(?P<day>\\d{1,2})\"\n    r\"[T ](?P<hour>\\d{1,2}):(?P<minute>\\d{1,2})\"\n    r\"(?::(?P<second>\\d{1,2})(?:\\.(?P<microsecond>\\d{1,6})\\d{0,6})?)?\"\n    r\"(?P<tzinfo>Z|[+-]\\d{2}(?::?\\d{2})?)?$\"\n)\n_iso8601_date_re = re.compile(r\"(?P<year>\\d{4})-(?P<month>\\d{1,2})-(?P<day>\\d{1,2})$\")\n_iso8601_time_re = re.compile(\n    r\"(?P<hour>\\d{1,2}):(?P<minute>\\d{1,2})\"\n    r\"(?::(?P<second>\\d{1,2})(?:\\.(?P<microsecond>\\d{1,6})\\d{0,6})?)?\"\n)\ndef get_fixed_timezone(offset):\n    \"\"\"Return a tzinfo instance with a fixed offset from UTC.\"\"\"\n    if isinstance(offset, dt.timedelta):\n        offset = offset.total_seconds() // 60\n    sign = \"-\" if offset < 0 else \"+\"\n    hhmm = \"%02d%02d\" % divmod(abs(offset), 60)\n    name = sign + hhmm\n    return dt.timezone(dt.timedelta(minutes=offset), name)\ndef from_iso_datetime(value):\n    \"\"\"Parse a string and return a datetime.datetime.\n    This function supports time zone offsets. When the input contains one,\n    the output uses a timezone with a fixed offset from UTC.\n    \"\"\"\n    match = _iso8601_datetime_re.match(value)\n    if not match:\n        raise ValueError(\"Not a valid ISO8601-formatted datetime string\")\n    kw = match.groupdict()\n    kw[\"microsecond\"] = kw[\"microsecond\"] and kw[\"microsecond\"].ljust(6, \"0\")\n    tzinfo = kw.pop(\"tzinfo\")\n    if tzinfo == \"Z\":\n        tzinfo = dt.timezone.utc\n    elif tzinfo is not None:\n        offset_mins = int(tzinfo[-2:]) if len(tzinfo) > 3 else 0\n        offset = 60 * int(tzinfo[1:3]) + offset_mins\n        if tzinfo[0] == \"-\":\n            offset = -offset\n        tzinfo = get_fixed_timezone(offset)\n    kw = {k: int(v) for k, v in kw.items() if v is not None}\n    kw[\"tzinfo\"] = tzinfo\n    return dt.datetime(**kw)\ndef from_iso_time(value):\n    \"\"\"Parse a string and return a datetime.time.\n    This function doesn't support time zone offsets.\n    \"\"\"\n    match = _iso8601_time_re.match(value)\n    if not match:\n        raise ValueError(\"Not a valid ISO8601-formatted time string\")\n    kw = match.groupdict()\n    kw[\"microsecond\"] = kw[\"microsecond\"] and kw[\"microsecond\"].ljust(6, \"0\")\n    kw = {k: int(v) for k, v in kw.items() if v is not None}\n    return dt.time(**kw)\ndef from_iso_date(value):\n    \"\"\"Parse a string and return a datetime.date.\"\"\"\n    match = _iso8601_date_re.match(value)\n    if not match:\n        raise ValueError(\"Not a valid ISO8601-formatted date string\")\n    kw = {k: int(v) for k, v in match.groupdict().items()}\n    return dt.date(**kw)\ndef isoformat(datetime):\n    \"\"\"Return the ISO8601-formatted representation of a datetime object.\n    :param datetime datetime: The datetime.\n    \"\"\"\n    return datetime.isoformat()\ndef to_iso_date(date):\n    return dt.date.isoformat(date)\ndef ensure_text_type(val):\n    if isinstance(val, bytes):\n        val = val.decode(\"utf-8\")\n    return str(val)\ndef pluck(dictlist, key):\n    \"\"\"Extracts a list of dictionary values from a list of dictionaries.\n    ::\n        >>> dlist = [{'id': 1, 'name': 'foo'}, {'id': 2, 'name': 'bar'}]\n        >>> pluck(dlist, 'id')\n        [1, 2]\n    \"\"\"\n    return [d[key] for d in dictlist]\n# Various utilities for pulling keyed values from objects\ndef get_value(obj, key, default=missing):\n    \"\"\"Helper for pulling a keyed value off various types of objects. Fields use\n    this method by default to access attributes of the source object. For object `x`\n    and attribute `i`, this method first tries to access `x[i]`, and then falls back to\n    `x.i` if an exception is raised.\n    .. warning::\n        If an object `x` does not raise an exception when `x[i]` does not exist,\n        `get_value` will never check the value `x.i`. Consider overriding\n        `marshmallow.fields.Field.get_value` in this case.\n    \"\"\"\n    if not isinstance(key, int) and \".\" in key:\n        return _get_value_for_keys(obj, key.split(\".\"), default)\n    else:\n        return _get_value_for_key(obj, key, default)\ndef _get_value_for_keys(obj, keys, default):\n    if len(keys) == 1:\n        return _get_value_for_key(obj, keys[0], default)\n    else:\n        return _get_value_for_keys(\n            _get_value_for_key(obj, keys[0], default), keys[1:], default\n        )\ndef _get_value_for_key(obj, key, default):\n    if not hasattr(obj, \"__getitem__\"):\n        return getattr(obj, key, default)\n    try:\n        return obj[key]\n    except (KeyError, IndexError, TypeError, AttributeError):\n        return getattr(obj, key, default)\ndef set_value(dct, key, value):\n    \"\"\"Set a value in a dict. If `key` contains a '.', it is assumed\n    be a path (i.e. dot-delimited string) to the value's location.\n    ::\n        >>> d = {}\n        >>> set_value(d, 'foo.bar', 42)\n        >>> d\n        {'foo': {'bar': 42}}\n    \"\"\"\n    if \".\" in key:\n        head, rest = key.split(\".\", 1)\n        target = dct.setdefault(head, {})\n        if not isinstance(target, dict):\n            raise ValueError(\n                \"Cannot set {key} in {head} \"\n                \"due to existing value: {target}\".format(\n                    key=key, head=head, target=target\n                )\n            )\n        set_value(target, rest, value)\n    else:\n        dct[key] = value\ndef callable_or_raise(obj):\n    \"\"\"Check that an object is callable, else raise a :exc:`ValueError`.\n    \"\"\"\n    if not callable(obj):\n        raise ValueError(\"Object {!r} is not callable.\".format(obj))\n    return obj\ndef _signature(func: typing.Callable) -> typing.List[str]:\n    return list(inspect.signature(func).parameters.keys())\ndef get_func_args(func: typing.Callable) -> typing.List[str]:\n    \"\"\"Given a callable, return a list of argument names. Handles\n    `functools.partial` objects and class-based callables.\n    .. versionchanged:: 3.0.0a1\n        Do not return bound arguments, eg. ``self``.\n    \"\"\"\n    if inspect.isfunction(func) or inspect.ismethod(func):\n        return _signature(func)\n    if isinstance(func, functools.partial):\n        return _signature(func.func)\n    # Callable class\n    return _signature(func.__call__)\ndef resolve_field_instance(cls_or_instance):\n    \"\"\"Return a Schema instance from a Schema class or instance.\n    :param type|Schema cls_or_instance: Marshmallow Schema class or instance.\n    \"\"\"\n    if isinstance(cls_or_instance, type):\n        if not issubclass(cls_or_instance, FieldABC):\n            raise FieldInstanceResolutionError\n        return cls_or_instance()\n    else:\n        if not isinstance(cls_or_instance, FieldABC):\n            raise FieldInstanceResolutionError\n        return cls_or_instance",
        "file_path": "src/marshmallow/utils.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 7,
        "score": 0.5605058670043945,
        "content": "from marshmallow.schema import Schema, SchemaOpts\nfrom . import fields\nfrom marshmallow.decorators import (\n    pre_dump,\n    post_dump,\n    pre_load,\n    post_load,\n    validates,\n    validates_schema,\n)\nfrom marshmallow.utils import EXCLUDE, INCLUDE, RAISE, pprint, missing\nfrom marshmallow.exceptions import ValidationError\nfrom distutils.version import LooseVersion\n__version__ = \"3.0.0\"\n__version_info__ = tuple(LooseVersion(__version__).version)\n__all__ = [\n    \"EXCLUDE\",\n    \"INCLUDE\",\n    \"RAISE\",\n    \"Schema\",\n    \"SchemaOpts\",\n    \"fields\",\n    \"validates\",\n    \"validates_schema\",\n    \"pre_dump\",\n    \"post_dump\",\n    \"pre_load\",\n    \"post_load\",\n    \"pprint\",\n    \"ValidationError\",\n    \"missing\",\n]",
        "file_path": "src/marshmallow/__init__.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 8,
        "score": 0.5453499555587769,
        "content": "            invalid_fields |= self.exclude - available_field_names\n        if invalid_fields:\n            message = \"Invalid fields for {}: {}.\".format(self, invalid_fields)\n            raise ValueError(message)\n        fields_dict = self.dict_class()\n        for field_name in field_names:\n            field_obj = self.declared_fields.get(field_name, ma_fields.Inferred())\n            self._bind_field(field_name, field_obj)\n            fields_dict[field_name] = field_obj\n        dump_data_keys = [\n            obj.data_key or name\n            for name, obj in fields_dict.items()\n            if not obj.load_only\n        ]\n        if len(dump_data_keys) != len(set(dump_data_keys)):\n            data_keys_duplicates = {\n                x for x in dump_data_keys if dump_data_keys.count(x) > 1\n            }\n            raise ValueError(\n                \"The data_key argument for one or more fields collides \"\n                \"with another field's name or data_key argument. \"\n                \"Check the following field names and \"\n                \"data_key arguments: {}\".format(list(data_keys_duplicates))\n            )\n        load_attributes = [\n            obj.attribute or name\n            for name, obj in fields_dict.items()\n            if not obj.dump_only\n        ]\n        if len(load_attributes) != len(set(load_attributes)):\n            attributes_duplicates = {\n                x for x in load_attributes if load_attributes.count(x) > 1\n            }\n            raise ValueError(\n                \"The attribute argument for one or more fields collides \"\n                \"with another field's name or attribute argument. \"\n                \"Check the following field names and \"\n                \"attribute arguments: {}\".format(list(attributes_duplicates))\n            )\n        return fields_dict\n    def on_bind_field(self, field_name, field_obj):\n        \"\"\"Hook to modify a field when it is bound to the `Schema`.\n        No-op by default.\n        \"\"\"\n        return None\n    def _bind_field(self, field_name, field_obj):\n        \"\"\"Bind field to the schema, setting any necessary attributes on the\n        field (e.g. parent and name).\n        Also set field load_only and dump_only values if field_name was\n        specified in ``class Meta``.\n        \"\"\"\n        try:\n            if field_name in self.load_only:\n                field_obj.load_only = True\n            if field_name in self.dump_only:\n                field_obj.dump_only = True\n            field_obj._bind_to_schema(field_name, self)\n            self.on_bind_field(field_name, field_obj)\n        except TypeError as error:\n            # field declared as a class, not an instance\n            if isinstance(field_obj, type) and issubclass(field_obj, base.FieldABC):\n                msg = (\n                    'Field for \"{}\" must be declared as a '\n                    \"Field instance, not a class. \"\n                    'Did you mean \"fields.{}()\"?'.format(field_name, field_obj.__name__)\n                )\n                raise TypeError(msg) from error\n    @lru_cache(maxsize=8)\n    def _has_processors(self, tag):\n        return self._hooks[(tag, True)] or self._hooks[(tag, False)]\n    def _invoke_dump_processors(self, tag, data, *, many, original_data=None):\n        # The pass_many post-dump processors may do things like add an envelope, so\n        # invoke those after invoking the non-pass_many processors which will expect\n        # to get a list of items.\n        data = self._invoke_processors(\n            tag, pass_many=False, data=data, many=many, original_data=original_data\n        )\n        data = self._invoke_processors(\n            tag, pass_many=True, data=data, many=many, original_data=original_data\n        )\n        return data\n    def _invoke_load_processors(self, tag, data, *, many, original_data, partial):\n        # This has to invert the order of the dump processors, so run the pass_many\n        # processors first.\n        data = self._invoke_processors(\n            tag,\n            pass_many=True,\n            data=data,\n            many=many,\n            original_data=original_data,\n            partial=partial,\n        )\n        data = self._invoke_processors(\n            tag,\n            pass_many=False,\n            data=data,\n            many=many,\n            original_data=original_data,\n            partial=partial,\n        )\n        return data\n    def _invoke_field_validators(self, *, error_store, data, many):\n        for attr_name in self._hooks[VALIDATES]:\n            validator = getattr(self, attr_name)\n            validator_kwargs = validator.__marshmallow_hook__[VALIDATES]\n            field_name = validator_kwargs[\"field_name\"]\n            try:\n                field_obj = self.fields[field_name]\n            except KeyError as error:\n                if field_name in self.declared_fields:\n                    continue\n                raise ValueError(\n                    '\"{}\" field does not exist.'.format(field_name)\n                ) from error\n            if many:\n                for idx, item in enumerate(data):\n                    try:\n                        value = item[field_obj.attribute or field_name]\n                    except KeyError:\n                        pass\n                    else:\n                        validated_value = self._call_and_store(\n                            getter_func=validator,\n                            data=value,\n                            field_name=field_obj.data_key or field_name,\n                            error_store=error_store,\n                            index=(idx if self.opts.index_errors else None),\n                        )\n                        if validated_value is missing:\n                            data[idx].pop(field_name, None)\n            else:\n                try:\n                    value = data[field_obj.attribute or field_name]\n                except KeyError:\n                    pass\n                else:\n                    validated_value = self._call_and_store(\n                        getter_func=validator,\n                        data=value,\n                        field_name=field_obj.data_key or field_name,\n                        error_store=error_store,\n                    )\n                    if validated_value is missing:\n                        data.pop(field_name, None)\n    def _invoke_schema_validators(\n        self,\n        *,\n        error_store,\n        pass_many,\n        data,\n        original_data,\n        many,\n        partial,\n        field_errors=False\n    ):\n        for attr_name in self._hooks[(VALIDATES_SCHEMA, pass_many)]:\n            validator = getattr(self, attr_name)\n            validator_kwargs = validator.__marshmallow_hook__[\n                (VALIDATES_SCHEMA, pass_many)\n            ]\n            if field_errors and validator_kwargs[\"skip_on_field_errors\"]:\n                continue\n            pass_original = validator_kwargs.get(\"pass_original\", False)\n            if many and not pass_many:\n                for idx, (item, orig) in enumerate(zip(data, original_data)):\n                    self._run_validator(\n                        validator,\n                        item,\n                        original_data=orig,\n                        error_store=error_store,\n                        many=many,\n                        partial=partial,\n                        index=idx,\n                        pass_original=pass_original,\n                    )\n            else:\n                self._run_validator(\n                    validator,\n                    data,\n                    original_data=original_data,\n                    error_store=error_store,\n                    many=many,\n                    pass_original=pass_original,\n                    partial=partial,\n                )\n    def _invoke_processors(\n        self, tag, *, pass_many, data, many, original_data=None, **kwargs\n    ):\n        key = (tag, pass_many)\n        for attr_name in self._hooks[key]:\n            # This will be a bound method.\n            processor = getattr(self, attr_name)\n            processor_kwargs = processor.__marshmallow_hook__[key]\n            pass_original = processor_kwargs.get(\"pass_original\", False)\n            if pass_many:\n                if pass_original:\n                    data = processor(data, original_data, many=many, **kwargs)\n                else:\n                    data = processor(data, many=many, **kwargs)\n            elif many:\n                if pass_original:\n                    data = [\n                        processor(item, original, many=many, **kwargs)\n                        for item, original in zip(data, original_data)\n                    ]\n                else:\n                    data = [processor(item, many=many, **kwargs) for item in data]\n            else:\n                if pass_original:\n                    data = processor(data, original_data, many=many, **kwargs)\n                else:\n                    data = processor(data, many=many, **kwargs)\n        return data\nclass Schema(BaseSchema, metaclass=SchemaMeta):\n    __doc__ = BaseSchema.__doc__",
        "file_path": "src/marshmallow/schema.py",
        "chunk_index": 2,
        "metadata": {}
      },
      {
        "rank": 9,
        "score": 0.5341177582740784,
        "content": "            for k, v in value.items():\n                if k in keys:\n                    result[keys[k]] = v\n        else:\n            for key, val in value.items():\n                try:\n                    deser_val = self.value_field.deserialize(val, **kwargs)\n                except ValidationError as error:\n                    errors[key][\"value\"] = error.messages\n                    if error.valid_data is not None and key in keys:\n                        result[keys[key]] = error.valid_data\n                else:\n                    if key in keys:\n                        result[keys[key]] = deser_val\n        if errors:\n            raise ValidationError(errors, valid_data=result)\n        return result\nclass Dict(Mapping):\n    \"\"\"A dict field. Supports dicts and dict-like objects. Extends\n    Mapping with dict as the mapping_type.\n    Example: ::\n        numbers = fields.Dict(keys=fields.Str(), values=fields.Float())\n    :param kwargs: The same keyword arguments that :class:`Mapping` receives.\n    .. versionadded:: 2.1.0\n    \"\"\"\n    mapping_type = dict\nclass Url(String):\n    \"\"\"A validated URL field. Validation occurs during both serialization and\n    deserialization.\n    :param default: Default value for the field if the attribute is not set.\n    :param str attribute: The name of the attribute to get the value from. If\n        `None`, assumes the attribute has the same name as the field.\n    :param bool relative: Whether to allow relative URLs.\n    :param bool require_tld: Whether to reject non-FQDN hostnames.\n    :param kwargs: The same keyword arguments that :class:`String` receives.\n    \"\"\"\n    default_error_messages = {\"invalid\": \"Not a valid URL.\"}\n    def __init__(self, *, relative=False, schemes=None, require_tld=True, **kwargs):\n        super().__init__(**kwargs)\n        self.relative = relative\n        self.require_tld = require_tld\n        # Insert validation into self.validators so that multiple errors can be\n        # stored.\n        self.validators.insert(\n            0,\n            validate.URL(\n                relative=self.relative,\n                schemes=schemes,\n                require_tld=self.require_tld,\n                error=self.error_messages[\"invalid\"],\n            ),\n        )\nclass Email(String):\n    \"\"\"A validated email field. Validation occurs during both serialization and\n    deserialization.\n    :param args: The same positional arguments that :class:`String` receives.\n    :param kwargs: The same keyword arguments that :class:`String` receives.\n    \"\"\"\n    default_error_messages = {\"invalid\": \"Not a valid email address.\"}\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        # Insert validation into self.validators so that multiple errors can be\n        # stored.\n        self.validators.insert(0, validate.Email(error=self.error_messages[\"invalid\"]))\nclass Method(Field):\n    \"\"\"A field that takes the value returned by a `Schema` method.\n    :param str serialize: The name of the Schema method from which\n        to retrieve the value. The method must take an argument ``obj``\n        (in addition to self) that is the object to be serialized.\n    :param str deserialize: Optional name of the Schema method for deserializing\n        a value The method must take a single argument ``value``, which is the\n        value to deserialize.\n    .. versionchanged:: 2.0.0\n        Removed optional ``context`` parameter on methods. Use ``self.context`` instead.\n    .. versionchanged:: 2.3.0\n        Deprecated ``method_name`` parameter in favor of ``serialize`` and allow\n        ``serialize`` to not be passed at all.\n    .. versionchanged:: 3.0.0\n        Removed ``method_name`` parameter.\n    \"\"\"\n    _CHECK_ATTRIBUTE = False\n    def __init__(self, serialize=None, deserialize=None, **kwargs):\n        # Set dump_only and load_only based on arguments\n        kwargs[\"dump_only\"] = bool(serialize) and not bool(deserialize)\n        kwargs[\"load_only\"] = bool(deserialize) and not bool(serialize)\n        super().__init__(**kwargs)\n        self.serialize_method_name = serialize\n        self.deserialize_method_name = deserialize\n    def _serialize(self, value, attr, obj, **kwargs):\n        if not self.serialize_method_name:\n            return missing_\n        method = utils.callable_or_raise(\n            getattr(self.parent, self.serialize_method_name, None)\n        )\n        return method(obj)\n    def _deserialize(self, value, attr, data, **kwargs):\n        if self.deserialize_method_name:\n            method = utils.callable_or_raise(\n                getattr(self.parent, self.deserialize_method_name, None)\n            )\n            return method(value)\n        return value\nclass Function(Field):\n    \"\"\"A field that takes the value returned by a function.\n    :param callable serialize: A callable from which to retrieve the value.\n        The function must take a single argument ``obj`` which is the object\n        to be serialized. It can also optionally take a ``context`` argument,\n        which is a dictionary of context variables passed to the serializer.\n        If no callable is provided then the ```load_only``` flag will be set\n        to True.\n    :param callable deserialize: A callable from which to retrieve the value.\n        The function must take a single argument ``value`` which is the value\n        to be deserialized. It can also optionally take a ``context`` argument,\n        which is a dictionary of context variables passed to the deserializer.\n        If no callable is provided then ```value``` will be passed through\n        unchanged.\n    .. versionchanged:: 2.3.0\n        Deprecated ``func`` parameter in favor of ``serialize``.\n    .. versionchanged:: 3.0.0a1\n        Removed ``func`` parameter.\n    \"\"\"\n    _CHECK_ATTRIBUTE = False\n    def __init__(self, serialize=None, deserialize=None, **kwargs):\n        # Set dump_only and load_only based on arguments\n        kwargs[\"dump_only\"] = bool(serialize) and not bool(deserialize)\n        kwargs[\"load_only\"] = bool(deserialize) and not bool(serialize)\n        super().__init__(**kwargs)\n        self.serialize_func = serialize and utils.callable_or_raise(serialize)\n        self.deserialize_func = deserialize and utils.callable_or_raise(deserialize)\n    def _serialize(self, value, attr, obj, **kwargs):\n        return self._call_or_raise(self.serialize_func, obj, attr)\n    def _deserialize(self, value, attr, data, **kwargs):\n        if self.deserialize_func:\n            return self._call_or_raise(self.deserialize_func, value, attr)\n        return value\n    def _call_or_raise(self, func, value, attr):\n        if len(utils.get_func_args(func)) > 1:\n            if self.parent.context is None:\n                msg = \"No context available for Function field {!r}\".format(attr)\n                raise ValidationError(msg)\n            return func(value, self.parent.context)\n        else:\n            return func(value)\nclass Constant(Field):\n    \"\"\"A field that (de)serializes to a preset constant.  If you only want the\n    constant added for serialization or deserialization, you should use\n    ``dump_only=True`` or ``load_only=True`` respectively.\n    :param constant: The constant to return for the field attribute.\n    .. versionadded:: 2.0.0\n    \"\"\"\n    _CHECK_ATTRIBUTE = False\n    def __init__(self, constant, **kwargs):\n        super().__init__(**kwargs)\n        self.constant = constant\n        self.missing = constant\n        self.default = constant\n    def _serialize(self, value, *args, **kwargs):\n        return self.constant\n    def _deserialize(self, value, *args, **kwargs):\n        return self.constant\nclass Inferred(Field):\n    \"\"\"A field that infers how to serialize, based on the value type.\n    .. warning::\n        This class is treated as private API.\n        Users should not need to use this class directly.\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        # We memoize the fields to avoid creating and binding new fields\n        # every time on serialization.\n        self._field_cache = {}\n    def _serialize(self, value, attr, obj, **kwargs):\n        field_cls = self.root.TYPE_MAPPING.get(type(value))\n        if field_cls is None:\n            field = super()\n        else:\n            field = self._field_cache.get(field_cls)\n            if field is None:\n                field = field_cls()\n                field._bind_to_schema(self.name, self.parent)\n                self._field_cache[field_cls] = field\n        return field._serialize(value, attr, obj, **kwargs)\n# Aliases\nURL = Url\nStr = String\nBool = Boolean\nInt = Integer",
        "file_path": "src/marshmallow/fields.py",
        "chunk_index": 3,
        "metadata": {}
      },
      {
        "rank": 10,
        "score": 0.5191068053245544,
        "content": "            Changed position of ``obj`` and ``attr``.\n        \"\"\"\n        return get_value(obj, attr, default)\n    ##### Serialization/Deserialization API #####\n    @staticmethod\n    def _call_and_store(getter_func, data, *, field_name, error_store, index=None):\n        \"\"\"Call ``getter_func`` with ``data`` as its argument, and store any `ValidationErrors`.\n        :param callable getter_func: Function for getting the serialized/deserialized\n            value from ``data``.\n        :param data: The data passed to ``getter_func``.\n        :param str field_name: Field name.\n        :param int index: Index of the item being validated, if validating a collection,\n            otherwise `None`.\n        \"\"\"\n        try:\n            value = getter_func(data)\n        except ValidationError as error:\n            error_store.store_error(error.messages, field_name, index=index)\n            # When a Nested field fails validation, the marshalled data is stored\n            # on the ValidationError's valid_data attribute\n            return error.valid_data or missing\n        return value\n    def _serialize(self, obj, *, many=False):\n        \"\"\"Serialize ``obj``.\n        :param obj: The object(s) to serialize.\n        :param bool many: `True` if ``data`` should be serialized as a collection.\n        :return: A dictionary of the serialized data\n        .. versionchanged:: 1.0.0\n            Renamed from ``marshal``.\n        \"\"\"\n        if many and obj is not None:\n            return [self._serialize(d, many=False) for d in obj]\n        ret = self.dict_class()\n        for attr_name, field_obj in self.dump_fields.items():\n            value = field_obj.serialize(attr_name, obj, accessor=self.get_attribute)\n            if value is missing:\n                continue\n            key = field_obj.data_key or attr_name\n            ret[key] = value\n        return ret\n    def dump(self, obj, *, many=None):\n        \"\"\"Serialize an object to native Python data types according to this\n        Schema's fields.\n        :param obj: The object to serialize.\n        :param bool many: Whether to serialize `obj` as a collection. If `None`, the value\n            for `self.many` is used.\n        :return: A dict of serialized data\n        :rtype: dict\n        .. versionadded:: 1.0.0\n        .. versionchanged:: 3.0.0b7\n            This method returns the serialized data rather than a ``(data, errors)`` duple.\n            A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised\n            if ``obj`` is invalid.\n        .. versionchanged:: 3.0.0rc9\n            Validation no longer occurs upon serialization.\n        \"\"\"\n        many = self.many if many is None else bool(many)\n        if many and is_iterable_but_not_string(obj):\n            obj = list(obj)\n        if self._has_processors(PRE_DUMP):\n            processed_obj = self._invoke_dump_processors(\n                PRE_DUMP, obj, many=many, original_data=obj\n            )\n        else:\n            processed_obj = obj\n        result = self._serialize(processed_obj, many=many)\n        if self._has_processors(POST_DUMP):\n            result = self._invoke_dump_processors(\n                POST_DUMP, result, many=many, original_data=obj\n            )\n        return result\n    def dumps(self, obj, *args, many=None, **kwargs):\n        \"\"\"Same as :meth:`dump`, except return a JSON-encoded string.\n        :param obj: The object to serialize.\n        :param bool many: Whether to serialize `obj` as a collection. If `None`, the value\n            for `self.many` is used.\n        :return: A ``json`` string\n        :rtype: str\n        .. versionadded:: 1.0.0\n        .. versionchanged:: 3.0.0b7\n            This method returns the serialized data rather than a ``(data, errors)`` duple.\n            A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised\n            if ``obj`` is invalid.\n        \"\"\"\n        serialized = self.dump(obj, many=many)\n        return self.opts.render_module.dumps(serialized, *args, **kwargs)\n    def _deserialize(\n        self, data, *, error_store, many=False, partial=False, unknown=RAISE, index=None\n    ):\n        \"\"\"Deserialize ``data``.\n        :param dict data: The data to deserialize.\n        :param ErrorStore error_store: Structure to store errors.\n        :param bool many: `True` if ``data`` should be deserialized as a collection.\n        :param bool|tuple partial: Whether to ignore missing fields and not require\n            any fields declared. Propagates down to ``Nested`` fields as well. If\n            its value is an iterable, only missing fields listed in that iterable\n            will be ignored. Use dot delimiters to specify nested fields.\n        :param unknown: Whether to exclude, include, or raise an error for unknown\n            fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`.\n        :param int index: Index of the item being serialized (for storing errors) if\n            serializing a collection, otherwise `None`.\n        :return: A dictionary of the deserialized data.\n        \"\"\"\n        index_errors = self.opts.index_errors\n        index = index if index_errors else None\n        if many:\n            if not is_collection(data):\n                error_store.store_error([self.error_messages[\"type\"]], index=index)\n                ret = []\n            else:\n                ret = [\n                    self._deserialize(\n                        d,\n                        error_store=error_store,\n                        many=False,\n                        partial=partial,\n                        unknown=unknown,\n                        index=idx,\n                    )\n                    for idx, d in enumerate(data)\n                ]\n            return ret\n        ret = self.dict_class()\n        # Check data is a dict\n        if not isinstance(data, Mapping):\n            error_store.store_error([self.error_messages[\"type\"]], index=index)\n        else:\n            partial_is_collection = is_collection(partial)\n            for attr_name, field_obj in self.load_fields.items():\n                field_name = field_obj.data_key or attr_name\n                raw_value = data.get(field_name, missing)\n                if raw_value is missing:\n                    # Ignore missing field if we're allowed to.\n                    if partial is True or (\n                        partial_is_collection and attr_name in partial\n                    ):\n                        continue\n                d_kwargs = {}\n                # Allow partial loading of nested schemas.\n                if partial_is_collection:\n                    prefix = field_name + \".\"\n                    len_prefix = len(prefix)\n                    sub_partial = [\n                        f[len_prefix:] for f in partial if f.startswith(prefix)\n                    ]\n                    d_kwargs[\"partial\"] = sub_partial\n                else:\n                    d_kwargs[\"partial\"] = partial\n                getter = lambda val: field_obj.deserialize(\n                    val, field_name, data, **d_kwargs\n                )\n                value = self._call_and_store(\n                    getter_func=getter,\n                    data=raw_value,\n                    field_name=field_name,\n                    error_store=error_store,\n                    index=index,\n                )\n                if value is not missing:\n                    key = field_obj.attribute or attr_name\n                    set_value(ret, key, value)\n            if unknown != EXCLUDE:\n                fields = {\n                    field_obj.data_key or field_name\n                    for field_name, field_obj in self.load_fields.items()\n                }\n                for key in set(data) - fields:\n                    value = data[key]\n                    if unknown == INCLUDE:\n                        set_value(ret, key, value)\n                    elif unknown == RAISE:\n                        error_store.store_error(\n                            [self.error_messages[\"unknown\"]],\n                            key,\n                            (index if index_errors else None),\n                        )\n        return ret\n    def load(self, data, *, many=None, partial=None, unknown=None):\n        \"\"\"Deserialize a data structure to an object defined by this Schema's fields.\n        :param dict data: The data to deserialize.\n        :param bool many: Whether to deserialize `data` as a collection. If `None`, the\n            value for `self.many` is used.\n        :param bool|tuple partial: Whether to ignore missing fields and not require\n            any fields declared. Propagates down to ``Nested`` fields as well. If\n            its value is an iterable, only missing fields listed in that iterable\n            will be ignored. Use dot delimiters to specify nested fields.\n        :param unknown: Whether to exclude, include, or raise an error for unknown\n            fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`.\n            If `None`, the value for `self.unknown` is used.\n        :return: A dict of deserialized data\n        :rtype: dict\n        .. versionadded:: 1.0.0\n        .. versionchanged:: 3.0.0b7\n            This method returns the deserialized data rather than a ``(data, errors)`` duple.\n            A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised\n            if invalid data are passed.\n        \"\"\"\n        return self._do_load(\n            data, many=many, partial=partial, unknown=unknown, postprocess=True\n        )\n    def loads(self, json_data, *, many=None, partial=None, unknown=None, **kwargs):\n        \"\"\"Same as :meth:`load`, except it takes a JSON string as input.\n        :param str json_data: A JSON string of the data to deserialize.\n        :param bool many: Whether to deserialize `obj` as a collection. If `None`, the\n            value for `self.many` is used.\n        :param bool|tuple partial: Whether to ignore missing fields and not require\n            any fields declared. Propagates down to ``Nested`` fields as well. If\n            its value is an iterable, only missing fields listed in that iterable\n            will be ignored. Use dot delimiters to specify nested fields.\n        :param unknown: Whether to exclude, include, or raise an error for unknown\n            fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`.\n            If `None`, the value for `self.unknown` is used.\n        :return: A dict of deserialized data\n        :rtype: dict\n        .. versionadded:: 1.0.0\n        .. versionchanged:: 3.0.0b7\n            This method returns the deserialized data rather than a ``(data, errors)`` duple.\n            A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised\n            if invalid data are passed.\n        \"\"\"\n        data = self.opts.render_module.loads(json_data, **kwargs)\n        return self.load(data, many=many, partial=partial, unknown=unknown)\n    def _run_validator(\n        self,\n        validator_func,\n        output,\n        *,\n        original_data,\n        error_store,\n        many,\n        partial,\n        pass_original,\n        index=None\n    ):\n        try:\n            if pass_original:  # Pass original, raw data (before unmarshalling)\n                validator_func(output, original_data, partial=partial, many=many)\n            else:\n                validator_func(output, partial=partial, many=many)\n        except ValidationError as err:\n            error_store.store_error(err.messages, err.field_name, index=index)\n    def validate(self, data, *, many=None, partial=None):\n        \"\"\"Validate `data` against the schema, returning a dictionary of\n        validation errors.\n        :param dict data: The data to validate.\n        :param bool many: Whether to validate `data` as a collection. If `None`, the\n            value for `self.many` is used.\n        :param bool|tuple partial: Whether to ignore missing fields and not require\n            any fields declared. Propagates down to ``Nested`` fields as well. If\n            its value is an iterable, only missing fields listed in that iterable\n            will be ignored. Use dot delimiters to specify nested fields.\n        :return: A dictionary of validation errors.\n        :rtype: dict\n        .. versionadded:: 1.1.0\n        \"\"\"\n        try:\n            self._do_load(data, many=many, partial=partial, postprocess=False)\n        except ValidationError as exc:\n            return exc.messages\n        return {}\n    ##### Private Helpers #####\n    def _do_load(\n        self, data, *, many=None, partial=None, unknown=None, postprocess=True\n    ):\n        \"\"\"Deserialize `data`, returning the deserialized result.\n        :param data: The data to deserialize.\n        :param bool many: Whether to deserialize `data` as a collection. If `None`, the\n            value for `self.many` is used.\n        :param bool|tuple partial: Whether to validate required fields. If its\n            value is an iterable, only fields listed in that iterable will be\n            ignored will be allowed missing. If `True`, all fields will be allowed missing.\n            If `None`, the value for `self.partial` is used.\n        :param unknown: Whether to exclude, include, or raise an error for unknown\n            fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`.\n            If `None`, the value for `self.unknown` is used.\n        :param bool postprocess: Whether to run post_load methods..\n        :return: A dict of deserialized data\n        :rtype: dict\n        \"\"\"\n        error_store = ErrorStore()\n        errors = {}\n        many = self.many if many is None else bool(many)\n        unknown = unknown or self.unknown\n        if partial is None:\n            partial = self.partial\n        # Run preprocessors\n        if self._has_processors(PRE_LOAD):\n            try:\n                processed_data = self._invoke_load_processors(\n                    PRE_LOAD, data, many=many, original_data=data, partial=partial\n                )\n            except ValidationError as err:\n                errors = err.normalized_messages()\n                result = None\n        else:\n            processed_data = data\n        if not errors:\n            # Deserialize data\n            result = self._deserialize(\n                processed_data,\n                error_store=error_store,\n                many=many,\n                partial=partial,\n                unknown=unknown,\n            )\n            # Run field-level validation\n            self._invoke_field_validators(\n                error_store=error_store, data=result, many=many\n            )\n            # Run schema-level validation\n            if self._has_processors(VALIDATES_SCHEMA):\n                field_errors = bool(error_store.errors)\n                self._invoke_schema_validators(\n                    error_store=error_store,\n                    pass_many=True,\n                    data=result,\n                    original_data=data,\n                    many=many,\n                    partial=partial,\n                    field_errors=field_errors,\n                )\n                self._invoke_schema_validators(\n                    error_store=error_store,\n                    pass_many=False,\n                    data=result,\n                    original_data=data,\n                    many=many,\n                    partial=partial,\n                    field_errors=field_errors,\n                )\n            errors = error_store.errors\n            # Run post processors\n            if not errors and postprocess and self._has_processors(POST_LOAD):\n                try:\n                    result = self._invoke_load_processors(\n                        POST_LOAD,\n                        result,\n                        many=many,\n                        original_data=data,\n                        partial=partial,\n                    )\n                except ValidationError as err:\n                    errors = err.normalized_messages()\n        if errors:\n            exc = ValidationError(errors, data=data, valid_data=result)\n            self.handle_error(exc, data, many=many, partial=partial)\n            raise exc\n        return result\n    def _normalize_nested_options(self):\n        \"\"\"Apply then flatten nested schema options\"\"\"\n        if self.only is not None:\n            # Apply the only option to nested fields.\n            self.__apply_nested_option(\"only\", self.only, \"intersection\")\n            # Remove the child field names from the only option.\n            self.only = self.set_class([field.split(\".\", 1)[0] for field in self.only])\n        if self.exclude:\n            # Apply the exclude option to nested fields.\n            self.__apply_nested_option(\"exclude\", self.exclude, \"union\")\n            # Remove the parent field names from the exclude option.\n            self.exclude = self.set_class(\n                [field for field in self.exclude if \".\" not in field]\n            )\n    def __apply_nested_option(self, option_name, field_names, set_operation):\n        \"\"\"Apply nested options to nested fields\"\"\"\n        # Split nested field names on the first dot.\n        nested_fields = [name.split(\".\", 1) for name in field_names if \".\" in name]\n        # Partition the nested field names by parent field.\n        nested_options = defaultdict(list)\n        for parent, nested_names in nested_fields:\n            nested_options[parent].append(nested_names)\n        # Apply the nested field options.\n        for key, options in iter(nested_options.items()):\n            new_options = self.set_class(options)\n            original_options = getattr(self.declared_fields[key], option_name, ())\n            if original_options:\n                if set_operation == \"union\":\n                    new_options |= self.set_class(original_options)\n                if set_operation == \"intersection\":\n                    new_options &= self.set_class(original_options)\n            setattr(self.declared_fields[key], option_name, new_options)\n    def _init_fields(self):\n        \"\"\"Update fields based on schema options.\"\"\"\n        if self.opts.fields:\n            available_field_names = self.set_class(self.opts.fields)\n        else:\n            available_field_names = self.set_class(self.declared_fields.keys())\n            if self.opts.additional:\n                available_field_names |= self.set_class(self.opts.additional)\n        invalid_fields = self.set_class()\n        if self.only is not None:\n            # Return only fields specified in only option\n            field_names = self.set_class(self.only)\n            invalid_fields |= field_names - available_field_names\n        else:\n            field_names = available_field_names\n        # If \"exclude\" option or param is specified, remove those fields.\n        if self.exclude:\n            # Note that this isn't available_field_names, since we want to\n            # apply \"only\" for the actual calculation.\n            field_names = field_names - self.exclude\n            invalid_fields |= self.exclude - available_field_names\n        if invalid_fields:\n            message = \"Invalid fields for {}: {}.\".format(self, invalid_fields)\n            raise ValueError(message)\n        fields_dict = self.dict_class()\n        for field_name in field_names:\n            field_obj = self.declared_fields.get(field_name, ma_fields.Inferred())\n            self._bind_field(field_name, field_obj)\n            fields_dict[field_name] = field_obj\n        dump_data_keys = [\n            obj.data_key or name\n            for name, obj in fields_dict.items()\n            if not obj.load_only\n        ]\n        if len(dump_data_keys) != len(set(dump_data_keys)):\n            data_keys_duplicates = {\n                x for x in dump_data_keys if dump_data_keys.count(x) > 1\n            }\n            raise ValueError(\n                \"The data_key argument for one or more fields collides \"\n                \"with another field's name or data_key argument. \"\n                \"Check the following field names and \"\n                \"data_key arguments: {}\".format(list(data_keys_duplicates))\n            )\n        load_attributes = [\n            obj.attribute or name\n            for name, obj in fields_dict.items()\n            if not obj.dump_only\n        ]\n        if len(load_attributes) != len(set(load_attributes)):\n            attributes_duplicates = {\n                x for x in load_attributes if load_attributes.count(x) > 1\n            }\n            raise ValueError(\n                \"The attribute argument for one or more fields collides \"\n                \"with another field's name or attribute argument. \"\n                \"Check the following field names and \"\n                \"attribute arguments: {}\".format(list(attributes_duplicates))\n            )\n        return fields_dict\n    def on_bind_field(self, field_name, field_obj):\n        \"\"\"Hook to modify a field when it is bound to the `Schema`.\n        No-op by default.\n        \"\"\"\n        return None\n    def _bind_field(self, field_name, field_obj):\n        \"\"\"Bind field to the schema, setting any necessary attributes on the\n        field (e.g. parent and name).\n        Also set field load_only and dump_only values if field_name was\n        specified in ``class Meta``.\n        \"\"\"\n        try:\n            if field_name in self.load_only:\n                field_obj.load_only = True\n            if field_name in self.dump_only:\n                field_obj.dump_only = True\n            field_obj._bind_to_schema(field_name, self)\n            self.on_bind_field(field_name, field_obj)\n        except TypeError as error:\n            # field declared as a class, not an instance\n            if isinstance(field_obj, type) and issubclass(field_obj, base.FieldABC):\n                msg = (\n                    'Field for \"{}\" must be declared as a '\n                    \"Field instance, not a class. \"\n                    'Did you mean \"fields.{}()\"?'.format(field_name, field_obj.__name__)\n                )\n                raise TypeError(msg) from error\n    @lru_cache(maxsize=8)\n    def _has_processors(self, tag):\n        return self._hooks[(tag, True)] or self._hooks[(tag, False)]\n    def _invoke_dump_processors(self, tag, data, *, many, original_data=None):\n        # The pass_many post-dump processors may do things like add an envelope, so\n        # invoke those after invoking the non-pass_many processors which will expect\n        # to get a list of items.\n        data = self._invoke_processors(\n            tag, pass_many=False, data=data, many=many, original_data=original_data\n        )\n        data = self._invoke_processors(\n            tag, pass_many=True, data=data, many=many, original_data=original_data\n        )\n        return data\n    def _invoke_load_processors(self, tag, data, *, many, original_data, partial):\n        # This has to invert the order of the dump processors, so run the pass_many\n        # processors first.\n        data = self._invoke_processors(\n            tag,\n            pass_many=True,\n            data=data,\n            many=many,\n            original_data=original_data,\n            partial=partial,\n        )\n        data = self._invoke_processors(\n            tag,\n            pass_many=False,\n            data=data,\n            many=many,\n            original_data=original_data,\n            partial=partial,\n        )",
        "file_path": "src/marshmallow/schema.py",
        "chunk_index": 1,
        "metadata": {}
      }
    ]
  },
  "marshmallow-code__marshmallow-1343": {
    "query": "[version 2.20.0] TypeError: 'NoneType' object is not subscriptable\nAfter update from version 2.19.5 to 2.20.0 I got error for code like:\r\n\r\n```python\r\nfrom marshmallow import Schema, fields, validates\r\n\r\n\r\nclass Bar(Schema):\r\n    value = fields.String()\r\n\r\n    @validates('value')  # <- issue here\r\n    def validate_value(self, value):\r\n        pass\r\n\r\n\r\nclass Foo(Schema):\r\n    bar = fields.Nested(Bar)\r\n\r\n\r\nsch = Foo()\r\n\r\nsch.validate({\r\n    'bar': 'invalid',\r\n})\r\n```\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/_/bug_mschema.py\", line 19, in <module>\r\n    'bar': 'invalid',\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/schema.py\", line 628, in validate\r\n    _, errors = self._do_load(data, many, partial=partial, postprocess=False)\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/schema.py\", line 670, in _do_load\r\n    index_errors=self.opts.index_errors,\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/marshalling.py\", line 292, in deserialize\r\n    index=(index if index_errors else None)\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/marshalling.py\", line 65, in call_and_store\r\n    value = getter_func(data)\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/marshalling.py\", line 285, in <lambda>\r\n    data\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/fields.py\", line 265, in deserialize\r\n    output = self._deserialize(value, attr, data)\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/fields.py\", line 465, in _deserialize\r\n    data, errors = self.schema.load(value)\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/schema.py\", line 588, in load\r\n    result, errors = self._do_load(data, many, partial=partial, postprocess=True)\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/schema.py\", line 674, in _do_load\r\n    self._invoke_field_validators(unmarshal, data=result, many=many)\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/schema.py\", line 894, in _invoke_field_validators\r\n    value = data[field_obj.attribute or field_name]\r\nTypeError: 'NoneType' object is not subscriptable\r\n```\n",
    "method": "sliding",
    "top_k": 10,
    "results": [
      {
        "rank": 1,
        "score": 0.6157238483428955,
        "content": "            'instead.', category=DeprecationWarning\n        )\n        cls.__accessor__ = func\n        return func\n    ##### Serialization/Deserialization API #####\n    def dump(self, obj, many=None, update_fields=True, **kwargs):\n        \"\"\"Serialize an object to native Python data types according to this\n        Schema's fields.\n        :param obj: The object to serialize.\n        :param bool many: Whether to serialize `obj` as a collection. If `None`, the value\n            for `self.many` is used.\n        :param bool update_fields: Whether to update the schema's field classes. Typically\n            set to `True`, but may be `False` when serializing a homogenous collection.\n            This parameter is used by `fields.Nested` to avoid multiple updates.\n        :return: A tuple of the form (``data``, ``errors``)\n        :rtype: `MarshalResult`, a `collections.namedtuple`\n        .. versionadded:: 1.0.0\n        \"\"\"\n        # Callable marshalling object\n        marshal = marshalling.Marshaller(prefix=self.prefix)\n        errors = {}\n        many = self.many if many is None else bool(many)\n        if many and utils.is_iterable_but_not_string(obj):\n            obj = list(obj)\n        if self._has_processors:\n            try:\n                processed_obj = self._invoke_dump_processors(\n                    PRE_DUMP,\n                    obj,\n                    many,\n                    original_data=obj)\n            except ValidationError as error:\n                errors = error.normalized_messages()\n                result = None\n        else:\n            processed_obj = obj\n        if not errors:\n            if update_fields:\n                obj_type = type(processed_obj)\n                if obj_type not in self._types_seen:\n                    self._update_fields(processed_obj, many=many)\n                    if not isinstance(processed_obj, Mapping):\n                        self._types_seen.add(obj_type)\n            try:\n                preresult = marshal(\n                    processed_obj,\n                    self.fields,\n                    many=many,\n                    # TODO: Remove self.__accessor__ in a later release\n                    accessor=self.get_attribute or self.__accessor__,\n                    dict_class=self.dict_class,\n                    index_errors=self.opts.index_errors,\n                    **kwargs\n                )\n            except ValidationError as error:\n                errors = marshal.errors\n                preresult = error.data\n            result = self._postprocess(preresult, many, obj=obj)\n        if not errors and self._has_processors:\n            try:\n                result = self._invoke_dump_processors(\n                    POST_DUMP,\n                    result,\n                    many,\n                    original_data=obj)\n            except ValidationError as error:\n                errors = error.normalized_messages()\n        if errors:\n            # TODO: Remove self.__error_handler__ in a later release\n            if self.__error_handler__ and callable(self.__error_handler__):\n                self.__error_handler__(errors, obj)\n            exc = ValidationError(\n                errors,\n                field_names=marshal.error_field_names,\n                fields=marshal.error_fields,\n                data=obj,\n                **marshal.error_kwargs\n            )\n            self.handle_error(exc, obj)\n            if self.strict:\n                raise exc\n        return MarshalResult(result, errors)\n    def dumps(self, obj, many=None, update_fields=True, *args, **kwargs):\n        \"\"\"Same as :meth:`dump`, except return a JSON-encoded string.\n        :param obj: The object to serialize.\n        :param bool many: Whether to serialize `obj` as a collection. If `None`, the value\n            for `self.many` is used.\n        :param bool update_fields: Whether to update the schema's field classes. Typically\n            set to `True`, but may be `False` when serializing a homogenous collection.\n            This parameter is used by `fields.Nested` to avoid multiple updates.\n        :return: A tuple of the form (``data``, ``errors``)\n        :rtype: `MarshalResult`, a `collections.namedtuple`\n        .. versionadded:: 1.0.0\n        \"\"\"\n        deserialized, errors = self.dump(obj, many=many, update_fields=update_fields)\n        ret = self.opts.json_module.dumps(deserialized, *args, **kwargs)\n        return MarshalResult(ret, errors)\n    def load(self, data, many=None, partial=None):\n        \"\"\"Deserialize a data structure to an object defined by this Schema's\n        fields and :meth:`make_object`.\n        :param dict data: The data to deserialize.\n        :param bool many: Whether to deserialize `data` as a collection. If `None`, the\n            value for `self.many` is used.\n        :param bool|tuple partial: Whether to ignore missing fields. If `None`,\n            the value for `self.partial` is used. If its value is an iterable,\n            only missing fields listed in that iterable will be ignored.\n        :return: A tuple of the form (``data``, ``errors``)\n        :rtype: `UnmarshalResult`, a `collections.namedtuple`\n        .. versionadded:: 1.0.0\n        \"\"\"\n        result, errors = self._do_load(data, many, partial=partial, postprocess=True)\n        return UnmarshalResult(data=result, errors=errors)\n    def loads(self, json_data, many=None, *args, **kwargs):\n        \"\"\"Same as :meth:`load`, except it takes a JSON string as input.\n        :param str json_data: A JSON string of the data to deserialize.\n        :param bool many: Whether to deserialize `obj` as a collection. If `None`, the\n            value for `self.many` is used.\n        :param bool|tuple partial: Whether to ignore missing fields. If `None`,\n            the value for `self.partial` is used. If its value is an iterable,\n            only missing fields listed in that iterable will be ignored.\n        :return: A tuple of the form (``data``, ``errors``)\n        :rtype: `UnmarshalResult`, a `collections.namedtuple`\n        .. versionadded:: 1.0.0\n        \"\"\"\n        # TODO: This avoids breaking backward compatibility if people were\n        # passing in positional args after `many` for use by `json.loads`, but\n        # ideally we shouldn't have to do this.\n        partial = kwargs.pop('partial', None)\n        data = self.opts.json_module.loads(json_data, *args, **kwargs)\n        return self.load(data, many=many, partial=partial)\n    def validate(self, data, many=None, partial=None):\n        \"\"\"Validate `data` against the schema, returning a dictionary of\n        validation errors.\n        :param dict data: The data to validate.\n        :param bool many: Whether to validate `data` as a collection. If `None`, the\n            value for `self.many` is used.\n        :param bool|tuple partial: Whether to ignore missing fields. If `None`,\n            the value for `self.partial` is used. If its value is an iterable,\n            only missing fields listed in that iterable will be ignored.\n        :return: A dictionary of validation errors.\n        :rtype: dict\n        .. versionadded:: 1.1.0\n        \"\"\"\n        _, errors = self._do_load(data, many, partial=partial, postprocess=False)\n        return errors\n    ##### Private Helpers #####\n    def _do_load(self, data, many=None, partial=None, postprocess=True):\n        \"\"\"Deserialize `data`, returning the deserialized result and a dictonary of\n        validation errors.\n        :param data: The data to deserialize.\n        :param bool many: Whether to deserialize `data` as a collection. If `None`, the\n            value for `self.many` is used.\n        :param bool|tuple partial: Whether to validate required fields. If its value is an iterable,\n            only fields listed in that iterable will be ignored will be allowed missing.\n            If `True`, all fields will be allowed missing.\n            If `None`, the value for `self.partial` is used.\n        :param bool postprocess: Whether to run post_load methods..\n        :return: A tuple of the form (`data`, `errors`)\n        \"\"\"\n        # Callable unmarshalling object\n        unmarshal = marshalling.Unmarshaller()\n        errors = {}\n        many = self.many if many is None else bool(many)\n        if partial is None:\n            partial = self.partial\n        try:\n            processed_data = self._invoke_load_processors(\n                PRE_LOAD,\n                data,\n                many,\n                original_data=data)\n        except ValidationError as err:\n            errors = err.normalized_messages()\n            result = None\n        if not errors:\n            try:\n                result = unmarshal(\n                    processed_data,\n                    self.fields,\n                    many=many,\n                    partial=partial,\n                    dict_class=self.dict_class,\n                    index_errors=self.opts.index_errors,\n                )\n            except ValidationError as error:\n                result = error.data\n            self._invoke_field_validators(unmarshal, data=result, many=many)\n            errors = unmarshal.errors\n            field_errors = bool(errors)\n            # Run schema-level migration\n            try:\n                self._invoke_validators(unmarshal, pass_many=True, data=result, original_data=data,\n                                        many=many, field_errors=field_errors)\n            except ValidationError as err:\n                errors.update(err.messages)\n            try:\n                self._invoke_validators(unmarshal, pass_many=False, data=result, original_data=data,\n                                        many=many, field_errors=field_errors)\n            except ValidationError as err:\n                errors.update(err.messages)\n        # Run post processors\n        if not errors and postprocess:\n            try:\n                result = self._invoke_load_processors(\n                    POST_LOAD,\n                    result,\n                    many,\n                    original_data=data)\n            except ValidationError as err:\n                errors = err.normalized_messages()\n        if errors:\n            # TODO: Remove self.__error_handler__ in a later release\n            if self.__error_handler__ and callable(self.__error_handler__):\n                self.__error_handler__(errors, data)\n            exc = ValidationError(\n                errors,\n                field_names=unmarshal.error_field_names,\n                fields=unmarshal.error_fields,\n                data=data,\n                **unmarshal.error_kwargs\n            )\n            self.handle_error(exc, data)\n            if self.strict:\n                raise exc\n        return result, errors\n    def _normalize_nested_options(self):\n        \"\"\"Apply then flatten nested schema options\"\"\"\n        if self.only is not None:\n            # Apply the only option to nested fields.\n            self.__apply_nested_option('only', self.only, 'intersection')\n            # Remove the child field names from the only option.\n            self.only = self.set_class(\n                [field.split('.', 1)[0] for field in self.only],\n            )\n        if self.exclude:\n            # Apply the exclude option to nested fields.\n            self.__apply_nested_option('exclude', self.exclude, 'union')\n            # Remove the parent field names from the exclude option.\n            self.exclude = self.set_class(\n                [field for field in self.exclude if '.' not in field],\n            )\n    def __apply_nested_option(self, option_name, field_names, set_operation):\n        \"\"\"Apply nested options to nested fields\"\"\"\n        # Split nested field names on the first dot.\n        nested_fields = [name.split('.', 1) for name in field_names if '.' in name]\n        # Partition the nested field names by parent field.\n        nested_options = defaultdict(list)\n        for parent, nested_names in nested_fields:\n            nested_options[parent].append(nested_names)\n        # Apply the nested field options.\n        for key, options in iter(nested_options.items()):\n            new_options = self.set_class(options)\n            original_options = getattr(self.declared_fields[key], option_name, ())\n            if original_options:\n                if set_operation == 'union':\n                    new_options |= self.set_class(original_options)\n                if set_operation == 'intersection':\n                    new_options &= self.set_class(original_options)\n            setattr(self.declared_fields[key], option_name, new_options)\n    def _update_fields(self, obj=None, many=False):\n        \"\"\"Update fields based on the passed in object.\"\"\"\n        if self.only is not None:\n            # Return only fields specified in only option\n            if self.opts.fields:\n                field_names = self.set_class(self.opts.fields) & self.set_class(self.only)\n            else:\n                field_names = self.set_class(self.only)\n        elif self.opts.fields:\n            # Return fields specified in fields option\n            field_names = self.set_class(self.opts.fields)\n        elif self.opts.additional:\n            # Return declared fields + additional fields\n            field_names = (self.set_class(self.declared_fields.keys()) |\n                            self.set_class(self.opts.additional))\n        else:\n            field_names = self.set_class(self.declared_fields.keys())\n        # If \"exclude\" option or param is specified, remove those fields\n        field_names -= self.exclude\n        ret = self.__filter_fields(field_names, obj, many=many)\n        # Set parents\n        self.__set_field_attrs(ret)\n        self.fields = ret\n        return self.fields\n    def on_bind_field(self, field_name, field_obj):\n        \"\"\"Hook to modify a field when it is bound to the `Schema`. No-op by default.\"\"\"\n        return None\n    def __set_field_attrs(self, fields_dict):\n        \"\"\"Bind fields to the schema, setting any necessary attributes\n        on the fields (e.g. parent and name).\n        Also set field load_only and dump_only values if field_name was\n        specified in ``class Meta``.\n        \"\"\"\n        for field_name, field_obj in iteritems(fields_dict):\n            try:\n                if field_name in self.load_only:\n                    field_obj.load_only = True\n                if field_name in self.dump_only:\n                    field_obj.dump_only = True\n                field_obj._add_to_schema(field_name, self)\n                self.on_bind_field(field_name, field_obj)\n            except TypeError:\n                # field declared as a class, not an instance\n                if (isinstance(field_obj, type) and\n                        issubclass(field_obj, base.FieldABC)):\n                    msg = ('Field for \"{0}\" must be declared as a '\n                           'Field instance, not a class. '\n                           'Did you mean \"fields.{1}()\"?'\n                           .format(field_name, field_obj.__name__))\n                    raise TypeError(msg)\n        return fields_dict\n    def __filter_fields(self, field_names, obj, many=False):\n        \"\"\"Return only those field_name:field_obj pairs specified by\n        ``field_names``.\n        :param set field_names: Field names to include in the final\n            return dictionary.\n        :param object|Mapping|list obj The object to base filtered fields on.\n        :returns: An dict of field_name:field_obj pairs.\n        \"\"\"\n        if obj and many:\n            try:  # list\n                obj = obj[0]\n            except IndexError:  # Nothing to serialize\n                return dict((k, v) for k, v in self.declared_fields.items() if k in field_names)\n        ret = self.dict_class()\n        for key in field_names:\n            if key in self.declared_fields:\n                ret[key] = self.declared_fields[key]\n            else:  # Implicit field creation (class Meta 'fields' or 'additional')\n                if obj:\n                    attribute_type = None\n                    try:\n                        if isinstance(obj, Mapping):\n                            attribute_type = type(obj[key])\n                        else:\n                            attribute_type = type(getattr(obj, key))\n                    except (AttributeError, KeyError) as err:\n                        err_type = type(err)\n                        raise err_type(\n                            '\"{0}\" is not a valid field for {1}.'.format(key, obj))\n                    field_obj = self.TYPE_MAPPING.get(attribute_type, fields.Field)()\n                else:  # Object is None\n                    field_obj = fields.Field()\n                # map key -> field (default to Raw)\n                ret[key] = field_obj\n        return ret\n    def _invoke_dump_processors(self, tag_name, data, many, original_data=None):\n        # The pass_many post-dump processors may do things like add an envelope, so\n        # invoke those after invoking the non-pass_many processors which will expect\n        # to get a list of items.\n        data = self._invoke_processors(tag_name, pass_many=False,\n            data=data, many=many, original_data=original_data)\n        data = self._invoke_processors(tag_name, pass_many=True,\n            data=data, many=many, original_data=original_data)\n        return data\n    def _invoke_load_processors(self, tag_name, data, many, original_data=None):\n        # This has to invert the order of the dump processors, so run the pass_many\n        # processors first.\n        data = self._invoke_processors(tag_name, pass_many=True,\n            data=data, many=many, original_data=original_data)\n        data = self._invoke_processors(tag_name, pass_many=False,\n            data=data, many=many, original_data=original_data)\n        return data\n    def _invoke_field_validators(self, unmarshal, data, many):\n        for attr_name in self.__processors__[(VALIDATES, False)]:\n            validator = getattr(self, attr_name)\n            validator_kwargs = validator.__marshmallow_kwargs__[(VALIDATES, False)]\n            field_name = validator_kwargs['field_name']\n            try:\n                field_obj = self.fields[field_name]\n            except KeyError:\n                if field_name in self.declared_fields:\n                    continue\n                raise ValueError('\"{0}\" field does not exist.'.format(field_name))\n            if many:\n                for idx, item in enumerate(data):\n                    try:\n                        value = item[field_obj.attribute or field_name]\n                    except KeyError:\n                        pass\n                    else:\n                        validated_value = unmarshal.call_and_store(\n                            getter_func=validator,\n                            data=value,\n                            field_name=field_obj.load_from or field_name,\n                            field_obj=field_obj,\n                            index=(idx if self.opts.index_errors else None)\n                        )\n                        if validated_value is missing:\n                            data[idx].pop(field_name, None)\n            else:\n                try:\n                    value = data[field_obj.attribute or field_name]\n                except KeyError:\n                    pass\n                else:\n                    validated_value = unmarshal.call_and_store(\n                        getter_func=validator,\n                        data=value,\n                        field_name=field_obj.load_from or field_name,\n                        field_obj=field_obj\n                    )\n                    if validated_value is missing:\n                        data.pop(field_name, None)\n    def _invoke_validators(\n            self, unmarshal, pass_many, data, original_data, many, field_errors=False):\n        errors = {}\n        for attr_name in self.__processors__[(VALIDATES_SCHEMA, pass_many)]:\n            validator = getattr(self, attr_name)\n            validator_kwargs = validator.__marshmallow_kwargs__[(VALIDATES_SCHEMA, pass_many)]\n            pass_original = validator_kwargs.get('pass_original', False)\n            skip_on_field_errors = validator_kwargs['skip_on_field_errors']\n            if skip_on_field_errors and field_errors:\n                continue\n            if pass_many:\n                validator = functools.partial(validator, many=many)\n            if many and not pass_many:\n                for idx, item in enumerate(data):\n                    try:\n                        unmarshal.run_validator(validator,\n                                                item, original_data, self.fields, many=many,\n                                                index=idx, pass_original=pass_original)\n                    except ValidationError as err:\n                        errors.update(err.messages)\n            else:\n                try:\n                    unmarshal.run_validator(validator,\n                                            data, original_data, self.fields, many=many,\n                                            pass_original=pass_original)\n                except ValidationError as err:\n                    errors.update(err.messages)\n        if errors:\n            raise ValidationError(errors)\n        return None\n    def _invoke_processors(self, tag_name, pass_many, data, many, original_data=None):\n        for attr_name in self.__processors__[(tag_name, pass_many)]:\n            # This will be a bound method.\n            processor = getattr(self, attr_name)\n            processor_kwargs = processor.__marshmallow_kwargs__[(tag_name, pass_many)]\n            pass_original = processor_kwargs.get('pass_original', False)\n            if pass_many:\n                if pass_original:\n                    data = utils.if_none(processor(data, many, original_data), data)\n                else:\n                    data = utils.if_none(processor(data, many), data)\n            elif many:\n                if pass_original:\n                    data = [utils.if_none(processor(item, original_data), item)\n                            for item in data]\n                else:\n                    data = [utils.if_none(processor(item), item) for item in data]\n            else:\n                if pass_original:\n                    data = utils.if_none(processor(data, original_data), data)\n                else:\n                    data = utils.if_none(processor(data), data)\n        return data\nclass Schema(with_metaclass(SchemaMeta, BaseSchema)):\n    __doc__ = BaseSchema.__doc__",
        "file_path": "src/marshmallow/schema.py",
        "chunk_index": 1,
        "metadata": {}
      },
      {
        "rank": 2,
        "score": 0.6145472526550293,
        "content": "# -*- coding: utf-8 -*-\n\"\"\"Exception classes for marshmallow-related errors.\"\"\"\nfrom marshmallow.compat import basestring\nclass MarshmallowError(Exception):\n    \"\"\"Base class for all marshmallow-related errors.\"\"\"\n    pass\nclass ValidationError(MarshmallowError):\n    \"\"\"Raised when validation fails on a field. Validators and custom fields should\n    raise this exception.\n    :param message: An error message, list of error messages, or dict of\n        error messages.\n    :param list field_names: Field names to store the error on.\n        If `None`, the error is stored in its default location.\n    :param list fields: `Field` objects to which the error applies.\n    \"\"\"\n    def __init__(self, message, field_names=None, fields=None, data=None, **kwargs):\n        if not isinstance(message, dict) and not isinstance(message, list):\n            messages = [message]\n        else:\n            messages = message\n        #: String, list, or dictionary of error messages.\n        #: If a `dict`, the keys will be field names and the values will be lists of\n        #: messages.\n        self.messages = messages\n        #: List of field objects which failed validation.\n        self.fields = fields\n        if isinstance(field_names, basestring):\n            #: List of field_names which failed validation.\n            self.field_names = [field_names]\n        else:  # fields is a list or None\n            self.field_names = field_names or []\n        # Store nested data\n        self.data = data\n        self.kwargs = kwargs\n        MarshmallowError.__init__(self, message)\n    def normalized_messages(self, no_field_name=\"_schema\"):\n        if isinstance(self.messages, dict):\n            return self.messages\n        if len(self.field_names) == 0:\n            return {no_field_name: self.messages}\n        return dict((name, self.messages) for name in self.field_names)\nclass RegistryError(NameError):\n    \"\"\"Raised when an invalid operation is performed on the serializer\n    class registry.\n    \"\"\"\n    pass",
        "file_path": "src/marshmallow/exceptions.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 3,
        "score": 0.6113420128822327,
        "content": "# -*- coding: utf-8 -*-\n\"\"\"Utility classes and values used for marshalling and unmarshalling objects to\nand from primitive types.\n.. warning::\n    This module is treated as private API.\n    Users should not need to use this module directly.\n\"\"\"\nfrom __future__ import unicode_literals\nimport collections\nfrom marshmallow.utils import is_collection, missing, set_value\nfrom marshmallow.compat import text_type, iteritems\nfrom marshmallow.exceptions import (\n    ValidationError,\n)\n__all__ = [\n    'Marshaller',\n    'Unmarshaller',\n]\n# Key used for field-level validation errors on nested fields\nFIELD = '_field'\nclass ErrorStore(object):\n    def __init__(self):\n        #: Dictionary of errors stored during serialization\n        self.errors = {}\n        #: List of `Field` objects which have validation errors\n        self.error_fields = []\n        #: List of field_names which have validation errors\n        self.error_field_names = []\n        #: True while (de)serializing a collection\n        self._pending = False\n        #: Dictionary of extra kwargs from user raised exception\n        self.error_kwargs = {}\n    def get_errors(self, index=None):\n        if index is not None:\n            errors = self.errors.get(index, {})\n            self.errors[index] = errors\n        else:\n            errors = self.errors\n        return errors\n    def call_and_store(self, getter_func, data, field_name, field_obj, index=None):\n        \"\"\"Call ``getter_func`` with ``data`` as its argument, and store any `ValidationErrors`.\n        :param callable getter_func: Function for getting the serialized/deserialized\n            value from ``data``.\n        :param data: The data passed to ``getter_func``.\n        :param str field_name: Field name.\n        :param FieldABC field_obj: Field object that performs the\n            serialization/deserialization behavior.\n        :param int index: Index of the item being validated, if validating a collection,\n            otherwise `None`.\n        \"\"\"\n        try:\n            value = getter_func(data)\n        except ValidationError as err:  # Store validation errors\n            self.error_kwargs.update(err.kwargs)\n            self.error_fields.append(field_obj)\n            self.error_field_names.append(field_name)\n            errors = self.get_errors(index=index)\n            # Warning: Mutation!\n            if isinstance(err.messages, dict):\n                errors[field_name] = err.messages\n            elif isinstance(errors.get(field_name), dict):\n                errors[field_name].setdefault(FIELD, []).extend(err.messages)\n            else:\n                errors.setdefault(field_name, []).extend(err.messages)\n            # When a Nested field fails validation, the marshalled data is stored\n            # on the ValidationError's data attribute\n            value = err.data or missing\n        return value\nclass Marshaller(ErrorStore):\n    \"\"\"Callable class responsible for serializing data and storing errors.\n    :param str prefix: Optional prefix that will be prepended to all the\n        serialized field names.\n    \"\"\"\n    def __init__(self, prefix=''):\n        self.prefix = prefix\n        ErrorStore.__init__(self)\n    def serialize(self, obj, fields_dict, many=False,\n                  accessor=None, dict_class=dict, index_errors=True, index=None):\n        \"\"\"Takes raw data (a dict, list, or other object) and a dict of\n        fields to output and serializes the data based on those fields.\n        :param obj: The actual object(s) from which the fields are taken from\n        :param dict fields_dict: Mapping of field names to :class:`Field` objects.\n        :param bool many: Set to `True` if ``data`` should be serialized as\n            a collection.\n        :param callable accessor: Function to use for getting values from ``obj``.\n        :param type dict_class: Dictionary class used to construct the output.\n        :param bool index_errors: Whether to store the index of invalid items in\n            ``self.errors`` when ``many=True``.\n        :param int index: Index of the item being serialized (for storing errors) if\n            serializing a collection, otherwise `None`.\n        :return: A dictionary of the marshalled data\n        .. versionchanged:: 1.0.0\n            Renamed from ``marshal``.\n        \"\"\"\n        if many and obj is not None:\n            self._pending = True\n            ret = [self.serialize(d, fields_dict, many=False,\n                                    dict_class=dict_class, accessor=accessor,\n                                    index=idx, index_errors=index_errors)\n                    for idx, d in enumerate(obj)]\n            self._pending = False\n            if self.errors:\n                raise ValidationError(\n                    self.errors,\n                    field_names=self.error_field_names,\n                    fields=self.error_fields,\n                    data=ret,\n                )\n            return ret\n        items = []\n        for attr_name, field_obj in iteritems(fields_dict):\n            if getattr(field_obj, 'load_only', False):\n                continue\n            key = ''.join([self.prefix or '', field_obj.dump_to or attr_name])\n            getter = lambda d: field_obj.serialize(attr_name, d, accessor=accessor)\n            value = self.call_and_store(\n                getter_func=getter,\n                data=obj,\n                field_name=key,\n                field_obj=field_obj,\n                index=(index if index_errors else None)\n            )\n            if value is missing:\n                continue\n            items.append((key, value))\n        ret = dict_class(items)\n        if self.errors and not self._pending:\n            raise ValidationError(\n                self.errors,\n                field_names=self.error_field_names,\n                fields=self.error_fields,\n                data=ret\n            )\n        return ret\n    # Make an instance callable\n    __call__ = serialize\n# Key used for schema-level validation errors\nSCHEMA = '_schema'\nclass Unmarshaller(ErrorStore):\n    \"\"\"Callable class responsible for deserializing data and storing errors.\n    .. versionadded:: 1.0.0\n    \"\"\"\n    default_schema_validation_error = 'Invalid data.'\n    def run_validator(self, validator_func, output,\n            original_data, fields_dict, index=None,\n            many=False, pass_original=False):\n        try:\n            if pass_original:  # Pass original, raw data (before unmarshalling)\n                res = validator_func(output, original_data)\n            else:\n                res = validator_func(output)\n            if res is False:\n                raise ValidationError(self.default_schema_validation_error)\n        except ValidationError as err:\n            errors = self.get_errors(index=index)\n            self.error_kwargs.update(err.kwargs)\n            # Store or reraise errors\n            if err.field_names:\n                field_names = err.field_names\n                field_objs = [fields_dict[each] if each in fields_dict else None\n                              for each in field_names]\n            else:\n                field_names = [SCHEMA]\n                field_objs = []\n            self.error_field_names = field_names\n            self.error_fields = field_objs\n            for field_name in field_names:\n                if isinstance(err.messages, (list, tuple)):\n                    # self.errors[field_name] may be a dict if schemas are nested\n                    if isinstance(errors.get(field_name), dict):\n                        errors[field_name].setdefault(\n                            SCHEMA, []\n                        ).extend(err.messages)\n                    else:\n                        errors.setdefault(field_name, []).extend(err.messages)\n                elif isinstance(err.messages, dict):\n                    errors.setdefault(field_name, []).append(err.messages)\n                else:\n                    errors.setdefault(field_name, []).append(text_type(err))\n    def deserialize(self, data, fields_dict, many=False, partial=False,\n            dict_class=dict, index_errors=True, index=None):\n        \"\"\"Deserialize ``data`` based on the schema defined by ``fields_dict``.\n        :param dict data: The data to deserialize.\n        :param dict fields_dict: Mapping of field names to :class:`Field` objects.\n        :param bool many: Set to `True` if ``data`` should be deserialized as\n            a collection.\n        :param bool|tuple partial: Whether to ignore missing fields. If its\n            value is an iterable, only missing fields listed in that iterable\n            will be ignored.\n        :param type dict_class: Dictionary class used to construct the output.\n        :param bool index_errors: Whether to store the index of invalid items in\n            ``self.errors`` when ``many=True``.\n        :param int index: Index of the item being serialized (for storing errors) if\n            serializing a collection, otherwise `None`.\n        :return: A dictionary of the deserialized data.\n        \"\"\"\n        if many and data is not None:\n            if not is_collection(data):\n                errors = self.get_errors(index=index)\n                self.error_field_names.append(SCHEMA)\n                errors[SCHEMA] = ['Invalid input type.']\n                ret = []\n            else:\n                self._pending = True\n                ret = [self.deserialize(d, fields_dict, many=False,\n                            partial=partial, dict_class=dict_class,\n                            index=idx, index_errors=index_errors)\n                        for idx, d in enumerate(data)]\n                self._pending = False\n                if self.errors:\n                    raise ValidationError(\n                        self.errors,\n                        field_names=self.error_field_names,\n                        fields=self.error_fields,\n                        data=ret,\n                    )\n            return ret\n        ret = dict_class()\n        if not isinstance(data, collections.Mapping):\n            errors = self.get_errors(index=index)\n            msg = 'Invalid input type.'\n            self.error_field_names = [SCHEMA]\n            errors = self.get_errors()\n            errors.setdefault(SCHEMA, []).append(msg)\n            return None\n        else:\n            partial_is_collection = is_collection(partial)\n            for attr_name, field_obj in iteritems(fields_dict):\n                if field_obj.dump_only:\n                    continue\n                raw_value = data.get(attr_name, missing)\n                field_name = attr_name\n                if raw_value is missing and field_obj.load_from:\n                    field_name = field_obj.load_from\n                    raw_value = data.get(field_obj.load_from, missing)\n                if raw_value is missing:\n                    # Ignore missing field if we're allowed to.\n                    if (\n                        partial is True or\n                        (partial_is_collection and attr_name in partial)\n                    ):\n                        continue\n                    _miss = field_obj.missing\n                    raw_value = _miss() if callable(_miss) else _miss\n                if raw_value is missing and not field_obj.required:\n                    continue\n                getter = lambda val: field_obj.deserialize(\n                    val,\n                    field_obj.load_from or attr_name,\n                    data\n                )\n                value = self.call_and_store(\n                    getter_func=getter,\n                    data=raw_value,\n                    field_name=field_name,\n                    field_obj=field_obj,\n                    index=(index if index_errors else None)\n                )\n                if value is not missing:\n                    key = fields_dict[attr_name].attribute or attr_name\n                    set_value(ret, key, value)\n        if self.errors and not self._pending:\n            raise ValidationError(\n                self.errors,\n                field_names=self.error_field_names,\n                fields=self.error_fields,\n                data=ret,\n            )\n        return ret\n    # Make an instance callable\n    __call__ = deserialize",
        "file_path": "src/marshmallow/marshalling.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 4,
        "score": 0.6047385931015015,
        "content": "# -*- coding: utf-8 -*-\n\"\"\"Field classes for various types of data.\"\"\"\nfrom __future__ import absolute_import, unicode_literals\nimport copy\nimport datetime as dt\nimport uuid\nimport warnings\nimport decimal\nfrom marshmallow import validate, utils, class_registry\nfrom marshmallow.base import FieldABC, SchemaABC\nfrom marshmallow.utils import missing as missing_\nfrom marshmallow.compat import text_type, basestring, Mapping\nfrom marshmallow.exceptions import ValidationError\nfrom marshmallow.validate import Validator\nfrom marshmallow.warnings import RemovedInMarshmallow3Warning\n__all__ = [\n    'Field',\n    'Raw',\n    'Nested',\n    'Dict',\n    'List',\n    'String',\n    'UUID',\n    'Number',\n    'Integer',\n    'Decimal',\n    'Boolean',\n    'FormattedString',\n    'Float',\n    'DateTime',\n    'LocalDateTime',\n    'Time',\n    'Date',\n    'TimeDelta',\n    'Url',\n    'URL',\n    'Email',\n    'Method',\n    'Function',\n    'Str',\n    'Bool',\n    'Int',\n    'Constant',\n]\nMISSING_ERROR_MESSAGE = (\n    'ValidationError raised by `{class_name}`, but error key `{key}` does '\n    'not exist in the `error_messages` dictionary.'\n)\n_RECURSIVE_NESTED = 'self'\nclass Field(FieldABC):\n    \"\"\"Basic field from which other fields should extend. It applies no\n    formatting by default, and should only be used in cases where\n    data does not need to be formatted before being serialized or deserialized.\n    On error, the name of the field will be returned.\n    :param default: If set, this value will be used during serialization if the input value\n        is missing. If not set, the field will be excluded from the serialized output if the\n        input value is missing. May be a value or a callable.\n    :param str attribute: The name of the attribute to get the value from. If\n        `None`, assumes the attribute has the same name as the field.\n    :param str load_from: Additional key to look for when deserializing. Will only\n        be checked if the field's name is not found on the input dictionary. If checked,\n        it will return this parameter on error.\n    :param str dump_to: Field name to use as a key when serializing.\n    :param callable validate: Validator or collection of validators that are called\n        during deserialization. Validator takes a field's input value as\n        its only parameter and returns a boolean.\n        If it returns `False`, an :exc:`ValidationError` is raised.\n    :param required: Raise a :exc:`ValidationError` if the field value\n        is not supplied during deserialization.\n    :param allow_none: Set this to `True` if `None` should be considered a valid value during\n        validation/deserialization. If ``missing=None`` and ``allow_none`` is unset,\n        will default to ``True``. Otherwise, the default is ``False``.\n    :param bool load_only: If `True` skip this field during serialization, otherwise\n        its value will be present in the serialized data.\n    :param bool dump_only: If `True` skip this field during deserialization, otherwise\n        its value will be present in the deserialized object. In the context of an\n        HTTP API, this effectively marks the field as \"read-only\".\n    :param missing: Default deserialization value for the field if the field is not\n        found in the input data. May be a value or a callable.\n    :param dict error_messages: Overrides for `Field.default_error_messages`.\n    :param metadata: Extra arguments to be stored as metadata.\n    .. versionchanged:: 2.0.0\n        Removed `error` parameter. Use ``error_messages`` instead.\n    .. versionchanged:: 2.0.0\n        Added `allow_none` parameter, which makes validation/deserialization of `None`\n        consistent across fields.\n    .. versionchanged:: 2.0.0\n        Added `load_only` and `dump_only` parameters, which allow field skipping\n        during the (de)serialization process.\n    .. versionchanged:: 2.0.0\n        Added `missing` parameter, which indicates the value for a field if the field\n        is not found during deserialization.\n    .. versionchanged:: 2.0.0\n        ``default`` value is only used if explicitly set. Otherwise, missing values\n        inputs are excluded from serialized output.\n    \"\"\"\n    # Some fields, such as Method fields and Function fields, are not expected\n    #  to exists as attributes on the objects to serialize. Set this to False\n    #  for those fields\n    _CHECK_ATTRIBUTE = True\n    _creation_index = 0  # Used for sorting\n    #: Default error messages for various kinds of errors. The keys in this dictionary\n    #: are passed to `Field.fail`. The values are error messages passed to\n    #: :exc:`marshmallow.ValidationError`.\n    default_error_messages = {\n        'required': 'Missing data for required field.',\n        'type': 'Invalid type.',  # used by Unmarshaller\n        'null': 'Field may not be null.',\n        'validator_failed': 'Invalid value.'\n    }\n    def __init__(self, default=missing_, attribute=None, load_from=None, dump_to=None,\n                 error=None, validate=None, required=False, allow_none=None, load_only=False,\n                 dump_only=False, missing=missing_, error_messages=None, **metadata):\n        self.default = default\n        self.attribute = attribute\n        self.load_from = load_from  # this flag is used by Unmarshaller\n        self.dump_to = dump_to  # this flag is used by Marshaller\n        self.validate = validate\n        if utils.is_iterable_but_not_string(validate):\n            if not utils.is_generator(validate):\n                self.validators = validate\n            else:\n                self.validators = list(validate)\n        elif callable(validate):\n            self.validators = [validate]\n        elif validate is None:\n            self.validators = []\n        else:\n            raise ValueError(\"The 'validate' parameter must be a callable \"\n                             \"or a collection of callables.\")\n        self.required = required\n        # If missing=None, None should be considered valid by default\n        if allow_none is None:\n            if missing is None:\n                self.allow_none = True\n            else:\n                self.allow_none = False\n        else:\n            self.allow_none = allow_none\n        self.load_only = load_only\n        self.dump_only = dump_only\n        self.missing = missing\n        self.metadata = metadata\n        self._creation_index = Field._creation_index\n        Field._creation_index += 1\n        # Collect default error message from self and parent classes\n        messages = {}\n        for cls in reversed(self.__class__.__mro__):\n            messages.update(getattr(cls, 'default_error_messages', {}))\n        messages.update(error_messages or {})\n        self.error_messages = messages\n    def __repr__(self):\n        return ('<fields.{ClassName}(default={self.default!r}, '\n                'attribute={self.attribute!r}, '\n                'validate={self.validate}, required={self.required}, '\n                'load_only={self.load_only}, dump_only={self.dump_only}, '\n                'missing={self.missing}, allow_none={self.allow_none}, '\n                'error_messages={self.error_messages})>'\n                .format(ClassName=self.__class__.__name__, self=self))\n    def get_value(self, attr, obj, accessor=None, default=missing_):\n        \"\"\"Return the value for a given key from an object.\"\"\"\n        # NOTE: Use getattr instead of direct attribute access here so that\n        # subclasses aren't required to define `attribute` member\n        attribute = getattr(self, 'attribute', None)\n        accessor_func = accessor or utils.get_value\n        check_key = attr if attribute is None else attribute\n        return accessor_func(check_key, obj, default)\n    def _validate(self, value):\n        \"\"\"Perform validation on ``value``. Raise a :exc:`ValidationError` if validation\n        does not succeed.\n        \"\"\"\n        errors = []\n        kwargs = {}\n        for validator in self.validators:\n            try:\n                r = validator(value)\n                if not isinstance(validator, Validator) and r is False:\n                    self.fail('validator_failed')\n            except ValidationError as err:\n                kwargs.update(err.kwargs)\n                if isinstance(err.messages, dict):\n                    errors.append(err.messages)\n                else:\n                    errors.extend(err.messages)\n        if errors:\n            raise ValidationError(errors, **kwargs)\n    # Hat tip to django-rest-framework.\n    def fail(self, key, **kwargs):\n        \"\"\"A helper method that simply raises a `ValidationError`.\n        \"\"\"\n        try:\n            msg = self.error_messages[key]\n        except KeyError:\n            class_name = self.__class__.__name__\n            msg = MISSING_ERROR_MESSAGE.format(class_name=class_name, key=key)\n            raise AssertionError(msg)\n        if isinstance(msg, basestring):\n            msg = msg.format(**kwargs)\n        raise ValidationError(msg)\n    def _validate_missing(self, value):\n        \"\"\"Validate missing values. Raise a :exc:`ValidationError` if\n        `value` should be considered missing.\n        \"\"\"\n        if value is missing_:\n            if hasattr(self, 'required') and self.required:\n                self.fail('required')\n        if value is None:\n            if hasattr(self, 'allow_none') and self.allow_none is not True:\n                self.fail('null')\n    def serialize(self, attr, obj, accessor=None):\n        \"\"\"Pulls the value for the given key from the object, applies the\n        field's formatting and returns the result.\n        :param str attr: The attibute or key to get from the object.\n        :param str obj: The object to pull the key from.\n        :param callable accessor: Function used to pull values from ``obj``.\n        :raise ValidationError: In case of formatting problem\n        \"\"\"\n        if self._CHECK_ATTRIBUTE:\n            value = self.get_value(attr, obj, accessor=accessor)\n            if value is missing_:\n                if hasattr(self, 'default'):\n                    if callable(self.default):\n                        return self.default()\n                    else:\n                        return self.default\n        else:\n            value = None\n        return self._serialize(value, attr, obj)\n    def deserialize(self, value, attr=None, data=None):\n        \"\"\"Deserialize ``value``.\n        :raise ValidationError: If an invalid value is passed or if a required value\n            is missing.\n        \"\"\"\n        # Validate required fields, deserialize, then validate\n        # deserialized value\n        self._validate_missing(value)\n        if getattr(self, 'allow_none', False) is True and value is None:\n            return None\n        output = self._deserialize(value, attr, data)\n        self._validate(output)\n        return output\n    # Methods for concrete classes to override.\n    def _add_to_schema(self, field_name, schema):\n        \"\"\"Update field with values from its parent schema. Called by\n            :meth:`__set_field_attrs <marshmallow.Schema.__set_field_attrs>`.\n        :param str field_name: Field name set in schema.\n        :param Schema schema: Parent schema.\n        \"\"\"\n        self.parent = self.parent or schema\n        self.name = self.name or field_name\n    def _serialize(self, value, attr, obj):\n        \"\"\"Serializes ``value`` to a basic Python datatype. Noop by default.\n        Concrete :class:`Field` classes should implement this method.\n        Example: ::\n            class TitleCase(Field):\n                def _serialize(self, value, attr, obj):\n                    if not value:\n                        return ''\n                    return unicode(value).title()\n        :param value: The value to be serialized.\n        :param str attr: The attribute or key on the object to be serialized.\n        :param object obj: The object the value was pulled from.\n        :raise ValidationError: In case of formatting or validation failure.\n        :return: The serialized value\n        \"\"\"\n        return value\n    def _deserialize(self, value, attr, data):\n        \"\"\"Deserialize value. Concrete :class:`Field` classes should implement this method.\n        :param value: The value to be deserialized.\n        :param str attr: The attribute/key in `data` to be deserialized.\n        :param dict data: The raw input data passed to the `Schema.load`.\n        :raise ValidationError: In case of formatting or validation failure.\n        :return: The deserialized value.\n        .. versionchanged:: 2.0.0\n            Added ``attr`` and ``data`` parameters.\n        \"\"\"\n        return value\n    # Properties\n    @property\n    def context(self):\n        \"\"\"The context dictionary for the parent :class:`Schema`.\"\"\"\n        return self.parent.context\n    @property\n    def root(self):\n        \"\"\"Reference to the `Schema` that this field belongs to even if it is buried in a `List`.\n        Return `None` for unbound fields.\n        \"\"\"\n        ret = self\n        while hasattr(ret, 'parent') and ret.parent:\n            ret = ret.parent\n        return ret if isinstance(ret, SchemaABC) else None\nclass Raw(Field):\n    \"\"\"Field that applies no formatting or validation.\"\"\"\n    pass\nclass Nested(Field):\n    \"\"\"Allows you to nest a :class:`Schema <marshmallow.Schema>`\n    inside a field.\n    Examples: ::\n        user = fields.Nested(UserSchema)\n        user2 = fields.Nested('UserSchema')  # Equivalent to above\n        collaborators = fields.Nested(UserSchema, many=True, only='id')\n        parent = fields.Nested('self')\n    When passing a `Schema <marshmallow.Schema>` instance as the first argument,\n    the instance's ``exclude``, ``only``, and ``many`` attributes will be respected.\n    Therefore, when passing the ``exclude``, ``only``, or ``many`` arguments to `fields.Nested`,\n    you should pass a `Schema <marshmallow.Schema>` class (not an instance) as the first argument.\n    ::\n        # Yes\n        author = fields.Nested(UserSchema, only=('id', 'name'))\n        # No\n        author = fields.Nested(UserSchema(), only=('id', 'name'))\n    :param Schema nested: The Schema class or class name (string)\n        to nest, or ``\"self\"`` to nest the :class:`Schema` within itself.\n    :param tuple exclude: A list or tuple of fields to exclude.\n    :param required: Raise an :exc:`ValidationError` during deserialization\n        if the field, *and* any required field values specified\n        in the `nested` schema, are not found in the data. If not a `bool`\n        (e.g. a `str`), the provided value will be used as the message of the\n        :exc:`ValidationError` instead of the default message.\n    :param only: A tuple or string of the field(s) to marshal. If `None`, all fields\n        will be marshalled. If a field name (string) is given, only a single\n        value will be returned as output instead of a dictionary.\n        This parameter takes precedence over ``exclude``.\n    :param bool many: Whether the field is a collection of objects.\n    :param kwargs: The same keyword arguments that :class:`Field` receives.\n    \"\"\"\n    def __init__(self, nested, default=missing_, exclude=tuple(), only=None, **kwargs):\n        self.nested = nested\n        self.only = only\n        self.exclude = exclude\n        self.many = kwargs.get('many', False)\n        self.__schema = None  # Cached Schema instance\n        self.__updated_fields = False\n        super(Nested, self).__init__(default=default, **kwargs)\n    @property\n    def schema(self):\n        \"\"\"The nested Schema object.\n        .. versionchanged:: 1.0.0\n            Renamed from `serializer` to `schema`\n        \"\"\"\n        if not self.__schema:\n            # Ensure that only parameter is a tuple\n            if isinstance(self.only, basestring):\n                only = (self.only,)\n            else:\n                only = self.only\n            # Inherit context from parent.\n            context = getattr(self.parent, 'context', {})\n            if isinstance(self.nested, SchemaABC):\n                self.__schema = self.nested\n                self.__schema.context.update(context)\n            elif isinstance(self.nested, type) and \\\n                    issubclass(self.nested, SchemaABC):\n                self.__schema = self.nested(many=self.many,\n                        only=only, exclude=self.exclude, context=context,\n                        load_only=self._nested_normalized_option('load_only'),\n                        dump_only=self._nested_normalized_option('dump_only'))\n            elif isinstance(self.nested, basestring):\n                if self.nested == _RECURSIVE_NESTED:\n                    parent_class = self.parent.__class__\n                    self.__schema = parent_class(many=self.many, only=only,\n                            exclude=self.exclude, context=context,\n                            load_only=self._nested_normalized_option('load_only'),\n                            dump_only=self._nested_normalized_option('dump_only'))\n                else:\n                    schema_class = class_registry.get_class(self.nested)\n                    self.__schema = schema_class(many=self.many,\n                            only=only, exclude=self.exclude, context=context,\n                            load_only=self._nested_normalized_option('load_only'),\n                            dump_only=self._nested_normalized_option('dump_only'))\n            else:\n                raise ValueError('Nested fields must be passed a '\n                                 'Schema, not {0}.'.format(self.nested.__class__))\n            self.__schema.ordered = getattr(self.parent, 'ordered', False)\n        return self.__schema\n    def _nested_normalized_option(self, option_name):\n        nested_field = '%s.' % self.name\n        return [field.split(nested_field, 1)[1]\n                for field in getattr(self.root, option_name, set())\n                if field.startswith(nested_field)]\n    def _serialize(self, nested_obj, attr, obj):\n        # Load up the schema first. This allows a RegistryError to be raised\n        # if an invalid schema name was passed\n        schema = self.schema\n        if nested_obj is None:\n            return None\n        if self.many and utils.is_iterable_but_not_string(nested_obj):\n            nested_obj = list(nested_obj)\n        if not self.__updated_fields:\n            schema._update_fields(obj=nested_obj, many=self.many)\n            self.__updated_fields = True\n        ret, errors = schema.dump(nested_obj, many=self.many,\n                update_fields=not self.__updated_fields)\n        if isinstance(self.only, basestring):  # self.only is a field name\n            only_field = self.schema.fields[self.only]\n            key = ''.join([self.schema.prefix or '', only_field.dump_to or self.only])\n            if self.many:\n                return utils.pluck(ret, key=key)\n            else:\n                return ret[key]\n        if errors:\n            raise ValidationError(errors, data=ret)\n        return ret\n    def _deserialize(self, value, attr, data):\n        if self.many and not utils.is_collection(value):\n            self.fail('type', input=value, type=value.__class__.__name__)\n        data, errors = self.schema.load(value)\n        if errors:\n            raise ValidationError(errors, data=data)\n        return data\n    def _validate_missing(self, value):\n        \"\"\"Validate missing values. Raise a :exc:`ValidationError` if\n        `value` should be considered missing.\n        \"\"\"\n        if value is missing_ and hasattr(self, 'required'):\n            if self.nested == _RECURSIVE_NESTED:\n                self.fail('required')\n            errors = self._check_required()\n            if errors:\n                raise ValidationError(errors)\n        else:\n            super(Nested, self)._validate_missing(value)\n    def _check_required(self):\n        errors = {}\n        if self.required:\n            for field_name, field in self.schema.fields.items():\n                if not field.required:\n                    continue\n                error_field_name = field.load_from or field_name\n                if (\n                    isinstance(field, Nested) and\n                    self.nested != _RECURSIVE_NESTED and\n                    field.nested != _RECURSIVE_NESTED\n                ):\n                    errors[error_field_name] = field._check_required()\n                else:\n                    try:\n                        field._validate_missing(field.missing)\n                    except ValidationError as ve:\n                        errors[error_field_name] = ve.messages\n            if self.many and errors:\n                errors = {0: errors}\n            # No inner errors; just raise required error like normal\n            if not errors:\n                self.fail('required')\n        return errors\nclass List(Field):\n    \"\"\"A list field, composed with another `Field` class or\n    instance.\n    Example: ::\n        numbers = fields.List(fields.Float())\n    :param Field cls_or_instance: A field class or instance.\n    :param bool default: Default value for serialization.\n    :param kwargs: The same keyword arguments that :class:`Field` receives.\n    .. versionchanged:: 2.0.0\n        The ``allow_none`` parameter now applies to deserialization and\n        has the same semantics as the other fields.\n    \"\"\"\n    default_error_messages = {\n        'invalid': 'Not a valid list.',\n    }\n    def __init__(self, cls_or_instance, **kwargs):\n        super(List, self).__init__(**kwargs)\n        if isinstance(cls_or_instance, type):\n            if not issubclass(cls_or_instance, FieldABC):\n                raise ValueError('The type of the list elements '\n                                           'must be a subclass of '\n                                           'marshmallow.base.FieldABC')\n            self.container = cls_or_instance()\n        else:\n            if not isinstance(cls_or_instance, FieldABC):\n                raise ValueError('The instances of the list '\n                                           'elements must be of type '\n                                           'marshmallow.base.FieldABC')\n            self.container = cls_or_instance\n    def get_value(self, attr, obj, accessor=None):\n        \"\"\"Return the value for a given key from an object.\"\"\"\n        value = super(List, self).get_value(attr, obj, accessor=accessor)\n        if self.container.attribute:\n            if utils.is_collection(value):\n                return [\n                    self.container.get_value(self.container.attribute, each)\n                    for each in value\n                ]\n            return self.container.get_value(self.container.attribute, value)\n        return value\n    def _add_to_schema(self, field_name, schema):\n        super(List, self)._add_to_schema(field_name, schema)\n        self.container = copy.deepcopy(self.container)\n        self.container.parent = self\n        self.container.name = field_name\n    def _serialize(self, value, attr, obj):\n        if value is None:\n            return None\n        if utils.is_collection(value):\n            return [self.container._serialize(each, attr, obj) for each in value]\n        return [self.container._serialize(value, attr, obj)]\n    def _deserialize(self, value, attr, data):\n        if not utils.is_collection(value):\n            self.fail('invalid')",
        "file_path": "src/marshmallow/fields.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 5,
        "score": 0.5948401093482971,
        "content": "# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nfrom marshmallow.schema import (\n    Schema,\n    SchemaOpts,\n    MarshalResult,\n    UnmarshalResult,\n)\nfrom . import fields\nfrom marshmallow.decorators import (\n    pre_dump, post_dump, pre_load, post_load, validates, validates_schema\n)\nfrom marshmallow.utils import pprint, missing\nfrom marshmallow.exceptions import ValidationError\nfrom distutils.version import LooseVersion\n__version__ = '2.20.0'\n__version_info__ = tuple(LooseVersion(__version__).version)\n__author__ = 'Steven Loria'\n__all__ = [\n    'Schema',\n    'SchemaOpts',\n    'fields',\n    'validates',\n    'validates_schema',\n    'pre_dump',\n    'post_dump',\n    'pre_load',\n    'post_load',\n    'pprint',\n    'MarshalResult',\n    'UnmarshalResult',\n    'ValidationError',\n    'missing',\n]",
        "file_path": "src/marshmallow/__init__.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 6,
        "score": 0.5907884240150452,
        "content": "        if errors:\n            raise ValidationError(errors, data=ret)\n        return ret\n    def _deserialize(self, value, attr, data):\n        if self.many and not utils.is_collection(value):\n            self.fail('type', input=value, type=value.__class__.__name__)\n        data, errors = self.schema.load(value)\n        if errors:\n            raise ValidationError(errors, data=data)\n        return data\n    def _validate_missing(self, value):\n        \"\"\"Validate missing values. Raise a :exc:`ValidationError` if\n        `value` should be considered missing.\n        \"\"\"\n        if value is missing_ and hasattr(self, 'required'):\n            if self.nested == _RECURSIVE_NESTED:\n                self.fail('required')\n            errors = self._check_required()\n            if errors:\n                raise ValidationError(errors)\n        else:\n            super(Nested, self)._validate_missing(value)\n    def _check_required(self):\n        errors = {}\n        if self.required:\n            for field_name, field in self.schema.fields.items():\n                if not field.required:\n                    continue\n                error_field_name = field.load_from or field_name\n                if (\n                    isinstance(field, Nested) and\n                    self.nested != _RECURSIVE_NESTED and\n                    field.nested != _RECURSIVE_NESTED\n                ):\n                    errors[error_field_name] = field._check_required()\n                else:\n                    try:\n                        field._validate_missing(field.missing)\n                    except ValidationError as ve:\n                        errors[error_field_name] = ve.messages\n            if self.many and errors:\n                errors = {0: errors}\n            # No inner errors; just raise required error like normal\n            if not errors:\n                self.fail('required')\n        return errors\nclass List(Field):\n    \"\"\"A list field, composed with another `Field` class or\n    instance.\n    Example: ::\n        numbers = fields.List(fields.Float())\n    :param Field cls_or_instance: A field class or instance.\n    :param bool default: Default value for serialization.\n    :param kwargs: The same keyword arguments that :class:`Field` receives.\n    .. versionchanged:: 2.0.0\n        The ``allow_none`` parameter now applies to deserialization and\n        has the same semantics as the other fields.\n    \"\"\"\n    default_error_messages = {\n        'invalid': 'Not a valid list.',\n    }\n    def __init__(self, cls_or_instance, **kwargs):\n        super(List, self).__init__(**kwargs)\n        if isinstance(cls_or_instance, type):\n            if not issubclass(cls_or_instance, FieldABC):\n                raise ValueError('The type of the list elements '\n                                           'must be a subclass of '\n                                           'marshmallow.base.FieldABC')\n            self.container = cls_or_instance()\n        else:\n            if not isinstance(cls_or_instance, FieldABC):\n                raise ValueError('The instances of the list '\n                                           'elements must be of type '\n                                           'marshmallow.base.FieldABC')\n            self.container = cls_or_instance\n    def get_value(self, attr, obj, accessor=None):\n        \"\"\"Return the value for a given key from an object.\"\"\"\n        value = super(List, self).get_value(attr, obj, accessor=accessor)\n        if self.container.attribute:\n            if utils.is_collection(value):\n                return [\n                    self.container.get_value(self.container.attribute, each)\n                    for each in value\n                ]\n            return self.container.get_value(self.container.attribute, value)\n        return value\n    def _add_to_schema(self, field_name, schema):\n        super(List, self)._add_to_schema(field_name, schema)\n        self.container = copy.deepcopy(self.container)\n        self.container.parent = self\n        self.container.name = field_name\n    def _serialize(self, value, attr, obj):\n        if value is None:\n            return None\n        if utils.is_collection(value):\n            return [self.container._serialize(each, attr, obj) for each in value]\n        return [self.container._serialize(value, attr, obj)]\n    def _deserialize(self, value, attr, data):\n        if not utils.is_collection(value):\n            self.fail('invalid')\n        result = []\n        errors = {}\n        for idx, each in enumerate(value):\n            try:\n                result.append(self.container.deserialize(each))\n            except ValidationError as e:\n                result.append(e.data)\n                errors.update({idx: e.messages})\n        if errors:\n            raise ValidationError(errors, data=result)\n        return result\nclass String(Field):\n    \"\"\"A string field.\n    :param kwargs: The same keyword arguments that :class:`Field` receives.\n    \"\"\"\n    default_error_messages = {\n        'invalid': 'Not a valid string.',\n        'invalid_utf8': 'Not a valid utf-8 string.'\n    }\n    def _serialize(self, value, attr, obj):\n        if value is None:\n            return None\n        return utils.ensure_text_type(value)\n    def _deserialize(self, value, attr, data):\n        if not isinstance(value, basestring):\n            self.fail('invalid')\n        try:\n            return utils.ensure_text_type(value)\n        except UnicodeDecodeError:\n            self.fail('invalid_utf8')\nclass UUID(String):\n    \"\"\"A UUID field.\"\"\"\n    default_error_messages = {\n        'invalid_uuid': 'Not a valid UUID.',\n        'invalid_guid': 'Not a valid UUID.'  # TODO: Remove this in marshmallow 3.0\n    }\n    def _validated(self, value):\n        \"\"\"Format the value or raise a :exc:`ValidationError` if an error occurs.\"\"\"\n        if value is None:\n            return None\n        if isinstance(value, uuid.UUID):\n            return value\n        try:\n            return uuid.UUID(value)\n        except (ValueError, AttributeError):\n            self.fail('invalid_uuid')\n    def _serialize(self, value, attr, obj):\n        validated = str(self._validated(value)) if value is not None else None\n        return super(String, self)._serialize(validated, attr, obj)\n    def _deserialize(self, value, attr, data):\n        return self._validated(value)\nclass Number(Field):\n    \"\"\"Base class for number fields.\n    :param bool as_string: If True, format the serialized value as a string.\n    :param kwargs: The same keyword arguments that :class:`Field` receives.\n    \"\"\"\n    num_type = float\n    default_error_messages = {\n        'invalid': 'Not a valid number.',\n        'too_large': 'Number too large.',\n    }\n    def __init__(self, as_string=False, **kwargs):\n        self.as_string = as_string\n        super(Number, self).__init__(**kwargs)\n    def _format_num(self, value):\n        \"\"\"Return the number value for value, given this field's `num_type`.\"\"\"\n        if value is None:\n            return None\n        return self.num_type(value)\n    def _validated(self, value):\n        \"\"\"Format the value or raise a :exc:`ValidationError` if an error occurs.\"\"\"\n        try:\n            return self._format_num(value)\n        except (TypeError, ValueError):\n            self.fail('invalid')\n        except OverflowError:\n            self.fail('too_large')\n    def _to_string(self, value):\n        return str(value)\n    def _serialize(self, value, attr, obj):\n        \"\"\"Return a string if `self.as_string=True`, otherwise return this field's `num_type`.\"\"\"\n        ret = self._validated(value)\n        return self._to_string(ret) if (self.as_string and ret not in (None, missing_)) else ret\n    def _deserialize(self, value, attr, data):\n        return self._validated(value)\nclass Integer(Number):\n    \"\"\"An integer field.\n    :param kwargs: The same keyword arguments that :class:`Number` receives.\n    \"\"\"\n    num_type = int\n    default_error_messages = {\n        'invalid': 'Not a valid integer.'\n    }\nclass Decimal(Number):\n    \"\"\"A field that (de)serializes to the Python ``decimal.Decimal`` type.\n    It's safe to use when dealing with money values, percentages, ratios\n    or other numbers where precision is critical.\n    .. warning::\n        This field serializes to a `decimal.Decimal` object by default. If you need\n        to render your data as JSON, keep in mind that the `json` module from the\n        standard library does not encode `decimal.Decimal`. Therefore, you must use\n        a JSON library that can handle decimals, such as `simplejson`, or serialize\n        to a string by passing ``as_string=True``.\n    .. warning::\n        If a JSON `float` value is passed to this field for deserialization it will\n        first be cast to its corresponding `string` value before being deserialized\n        to a `decimal.Decimal` object. The default `__str__` implementation of the\n        built-in Python `float` type may apply a destructive transformation upon\n        its input data and therefore cannot be relied upon to preserve precision.\n        To avoid this, you can instead pass a JSON `string` to be deserialized\n        directly.\n    :param int places: How many decimal places to quantize the value. If `None`, does\n        not quantize the value.\n    :param rounding: How to round the value during quantize, for example\n        `decimal.ROUND_UP`. If None, uses the rounding value from\n        the current thread's context.\n    :param bool allow_nan: If `True`, `NaN`, `Infinity` and `-Infinity` are allowed,\n        even though they are illegal according to the JSON specification.\n    :param bool as_string: If True, serialize to a string instead of a Python\n        `decimal.Decimal` type.\n    :param kwargs: The same keyword arguments that :class:`Number` receives.\n    .. versionadded:: 1.2.0\n    \"\"\"\n    num_type = decimal.Decimal\n    default_error_messages = {\n        'special': 'Special numeric values are not permitted.',\n    }\n    def __init__(self, places=None, rounding=None, allow_nan=False, as_string=False, **kwargs):\n        self.places = decimal.Decimal((0, (1,), -places)) if places is not None else None\n        self.rounding = rounding\n        self.allow_nan = allow_nan\n        super(Decimal, self).__init__(as_string=as_string, **kwargs)\n    # override Number\n    def _format_num(self, value):\n        if value is None:\n            return None\n        num = decimal.Decimal(str(value))\n        if self.allow_nan:\n            if num.is_nan():\n                return decimal.Decimal('NaN')  # avoid sNaN, -sNaN and -NaN\n        else:\n            if num.is_nan() or num.is_infinite():\n                self.fail('special')\n        if self.places is not None and num.is_finite():\n            num = num.quantize(self.places, rounding=self.rounding)\n        return num\n    # override Number\n    def _validated(self, value):\n        try:\n            return super(Decimal, self)._validated(value)\n        except decimal.InvalidOperation:\n            self.fail('invalid')\n    # override Number\n    def _to_string(self, value):\n        return format(value, 'f')\nclass Boolean(Field):\n    \"\"\"A boolean field.\n    :param kwargs: The same keyword arguments that :class:`Field` receives.\n    \"\"\"\n    #: Values that will (de)serialize to `True`. If an empty set, any non-falsy\n    #  value will deserialize to `True`.\n    truthy = set(('t', 'T', 'true', 'True', 'TRUE', '1', 1, True))\n    #: Values that will (de)serialize to `False`.\n    falsy = set(('f', 'F', 'false', 'False', 'FALSE', '0', 0, 0.0, False))\n    default_error_messages = {\n        'invalid': 'Not a valid boolean.'\n    }\n    def _serialize(self, value, attr, obj):\n        if value is None:\n            return None\n        elif value in self.truthy:\n            return True\n        elif value in self.falsy:\n            return False\n        return bool(value)\n    def _deserialize(self, value, attr, data):\n        if not self.truthy:\n            return bool(value)\n        else:\n            try:\n                if value in self.truthy:\n                    return True\n                elif value in self.falsy:\n                    return False\n            except TypeError:\n                pass\n        self.fail('invalid')\nclass FormattedString(Field):\n    \"\"\"Interpolate other values from the object into this field. The syntax for\n    the source string is the same as the string `str.format` method\n    from the python stdlib.\n    ::\n        class UserSchema(Schema):\n            name = fields.String()\n            greeting = fields.FormattedString('Hello {name}')\n        ser = UserSchema()\n        res = ser.dump(user)\n        res.data  # => {'name': 'Monty', 'greeting': 'Hello Monty'}\n    \"\"\"\n    default_error_messages = {\n        'format': 'Cannot format string with given data.'\n    }\n    _CHECK_ATTRIBUTE = False\n    def __init__(self, src_str, *args, **kwargs):\n        warnings.warn('FormattedString is deprecated and will be removed in marshmallow 3. '\n                      'Use a Method or Function field instead.', RemovedInMarshmallow3Warning)\n        Field.__init__(self, *args, **kwargs)\n        self.src_str = text_type(src_str)\n    def _serialize(self, value, attr, obj):\n        try:\n            data = utils.to_marshallable_type(obj)\n            return self.src_str.format(**data)\n        except (TypeError, IndexError):\n            self.fail('format')\nclass Float(Number):\n    \"\"\"\n    A double as IEEE-754 double precision string.\n    :param bool as_string: If True, format the value as a string.\n    :param kwargs: The same keyword arguments that :class:`Number` receives.\n    \"\"\"\n    num_type = float\nclass DateTime(Field):\n    \"\"\"A formatted datetime string in UTC.\n    Example: ``'2014-12-22T03:12:58.019077+00:00'``\n    Timezone-naive `datetime` objects are converted to\n    UTC (+00:00) by :meth:`Schema.dump <marshmallow.Schema.dump>`.\n    :meth:`Schema.load <marshmallow.Schema.load>` returns `datetime`\n    objects that are timezone-aware.\n    :param str format: Either ``\"rfc\"`` (for RFC822), ``\"iso\"`` (for ISO8601),\n        or a date format string. If `None`, defaults to \"iso\".\n    :param kwargs: The same keyword arguments that :class:`Field` receives.\n    \"\"\"\n    DATEFORMAT_SERIALIZATION_FUNCS = {\n        'iso': utils.isoformat,\n        'iso8601': utils.isoformat,\n        'rfc': utils.rfcformat,\n        'rfc822': utils.rfcformat,\n    }\n    DATEFORMAT_DESERIALIZATION_FUNCS = {\n        'iso': utils.from_iso,\n        'iso8601': utils.from_iso,\n        'rfc': utils.from_rfc,\n        'rfc822': utils.from_rfc,\n    }\n    DEFAULT_FORMAT = 'iso'\n    localtime = False\n    default_error_messages = {\n        'invalid': 'Not a valid datetime.',\n        'format': '\"{input}\" cannot be formatted as a datetime.',\n    }\n    def __init__(self, format=None, **kwargs):\n        super(DateTime, self).__init__(**kwargs)\n        # Allow this to be None. It may be set later in the ``_serialize``\n        # or ``_desrialize`` methods This allows a Schema to dynamically set the\n        # dateformat, e.g. from a Meta option\n        self.dateformat = format\n    def _add_to_schema(self, field_name, schema):\n        super(DateTime, self)._add_to_schema(field_name, schema)\n        self.dateformat = self.dateformat or schema.opts.dateformat\n    def _serialize(self, value, attr, obj):\n        if value is None:\n            return None\n        self.dateformat = self.dateformat or self.DEFAULT_FORMAT\n        format_func = self.DATEFORMAT_SERIALIZATION_FUNCS.get(self.dateformat, None)\n        if format_func:\n            try:\n                return format_func(value, localtime=self.localtime)\n            except (AttributeError, ValueError):\n                self.fail('format', input=value)\n        else:\n            return value.strftime(self.dateformat)\n    def _deserialize(self, value, attr, data):\n        if not value:  # Falsy values, e.g. '', None, [] are not valid\n            raise self.fail('invalid')\n        self.dateformat = self.dateformat or self.DEFAULT_FORMAT\n        func = self.DATEFORMAT_DESERIALIZATION_FUNCS.get(self.dateformat)\n        if func:\n            try:\n                return func(value)\n            except (TypeError, AttributeError, ValueError):\n                raise self.fail('invalid')\n        elif self.dateformat:\n            try:\n                return dt.datetime.strptime(value, self.dateformat)\n            except (TypeError, AttributeError, ValueError):\n                raise self.fail('invalid')\n        elif utils.dateutil_available:\n            try:\n                return utils.from_datestring(value)\n            except TypeError:\n                raise self.fail('invalid')\n        else:\n            warnings.warn('It is recommended that you install python-dateutil '\n                          'for improved datetime deserialization.')\n            raise self.fail('invalid')\nclass LocalDateTime(DateTime):\n    \"\"\"A formatted datetime string in localized time, relative to UTC.\n        ex. ``\"Sun, 10 Nov 2013 08:23:45 -0600\"``\n    Takes the same arguments as :class:`DateTime <marshmallow.fields.DateTime>`.\n    \"\"\"\n    localtime = True\nclass Time(Field):\n    \"\"\"ISO8601-formatted time string.\n    :param kwargs: The same keyword arguments that :class:`Field` receives.\n    \"\"\"\n    default_error_messages = {\n        'invalid': 'Not a valid time.',\n        'format': '\"{input}\" cannot be formatted as a time.',\n    }\n    def _serialize(self, value, attr, obj):\n        if value is None:\n            return None\n        try:\n            ret = value.isoformat()\n        except AttributeError:\n            self.fail('format', input=value)\n        if value.microsecond:\n            return ret[:15]\n        return ret\n    def _deserialize(self, value, attr, data):\n        \"\"\"Deserialize an ISO8601-formatted time to a :class:`datetime.time` object.\"\"\"\n        if not value:   # falsy values are invalid\n            self.fail('invalid')\n        try:\n            return utils.from_iso_time(value)\n        except (AttributeError, TypeError, ValueError):\n            self.fail('invalid')\nclass Date(Field):\n    \"\"\"ISO8601-formatted date string.\n    :param kwargs: The same keyword arguments that :class:`Field` receives.\n    \"\"\"\n    default_error_messages = {\n        'invalid': 'Not a valid date.',\n        'format': '\"{input}\" cannot be formatted as a date.',\n    }\n    def _serialize(self, value, attr, obj):\n        if value is None:\n            return None\n        try:\n            return value.isoformat()\n        except AttributeError:\n            self.fail('format', input=value)\n        return value\n    def _deserialize(self, value, attr, data):\n        \"\"\"Deserialize an ISO8601-formatted date string to a\n        :class:`datetime.date` object.\n        \"\"\"\n        if not value:  # falsy values are invalid\n            self.fail('invalid')\n        try:\n            return utils.from_iso_date(value)\n        except (AttributeError, TypeError, ValueError):\n            self.fail('invalid')\nclass TimeDelta(Field):\n    \"\"\"A field that (de)serializes a :class:`datetime.timedelta` object to an\n    integer and vice versa. The integer can represent the number of days,\n    seconds or microseconds.\n    :param str precision: Influences how the integer is interpreted during\n        (de)serialization. Must be 'days', 'seconds' or 'microseconds'.\n    :param str error: Error message stored upon validation failure.\n    :param kwargs: The same keyword arguments that :class:`Field` receives.\n    .. versionchanged:: 2.0.0\n        Always serializes to an integer value to avoid rounding errors.\n        Add `precision` parameter.\n    \"\"\"\n    DAYS = 'days'\n    SECONDS = 'seconds'\n    MICROSECONDS = 'microseconds'\n    default_error_messages = {\n        'invalid': 'Not a valid period of time.',\n        'format': '{input!r} cannot be formatted as a timedelta.'\n    }\n    def __init__(self, precision='seconds', error=None, **kwargs):\n        precision = precision.lower()\n        units = (self.DAYS, self.SECONDS, self.MICROSECONDS)\n        if precision not in units:\n            msg = 'The precision must be \"{0}\", \"{1}\" or \"{2}\".'.format(*units)\n            raise ValueError(msg)\n        self.precision = precision\n        super(TimeDelta, self).__init__(error=error, **kwargs)\n    def _serialize(self, value, attr, obj):\n        if value is None:\n            return None\n        try:\n            days = value.days\n            if self.precision == self.DAYS:\n                return days\n            else:\n                seconds = days * 86400 + value.seconds\n                if self.precision == self.SECONDS:\n                    return seconds\n                else:  # microseconds\n                    return seconds * 10**6 + value.microseconds  # flake8: noqa\n        except AttributeError:\n            self.fail('format', input=value)\n    def _deserialize(self, value, attr, data):\n        try:\n            value = int(value)\n        except (TypeError, ValueError):\n            self.fail('invalid')",
        "file_path": "src/marshmallow/fields.py",
        "chunk_index": 1,
        "metadata": {}
      },
      {
        "rank": 7,
        "score": 0.5862753987312317,
        "content": "# -*- coding: utf-8 -*-\n\"\"\"The :class:`Schema` class, including its metaclass and options (class Meta).\"\"\"\nfrom __future__ import absolute_import, unicode_literals\nfrom collections import defaultdict, namedtuple\nimport copy\nimport datetime as dt\nimport decimal\nimport inspect\nimport json\nimport uuid\nimport warnings\nimport functools\nfrom marshmallow import base, fields, utils, class_registry, marshalling\nfrom marshmallow.compat import (with_metaclass, iteritems, text_type,\n                                binary_type, Mapping, OrderedDict)\nfrom marshmallow.exceptions import ValidationError\nfrom marshmallow.orderedset import OrderedSet\nfrom marshmallow.decorators import (PRE_DUMP, POST_DUMP, PRE_LOAD, POST_LOAD,\n                                    VALIDATES, VALIDATES_SCHEMA)\nfrom marshmallow.utils import missing\nfrom marshmallow.warnings import RemovedInMarshmallow3Warning, ChangedInMarshmallow3Warning\n#: Return type of :meth:`Schema.dump` including serialized data and errors\nMarshalResult = namedtuple('MarshalResult', ['data', 'errors'])\n#: Return type of :meth:`Schema.load`, including deserialized data and errors\nUnmarshalResult = namedtuple('UnmarshalResult', ['data', 'errors'])\ndef _get_fields(attrs, field_class, pop=False, ordered=False):\n    \"\"\"Get fields from a class. If ordered=True, fields will sorted by creation index.\n    :param attrs: Mapping of class attributes\n    :param type field_class: Base field class\n    :param bool pop: Remove matching fields\n    \"\"\"\n    getter = getattr(attrs, 'pop' if pop else 'get')\n    fields = [\n        (field_name, getter(field_name))\n        for field_name, field_value in list(iteritems(attrs))\n        if utils.is_instance_or_subclass(field_value, field_class)\n    ]\n    if ordered:\n        return sorted(\n            fields,\n            key=lambda pair: pair[1]._creation_index,\n        )\n    else:\n        return fields\n# This function allows Schemas to inherit from non-Schema classes and ensures\n#   inheritance according to the MRO\ndef _get_fields_by_mro(klass, field_class, ordered=False):\n    \"\"\"Collect fields from a class, following its method resolution order. The\n    class itself is excluded from the search; only its parents are checked. Get\n    fields from ``_declared_fields`` if available, else use ``__dict__``.\n    :param type klass: Class whose fields to retrieve\n    :param type field_class: Base field class\n    \"\"\"\n    mro = inspect.getmro(klass)\n    # Loop over mro in reverse to maintain correct order of fields\n    return sum(\n        (\n            _get_fields(\n                getattr(base, '_declared_fields', base.__dict__),\n                field_class,\n                ordered=ordered\n            )\n            for base in mro[:0:-1]\n        ),\n        [],\n    )\nclass SchemaMeta(type):\n    \"\"\"Metaclass for the Schema class. Binds the declared fields to\n    a ``_declared_fields`` attribute, which is a dictionary mapping attribute\n    names to field objects. Also sets the ``opts`` class attribute, which is\n    the Schema class's ``class Meta`` options.\n    \"\"\"\n    def __new__(mcs, name, bases, attrs):\n        meta = attrs.get('Meta')\n        ordered = getattr(meta, 'ordered', False)\n        if not ordered:\n            # Inherit 'ordered' option\n            # Warning: We loop through bases instead of MRO because we don't\n            # yet have access to the class object\n            # (i.e. can't call super before we have fields)\n            for base_ in bases:\n                if hasattr(base_, 'Meta') and hasattr(base_.Meta, 'ordered'):\n                    ordered = base_.Meta.ordered\n                    break\n            else:\n                ordered = False\n        cls_fields = _get_fields(attrs, base.FieldABC, pop=True, ordered=ordered)\n        klass = super(SchemaMeta, mcs).__new__(mcs, name, bases, attrs)\n        inherited_fields = _get_fields_by_mro(klass, base.FieldABC, ordered=ordered)\n        # Use getattr rather than attrs['Meta'] so that we get inheritance for free\n        meta = getattr(klass, 'Meta')\n        # Set klass.opts in __new__ rather than __init__ so that it is accessible in\n        # get_declared_fields\n        klass.opts = klass.OPTIONS_CLASS(meta)\n        # Pass the inherited `ordered` into opts\n        klass.opts.ordered = ordered\n        # Add fields specifid in the `include` class Meta option\n        cls_fields += list(klass.opts.include.items())\n        dict_cls = OrderedDict if ordered else dict\n        # Assign _declared_fields on class\n        klass._declared_fields = mcs.get_declared_fields(\n            klass=klass,\n            cls_fields=cls_fields,\n            inherited_fields=inherited_fields,\n            dict_cls=dict_cls\n        )\n        return klass\n    @classmethod\n    def get_declared_fields(mcs, klass, cls_fields, inherited_fields, dict_cls):\n        \"\"\"Returns a dictionary of field_name => `Field` pairs declard on the class.\n        This is exposed mainly so that plugins can add additional fields, e.g. fields\n        computed from class Meta options.\n        :param type klass: The class object.\n        :param dict cls_fields: The fields declared on the class, including those added\n            by the ``include`` class Meta option.\n        :param dict inherited_fileds: Inherited fields.\n        :param type dict_class: Either `dict` or `OrderedDict`, depending on the whether\n            the user specified `ordered=True`.\n        \"\"\"\n        return dict_cls(inherited_fields + cls_fields)\n    # NOTE: self is the class object\n    def __init__(self, name, bases, attrs):\n        super(SchemaMeta, self).__init__(name, bases, attrs)\n        if name:\n            class_registry.register(name, self)\n        self._resolve_processors()\n    def _resolve_processors(self):\n        \"\"\"Add in the decorated processors\n        By doing this after constructing the class, we let standard inheritance\n        do all the hard work.\n        \"\"\"\n        mro = inspect.getmro(self)\n        self._has_processors = False\n        self.__processors__ = defaultdict(list)\n        for attr_name in dir(self):\n            # Need to look up the actual descriptor, not whatever might be\n            # bound to the class. This needs to come from the __dict__ of the\n            # declaring class.\n            for parent in mro:\n                try:\n                    attr = parent.__dict__[attr_name]\n                except KeyError:\n                    continue\n                else:\n                    break\n            else:\n                # In case we didn't find the attribute and didn't break above.\n                # We should never hit this - it's just here for completeness\n                # to exclude the possibility of attr being undefined.\n                continue\n            try:\n                processor_tags = attr.__marshmallow_tags__\n            except AttributeError:\n                continue\n            self._has_processors = bool(processor_tags)\n            for tag in processor_tags:\n                # Use name here so we can get the bound method later, in case\n                # the processor was a descriptor or something.\n                self.__processors__[tag].append(attr_name)\nclass SchemaOpts(object):\n    \"\"\"class Meta options for the :class:`Schema`. Defines defaults.\"\"\"\n    def __init__(self, meta):\n        self.fields = getattr(meta, 'fields', ())\n        if not isinstance(self.fields, (list, tuple)):\n            raise ValueError(\"`fields` option must be a list or tuple.\")\n        self.additional = getattr(meta, 'additional', ())\n        if not isinstance(self.additional, (list, tuple)):\n            raise ValueError(\"`additional` option must be a list or tuple.\")\n        if self.fields and self.additional:\n            raise ValueError(\"Cannot set both `fields` and `additional` options\"\n                            \" for the same Schema.\")\n        self.exclude = getattr(meta, 'exclude', ())\n        if not isinstance(self.exclude, (list, tuple)):\n            raise ValueError(\"`exclude` must be a list or tuple.\")\n        self.strict = getattr(meta, 'strict', False)\n        if hasattr(meta, 'dateformat'):\n            warnings.warn(\n                \"The dateformat option is renamed to datetimeformat in marshmallow 3.\",\n                ChangedInMarshmallow3Warning\n            )\n        self.dateformat = getattr(meta, 'dateformat', None)\n        if hasattr(meta, 'json_module'):\n            warnings.warn(\n                \"The json_module option is renamed to render_module in marshmallow 3.\",\n                ChangedInMarshmallow3Warning\n            )\n        self.json_module = getattr(meta, 'json_module', json)\n        if hasattr(meta, 'skip_missing'):\n            warnings.warn(\n                'The skip_missing option is no longer necessary. Missing inputs passed to '\n                'Schema.dump will be excluded from the serialized output by default.',\n                UserWarning\n            )\n        self.ordered = getattr(meta, 'ordered', False)\n        self.index_errors = getattr(meta, 'index_errors', True)\n        self.include = getattr(meta, 'include', {})\n        self.load_only = getattr(meta, 'load_only', ())\n        self.dump_only = getattr(meta, 'dump_only', ())\nclass BaseSchema(base.SchemaABC):\n    \"\"\"Base schema class with which to define custom schemas.\n    Example usage:\n    .. code-block:: python\n        import datetime as dt\n        from marshmallow import Schema, fields\n        class Album(object):\n            def __init__(self, title, release_date):\n                self.title = title\n                self.release_date = release_date\n        class AlbumSchema(Schema):\n            title = fields.Str()\n            release_date = fields.Date()\n        # Or, equivalently\n        class AlbumSchema2(Schema):\n            class Meta:\n                fields = (\"title\", \"release_date\")\n        album = Album(\"Beggars Banquet\", dt.date(1968, 12, 6))\n        schema = AlbumSchema()\n        data, errors = schema.dump(album)\n        data  # {'release_date': '1968-12-06', 'title': 'Beggars Banquet'}\n    :param dict extra: A dict of extra attributes to bind to the serialized result.\n    :param tuple|list only: Whitelist of fields to select when instantiating the Schema.\n        If None, all fields are used.\n        Nested fields can be represented with dot delimiters.\n    :param tuple|list exclude: Blacklist of fields to exclude when instantiating the Schema.\n        If a field appears in both `only` and `exclude`, it is not used.\n        Nested fields can be represented with dot delimiters.\n    :param str prefix: Optional prefix that will be prepended to all the\n        serialized field names.\n    :param bool strict: If `True`, raise errors if invalid data are passed in\n        instead of failing silently and storing the errors.\n    :param bool many: Should be set to `True` if ``obj`` is a collection\n        so that the object will be serialized to a list.\n    :param dict context: Optional context passed to :class:`fields.Method` and\n        :class:`fields.Function` fields.\n    :param tuple|list load_only: Fields to skip during serialization (write-only fields)\n    :param tuple|list dump_only: Fields to skip during deserialization (read-only fields)\n    :param bool|tuple partial: Whether to ignore missing fields. If its value\n        is an iterable, only missing fields listed in that iterable will be\n        ignored.\n    .. versionchanged:: 2.0.0\n        `__validators__`, `__preprocessors__`, and `__data_handlers__` are removed in favor of\n        `marshmallow.decorators.validates_schema`,\n        `marshmallow.decorators.pre_load` and `marshmallow.decorators.post_dump`.\n        `__accessor__` and `__error_handler__` are deprecated. Implement the\n        `handle_error` and `get_attribute` methods instead.\n        \"\"\"\n    TYPE_MAPPING = {\n        text_type: fields.String,\n        binary_type: fields.String,\n        dt.datetime: fields.DateTime,\n        float: fields.Float,\n        bool: fields.Boolean,\n        tuple: fields.Raw,\n        list: fields.Raw,\n        set: fields.Raw,\n        int: fields.Integer,\n        uuid.UUID: fields.UUID,\n        dt.time: fields.Time,\n        dt.date: fields.Date,\n        dt.timedelta: fields.TimeDelta,\n        decimal.Decimal: fields.Decimal,\n    }\n    OPTIONS_CLASS = SchemaOpts\n    #: DEPRECATED: Custom error handler function. May be `None`.\n    __error_handler__ = None\n    #: DEPRECATED: Function used to get values of an object.\n    __accessor__ = None\n    class Meta(object):\n        \"\"\"Options object for a Schema.\n        Example usage: ::\n            class Meta:\n                fields = (\"id\", \"email\", \"date_created\")\n                exclude = (\"password\", \"secret_attribute\")\n        Available options:\n        - ``fields``: Tuple or list of fields to include in the serialized result.\n        - ``additional``: Tuple or list of fields to include *in addition* to the\n            explicitly declared fields. ``additional`` and ``fields`` are\n            mutually-exclusive options.\n        - ``include``: Dictionary of additional fields to include in the schema. It is\n            usually better to define fields as class variables, but you may need to\n            use this option, e.g., if your fields are Python keywords. May be an\n            `OrderedDict`.\n        - ``exclude``: Tuple or list of fields to exclude in the serialized result.\n            Nested fields can be represented with dot delimiters.\n        - ``dateformat``: Date format for all DateTime fields that do not have their\n            date format explicitly specified.\n        - ``strict``: If `True`, raise errors during marshalling rather than\n            storing them.\n        - ``json_module``: JSON module to use for `loads` and `dumps`.\n            Defaults to the ``json`` module in the stdlib.\n        - ``ordered``: If `True`, order serialization output according to the\n            order in which fields were declared. Output of `Schema.dump` will be a\n            `collections.OrderedDict`.\n        - ``index_errors``: If `True`, errors dictionaries will include the index\n            of invalid items in a collection.\n        - ``load_only``: Tuple or list of fields to exclude from serialized results.\n        - ``dump_only``: Tuple or list of fields to exclude from deserialization\n        \"\"\"\n        pass\n    def __init__(self, extra=None, only=None, exclude=(), prefix='', strict=None,\n                 many=False, context=None, load_only=(), dump_only=(),\n                 partial=False):\n        # copy declared fields from metaclass\n        self.declared_fields = copy.deepcopy(self._declared_fields)\n        self.many = many\n        self.only = only\n        self.exclude = set(self.opts.exclude) | set(exclude)\n        if prefix:\n            warnings.warn(\n                'The `prefix` argument is deprecated. Use a post_dump '\n                'method to insert a prefix instead.',\n                RemovedInMarshmallow3Warning\n            )\n        self.prefix = prefix\n        self.strict = strict if strict is not None else self.opts.strict\n        self.ordered = self.opts.ordered\n        self.load_only = set(load_only) or set(self.opts.load_only)\n        self.dump_only = set(dump_only) or set(self.opts.dump_only)\n        self.partial = partial\n        #: Dictionary mapping field_names -> :class:`Field` objects\n        self.fields = self.dict_class()\n        if extra:\n            warnings.warn(\n                'The `extra` argument is deprecated. Use a post_dump '\n                'method to add additional data instead.',\n                RemovedInMarshmallow3Warning\n            )\n        self.extra = extra\n        self.context = context or {}\n        self._normalize_nested_options()\n        self._types_seen = set()\n        self._update_fields(many=many)\n    def __repr__(self):\n        return '<{ClassName}(many={self.many}, strict={self.strict})>'.format(\n            ClassName=self.__class__.__name__, self=self\n        )\n    def _postprocess(self, data, many, obj):\n        if self.extra:\n            if many:\n                for each in data:\n                    each.update(self.extra)\n            else:\n                data.update(self.extra)\n        return data\n    @property\n    def dict_class(self):\n        return OrderedDict if self.ordered else dict\n    @property\n    def set_class(self):\n        return OrderedSet if self.ordered else set\n    ##### Override-able methods #####\n    def handle_error(self, error, data):\n        \"\"\"Custom error handler function for the schema.\n        :param ValidationError error: The `ValidationError` raised during (de)serialization.\n        :param data: The original input data.\n        .. versionadded:: 2.0.0\n        \"\"\"\n        pass\n    def get_attribute(self, attr, obj, default):\n        \"\"\"Defines how to pull values from an object to serialize.\n        .. versionadded:: 2.0.0\n        \"\"\"\n        return utils.get_value(attr, obj, default)\n    ##### Handler decorators (deprecated) #####\n    @classmethod\n    def error_handler(cls, func):\n        \"\"\"Decorator that registers an error handler function for the schema.\n        The function receives the :class:`Schema` instance, a dictionary of errors,\n        and the serialized object (if serializing data) or data dictionary (if\n        deserializing data) as arguments.\n        Example: ::\n            class UserSchema(Schema):\n                email = fields.Email()\n            @UserSchema.error_handler\n            def handle_errors(schema, errors, obj):\n                raise ValueError('An error occurred while marshalling {}'.format(obj))\n            user = User(email='invalid')\n            UserSchema().dump(user)  # => raises ValueError\n            UserSchema().load({'email': 'bademail'})  # raises ValueError\n        .. versionadded:: 0.7.0\n        .. deprecated:: 2.0.0\n            Set the ``error_handler`` class Meta option instead.\n        \"\"\"\n        warnings.warn(\n            'Schema.error_handler is deprecated. Set the error_handler class Meta option '\n            'instead.', category=DeprecationWarning\n        )\n        cls.__error_handler__ = func\n        return func\n    @classmethod\n    def accessor(cls, func):\n        \"\"\"Decorator that registers a function for pulling values from an object\n        to serialize. The function receives the :class:`Schema` instance, the\n        ``key`` of the value to get, the ``obj`` to serialize, and an optional\n        ``default`` value.\n        .. deprecated:: 2.0.0\n            Set the ``error_handler`` class Meta option instead.\n        \"\"\"\n        warnings.warn(\n            'Schema.accessor is deprecated. Set the accessor class Meta option '\n            'instead.', category=DeprecationWarning\n        )\n        cls.__accessor__ = func\n        return func\n    ##### Serialization/Deserialization API #####\n    def dump(self, obj, many=None, update_fields=True, **kwargs):\n        \"\"\"Serialize an object to native Python data types according to this\n        Schema's fields.\n        :param obj: The object to serialize.\n        :param bool many: Whether to serialize `obj` as a collection. If `None`, the value\n            for `self.many` is used.\n        :param bool update_fields: Whether to update the schema's field classes. Typically\n            set to `True`, but may be `False` when serializing a homogenous collection.\n            This parameter is used by `fields.Nested` to avoid multiple updates.\n        :return: A tuple of the form (``data``, ``errors``)\n        :rtype: `MarshalResult`, a `collections.namedtuple`\n        .. versionadded:: 1.0.0\n        \"\"\"\n        # Callable marshalling object\n        marshal = marshalling.Marshaller(prefix=self.prefix)\n        errors = {}\n        many = self.many if many is None else bool(many)\n        if many and utils.is_iterable_but_not_string(obj):\n            obj = list(obj)\n        if self._has_processors:\n            try:\n                processed_obj = self._invoke_dump_processors(\n                    PRE_DUMP,\n                    obj,\n                    many,\n                    original_data=obj)\n            except ValidationError as error:\n                errors = error.normalized_messages()\n                result = None\n        else:\n            processed_obj = obj\n        if not errors:\n            if update_fields:\n                obj_type = type(processed_obj)\n                if obj_type not in self._types_seen:\n                    self._update_fields(processed_obj, many=many)\n                    if not isinstance(processed_obj, Mapping):\n                        self._types_seen.add(obj_type)\n            try:\n                preresult = marshal(\n                    processed_obj,\n                    self.fields,\n                    many=many,\n                    # TODO: Remove self.__accessor__ in a later release\n                    accessor=self.get_attribute or self.__accessor__,\n                    dict_class=self.dict_class,\n                    index_errors=self.opts.index_errors,\n                    **kwargs\n                )\n            except ValidationError as error:\n                errors = marshal.errors\n                preresult = error.data\n            result = self._postprocess(preresult, many, obj=obj)\n        if not errors and self._has_processors:\n            try:\n                result = self._invoke_dump_processors(\n                    POST_DUMP,\n                    result,\n                    many,\n                    original_data=obj)\n            except ValidationError as error:\n                errors = error.normalized_messages()\n        if errors:\n            # TODO: Remove self.__error_handler__ in a later release\n            if self.__error_handler__ and callable(self.__error_handler__):\n                self.__error_handler__(errors, obj)\n            exc = ValidationError(\n                errors,\n                field_names=marshal.error_field_names,\n                fields=marshal.error_fields,\n                data=obj,\n                **marshal.error_kwargs\n            )\n            self.handle_error(exc, obj)\n            if self.strict:\n                raise exc\n        return MarshalResult(result, errors)\n    def dumps(self, obj, many=None, update_fields=True, *args, **kwargs):\n        \"\"\"Same as :meth:`dump`, except return a JSON-encoded string.\n        :param obj: The object to serialize.\n        :param bool many: Whether to serialize `obj` as a collection. If `None`, the value\n            for `self.many` is used.\n        :param bool update_fields: Whether to update the schema's field classes. Typically\n            set to `True`, but may be `False` when serializing a homogenous collection.\n            This parameter is used by `fields.Nested` to avoid multiple updates.\n        :return: A tuple of the form (``data``, ``errors``)\n        :rtype: `MarshalResult`, a `collections.namedtuple`\n        .. versionadded:: 1.0.0\n        \"\"\"\n        deserialized, errors = self.dump(obj, many=many, update_fields=update_fields)\n        ret = self.opts.json_module.dumps(deserialized, *args, **kwargs)\n        return MarshalResult(ret, errors)\n    def load(self, data, many=None, partial=None):\n        \"\"\"Deserialize a data structure to an object defined by this Schema's\n        fields and :meth:`make_object`.",
        "file_path": "src/marshmallow/schema.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 8,
        "score": 0.5459198355674744,
        "content": "# -*- coding: utf-8 -*-\n\"\"\"Utility methods for marshmallow.\"\"\"\nfrom __future__ import absolute_import, unicode_literals\nimport datetime\nimport inspect\nimport json\nimport re\nimport time\nimport types\nfrom calendar import timegm\nfrom decimal import Decimal, ROUND_HALF_EVEN, Context, Inexact\nfrom email.utils import formatdate, parsedate\nfrom pprint import pprint as py_pprint\nfrom marshmallow.compat import OrderedDict, binary_type, text_type\nfrom marshmallow.compat import get_func_args as compat_get_func_args\nfrom marshmallow.compat import Mapping, Iterable\nfrom marshmallow.warnings import unused_and_removed_in_ma3\ndateutil_available = False\ntry:\n    from dateutil import parser\n    dateutil_available = True\nexcept ImportError:\n    dateutil_available = False\nclass _Missing(object):\n    def __bool__(self):\n        return False\n    __nonzero__ = __bool__  # PY2 compat\n    def __copy__(self):\n        return self\n    def __deepcopy__(self, _):\n        return self\n    def __repr__(self):\n        return '<marshmallow.missing>'\n# Singleton value that indicates that a field's value is missing from input\n# dict passed to :meth:`Schema.load`. If the field's value is not required,\n# it's ``default`` value is used.\nmissing = _Missing()\ndef is_generator(obj):\n    \"\"\"Return True if ``obj`` is a generator\n    \"\"\"\n    return inspect.isgeneratorfunction(obj) or inspect.isgenerator(obj)\ndef is_iterable_but_not_string(obj):\n    \"\"\"Return True if ``obj`` is an iterable object that isn't a string.\"\"\"\n    return (\n        (isinstance(obj, Iterable) and not hasattr(obj, \"strip\")) or is_generator(obj)\n    )\n@unused_and_removed_in_ma3\ndef is_indexable_but_not_string(obj):\n    \"\"\"Return True if ``obj`` is indexable but isn't a string.\"\"\"\n    return not hasattr(obj, \"strip\") and hasattr(obj, \"__getitem__\")\ndef is_collection(obj):\n    \"\"\"Return True if ``obj`` is a collection type, e.g list, tuple, queryset.\"\"\"\n    return is_iterable_but_not_string(obj) and not isinstance(obj, Mapping)\ndef is_instance_or_subclass(val, class_):\n    \"\"\"Return True if ``val`` is either a subclass or instance of ``class_``.\"\"\"\n    try:\n        return issubclass(val, class_)\n    except TypeError:\n        return isinstance(val, class_)\ndef is_keyed_tuple(obj):\n    \"\"\"Return True if ``obj`` has keyed tuple behavior, such as\n    namedtuples or SQLAlchemy's KeyedTuples.\n    \"\"\"\n    return isinstance(obj, tuple) and hasattr(obj, '_fields')\n@unused_and_removed_in_ma3\ndef float_to_decimal(f):\n    \"\"\"Convert a floating point number to a Decimal with no loss of information.\n        See: http://docs.python.org/release/2.6.7/library/decimal.html#decimal-faq\n    \"\"\"\n    n, d = f.as_integer_ratio()\n    numerator, denominator = Decimal(n), Decimal(d)\n    ctx = Context(prec=60)\n    result = ctx.divide(numerator, denominator)\n    while ctx.flags[Inexact]:\n        ctx.flags[Inexact] = False\n        ctx.prec *= 2\n        result = ctx.divide(numerator, denominator)\n    return result\nZERO_DECIMAL = Decimal()\n@unused_and_removed_in_ma3\ndef decimal_to_fixed(value, precision):\n    \"\"\"Convert a `Decimal` to a fixed-precision number as a string.\"\"\"\n    return text_type(value.quantize(precision, rounding=ROUND_HALF_EVEN))\ndef to_marshallable_type(obj, field_names=None):\n    \"\"\"Helper for converting an object to a dictionary only if it is not\n    dictionary already or an indexable object nor a simple type\"\"\"\n    if obj is None:\n        return None  # make it idempotent for None\n    if hasattr(obj, '__marshallable__'):\n        return obj.__marshallable__()\n    if hasattr(obj, '__getitem__') and not is_keyed_tuple(obj):\n        return obj  # it is indexable it is ok\n    if isinstance(obj, types.GeneratorType):\n        return list(obj)\n    if field_names:\n        # exclude field names that aren't actual attributes of the object\n        attrs = set(dir(obj)) & set(field_names)\n    else:\n        attrs = set(dir(obj))\n    return dict([(attr, getattr(obj, attr, None)) for attr in attrs\n                  if not attr.startswith(\"__\") and not attr.endswith(\"__\")])\ndef pprint(obj, *args, **kwargs):\n    \"\"\"Pretty-printing function that can pretty-print OrderedDicts\n    like regular dictionaries. Useful for printing the output of\n    :meth:`marshmallow.Schema.dump`.\n    \"\"\"\n    if isinstance(obj, OrderedDict):\n        print(json.dumps(obj, *args, **kwargs))\n    else:\n        py_pprint(obj, *args, **kwargs)\n# From pytz: http://pytz.sourceforge.net/\nZERO = datetime.timedelta(0)\nHOUR = datetime.timedelta(hours=1)\nclass UTC(datetime.tzinfo):\n    \"\"\"UTC\n    Optimized UTC implementation. It unpickles using the single module global\n    instance defined beneath this class declaration.\n    \"\"\"\n    zone = \"UTC\"\n    _utcoffset = ZERO\n    _dst = ZERO\n    _tzname = zone\n    def fromutc(self, dt):\n        if dt.tzinfo is None:\n            return self.localize(dt)\n        return super(utc.__class__, self).fromutc(dt)\n    def utcoffset(self, dt):\n        return ZERO\n    def tzname(self, dt):\n        return \"UTC\"\n    def dst(self, dt):\n        return ZERO\n    def localize(self, dt, is_dst=False):\n        '''Convert naive time to local time'''\n        if dt.tzinfo is not None:\n            raise ValueError('Not naive datetime (tzinfo is already set)')\n        return dt.replace(tzinfo=self)\n    def normalize(self, dt, is_dst=False):\n        '''Correct the timezone information on the given datetime'''\n        if dt.tzinfo is self:\n            return dt\n        if dt.tzinfo is None:\n            raise ValueError('Naive time - no tzinfo set')\n        return dt.astimezone(self)\n    def __repr__(self):\n        return \"<UTC>\"\n    def __str__(self):\n        return \"UTC\"\nUTC = utc = UTC()  # UTC is a singleton\ndef local_rfcformat(dt):\n    \"\"\"Return the RFC822-formatted representation of a timezone-aware datetime\n    with the UTC offset.\n    \"\"\"\n    weekday = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"][dt.weekday()]\n    month = [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\",\n             \"Oct\", \"Nov\", \"Dec\"][dt.month - 1]\n    tz_offset = dt.strftime(\"%z\")\n    return \"%s, %02d %s %04d %02d:%02d:%02d %s\" % (weekday, dt.day, month,\n        dt.year, dt.hour, dt.minute, dt.second, tz_offset)\ndef rfcformat(dt, localtime=False):\n    \"\"\"Return the RFC822-formatted representation of a datetime object.\n    :param datetime dt: The datetime.\n    :param bool localtime: If ``True``, return the date relative to the local\n        timezone instead of UTC, displaying the proper offset,\n        e.g. \"Sun, 10 Nov 2013 08:23:45 -0600\"\n    \"\"\"\n    if not localtime:\n        return formatdate(timegm(dt.utctimetuple()))\n    else:\n        return local_rfcformat(dt)\n# From Django\n_iso8601_re = re.compile(\n    r'(?P<year>\\d{4})-(?P<month>\\d{1,2})-(?P<day>\\d{1,2})'\n    r'[T ](?P<hour>\\d{1,2}):(?P<minute>\\d{1,2})'\n    r'(?::(?P<second>\\d{1,2})(?:\\.(?P<microsecond>\\d{1,6})\\d{0,6})?)?'\n    r'(?P<tzinfo>Z|[+-]\\d{2}(?::?\\d{2})?)?$'\n)\ndef isoformat(dt, localtime=False, *args, **kwargs):\n    \"\"\"Return the ISO8601-formatted UTC representation of a datetime object.\n    \"\"\"\n    if localtime and dt.tzinfo is not None:\n        localized = dt\n    else:\n        if dt.tzinfo is None:\n            localized = UTC.localize(dt)\n        else:\n            localized = dt.astimezone(UTC)\n    return localized.isoformat(*args, **kwargs)\ndef from_datestring(datestring):\n    \"\"\"Parse an arbitrary datestring and return a datetime object using\n    dateutils' parser.\n    \"\"\"\n    if dateutil_available:\n        return parser.parse(datestring)\n    else:\n        raise RuntimeError('from_datestring requires the python-dateutil library')\ndef from_rfc(datestring, use_dateutil=True):\n    \"\"\"Parse a RFC822-formatted datetime string and return a datetime object.\n    Use dateutil's parser if possible.\n    https://stackoverflow.com/questions/885015/how-to-parse-a-rfc-2822-date-time-into-a-python-datetime\n    \"\"\"\n    # Use dateutil's parser if possible\n    if dateutil_available and use_dateutil:\n        return parser.parse(datestring)\n    else:\n        parsed = parsedate(datestring)  # as a tuple\n        timestamp = time.mktime(parsed)\n        return datetime.datetime.fromtimestamp(timestamp)\ndef from_iso(datestring, use_dateutil=True):\n    \"\"\"Parse an ISO8601-formatted datetime string and return a datetime object.\n    Use dateutil's parser if possible and return a timezone-aware datetime.\n    \"\"\"\n    if not _iso8601_re.match(datestring):\n        raise ValueError('Not a valid ISO8601-formatted string')\n    # Use dateutil's parser if possible\n    if dateutil_available and use_dateutil:\n        return parser.parse(datestring)\n    else:\n        # Strip off timezone info.\n        if '.' in datestring:\n            # datestring contains microseconds\n            (dt_nomstz, mstz) = datestring.split('.')\n            ms_notz = mstz[:len(mstz) - len(mstz.lstrip('0123456789'))]\n            datestring = '.'.join((dt_nomstz, ms_notz))\n            return datetime.datetime.strptime(datestring[:26], '%Y-%m-%dT%H:%M:%S.%f')\n        return datetime.datetime.strptime(datestring[:19], '%Y-%m-%dT%H:%M:%S')\ndef from_iso_time(timestring, use_dateutil=True):\n    \"\"\"Parse an ISO8601-formatted datetime string and return a datetime.time\n    object.\n    \"\"\"\n    if dateutil_available and use_dateutil:\n        return parser.parse(timestring).time()\n    else:\n        if len(timestring) > 8:  # has microseconds\n            fmt = '%H:%M:%S.%f'\n        else:\n            fmt = '%H:%M:%S'\n        return datetime.datetime.strptime(timestring, fmt).time()\ndef from_iso_date(datestring, use_dateutil=True):\n    if dateutil_available and use_dateutil:\n        return parser.parse(datestring).date()\n    else:\n        return datetime.datetime.strptime(datestring[:10], '%Y-%m-%d').date()\ndef ensure_text_type(val):\n    if isinstance(val, binary_type):\n        val = val.decode('utf-8')\n    return text_type(val)\ndef pluck(dictlist, key):\n    \"\"\"Extracts a list of dictionary values from a list of dictionaries.\n    ::\n        >>> dlist = [{'id': 1, 'name': 'foo'}, {'id': 2, 'name': 'bar'}]\n        >>> pluck(dlist, 'id')\n        [1, 2]\n    \"\"\"\n    return [d[key] for d in dictlist]\n# Various utilities for pulling keyed values from objects\ndef get_value(key, obj, default=missing):\n    \"\"\"Helper for pulling a keyed value off various types of objects\"\"\"\n    if isinstance(key, int):\n        return _get_value_for_key(key, obj, default)\n    else:\n        return _get_value_for_keys(key.split('.'), obj, default)\ndef _get_value_for_keys(keys, obj, default):\n    if len(keys) == 1:\n        return _get_value_for_key(keys[0], obj, default)\n    else:\n        return _get_value_for_keys(\n            keys[1:], _get_value_for_key(keys[0], obj, default), default)\ndef _get_value_for_key(key, obj, default):\n    try:\n        return obj[key]\n    except (KeyError, AttributeError, IndexError, TypeError):\n        try:\n            attr = getattr(obj, key)\n            return attr() if callable(attr) else attr\n        except AttributeError:\n            return default\n    return default\ndef set_value(dct, key, value):\n    \"\"\"Set a value in a dict. If `key` contains a '.', it is assumed\n    be a path (i.e. dot-delimited string) to the value's location.\n    ::\n        >>> d = {}\n        >>> set_value(d, 'foo.bar', 42)\n        >>> d\n        {'foo': {'bar': 42}}\n    \"\"\"\n    if '.' in key:\n        head, rest = key.split('.', 1)\n        target = dct.setdefault(head, {})\n        if not isinstance(target, dict):\n            raise ValueError(\n                'Cannot set {key} in {head} '\n                'due to existing value: {target}'.format(key=key, head=head, target=target)\n            )\n        set_value(target, rest, value)\n    else:\n        dct[key] = value\ndef callable_or_raise(obj):\n    \"\"\"Check that an object is callable, else raise a :exc:`ValueError`.\n    \"\"\"\n    if not callable(obj):\n        raise ValueError('Object {0!r} is not callable.'.format(obj))\n    return obj\nget_func_args = compat_get_func_args\n\"\"\"Given a callable, return a list of argument names.\nHandles `functools.partial` objects and callable objects.\n\"\"\"\ndef if_none(value, default):\n    return value if value is not None else default",
        "file_path": "src/marshmallow/utils.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 9,
        "score": 0.5103336572647095,
        "content": "# -*- coding: utf-8 -*-\n\"\"\"Decorators for registering schema pre-processing and post-processing methods.\nThese should be imported from the top-level `marshmallow` module.\nExample: ::\n    from marshmallow import (\n        Schema, pre_load, pre_dump, post_load, validates_schema,\n        validates, fields, ValidationError\n    )\n    class UserSchema(Schema):\n        email = fields.Str(required=True)\n        age = fields.Integer(required=True)\n        @post_load\n        def lowerstrip_email(self, item):\n            item['email'] = item['email'].lower().strip()\n            return item\n        @pre_load(pass_many=True)\n        def remove_envelope(self, data, many):\n            namespace = 'results' if many else 'result'\n            return data[namespace]\n        @post_dump(pass_many=True)\n        def add_envelope(self, data, many):\n            namespace = 'results' if many else 'result'\n            return {namespace: data}\n        @validates_schema\n        def validate_email(self, data):\n            if len(data['email']) < 3:\n                raise ValidationError('Email must be more than 3 characters', 'email')\n        @validates('age')\n        def validate_age(self, data):\n            if data < 14:\n                raise ValidationError('Too young!')\n.. note::\n    These decorators only work with instance methods. Class and static\n    methods are not supported.\n.. warning::\n    The invocation order of decorated methods of the same type is not guaranteed.\n    If you need to guarantee order of different processing steps, you should put\n    them in the same processing method.\n\"\"\"\nfrom __future__ import unicode_literals\nimport functools\nPRE_DUMP = 'pre_dump'\nPOST_DUMP = 'post_dump'\nPRE_LOAD = 'pre_load'\nPOST_LOAD = 'post_load'\nVALIDATES = 'validates'\nVALIDATES_SCHEMA = 'validates_schema'\ndef validates(field_name):\n    \"\"\"Register a field validator.\n    :param str field_name: Name of the field that the method validates.\n    \"\"\"\n    return tag_processor(VALIDATES, None, False, field_name=field_name)\ndef validates_schema(fn=None, pass_many=False, pass_original=False, skip_on_field_errors=False):\n    \"\"\"Register a schema-level validator.\n    By default, receives a single object at a time, regardless of whether ``many=True``\n    is passed to the `Schema`. If ``pass_many=True``, the raw data (which may be a collection)\n    and the value for ``many`` is passed.\n    If ``pass_original=True``, the original data (before unmarshalling) will be passed as\n    an additional argument to the method.\n    If ``skip_on_field_errors=True``, this validation method will be skipped whenever\n    validation errors have been detected when validating fields.\n    \"\"\"\n    return tag_processor(VALIDATES_SCHEMA, fn, pass_many, pass_original=pass_original,\n                         skip_on_field_errors=skip_on_field_errors)\ndef pre_dump(fn=None, pass_many=False):\n    \"\"\"Register a method to invoke before serializing an object. The method\n    receives the object to be serialized and returns the processed object.\n    By default, receives a single object at a time, regardless of whether ``many=True``\n    is passed to the `Schema`. If ``pass_many=True``, the raw data (which may be a collection)\n    and the value for ``many`` is passed.\n    \"\"\"\n    return tag_processor(PRE_DUMP, fn, pass_many)\ndef post_dump(fn=None, pass_many=False, pass_original=False):\n    \"\"\"Register a method to invoke after serializing an object. The method\n    receives the serialized object and returns the processed object.\n    By default, receives a single object at a time, transparently handling the ``many``\n    argument passed to the Schema. If ``pass_many=True``, the raw data\n    (which may be a collection) and the value for ``many`` is passed.\n    \"\"\"\n    return tag_processor(POST_DUMP, fn, pass_many, pass_original=pass_original)\ndef pre_load(fn=None, pass_many=False):\n    \"\"\"Register a method to invoke before deserializing an object. The method\n    receives the data to be deserialized and returns the processed data.\n    By default, receives a single datum at a time, transparently handling the ``many``\n    argument passed to the Schema. If ``pass_many=True``, the raw data\n    (which may be a collection) and the value for ``many`` is passed.\n    \"\"\"\n    return tag_processor(PRE_LOAD, fn, pass_many)\ndef post_load(fn=None, pass_many=False, pass_original=False):\n    \"\"\"Register a method to invoke after deserializing an object. The method\n    receives the deserialized data and returns the processed data.\n    By default, receives a single datum at a time, transparently handling the ``many``\n    argument passed to the Schema. If ``pass_many=True``, the raw data\n    (which may be a collection) and the value for ``many`` is passed.\n    \"\"\"\n    return tag_processor(POST_LOAD, fn, pass_many, pass_original=pass_original)\ndef tag_processor(tag_name, fn, pass_many, **kwargs):\n    \"\"\"Tags decorated processor function to be picked up later.\n    .. note::\n        Currently ony works with functions and instance methods. Class and\n        static methods are not supported.\n    :return: Decorated function if supplied, else this decorator with its args\n        bound.\n    \"\"\"\n    # Allow using this as either a decorator or a decorator factory.\n    if fn is None:\n        return functools.partial(\n            tag_processor, tag_name, pass_many=pass_many, **kwargs\n        )\n    # Set a marshmallow_tags attribute instead of wrapping in some class,\n    # because I still want this to end up as a normal (unbound) method.\n    try:\n        marshmallow_tags = fn.__marshmallow_tags__\n    except AttributeError:\n        fn.__marshmallow_tags__ = marshmallow_tags = set()\n    # Also save the kwargs for the tagged function on\n    # __marshmallow_kwargs__, keyed by (<tag_name>, <pass_many>)\n    try:\n        marshmallow_kwargs = fn.__marshmallow_kwargs__\n    except AttributeError:\n        fn.__marshmallow_kwargs__ = marshmallow_kwargs = {}\n    marshmallow_tags.add((tag_name, pass_many))\n    marshmallow_kwargs[(tag_name, pass_many)] = kwargs\n    return fn",
        "file_path": "src/marshmallow/decorators.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 10,
        "score": 0.5044437646865845,
        "content": "    localtime = True\nclass Time(Field):\n    \"\"\"ISO8601-formatted time string.\n    :param kwargs: The same keyword arguments that :class:`Field` receives.\n    \"\"\"\n    default_error_messages = {\n        'invalid': 'Not a valid time.',\n        'format': '\"{input}\" cannot be formatted as a time.',\n    }\n    def _serialize(self, value, attr, obj):\n        if value is None:\n            return None\n        try:\n            ret = value.isoformat()\n        except AttributeError:\n            self.fail('format', input=value)\n        if value.microsecond:\n            return ret[:15]\n        return ret\n    def _deserialize(self, value, attr, data):\n        \"\"\"Deserialize an ISO8601-formatted time to a :class:`datetime.time` object.\"\"\"\n        if not value:   # falsy values are invalid\n            self.fail('invalid')\n        try:\n            return utils.from_iso_time(value)\n        except (AttributeError, TypeError, ValueError):\n            self.fail('invalid')\nclass Date(Field):\n    \"\"\"ISO8601-formatted date string.\n    :param kwargs: The same keyword arguments that :class:`Field` receives.\n    \"\"\"\n    default_error_messages = {\n        'invalid': 'Not a valid date.',\n        'format': '\"{input}\" cannot be formatted as a date.',\n    }\n    def _serialize(self, value, attr, obj):\n        if value is None:\n            return None\n        try:\n            return value.isoformat()\n        except AttributeError:\n            self.fail('format', input=value)\n        return value\n    def _deserialize(self, value, attr, data):\n        \"\"\"Deserialize an ISO8601-formatted date string to a\n        :class:`datetime.date` object.\n        \"\"\"\n        if not value:  # falsy values are invalid\n            self.fail('invalid')\n        try:\n            return utils.from_iso_date(value)\n        except (AttributeError, TypeError, ValueError):\n            self.fail('invalid')\nclass TimeDelta(Field):\n    \"\"\"A field that (de)serializes a :class:`datetime.timedelta` object to an\n    integer and vice versa. The integer can represent the number of days,\n    seconds or microseconds.\n    :param str precision: Influences how the integer is interpreted during\n        (de)serialization. Must be 'days', 'seconds' or 'microseconds'.\n    :param str error: Error message stored upon validation failure.\n    :param kwargs: The same keyword arguments that :class:`Field` receives.\n    .. versionchanged:: 2.0.0\n        Always serializes to an integer value to avoid rounding errors.\n        Add `precision` parameter.\n    \"\"\"\n    DAYS = 'days'\n    SECONDS = 'seconds'\n    MICROSECONDS = 'microseconds'\n    default_error_messages = {\n        'invalid': 'Not a valid period of time.',\n        'format': '{input!r} cannot be formatted as a timedelta.'\n    }\n    def __init__(self, precision='seconds', error=None, **kwargs):\n        precision = precision.lower()\n        units = (self.DAYS, self.SECONDS, self.MICROSECONDS)\n        if precision not in units:\n            msg = 'The precision must be \"{0}\", \"{1}\" or \"{2}\".'.format(*units)\n            raise ValueError(msg)\n        self.precision = precision\n        super(TimeDelta, self).__init__(error=error, **kwargs)\n    def _serialize(self, value, attr, obj):\n        if value is None:\n            return None\n        try:\n            days = value.days\n            if self.precision == self.DAYS:\n                return days\n            else:\n                seconds = days * 86400 + value.seconds\n                if self.precision == self.SECONDS:\n                    return seconds\n                else:  # microseconds\n                    return seconds * 10**6 + value.microseconds  # flake8: noqa\n        except AttributeError:\n            self.fail('format', input=value)\n    def _deserialize(self, value, attr, data):\n        try:\n            value = int(value)\n        except (TypeError, ValueError):\n            self.fail('invalid')\n        kwargs = {self.precision: value}\n        try:\n            return dt.timedelta(**kwargs)\n        except OverflowError:\n            self.fail('invalid')\nclass Dict(Field):\n    \"\"\"A dict field. Supports dicts and dict-like objects.\n    .. note::\n        This field is only appropriate when the structure of\n        nested data is not known. For structured data, use\n        `Nested`.\n    .. versionadded:: 2.1.0\n    \"\"\"\n    default_error_messages = {\n        'invalid': 'Not a valid mapping type.'\n    }\n    def _deserialize(self, value, attr, data):\n        if isinstance(value, Mapping):\n            return value\n        else:\n            self.fail('invalid')\nclass ValidatedField(Field):\n    \"\"\"A field that validates input on serialization.\"\"\"\n    def _validated(self, value):\n        raise NotImplementedError('Must implement _validate method')\n    def _serialize(self, value, *args, **kwargs):\n        ret = super(ValidatedField, self)._serialize(value, *args, **kwargs)\n        return self._validated(ret)\nclass Url(ValidatedField, String):\n    \"\"\"A validated URL field. Validation occurs during both serialization and\n    deserialization.\n    :param default: Default value for the field if the attribute is not set.\n    :param str attribute: The name of the attribute to get the value from. If\n        `None`, assumes the attribute has the same name as the field.\n    :param bool relative: Allow relative URLs.\n    :param kwargs: The same keyword arguments that :class:`String` receives.\n    \"\"\"\n    default_error_messages = {'invalid': 'Not a valid URL.'}\n    def __init__(self, relative=False, schemes=None, **kwargs):\n        String.__init__(self, **kwargs)\n        self.relative = relative\n        # Insert validation into self.validators so that multiple errors can be\n        # stored.\n        self.validators.insert(0, validate.URL(\n            relative=self.relative,\n            schemes=schemes,\n            error=self.error_messages['invalid']\n        ))\n    def _validated(self, value):\n        if value is None:\n            return None\n        return validate.URL(\n            relative=self.relative,\n            error=self.error_messages['invalid']\n        )(value)\nclass Email(ValidatedField, String):\n    \"\"\"A validated email field. Validation occurs during both serialization and\n    deserialization.\n    :param args: The same positional arguments that :class:`String` receives.\n    :param kwargs: The same keyword arguments that :class:`String` receives.\n    \"\"\"\n    default_error_messages = {'invalid': 'Not a valid email address.'}\n    def __init__(self, *args, **kwargs):\n        String.__init__(self, *args, **kwargs)\n        # Insert validation into self.validators so that multiple errors can be\n        # stored.\n        self.validators.insert(0, validate.Email(error=self.error_messages['invalid']))\n    def _validated(self, value):\n        if value is None:\n            return None\n        return validate.Email(\n            error=self.error_messages['invalid']\n        )(value)\nclass Method(Field):\n    \"\"\"A field that takes the value returned by a `Schema` method.\n    :param str method_name: The name of the Schema method from which\n        to retrieve the value. The method must take an argument ``obj``\n        (in addition to self) that is the object to be serialized.\n    :param str deserialize: Optional name of the Schema method for deserializing\n        a value The method must take a single argument ``value``, which is the\n        value to deserialize.\n    .. versionchanged:: 2.0.0\n        Removed optional ``context`` parameter on methods. Use ``self.context`` instead.\n    .. versionchanged:: 2.3.0\n        Deprecated ``method_name`` parameter in favor of ``serialize`` and allow\n        ``serialize`` to not be passed at all.\n    \"\"\"\n    _CHECK_ATTRIBUTE = False\n    def __init__(self, serialize=None, deserialize=None, method_name=None, **kwargs):\n        if method_name is not None:\n            warnings.warn('\"method_name\" argument of fields.Method is deprecated. '\n                          'Use the \"serialize\" argument instead.', RemovedInMarshmallow3Warning)\n        self.serialize_method_name = self.method_name = serialize or method_name\n        self.deserialize_method_name = deserialize\n        super(Method, self).__init__(**kwargs)\n    def _serialize(self, value, attr, obj):\n        if not self.serialize_method_name:\n            return missing_\n        method = utils.callable_or_raise(\n            getattr(self.parent, self.serialize_method_name, None)\n        )\n        try:\n            return method(obj)\n        except AttributeError:\n            pass\n        return missing_\n    def _deserialize(self, value, attr, data):\n        if self.deserialize_method_name:\n            try:\n                method = utils.callable_or_raise(\n                    getattr(self.parent, self.deserialize_method_name, None)\n                )\n                return method(value)\n            except AttributeError:\n                pass\n        return value\nclass Function(Field):\n    \"\"\"A field that takes the value returned by a function.\n    :param callable serialize: A callable from which to retrieve the value.\n        The function must take a single argument ``obj`` which is the object\n        to be serialized. It can also optionally take a ``context`` argument,\n        which is a dictionary of context variables passed to the serializer.\n        If no callable is provided then the ```load_only``` flag will be set\n        to True.\n    :param callable deserialize: A callable from which to retrieve the value.\n        The function must take a single argument ``value`` which is the value\n        to be deserialized. It can also optionally take a ``context`` argument,\n        which is a dictionary of context variables passed to the deserializer.\n        If no callable is provided then ```value``` will be passed through\n        unchanged.\n    :param callable func: This argument is to be deprecated. It exists for\n        backwards compatiblity. Use serialize instead.\n    .. versionchanged:: 2.3.0\n        Deprecated ``func`` parameter in favor of ``serialize``.\n    \"\"\"\n    _CHECK_ATTRIBUTE = False\n    def __init__(self, serialize=None, deserialize=None, func=None, **kwargs):\n        if func:\n            warnings.warn('\"func\" argument of fields.Function is deprecated. '\n                          'Use the \"serialize\" argument instead.', RemovedInMarshmallow3Warning)\n            serialize = func\n        super(Function, self).__init__(**kwargs)\n        self.serialize_func = self.func = serialize and utils.callable_or_raise(serialize)\n        self.deserialize_func = deserialize and utils.callable_or_raise(deserialize)\n    def _serialize(self, value, attr, obj):\n        try:\n            return self._call_or_raise(self.serialize_func, obj, attr)\n        except AttributeError:  # the object is not expected to have the attribute\n            pass\n        return missing_\n    def _deserialize(self, value, attr, data):\n        if self.deserialize_func:\n            return self._call_or_raise(self.deserialize_func, value, attr)\n        return value\n    def _call_or_raise(self, func, value, attr):\n        if len(utils.get_func_args(func)) > 1:\n            if self.parent.context is None:\n                msg = 'No context available for Function field {0!r}'.format(attr)\n                raise ValidationError(msg)\n            return func(value, self.parent.context)\n        else:\n            return func(value)\nclass Constant(Field):\n    \"\"\"A field that (de)serializes to a preset constant.  If you only want the\n    constant added for serialization or deserialization, you should use\n    ``dump_only=True`` or ``load_only=True`` respectively.\n    :param constant: The constant to return for the field attribute.\n    .. versionadded:: 2.0.0\n    \"\"\"\n    _CHECK_ATTRIBUTE = False\n    def __init__(self, constant, **kwargs):\n        super(Constant, self).__init__(**kwargs)\n        self.constant = constant\n        self.missing = constant\n        self.default = constant\n    def _serialize(self, value, *args, **kwargs):\n        return self.constant\n    def _deserialize(self, value, *args, **kwargs):\n        return self.constant\n# Aliases\nURL = Url\nStr = String\nBool = Boolean\nInt = Integer",
        "file_path": "src/marshmallow/fields.py",
        "chunk_index": 2,
        "metadata": {}
      }
    ]
  },
  "pylint-dev__astroid-1978": {
    "query": "Deprecation warnings from numpy\n### Steps to reproduce\r\n\r\n1. Run pylint over the following test case:\r\n\r\n```\r\n\"\"\"Test case\"\"\"\r\n\r\nimport numpy as np\r\nvalue = np.random.seed(1234)\r\n```\r\n\r\n### Current behavior\r\n```\r\n/home/bje/source/nemo/myenv/lib/python3.10/site-packages/astroid/raw_building.py:470: FutureWarning: In the future `np.long` will be defined as the corresponding NumPy scalar.  (This may have returned Python scalars in past versions.\r\n  getattr(sys.modules[modname], name)\r\n/home/bje/source/nemo/myenv/lib/python3.10/site-packages/astroid/raw_building.py:470: FutureWarning: In the future `np.long` will be defined as the corresponding NumPy scalar.  (This may have returned Python scalars in past versions.\r\n  getattr(sys.modules[modname], name)\r\n```\r\n\r\n### Expected behavior\r\nThere should be no future warnings.\r\n\r\n### python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\" output\r\n2.12.13\n",
    "method": "sliding",
    "top_k": 10,
    "results": [
      {
        "rank": 1,
        "score": 0.6195396780967712,
        "content": "# Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n# For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n# Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n\"\"\"Different utilities for the numpy brains.\"\"\"\nfrom __future__ import annotations\nfrom astroid.builder import extract_node\nfrom astroid.context import InferenceContext\nfrom astroid.nodes.node_classes import Attribute, Import, Name, NodeNG\n# Class subscript is available in numpy starting with version 1.20.0\nNUMPY_VERSION_TYPE_HINTS_SUPPORT = (\"1\", \"20\", \"0\")\ndef numpy_supports_type_hints() -> bool:\n    \"\"\"Returns True if numpy supports type hints.\"\"\"\n    np_ver = _get_numpy_version()\n    return np_ver and np_ver > NUMPY_VERSION_TYPE_HINTS_SUPPORT\ndef _get_numpy_version() -> tuple[str, str, str]:\n    \"\"\"\n    Return the numpy version number if numpy can be imported.\n    Otherwise returns ('0', '0', '0')\n    \"\"\"\n    try:\n        import numpy  # pylint: disable=import-outside-toplevel\n        return tuple(numpy.version.version.split(\".\"))\n    except (ImportError, AttributeError):\n        return (\"0\", \"0\", \"0\")\ndef infer_numpy_member(src, node, context: InferenceContext | None = None):\n    node = extract_node(src)\n    return node.infer(context=context)\ndef _is_a_numpy_module(node: Name) -> bool:\n    \"\"\"\n    Returns True if the node is a representation of a numpy module.\n    For example in :\n        import numpy as np\n        x = np.linspace(1, 2)\n    The node <Name.np> is a representation of the numpy module.\n    :param node: node to test\n    :return: True if the node is a representation of the numpy module.\n    \"\"\"\n    module_nickname = node.name\n    potential_import_target = [\n        x for x in node.lookup(module_nickname)[1] if isinstance(x, Import)\n    ]\n    return any(\n        (\"numpy\", module_nickname) in target.names or (\"numpy\", None) in target.names\n        for target in potential_import_target\n    )\ndef looks_like_numpy_member(member_name: str, node: NodeNG) -> bool:\n    \"\"\"\n    Returns True if the node is a member of numpy whose\n    name is member_name.\n    :param member_name: name of the member\n    :param node: node to test\n    :return: True if the node is a member of numpy\n    \"\"\"\n    if (\n        isinstance(node, Attribute)\n        and node.attrname == member_name\n        and isinstance(node.expr, Name)\n        and _is_a_numpy_module(node.expr)\n    ):\n        return True\n    if (\n        isinstance(node, Name)\n        and node.name == member_name\n        and node.root().name.startswith(\"numpy\")\n    ):\n        return True\n    return False",
        "file_path": "astroid/brain/brain_numpy_utils.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 2,
        "score": 0.5855466723442078,
        "content": "# Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n# For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n# Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n\"\"\"Astroid hooks for numpy.core.function_base module.\"\"\"\nimport functools\nfrom astroid.brain.brain_numpy_utils import infer_numpy_member, looks_like_numpy_member\nfrom astroid.inference_tip import inference_tip\nfrom astroid.manager import AstroidManager\nfrom astroid.nodes.node_classes import Attribute\nMETHODS_TO_BE_INFERRED = {\n    \"linspace\": \"\"\"def linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None, axis=0):\n            return numpy.ndarray([0, 0])\"\"\",\n    \"logspace\": \"\"\"def logspace(start, stop, num=50, endpoint=True, base=10.0, dtype=None, axis=0):\n            return numpy.ndarray([0, 0])\"\"\",\n    \"geomspace\": \"\"\"def geomspace(start, stop, num=50, endpoint=True, dtype=None, axis=0):\n            return numpy.ndarray([0, 0])\"\"\",\n}\nfor func_name, func_src in METHODS_TO_BE_INFERRED.items():\n    inference_function = functools.partial(infer_numpy_member, func_src)\n    AstroidManager().register_transform(\n        Attribute,\n        inference_tip(inference_function),\n        functools.partial(looks_like_numpy_member, func_name),\n    )",
        "file_path": "astroid/brain/brain_numpy_core_function_base.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 3,
        "score": 0.5740030407905579,
        "content": "# Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n# For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n# Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n\"\"\"Astroid hooks for numpy.core.numeric module.\"\"\"\nimport functools\nfrom astroid.brain.brain_numpy_utils import infer_numpy_member, looks_like_numpy_member\nfrom astroid.brain.helpers import register_module_extender\nfrom astroid.builder import parse\nfrom astroid.inference_tip import inference_tip\nfrom astroid.manager import AstroidManager\nfrom astroid.nodes.node_classes import Attribute\ndef numpy_core_numeric_transform():\n    return parse(\n        \"\"\"\n    # different functions defined in numeric.py\n    import numpy\n    def zeros_like(a, dtype=None, order='K', subok=True, shape=None): return numpy.ndarray((0, 0))\n    def ones_like(a, dtype=None, order='K', subok=True, shape=None): return numpy.ndarray((0, 0))\n    def full_like(a, fill_value, dtype=None, order='K', subok=True, shape=None): return numpy.ndarray((0, 0))\n        \"\"\"\n    )\nregister_module_extender(\n    AstroidManager(), \"numpy.core.numeric\", numpy_core_numeric_transform\n)\nMETHODS_TO_BE_INFERRED = {\n    \"ones\": \"\"\"def ones(shape, dtype=None, order='C'):\n            return numpy.ndarray([0, 0])\"\"\"\n}\nfor method_name, function_src in METHODS_TO_BE_INFERRED.items():\n    inference_function = functools.partial(infer_numpy_member, function_src)\n    AstroidManager().register_transform(\n        Attribute,\n        inference_tip(inference_function),\n        functools.partial(looks_like_numpy_member, method_name),\n    )",
        "file_path": "astroid/brain/brain_numpy_core_numeric.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 4,
        "score": 0.5547617077827454,
        "content": "# Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n# For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n# Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n# TODO(hippo91) : correct the methods signature.\n\"\"\"Astroid hooks for numpy.core.numerictypes module.\"\"\"\nfrom astroid.brain.brain_numpy_utils import numpy_supports_type_hints\nfrom astroid.brain.helpers import register_module_extender\nfrom astroid.builder import parse\nfrom astroid.manager import AstroidManager\ndef numpy_core_numerictypes_transform():\n    # TODO: Uniformize the generic API with the ndarray one.\n    #       According to numpy doc the generic object should expose\n    #       the same API than ndarray. This has been done here partially\n    #       through the astype method.\n    generic_src = \"\"\"\n    class generic(object):\n        def __init__(self, value):\n            self.T = np.ndarray([0, 0])\n            self.base = None\n            self.data = None\n            self.dtype = None\n            self.flags = None\n            # Should be a numpy.flatiter instance but not available for now\n            # Putting an array instead so that iteration and indexing are authorized\n            self.flat = np.ndarray([0, 0])\n            self.imag = None\n            self.itemsize = None\n            self.nbytes = None\n            self.ndim = None\n            self.real = None\n            self.size = None\n            self.strides = None\n        def all(self): return uninferable\n        def any(self): return uninferable\n        def argmax(self): return uninferable\n        def argmin(self): return uninferable\n        def argsort(self): return uninferable\n        def astype(self, dtype, order='K', casting='unsafe', subok=True, copy=True): return np.ndarray([0, 0])\n        def base(self): return uninferable\n        def byteswap(self): return uninferable\n        def choose(self): return uninferable\n        def clip(self): return uninferable\n        def compress(self): return uninferable\n        def conj(self): return uninferable\n        def conjugate(self): return uninferable\n        def copy(self): return uninferable\n        def cumprod(self): return uninferable\n        def cumsum(self): return uninferable\n        def data(self): return uninferable\n        def diagonal(self): return uninferable\n        def dtype(self): return uninferable\n        def dump(self): return uninferable\n        def dumps(self): return uninferable\n        def fill(self): return uninferable\n        def flags(self): return uninferable\n        def flat(self): return uninferable\n        def flatten(self): return uninferable\n        def getfield(self): return uninferable\n        def imag(self): return uninferable\n        def item(self): return uninferable\n        def itemset(self): return uninferable\n        def itemsize(self): return uninferable\n        def max(self): return uninferable\n        def mean(self): return uninferable\n        def min(self): return uninferable\n        def nbytes(self): return uninferable\n        def ndim(self): return uninferable\n        def newbyteorder(self): return uninferable\n        def nonzero(self): return uninferable\n        def prod(self): return uninferable\n        def ptp(self): return uninferable\n        def put(self): return uninferable\n        def ravel(self): return uninferable\n        def real(self): return uninferable\n        def repeat(self): return uninferable\n        def reshape(self): return uninferable\n        def resize(self): return uninferable\n        def round(self): return uninferable\n        def searchsorted(self): return uninferable\n        def setfield(self): return uninferable\n        def setflags(self): return uninferable\n        def shape(self): return uninferable\n        def size(self): return uninferable\n        def sort(self): return uninferable\n        def squeeze(self): return uninferable\n        def std(self): return uninferable\n        def strides(self): return uninferable\n        def sum(self): return uninferable\n        def swapaxes(self): return uninferable\n        def take(self): return uninferable\n        def tobytes(self): return uninferable\n        def tofile(self): return uninferable\n        def tolist(self): return uninferable\n        def tostring(self): return uninferable\n        def trace(self): return uninferable\n        def transpose(self): return uninferable\n        def var(self): return uninferable\n        def view(self): return uninferable\n        \"\"\"\n    if numpy_supports_type_hints():\n        generic_src += \"\"\"\n        @classmethod\n        def __class_getitem__(cls, value):\n            return cls\n        \"\"\"\n    return parse(\n        generic_src\n        + \"\"\"\n    class dtype(object):\n        def __init__(self, obj, align=False, copy=False):\n            self.alignment = None\n            self.base = None\n            self.byteorder = None\n            self.char = None\n            self.descr = None\n            self.fields = None\n            self.flags = None\n            self.hasobject = None\n            self.isalignedstruct = None\n            self.isbuiltin = None\n            self.isnative = None\n            self.itemsize = None\n            self.kind = None\n            self.metadata = None\n            self.name = None\n            self.names = None\n            self.num = None\n            self.shape = None\n            self.str = None\n            self.subdtype = None\n            self.type = None\n        def newbyteorder(self, new_order='S'): return uninferable\n        def __neg__(self): return uninferable\n    class busdaycalendar(object):\n        def __init__(self, weekmask='1111100', holidays=None):\n            self.holidays = None\n            self.weekmask = None\n    class flexible(generic): pass\n    class bool_(generic): pass\n    class number(generic):\n        def __neg__(self): return uninferable\n    class datetime64(generic):\n        def __init__(self, nb, unit=None): pass\n    class void(flexible):\n        def __init__(self, *args, **kwargs):\n            self.base = None\n            self.dtype = None\n            self.flags = None\n        def getfield(self): return uninferable\n        def setfield(self): return uninferable\n    class character(flexible): pass\n    class integer(number):\n        def __init__(self, value):\n           self.denominator = None\n           self.numerator = None\n    class inexact(number): pass\n    class str_(str, character):\n        def maketrans(self, x, y=None, z=None): return uninferable\n    class bytes_(bytes, character):\n        def fromhex(self, string): return uninferable\n        def maketrans(self, frm, to): return uninferable\n    class signedinteger(integer): pass\n    class unsignedinteger(integer): pass\n    class complexfloating(inexact): pass\n    class floating(inexact): pass\n    class float64(floating, float):\n        def fromhex(self, string): return uninferable\n    class uint64(unsignedinteger): pass\n    class complex64(complexfloating): pass\n    class int16(signedinteger): pass\n    class float96(floating): pass\n    class int8(signedinteger): pass\n    class uint32(unsignedinteger): pass\n    class uint8(unsignedinteger): pass\n    class _typedict(dict): pass\n    class complex192(complexfloating): pass\n    class timedelta64(signedinteger):\n        def __init__(self, nb, unit=None): pass\n    class int32(signedinteger): pass\n    class uint16(unsignedinteger): pass\n    class float32(floating): pass\n    class complex128(complexfloating, complex): pass\n    class float16(floating): pass\n    class int64(signedinteger): pass\n    buffer_type = memoryview\n    bool8 = bool_\n    byte = int8\n    bytes0 = bytes_\n    cdouble = complex128\n    cfloat = complex128\n    clongdouble = complex192\n    clongfloat = complex192\n    complex_ = complex128\n    csingle = complex64\n    double = float64\n    float_ = float64\n    half = float16\n    int0 = int32\n    int_ = int32\n    intc = int32\n    intp = int32\n    long = int32\n    longcomplex = complex192\n    longdouble = float96\n    longfloat = float96\n    longlong = int64\n    object0 = object_\n    object_ = object_\n    short = int16\n    single = float32\n    singlecomplex = complex64\n    str0 = str_\n    string_ = bytes_\n    ubyte = uint8\n    uint = uint32\n    uint0 = uint32\n    uintc = uint32\n    uintp = uint32\n    ulonglong = uint64\n    unicode = str_\n    unicode_ = str_\n    ushort = uint16\n    void0 = void\n    \"\"\"\n    )\nregister_module_extender(\n    AstroidManager(), \"numpy.core.numerictypes\", numpy_core_numerictypes_transform\n)",
        "file_path": "astroid/brain/brain_numpy_core_numerictypes.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 5,
        "score": 0.5529847741127014,
        "content": "# Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n# For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n# Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n\"\"\"Astroid hooks for numpy.core.multiarray module.\"\"\"\nimport functools\nfrom astroid.brain.brain_numpy_utils import infer_numpy_member, looks_like_numpy_member\nfrom astroid.brain.helpers import register_module_extender\nfrom astroid.builder import parse\nfrom astroid.inference_tip import inference_tip\nfrom astroid.manager import AstroidManager\nfrom astroid.nodes.node_classes import Attribute, Name\ndef numpy_core_multiarray_transform():\n    return parse(\n        \"\"\"\n    # different functions defined in multiarray.py\n    def inner(a, b):\n        return numpy.ndarray([0, 0])\n    def vdot(a, b):\n        return numpy.ndarray([0, 0])\n        \"\"\"\n    )\nregister_module_extender(\n    AstroidManager(), \"numpy.core.multiarray\", numpy_core_multiarray_transform\n)\nMETHODS_TO_BE_INFERRED = {\n    \"array\": \"\"\"def array(object, dtype=None, copy=True, order='K', subok=False, ndmin=0):\n            return numpy.ndarray([0, 0])\"\"\",\n    \"dot\": \"\"\"def dot(a, b, out=None):\n            return numpy.ndarray([0, 0])\"\"\",\n    \"empty_like\": \"\"\"def empty_like(a, dtype=None, order='K', subok=True):\n            return numpy.ndarray((0, 0))\"\"\",\n    \"concatenate\": \"\"\"def concatenate(arrays, axis=None, out=None):\n            return numpy.ndarray((0, 0))\"\"\",\n    \"where\": \"\"\"def where(condition, x=None, y=None):\n            return numpy.ndarray([0, 0])\"\"\",\n    \"empty\": \"\"\"def empty(shape, dtype=float, order='C'):\n            return numpy.ndarray([0, 0])\"\"\",\n    \"bincount\": \"\"\"def bincount(x, weights=None, minlength=0):\n            return numpy.ndarray([0, 0])\"\"\",\n    \"busday_count\": \"\"\"def busday_count(\n        begindates, enddates, weekmask='1111100', holidays=[], busdaycal=None, out=None\n    ):\n        return numpy.ndarray([0, 0])\"\"\",\n    \"busday_offset\": \"\"\"def busday_offset(\n        dates, offsets, roll='raise', weekmask='1111100', holidays=None,\n        busdaycal=None, out=None\n    ):\n        return numpy.ndarray([0, 0])\"\"\",\n    \"can_cast\": \"\"\"def can_cast(from_, to, casting='safe'):\n            return True\"\"\",\n    \"copyto\": \"\"\"def copyto(dst, src, casting='same_kind', where=True):\n            return None\"\"\",\n    \"datetime_as_string\": \"\"\"def datetime_as_string(arr, unit=None, timezone='naive', casting='same_kind'):\n            return numpy.ndarray([0, 0])\"\"\",\n    \"is_busday\": \"\"\"def is_busday(dates, weekmask='1111100', holidays=None, busdaycal=None, out=None):\n            return numpy.ndarray([0, 0])\"\"\",\n    \"lexsort\": \"\"\"def lexsort(keys, axis=-1):\n            return numpy.ndarray([0, 0])\"\"\",\n    \"may_share_memory\": \"\"\"def may_share_memory(a, b, max_work=None):\n            return True\"\"\",\n    # Not yet available because dtype is not yet present in those brains\n    #     \"min_scalar_type\": \"\"\"def min_scalar_type(a):\n    #             return numpy.dtype('int16')\"\"\",\n    \"packbits\": \"\"\"def packbits(a, axis=None, bitorder='big'):\n            return numpy.ndarray([0, 0])\"\"\",\n    # Not yet available because dtype is not yet present in those brains\n    #     \"result_type\": \"\"\"def result_type(*arrays_and_dtypes):\n    #             return numpy.dtype('int16')\"\"\",\n    \"shares_memory\": \"\"\"def shares_memory(a, b, max_work=None):\n            return True\"\"\",\n    \"unpackbits\": \"\"\"def unpackbits(a, axis=None, count=None, bitorder='big'):\n            return numpy.ndarray([0, 0])\"\"\",\n    \"unravel_index\": \"\"\"def unravel_index(indices, shape, order='C'):\n            return (numpy.ndarray([0, 0]),)\"\"\",\n    \"zeros\": \"\"\"def zeros(shape, dtype=float, order='C'):\n            return numpy.ndarray([0, 0])\"\"\",\n}\nfor method_name, function_src in METHODS_TO_BE_INFERRED.items():\n    inference_function = functools.partial(infer_numpy_member, function_src)\n    AstroidManager().register_transform(\n        Attribute,\n        inference_tip(inference_function),\n        functools.partial(looks_like_numpy_member, method_name),\n    )\n    AstroidManager().register_transform(\n        Name,\n        inference_tip(inference_function),\n        functools.partial(looks_like_numpy_member, method_name),\n    )",
        "file_path": "astroid/brain/brain_numpy_core_multiarray.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 6,
        "score": 0.5376266241073608,
        "content": "# Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n# For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n# Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n# Note: starting with version 1.18 numpy module has `__getattr__` method which prevent\n# `pylint` to emit `no-member` message for all numpy's attributes. (see pylint's module\n# typecheck in `_emit_no_member` function)\n\"\"\"Astroid hooks for numpy.core.umath module.\"\"\"\nfrom astroid.brain.helpers import register_module_extender\nfrom astroid.builder import parse\nfrom astroid.manager import AstroidManager\ndef numpy_core_umath_transform():\n    ufunc_optional_keyword_arguments = (\n        \"\"\"out=None, where=True, casting='same_kind', order='K', \"\"\"\n        \"\"\"dtype=None, subok=True\"\"\"\n    )\n    return parse(\n        \"\"\"\n    class FakeUfunc:\n        def __init__(self):\n            self.__doc__ = str()\n            self.__name__ = str()\n            self.nin = 0\n            self.nout = 0\n            self.nargs = 0\n            self.ntypes = 0\n            self.types = None\n            self.identity = None\n            self.signature = None\n        @classmethod\n        def reduce(cls, a, axis=None, dtype=None, out=None):\n            return numpy.ndarray([0, 0])\n        @classmethod\n        def accumulate(cls, array, axis=None, dtype=None, out=None):\n            return numpy.ndarray([0, 0])\n        @classmethod\n        def reduceat(cls, a, indices, axis=None, dtype=None, out=None):\n            return numpy.ndarray([0, 0])\n        @classmethod\n        def outer(cls, A, B, **kwargs):\n            return numpy.ndarray([0, 0])\n        @classmethod\n        def at(cls, a, indices, b=None):\n            return numpy.ndarray([0, 0])\n    class FakeUfuncOneArg(FakeUfunc):\n        def __call__(self, x, {opt_args:s}):\n            return numpy.ndarray([0, 0])\n    class FakeUfuncOneArgBis(FakeUfunc):\n        def __call__(self, x, {opt_args:s}):\n            return numpy.ndarray([0, 0]), numpy.ndarray([0, 0])\n    class FakeUfuncTwoArgs(FakeUfunc):\n        def __call__(self, x1, x2, {opt_args:s}):\n            return numpy.ndarray([0, 0])\n    # Constants\n    e = 2.718281828459045\n    euler_gamma = 0.5772156649015329\n    # One arg functions with optional kwargs\n    arccos = FakeUfuncOneArg()\n    arccosh = FakeUfuncOneArg()\n    arcsin = FakeUfuncOneArg()\n    arcsinh = FakeUfuncOneArg()\n    arctan = FakeUfuncOneArg()\n    arctanh = FakeUfuncOneArg()\n    cbrt = FakeUfuncOneArg()\n    conj = FakeUfuncOneArg()\n    conjugate = FakeUfuncOneArg()\n    cosh = FakeUfuncOneArg()\n    deg2rad = FakeUfuncOneArg()\n    degrees = FakeUfuncOneArg()\n    exp2 = FakeUfuncOneArg()\n    expm1 = FakeUfuncOneArg()\n    fabs = FakeUfuncOneArg()\n    frexp = FakeUfuncOneArgBis()\n    isfinite = FakeUfuncOneArg()\n    isinf = FakeUfuncOneArg()\n    log = FakeUfuncOneArg()\n    log1p = FakeUfuncOneArg()\n    log2 = FakeUfuncOneArg()\n    logical_not = FakeUfuncOneArg()\n    modf = FakeUfuncOneArgBis()\n    negative = FakeUfuncOneArg()\n    positive = FakeUfuncOneArg()\n    rad2deg = FakeUfuncOneArg()\n    radians = FakeUfuncOneArg()\n    reciprocal = FakeUfuncOneArg()\n    rint = FakeUfuncOneArg()\n    sign = FakeUfuncOneArg()\n    signbit = FakeUfuncOneArg()\n    sinh = FakeUfuncOneArg()\n    spacing = FakeUfuncOneArg()\n    square = FakeUfuncOneArg()\n    tan = FakeUfuncOneArg()\n    tanh = FakeUfuncOneArg()\n    trunc = FakeUfuncOneArg()\n    # Two args functions with optional kwargs\n    add = FakeUfuncTwoArgs()\n    bitwise_and = FakeUfuncTwoArgs()\n    bitwise_or = FakeUfuncTwoArgs()\n    bitwise_xor = FakeUfuncTwoArgs()\n    copysign = FakeUfuncTwoArgs()\n    divide = FakeUfuncTwoArgs()\n    divmod = FakeUfuncTwoArgs()\n    equal = FakeUfuncTwoArgs()\n    float_power = FakeUfuncTwoArgs()\n    floor_divide = FakeUfuncTwoArgs()\n    fmax = FakeUfuncTwoArgs()\n    fmin = FakeUfuncTwoArgs()\n    fmod = FakeUfuncTwoArgs()\n    greater = FakeUfuncTwoArgs()\n    gcd = FakeUfuncTwoArgs()\n    hypot = FakeUfuncTwoArgs()\n    heaviside = FakeUfuncTwoArgs()\n    lcm = FakeUfuncTwoArgs()\n    ldexp = FakeUfuncTwoArgs()\n    left_shift = FakeUfuncTwoArgs()\n    less = FakeUfuncTwoArgs()\n    logaddexp = FakeUfuncTwoArgs()\n    logaddexp2 = FakeUfuncTwoArgs()\n    logical_and = FakeUfuncTwoArgs()\n    logical_or = FakeUfuncTwoArgs()\n    logical_xor = FakeUfuncTwoArgs()\n    maximum = FakeUfuncTwoArgs()\n    minimum = FakeUfuncTwoArgs()\n    multiply = FakeUfuncTwoArgs()\n    nextafter = FakeUfuncTwoArgs()\n    not_equal = FakeUfuncTwoArgs()\n    power = FakeUfuncTwoArgs()\n    remainder = FakeUfuncTwoArgs()\n    right_shift = FakeUfuncTwoArgs()\n    subtract = FakeUfuncTwoArgs()\n    true_divide = FakeUfuncTwoArgs()\n    \"\"\".format(\n            opt_args=ufunc_optional_keyword_arguments\n        )\n    )\nregister_module_extender(\n    AstroidManager(), \"numpy.core.umath\", numpy_core_umath_transform\n)",
        "file_path": "astroid/brain/brain_numpy_core_umath.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 7,
        "score": 0.537498950958252,
        "content": "# Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n# For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n# Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\nimport importlib\nimport sys\nimport warnings\nfrom typing import Any\nimport lazy_object_proxy\nif sys.version_info >= (3, 8):\n    from typing import Literal\nelse:\n    from typing_extensions import Literal\ndef lazy_descriptor(obj):\n    class DescriptorProxy(lazy_object_proxy.Proxy):\n        def __get__(self, instance, owner=None):\n            return self.__class__.__get__(self, instance)\n    return DescriptorProxy(obj)\ndef lazy_import(module_name: str) -> lazy_object_proxy.Proxy:\n    return lazy_object_proxy.Proxy(\n        lambda: importlib.import_module(\".\" + module_name, \"astroid\")\n    )\n@object.__new__\nclass Uninferable:\n    \"\"\"Special inference object, which is returned when inference fails.\"\"\"\n    def __repr__(self) -> str:\n        return \"Uninferable\"\n    __str__ = __repr__\n    def __getattribute__(self, name: str) -> Any:\n        if name == \"next\":\n            raise AttributeError(\"next method should not be called\")\n        if name.startswith(\"__\") and name.endswith(\"__\"):\n            return object.__getattribute__(self, name)\n        if name == \"accept\":\n            return object.__getattribute__(self, name)\n        return self\n    def __call__(self, *args, **kwargs):\n        return self\n    def __bool__(self) -> Literal[False]:\n        return False\n    __nonzero__ = __bool__\n    def accept(self, visitor):\n        return visitor.visit_uninferable(self)\nclass BadOperationMessage:\n    \"\"\"Object which describes a TypeError occurred somewhere in the inference chain.\n    This is not an exception, but a container object which holds the types and\n    the error which occurred.\n    \"\"\"\nclass BadUnaryOperationMessage(BadOperationMessage):\n    \"\"\"Object which describes operational failures on UnaryOps.\"\"\"\n    def __init__(self, operand, op, error):\n        self.operand = operand\n        self.op = op\n        self.error = error\n    @property\n    def _object_type_helper(self):\n        helpers = lazy_import(\"helpers\")\n        return helpers.object_type\n    def _object_type(self, obj):\n        objtype = self._object_type_helper(obj)\n        if objtype is Uninferable:\n            return None\n        return objtype\n    def __str__(self) -> str:\n        if hasattr(self.operand, \"name\"):\n            operand_type = self.operand.name\n        else:\n            object_type = self._object_type(self.operand)\n            if hasattr(object_type, \"name\"):\n                operand_type = object_type.name\n            else:\n                # Just fallback to as_string\n                operand_type = object_type.as_string()\n        msg = \"bad operand type for unary {}: {}\"\n        return msg.format(self.op, operand_type)\nclass BadBinaryOperationMessage(BadOperationMessage):\n    \"\"\"Object which describes type errors for BinOps.\"\"\"\n    def __init__(self, left_type, op, right_type):\n        self.left_type = left_type\n        self.right_type = right_type\n        self.op = op\n    def __str__(self) -> str:\n        msg = \"unsupported operand type(s) for {}: {!r} and {!r}\"\n        return msg.format(self.op, self.left_type.name, self.right_type.name)\ndef _instancecheck(cls, other) -> bool:\n    wrapped = cls.__wrapped__\n    other_cls = other.__class__\n    is_instance_of = wrapped is other_cls or issubclass(other_cls, wrapped)\n    warnings.warn(\n        \"%r is deprecated and slated for removal in astroid \"\n        \"2.0, use %r instead\" % (cls.__class__.__name__, wrapped.__name__),\n        PendingDeprecationWarning,\n        stacklevel=2,\n    )\n    return is_instance_of\ndef proxy_alias(alias_name, node_type):\n    \"\"\"Get a Proxy from the given name to the given node type.\"\"\"\n    proxy = type(\n        alias_name,\n        (lazy_object_proxy.Proxy,),\n        {\n            \"__class__\": object.__dict__[\"__class__\"],\n            \"__instancecheck__\": _instancecheck,\n        },\n    )\n    return proxy(lambda: node_type)\ndef check_warnings_filter() -> bool:\n    \"\"\"Return True if any other than the default DeprecationWarning filter is enabled.\n    https://docs.python.org/3/library/warnings.html#default-warning-filter\n    \"\"\"\n    return any(\n        issubclass(DeprecationWarning, filter[2])\n        and filter[0] != \"ignore\"\n        and filter[3] != \"__main__\"\n        for filter in warnings.filters\n    )",
        "file_path": "astroid/util.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 8,
        "score": 0.518958330154419,
        "content": "# Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n# For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n# Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n\"\"\"Astroid hooks for numpy ndarray class.\"\"\"\nfrom __future__ import annotations\nfrom astroid.brain.brain_numpy_utils import numpy_supports_type_hints\nfrom astroid.builder import extract_node\nfrom astroid.context import InferenceContext\nfrom astroid.inference_tip import inference_tip\nfrom astroid.manager import AstroidManager\nfrom astroid.nodes.node_classes import Attribute\ndef infer_numpy_ndarray(node, context: InferenceContext | None = None):\n    ndarray = \"\"\"\n    class ndarray(object):\n        def __init__(self, shape, dtype=float, buffer=None, offset=0,\n                     strides=None, order=None):\n            self.T = numpy.ndarray([0, 0])\n            self.base = None\n            self.ctypes = None\n            self.data = None\n            self.dtype = None\n            self.flags = None\n            # Should be a numpy.flatiter instance but not available for now\n            # Putting an array instead so that iteration and indexing are authorized\n            self.flat = np.ndarray([0, 0])\n            self.imag = np.ndarray([0, 0])\n            self.itemsize = None\n            self.nbytes = None\n            self.ndim = None\n            self.real = np.ndarray([0, 0])\n            self.shape = numpy.ndarray([0, 0])\n            self.size = None\n            self.strides = None\n        def __abs__(self): return numpy.ndarray([0, 0])\n        def __add__(self, value): return numpy.ndarray([0, 0])\n        def __and__(self, value): return numpy.ndarray([0, 0])\n        def __array__(self, dtype=None): return numpy.ndarray([0, 0])\n        def __array_wrap__(self, obj): return numpy.ndarray([0, 0])\n        def __contains__(self, key): return True\n        def __copy__(self): return numpy.ndarray([0, 0])\n        def __deepcopy__(self, memo): return numpy.ndarray([0, 0])\n        def __divmod__(self, value): return (numpy.ndarray([0, 0]), numpy.ndarray([0, 0]))\n        def __eq__(self, value): return numpy.ndarray([0, 0])\n        def __float__(self): return 0.\n        def __floordiv__(self): return numpy.ndarray([0, 0])\n        def __ge__(self, value): return numpy.ndarray([0, 0])\n        def __getitem__(self, key): return uninferable\n        def __gt__(self, value): return numpy.ndarray([0, 0])\n        def __iadd__(self, value): return numpy.ndarray([0, 0])\n        def __iand__(self, value): return numpy.ndarray([0, 0])\n        def __ifloordiv__(self, value): return numpy.ndarray([0, 0])\n        def __ilshift__(self, value): return numpy.ndarray([0, 0])\n        def __imod__(self, value): return numpy.ndarray([0, 0])\n        def __imul__(self, value): return numpy.ndarray([0, 0])\n        def __int__(self): return 0\n        def __invert__(self): return numpy.ndarray([0, 0])\n        def __ior__(self, value): return numpy.ndarray([0, 0])\n        def __ipow__(self, value): return numpy.ndarray([0, 0])\n        def __irshift__(self, value): return numpy.ndarray([0, 0])\n        def __isub__(self, value): return numpy.ndarray([0, 0])\n        def __itruediv__(self, value): return numpy.ndarray([0, 0])\n        def __ixor__(self, value): return numpy.ndarray([0, 0])\n        def __le__(self, value): return numpy.ndarray([0, 0])\n        def __len__(self): return 1\n        def __lshift__(self, value): return numpy.ndarray([0, 0])\n        def __lt__(self, value): return numpy.ndarray([0, 0])\n        def __matmul__(self, value): return numpy.ndarray([0, 0])\n        def __mod__(self, value): return numpy.ndarray([0, 0])\n        def __mul__(self, value): return numpy.ndarray([0, 0])\n        def __ne__(self, value): return numpy.ndarray([0, 0])\n        def __neg__(self): return numpy.ndarray([0, 0])\n        def __or__(self, value): return numpy.ndarray([0, 0])\n        def __pos__(self): return numpy.ndarray([0, 0])\n        def __pow__(self): return numpy.ndarray([0, 0])\n        def __repr__(self): return str()\n        def __rshift__(self): return numpy.ndarray([0, 0])\n        def __setitem__(self, key, value): return uninferable\n        def __str__(self): return str()\n        def __sub__(self, value): return numpy.ndarray([0, 0])\n        def __truediv__(self, value): return numpy.ndarray([0, 0])\n        def __xor__(self, value): return numpy.ndarray([0, 0])\n        def all(self, axis=None, out=None, keepdims=False): return np.ndarray([0, 0])\n        def any(self, axis=None, out=None, keepdims=False): return np.ndarray([0, 0])\n        def argmax(self, axis=None, out=None): return np.ndarray([0, 0])\n        def argmin(self, axis=None, out=None): return np.ndarray([0, 0])\n        def argpartition(self, kth, axis=-1, kind='introselect', order=None): return np.ndarray([0, 0])\n        def argsort(self, axis=-1, kind='quicksort', order=None): return np.ndarray([0, 0])\n        def astype(self, dtype, order='K', casting='unsafe', subok=True, copy=True): return np.ndarray([0, 0])\n        def byteswap(self, inplace=False): return np.ndarray([0, 0])\n        def choose(self, choices, out=None, mode='raise'): return np.ndarray([0, 0])\n        def clip(self, min=None, max=None, out=None): return np.ndarray([0, 0])\n        def compress(self, condition, axis=None, out=None): return np.ndarray([0, 0])\n        def conj(self): return np.ndarray([0, 0])\n        def conjugate(self): return np.ndarray([0, 0])\n        def copy(self, order='C'): return np.ndarray([0, 0])\n        def cumprod(self, axis=None, dtype=None, out=None): return np.ndarray([0, 0])\n        def cumsum(self, axis=None, dtype=None, out=None): return np.ndarray([0, 0])\n        def diagonal(self, offset=0, axis1=0, axis2=1): return np.ndarray([0, 0])\n        def dot(self, b, out=None): return np.ndarray([0, 0])\n        def dump(self, file): return None\n        def dumps(self): return str()\n        def fill(self, value): return None\n        def flatten(self, order='C'): return np.ndarray([0, 0])\n        def getfield(self, dtype, offset=0): return np.ndarray([0, 0])\n        def item(self, *args): return uninferable\n        def itemset(self, *args): return None\n        def max(self, axis=None, out=None): return np.ndarray([0, 0])\n        def mean(self, axis=None, dtype=None, out=None, keepdims=False): return np.ndarray([0, 0])\n        def min(self, axis=None, out=None, keepdims=False): return np.ndarray([0, 0])\n        def newbyteorder(self, new_order='S'): return np.ndarray([0, 0])\n        def nonzero(self): return (1,)\n        def partition(self, kth, axis=-1, kind='introselect', order=None): return None\n        def prod(self, axis=None, dtype=None, out=None, keepdims=False): return np.ndarray([0, 0])\n        def ptp(self, axis=None, out=None): return np.ndarray([0, 0])\n        def put(self, indices, values, mode='raise'): return None\n        def ravel(self, order='C'): return np.ndarray([0, 0])\n        def repeat(self, repeats, axis=None): return np.ndarray([0, 0])\n        def reshape(self, shape, order='C'): return np.ndarray([0, 0])\n        def resize(self, new_shape, refcheck=True): return None\n        def round(self, decimals=0, out=None): return np.ndarray([0, 0])\n        def searchsorted(self, v, side='left', sorter=None): return np.ndarray([0, 0])\n        def setfield(self, val, dtype, offset=0): return None\n        def setflags(self, write=None, align=None, uic=None): return None\n        def sort(self, axis=-1, kind='quicksort', order=None): return None\n        def squeeze(self, axis=None): return np.ndarray([0, 0])\n        def std(self, axis=None, dtype=None, out=None, ddof=0, keepdims=False): return np.ndarray([0, 0])\n        def sum(self, axis=None, dtype=None, out=None, keepdims=False): return np.ndarray([0, 0])\n        def swapaxes(self, axis1, axis2): return np.ndarray([0, 0])\n        def take(self, indices, axis=None, out=None, mode='raise'): return np.ndarray([0, 0])\n        def tobytes(self, order='C'): return b''\n        def tofile(self, fid, sep=\"\", format=\"%s\"): return None\n        def tolist(self, ): return []\n        def tostring(self, order='C'): return b''\n        def trace(self, offset=0, axis1=0, axis2=1, dtype=None, out=None): return np.ndarray([0, 0])\n        def transpose(self, *axes): return np.ndarray([0, 0])\n        def var(self, axis=None, dtype=None, out=None, ddof=0, keepdims=False): return np.ndarray([0, 0])\n        def view(self, dtype=None, type=None): return np.ndarray([0, 0])\n    \"\"\"\n    if numpy_supports_type_hints():\n        ndarray += \"\"\"\n        @classmethod\n        def __class_getitem__(cls, value):\n            return cls\n        \"\"\"\n    node = extract_node(ndarray)\n    return node.infer(context=context)\ndef _looks_like_numpy_ndarray(node) -> bool:\n    return isinstance(node, Attribute) and node.attrname == \"ndarray\"\nAstroidManager().register_transform(\n    Attribute,\n    inference_tip(infer_numpy_ndarray),\n    _looks_like_numpy_ndarray,\n)",
        "file_path": "astroid/brain/brain_numpy_ndarray.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 9,
        "score": 0.4993884563446045,
        "content": "# Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n# For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n# Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n\"\"\"Astroid hooks for numpy.core.fromnumeric module.\"\"\"\nfrom astroid.brain.helpers import register_module_extender\nfrom astroid.builder import parse\nfrom astroid.manager import AstroidManager\ndef numpy_core_fromnumeric_transform():\n    return parse(\n        \"\"\"\n    def sum(a, axis=None, dtype=None, out=None, keepdims=None, initial=None):\n        return numpy.ndarray([0, 0])\n    \"\"\"\n    )\nregister_module_extender(\n    AstroidManager(), \"numpy.core.fromnumeric\", numpy_core_fromnumeric_transform\n)",
        "file_path": "astroid/brain/brain_numpy_core_fromnumeric.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 10,
        "score": 0.4942384362220764,
        "content": "# Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n# For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n# Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n\"\"\"Astroid hooks for numpy ma module.\"\"\"\nfrom astroid.brain.helpers import register_module_extender\nfrom astroid.builder import parse\nfrom astroid.manager import AstroidManager\ndef numpy_ma_transform():\n    \"\"\"\n    Infer the call of various numpy.ma functions.\n    :param node: node to infer\n    :param context: inference context\n    \"\"\"\n    return parse(\n        \"\"\"\n    import numpy.ma\n    def masked_where(condition, a, copy=True):\n        return numpy.ma.masked_array(a, mask=[])\n    def masked_invalid(a, copy=True):\n        return numpy.ma.masked_array(a, mask=[])\n    \"\"\"\n    )\nregister_module_extender(AstroidManager(), \"numpy.ma\", numpy_ma_transform)",
        "file_path": "astroid/brain/brain_numpy_ma.py",
        "chunk_index": 0,
        "metadata": {}
      }
    ]
  },
  "pylint-dev__astroid-1333": {
    "query": "astroid 2.9.1 breaks pylint with missing __init__.py: F0010: error while code parsing: Unable to load file __init__.py\n### Steps to reproduce\r\n> Steps provided are for Windows 11, but initial problem found in Ubuntu 20.04\r\n\r\n> Update 2022-01-04: Corrected repro steps and added more environment details\r\n\r\n1. Set up simple repo with following structure (all files can be empty):\r\n```\r\nroot_dir/\r\n|--src/\r\n|----project/ # Notice the missing __init__.py\r\n|------file.py # It can be empty, but I added `import os` at the top\r\n|----__init__.py\r\n```\r\n2. Open a command prompt\r\n3. `cd root_dir`\r\n4. `python -m venv venv`\r\n5. `venv/Scripts/activate`\r\n6. `pip install pylint astroid==2.9.1` # I also repro'd on the latest, 2.9.2\r\n7. `pylint src/project` # Updated from `pylint src`\r\n8. Observe failure:\r\n```\r\nsrc\\project\\__init__.py:1:0: F0010: error while code parsing: Unable to load file src\\project\\__init__.py:\r\n```\r\n\r\n### Current behavior\r\nFails with `src\\project\\__init__.py:1:0: F0010: error while code parsing: Unable to load file src\\project\\__init__.py:`\r\n\r\n### Expected behavior\r\nDoes not fail with error.\r\n> If you replace step 6 with `pip install pylint astroid==2.9.0`, you get no failure with an empty output - since no files have content\r\n\r\n### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\r\n2.9.1\r\n\r\n`python 3.9.1`\r\n`pylint 2.12.2 `\r\n\r\n\r\n\r\nThis issue has been observed with astroid `2.9.1` and `2.9.2`\n",
    "method": "sliding",
    "top_k": 10,
    "results": [
      {
        "rank": 1,
        "score": 0.6676849126815796,
        "content": "# Copyright (c) 2006-2014 LOGILAB S.A. (Paris, FRANCE) <contact@logilab.fr>\n# Copyright (c) 2014-2020 Claudiu Popa <pcmanticore@gmail.com>\n# Copyright (c) 2014 Google, Inc.\n# Copyright (c) 2015-2017 Ceridwen <ceridwenv@gmail.com>\n# Copyright (c) 2015 Florian Bruhin <me@the-compiler.org>\n# Copyright (c) 2015 Radosław Ganczarek <radoslaw@ganczarek.in>\n# Copyright (c) 2016 Moises Lopez <moylop260@vauxoo.com>\n# Copyright (c) 2017 Hugo <hugovk@users.noreply.github.com>\n# Copyright (c) 2017 Łukasz Rogalski <rogalski.91@gmail.com>\n# Copyright (c) 2017 Calen Pennington <cale@edx.org>\n# Copyright (c) 2018 Ville Skyttä <ville.skytta@iki.fi>\n# Copyright (c) 2018 Ashley Whetter <ashley@awhetter.co.uk>\n# Copyright (c) 2018 Bryce Guinta <bryce.paul.guinta@gmail.com>\n# Copyright (c) 2019 Uilian Ries <uilianries@gmail.com>\n# Copyright (c) 2019 Thomas Hisch <t.hisch@gmail.com>\n# Copyright (c) 2020-2021 hippo91 <guillaume.peillex@gmail.com>\n# Copyright (c) 2020 David Gilman <davidgilman1@gmail.com>\n# Copyright (c) 2020 Konrad Weihmann <kweihmann@outlook.com>\n# Copyright (c) 2020 Felix Mölder <felix.moelder@uni-due.de>\n# Copyright (c) 2020 Michael <michael-k@users.noreply.github.com>\n# Copyright (c) 2021 Pierre Sassoulas <pierre.sassoulas@gmail.com>\n# Copyright (c) 2021 Marc Mueller <30130371+cdce8p@users.noreply.github.com>\n# Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n# For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n__version__ = \"2.10.0-dev0\"\nversion = __version__",
        "file_path": "astroid/__pkginfo__.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 2,
        "score": 0.6270609498023987,
        "content": "# Copyright (c) 2007, 2009-2010, 2013 LOGILAB S.A. (Paris, FRANCE) <contact@logilab.fr>\n# Copyright (c) 2014 Google, Inc.\n# Copyright (c) 2015-2018, 2020 Claudiu Popa <pcmanticore@gmail.com>\n# Copyright (c) 2015-2016 Ceridwen <ceridwenv@gmail.com>\n# Copyright (c) 2016 Derek Gustafson <degustaf@gmail.com>\n# Copyright (c) 2018 Bryce Guinta <bryce.paul.guinta@gmail.com>\n# Copyright (c) 2020-2021 hippo91 <guillaume.peillex@gmail.com>\n# Copyright (c) 2021 Pierre Sassoulas <pierre.sassoulas@gmail.com>\n# Copyright (c) 2021 Daniël van Noord <13665637+DanielNoord@users.noreply.github.com>\n# Copyright (c) 2021 Marc Mueller <30130371+cdce8p@users.noreply.github.com>\n# Copyright (c) 2021 Andrew Haigh <hello@nelf.in>\n# Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n# For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n\"\"\"this module contains exceptions used in the astroid library\n\"\"\"\nfrom typing import TYPE_CHECKING\nfrom astroid import util\nif TYPE_CHECKING:\n    from astroid import nodes\n__all__ = (\n    \"AstroidBuildingError\",\n    \"AstroidBuildingException\",\n    \"AstroidError\",\n    \"AstroidImportError\",\n    \"AstroidIndexError\",\n    \"AstroidSyntaxError\",\n    \"AstroidTypeError\",\n    \"AstroidValueError\",\n    \"AttributeInferenceError\",\n    \"BinaryOperationError\",\n    \"DuplicateBasesError\",\n    \"InconsistentMroError\",\n    \"InferenceError\",\n    \"InferenceOverwriteError\",\n    \"MroError\",\n    \"NameInferenceError\",\n    \"NoDefault\",\n    \"NotFoundError\",\n    \"OperationError\",\n    \"ResolveError\",\n    \"SuperArgumentTypeError\",\n    \"SuperError\",\n    \"TooManyLevelsError\",\n    \"UnaryOperationError\",\n    \"UnresolvableName\",\n    \"UseInferenceDefault\",\n)\nclass AstroidError(Exception):\n    \"\"\"base exception class for all astroid related exceptions\n    AstroidError and its subclasses are structured, intended to hold\n    objects representing state when the exception is thrown.  Field\n    values are passed to the constructor as keyword-only arguments.\n    Each subclass has its own set of standard fields, but use your\n    best judgment to decide whether a specific exception instance\n    needs more or fewer fields for debugging.  Field values may be\n    used to lazily generate the error message: self.message.format()\n    will be called with the field names and values supplied as keyword\n    arguments.\n    \"\"\"\n    def __init__(self, message=\"\", **kws):\n        super().__init__(message)\n        self.message = message\n        for key, value in kws.items():\n            setattr(self, key, value)\n    def __str__(self):\n        return self.message.format(**vars(self))\nclass AstroidBuildingError(AstroidError):\n    \"\"\"exception class when we are unable to build an astroid representation\n    Standard attributes:\n        modname: Name of the module that AST construction failed for.\n        error: Exception raised during construction.\n    \"\"\"\n    def __init__(self, message=\"Failed to import module {modname}.\", **kws):\n        super().__init__(message, **kws)\nclass AstroidImportError(AstroidBuildingError):\n    \"\"\"Exception class used when a module can't be imported by astroid.\"\"\"\nclass TooManyLevelsError(AstroidImportError):\n    \"\"\"Exception class which is raised when a relative import was beyond the top-level.\n    Standard attributes:\n        level: The level which was attempted.\n        name: the name of the module on which the relative import was attempted.\n    \"\"\"\n    level = None\n    name = None\n    def __init__(\n        self,\n        message=\"Relative import with too many levels \" \"({level}) for module {name!r}\",\n        **kws,\n    ):\n        super().__init__(message, **kws)\nclass AstroidSyntaxError(AstroidBuildingError):\n    \"\"\"Exception class used when a module can't be parsed.\"\"\"\nclass NoDefault(AstroidError):\n    \"\"\"raised by function's `default_value` method when an argument has\n    no default value\n    Standard attributes:\n        func: Function node.\n        name: Name of argument without a default.\n    \"\"\"\n    func = None\n    name = None\n    def __init__(self, message=\"{func!r} has no default for {name!r}.\", **kws):\n        super().__init__(message, **kws)\nclass ResolveError(AstroidError):\n    \"\"\"Base class of astroid resolution/inference error.\n    ResolveError is not intended to be raised.\n    Standard attributes:\n        context: InferenceContext object.\n    \"\"\"\n    context = None\nclass MroError(ResolveError):\n    \"\"\"Error raised when there is a problem with method resolution of a class.\n    Standard attributes:\n        mros: A sequence of sequences containing ClassDef nodes.\n        cls: ClassDef node whose MRO resolution failed.\n        context: InferenceContext object.\n    \"\"\"\n    mros = ()\n    cls = None\n    def __str__(self):\n        mro_names = \", \".join(f\"({', '.join(b.name for b in m)})\" for m in self.mros)\n        return self.message.format(mros=mro_names, cls=self.cls)\nclass DuplicateBasesError(MroError):\n    \"\"\"Error raised when there are duplicate bases in the same class bases.\"\"\"\nclass InconsistentMroError(MroError):\n    \"\"\"Error raised when a class's MRO is inconsistent.\"\"\"\nclass SuperError(ResolveError):\n    \"\"\"Error raised when there is a problem with a *super* call.\n    Standard attributes:\n        *super_*: The Super instance that raised the exception.\n        context: InferenceContext object.\n    \"\"\"\n    super_ = None\n    def __str__(self):\n        return self.message.format(**vars(self.super_))\nclass InferenceError(ResolveError):\n    \"\"\"raised when we are unable to infer a node\n    Standard attributes:\n        node: The node inference was called on.\n        context: InferenceContext object.\n    \"\"\"\n    node = None\n    context = None\n    def __init__(self, message=\"Inference failed for {node!r}.\", **kws):\n        super().__init__(message, **kws)\n# Why does this inherit from InferenceError rather than ResolveError?\n# Changing it causes some inference tests to fail.\nclass NameInferenceError(InferenceError):\n    \"\"\"Raised when a name lookup fails, corresponds to NameError.\n    Standard attributes:\n        name: The name for which lookup failed, as a string.\n        scope: The node representing the scope in which the lookup occurred.\n        context: InferenceContext object.\n    \"\"\"\n    name = None\n    scope = None\n    def __init__(self, message=\"{name!r} not found in {scope!r}.\", **kws):\n        super().__init__(message, **kws)\nclass AttributeInferenceError(ResolveError):\n    \"\"\"Raised when an attribute lookup fails, corresponds to AttributeError.\n    Standard attributes:\n        target: The node for which lookup failed.\n        attribute: The attribute for which lookup failed, as a string.\n        context: InferenceContext object.\n    \"\"\"\n    target = None\n    attribute = None\n    def __init__(self, message=\"{attribute!r} not found on {target!r}.\", **kws):\n        super().__init__(message, **kws)\nclass UseInferenceDefault(Exception):\n    \"\"\"exception to be raised in custom inference function to indicate that it\n    should go back to the default behaviour\n    \"\"\"\nclass _NonDeducibleTypeHierarchy(Exception):\n    \"\"\"Raised when is_subtype / is_supertype can't deduce the relation between two types.\"\"\"\nclass AstroidIndexError(AstroidError):\n    \"\"\"Raised when an Indexable / Mapping does not have an index / key.\"\"\"\nclass AstroidTypeError(AstroidError):\n    \"\"\"Raised when a TypeError would be expected in Python code.\"\"\"\nclass AstroidValueError(AstroidError):\n    \"\"\"Raised when a ValueError would be expected in Python code.\"\"\"\nclass InferenceOverwriteError(AstroidError):\n    \"\"\"Raised when an inference tip is overwritten\n    Currently only used for debugging.\n    \"\"\"\nclass ParentMissingError(AstroidError):\n    \"\"\"Raised when a node which is expected to have a parent attribute is missing one\n    Standard attributes:\n        target: The node for which the parent lookup failed.\n    \"\"\"\n    def __init__(self, target: \"nodes.NodeNG\") -> None:\n        self.target = target\n        super().__init__(message=f\"Parent not found on {target!r}.\")\nclass StatementMissing(ParentMissingError):\n    \"\"\"Raised when a call to node.statement() does not return a node. This is because\n    a node in the chain does not have a parent attribute and therefore does not\n    return a node for statement().\n    Standard attributes:\n        target: The node for which the parent lookup failed.\n    \"\"\"\n    def __init__(self, target: \"nodes.NodeNG\") -> None:\n        # pylint: disable-next=bad-super-call\n        # https://github.com/PyCQA/pylint/issues/2903\n        # https://github.com/PyCQA/astroid/pull/1217#discussion_r744149027\n        super(ParentMissingError, self).__init__(\n            message=f\"Statement not found on {target!r}\"\n        )\n# Backwards-compatibility aliases\nOperationError = util.BadOperationMessage\nUnaryOperationError = util.BadUnaryOperationMessage\nBinaryOperationError = util.BadBinaryOperationMessage\nSuperArgumentTypeError = SuperError\nUnresolvableName = NameInferenceError\nNotFoundError = AttributeInferenceError\nAstroidBuildingException = AstroidBuildingError",
        "file_path": "astroid/exceptions.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 3,
        "score": 0.5909662246704102,
        "content": "# Copyright (c) 2006-2011, 2013-2014 LOGILAB S.A. (Paris, FRANCE) <contact@logilab.fr>\n# Copyright (c) 2014-2020 Claudiu Popa <pcmanticore@gmail.com>\n# Copyright (c) 2014 BioGeek <jeroen.vangoey@gmail.com>\n# Copyright (c) 2014 Google, Inc.\n# Copyright (c) 2014 Eevee (Alex Munroe) <amunroe@yelp.com>\n# Copyright (c) 2015-2016 Ceridwen <ceridwenv@gmail.com>\n# Copyright (c) 2016 Derek Gustafson <degustaf@gmail.com>\n# Copyright (c) 2017 Iva Miholic <ivamiho@gmail.com>\n# Copyright (c) 2018 Bryce Guinta <bryce.paul.guinta@gmail.com>\n# Copyright (c) 2018 Nick Drozd <nicholasdrozd@gmail.com>\n# Copyright (c) 2019 Raphael Gaschignard <raphael@makeleaps.com>\n# Copyright (c) 2020-2021 hippo91 <guillaume.peillex@gmail.com>\n# Copyright (c) 2020 Raphael Gaschignard <raphael@rtpg.co>\n# Copyright (c) 2020 Anubhav <35621759+anubh-v@users.noreply.github.com>\n# Copyright (c) 2020 Ashley Whetter <ashley@awhetter.co.uk>\n# Copyright (c) 2021 Pierre Sassoulas <pierre.sassoulas@gmail.com>\n# Copyright (c) 2021 Daniël van Noord <13665637+DanielNoord@users.noreply.github.com>\n# Copyright (c) 2021 grayjk <grayjk@gmail.com>\n# Copyright (c) 2021 Marc Mueller <30130371+cdce8p@users.noreply.github.com>\n# Copyright (c) 2021 Andrew Haigh <hello@nelf.in>\n# Copyright (c) 2021 DudeNr33 <3929834+DudeNr33@users.noreply.github.com>\n# Copyright (c) 2021 pre-commit-ci[bot] <bot@noreply.github.com>\n# Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n# For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n\"\"\"astroid manager: avoid multiple astroid build of a same module when\npossible by providing a class responsible to get astroid representation\nfrom various source and using a cache of built modules)\n\"\"\"\nimport os\nimport types\nimport zipimport\nfrom typing import TYPE_CHECKING, ClassVar, List, Optional\nfrom astroid.exceptions import AstroidBuildingError, AstroidImportError\nfrom astroid.interpreter._import import spec\nfrom astroid.modutils import (\n    NoSourceFile,\n    file_info_from_modpath,\n    get_source_file,\n    is_module_name_part_of_extension_package_whitelist,\n    is_python_source,\n    is_standard_module,\n    load_module_from_name,\n    modpath_from_file,\n)\nfrom astroid.transforms import TransformVisitor\nif TYPE_CHECKING:\n    from astroid import nodes\nZIP_IMPORT_EXTS = (\".zip\", \".egg\", \".whl\", \".pyz\", \".pyzw\")\ndef safe_repr(obj):\n    try:\n        return repr(obj)\n    except Exception:  # pylint: disable=broad-except\n        return \"???\"\nclass AstroidManager:\n    \"\"\"Responsible to build astroid from files or modules.\n    Use the Borg (singleton) pattern.\n    \"\"\"\n    name = \"astroid loader\"\n    brain = {}\n    max_inferable_values: ClassVar[int] = 100\n    def __init__(self):\n        self.__dict__ = AstroidManager.brain\n        if not self.__dict__:\n            # NOTE: cache entries are added by the [re]builder\n            self.astroid_cache = {}\n            self._mod_file_cache = {}\n            self._failed_import_hooks = []\n            self.always_load_extensions = False\n            self.optimize_ast = False\n            self.extension_package_whitelist = set()\n            self._transform = TransformVisitor()\n    @property\n    def register_transform(self):\n        # This and unregister_transform below are exported for convenience\n        return self._transform.register_transform\n    @property\n    def unregister_transform(self):\n        return self._transform.unregister_transform\n    @property\n    def builtins_module(self):\n        return self.astroid_cache[\"builtins\"]\n    def visit_transforms(self, node):\n        \"\"\"Visit the transforms and apply them to the given *node*.\"\"\"\n        return self._transform.visit(node)\n    def ast_from_file(self, filepath, modname=None, fallback=True, source=False):\n        \"\"\"given a module name, return the astroid object\"\"\"\n        try:\n            filepath = get_source_file(filepath, include_no_ext=True)\n            source = True\n        except NoSourceFile:\n            pass\n        if modname is None:\n            try:\n                modname = \".\".join(modpath_from_file(filepath))\n            except ImportError:\n                modname = filepath\n        if (\n            modname in self.astroid_cache\n            and self.astroid_cache[modname].file == filepath\n        ):\n            return self.astroid_cache[modname]\n        if source:\n            # pylint: disable=import-outside-toplevel; circular import\n            from astroid.builder import AstroidBuilder\n            return AstroidBuilder(self).file_build(filepath, modname)\n        if fallback and modname:\n            return self.ast_from_module_name(modname)\n        raise AstroidBuildingError(\"Unable to build an AST for {path}.\", path=filepath)\n    def ast_from_string(self, data, modname=\"\", filepath=None):\n        \"\"\"Given some source code as a string, return its corresponding astroid object\"\"\"\n        # pylint: disable=import-outside-toplevel; circular import\n        from astroid.builder import AstroidBuilder\n        return AstroidBuilder(self).string_build(data, modname, filepath)\n    def _build_stub_module(self, modname):\n        # pylint: disable=import-outside-toplevel; circular import\n        from astroid.builder import AstroidBuilder\n        return AstroidBuilder(self).string_build(\"\", modname)\n    def _build_namespace_module(self, modname: str, path: List[str]) -> \"nodes.Module\":\n        # pylint: disable=import-outside-toplevel; circular import\n        from astroid.builder import build_namespace_package_module\n        return build_namespace_package_module(modname, path)\n    def _can_load_extension(self, modname: str) -> bool:\n        if self.always_load_extensions:\n            return True\n        if is_standard_module(modname):\n            return True\n        return is_module_name_part_of_extension_package_whitelist(\n            modname, self.extension_package_whitelist\n        )\n    def ast_from_module_name(self, modname, context_file=None):\n        \"\"\"given a module name, return the astroid object\"\"\"\n        if modname in self.astroid_cache:\n            return self.astroid_cache[modname]\n        if modname == \"__main__\":\n            return self._build_stub_module(modname)\n        if context_file:\n            old_cwd = os.getcwd()\n            os.chdir(os.path.dirname(context_file))\n        try:\n            found_spec = self.file_from_module_name(modname, context_file)\n            if found_spec.type == spec.ModuleType.PY_ZIPMODULE:\n                module = self.zip_import_data(found_spec.location)\n                if module is not None:\n                    return module\n            elif found_spec.type in (\n                spec.ModuleType.C_BUILTIN,\n                spec.ModuleType.C_EXTENSION,\n            ):\n                if (\n                    found_spec.type == spec.ModuleType.C_EXTENSION\n                    and not self._can_load_extension(modname)\n                ):\n                    return self._build_stub_module(modname)\n                try:\n                    module = load_module_from_name(modname)\n                except Exception as e:\n                    raise AstroidImportError(\n                        \"Loading {modname} failed with:\\n{error}\",\n                        modname=modname,\n                        path=found_spec.location,\n                    ) from e\n                return self.ast_from_module(module, modname)\n            elif found_spec.type == spec.ModuleType.PY_COMPILED:\n                raise AstroidImportError(\n                    \"Unable to load compiled module {modname}.\",\n                    modname=modname,\n                    path=found_spec.location,\n                )\n            elif found_spec.type == spec.ModuleType.PY_NAMESPACE:\n                return self._build_namespace_module(\n                    modname, found_spec.submodule_search_locations\n                )\n            elif found_spec.type == spec.ModuleType.PY_FROZEN:\n                return self._build_stub_module(modname)\n            if found_spec.location is None:\n                raise AstroidImportError(\n                    \"Can't find a file for module {modname}.\", modname=modname\n                )\n            return self.ast_from_file(found_spec.location, modname, fallback=False)\n        except AstroidBuildingError as e:\n            for hook in self._failed_import_hooks:\n                try:\n                    return hook(modname)\n                except AstroidBuildingError:\n                    pass\n            raise e\n        finally:\n            if context_file:\n                os.chdir(old_cwd)\n    def zip_import_data(self, filepath):\n        if zipimport is None:\n            return None\n        # pylint: disable=import-outside-toplevel; circular import\n        from astroid.builder import AstroidBuilder\n        builder = AstroidBuilder(self)\n        for ext in ZIP_IMPORT_EXTS:\n            try:\n                eggpath, resource = filepath.rsplit(ext + os.path.sep, 1)\n            except ValueError:\n                continue\n            try:\n                # pylint: disable-next=no-member\n                importer = zipimport.zipimporter(eggpath + ext)\n                zmodname = resource.replace(os.path.sep, \".\")\n                if importer.is_package(resource):\n                    zmodname = zmodname + \".__init__\"\n                module = builder.string_build(\n                    importer.get_source(resource), zmodname, filepath\n                )\n                return module\n            except Exception:  # pylint: disable=broad-except\n                continue\n        return None\n    def file_from_module_name(self, modname, contextfile):\n        try:\n            value = self._mod_file_cache[(modname, contextfile)]\n        except KeyError:\n            try:\n                value = file_info_from_modpath(\n                    modname.split(\".\"), context_file=contextfile\n                )\n            except ImportError as e:\n                value = AstroidImportError(\n                    \"Failed to import module {modname} with error:\\n{error}.\",\n                    modname=modname,\n                    # we remove the traceback here to save on memory usage (since these exceptions are cached)\n                    error=e.with_traceback(None),\n                )\n            self._mod_file_cache[(modname, contextfile)] = value\n        if isinstance(value, AstroidBuildingError):\n            # we remove the traceback here to save on memory usage (since these exceptions are cached)\n            raise value.with_traceback(None)\n        return value\n    def ast_from_module(self, module: types.ModuleType, modname: Optional[str] = None):\n        \"\"\"given an imported module, return the astroid object\"\"\"\n        modname = modname or module.__name__\n        if modname in self.astroid_cache:\n            return self.astroid_cache[modname]\n        try:\n            # some builtin modules don't have __file__ attribute\n            filepath = module.__file__\n            if is_python_source(filepath):\n                return self.ast_from_file(filepath, modname)\n        except AttributeError:\n            pass\n        # pylint: disable=import-outside-toplevel; circular import\n        from astroid.builder import AstroidBuilder\n        return AstroidBuilder(self).module_build(module, modname)\n    def ast_from_class(self, klass, modname=None):\n        \"\"\"get astroid for the given class\"\"\"\n        if modname is None:\n            try:\n                modname = klass.__module__\n            except AttributeError as exc:\n                raise AstroidBuildingError(\n                    \"Unable to get module for class {class_name}.\",\n                    cls=klass,\n                    class_repr=safe_repr(klass),\n                    modname=modname,\n                ) from exc\n        modastroid = self.ast_from_module_name(modname)\n        return modastroid.getattr(klass.__name__)[0]  # XXX\n    def infer_ast_from_something(self, obj, context=None):\n        \"\"\"infer astroid for the given class\"\"\"\n        if hasattr(obj, \"__class__\") and not isinstance(obj, type):\n            klass = obj.__class__\n        else:\n            klass = obj\n        try:\n            modname = klass.__module__\n        except AttributeError as exc:\n            raise AstroidBuildingError(\n                \"Unable to get module for {class_repr}.\",\n                cls=klass,\n                class_repr=safe_repr(klass),\n            ) from exc\n        except Exception as exc:\n            raise AstroidImportError(\n                \"Unexpected error while retrieving module for {class_repr}:\\n\"\n                \"{error}\",\n                cls=klass,\n                class_repr=safe_repr(klass),\n            ) from exc\n        try:\n            name = klass.__name__\n        except AttributeError as exc:\n            raise AstroidBuildingError(\n                \"Unable to get name for {class_repr}:\\n\",\n                cls=klass,\n                class_repr=safe_repr(klass),\n            ) from exc\n        except Exception as exc:\n            raise AstroidImportError(\n                \"Unexpected error while retrieving name for {class_repr}:\\n\" \"{error}\",\n                cls=klass,\n                class_repr=safe_repr(klass),\n            ) from exc\n        # take care, on living object __module__ is regularly wrong :(\n        modastroid = self.ast_from_module_name(modname)\n        if klass is obj:\n            for inferred in modastroid.igetattr(name, context):\n                yield inferred\n        else:\n            for inferred in modastroid.igetattr(name, context):\n                yield inferred.instantiate_class()\n    def register_failed_import_hook(self, hook):\n        \"\"\"Registers a hook to resolve imports that cannot be found otherwise.\n        `hook` must be a function that accepts a single argument `modname` which\n        contains the name of the module or package that could not be imported.\n        If `hook` can resolve the import, must return a node of type `astroid.Module`,\n        otherwise, it must raise `AstroidBuildingError`.\n        \"\"\"\n        self._failed_import_hooks.append(hook)\n    def cache_module(self, module):\n        \"\"\"Cache a module if no module with the same name is known yet.\"\"\"\n        self.astroid_cache.setdefault(module.name, module)\n    def bootstrap(self):\n        \"\"\"Bootstrap the required AST modules needed for the manager to work\n        The bootstrap usually involves building the AST for the builtins\n        module, which is required by the rest of astroid to work correctly.\n        \"\"\"\n        from astroid import raw_building  # pylint: disable=import-outside-toplevel\n        raw_building._astroid_bootstrapping()\n    def clear_cache(self):\n        \"\"\"Clear the underlying cache. Also bootstraps the builtins module.\"\"\"\n        self.astroid_cache.clear()\n        self.bootstrap()",
        "file_path": "astroid/manager.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 4,
        "score": 0.5868119597434998,
        "content": "# Copyright (c) 2006-2013, 2015 LOGILAB S.A. (Paris, FRANCE) <contact@logilab.fr>\n# Copyright (c) 2014 Google, Inc.\n# Copyright (c) 2014 Eevee (Alex Munroe) <amunroe@yelp.com>\n# Copyright (c) 2015-2016, 2018, 2020 Claudiu Popa <pcmanticore@gmail.com>\n# Copyright (c) 2015-2016 Ceridwen <ceridwenv@gmail.com>\n# Copyright (c) 2016 Derek Gustafson <degustaf@gmail.com>\n# Copyright (c) 2016 Moises Lopez <moylop260@vauxoo.com>\n# Copyright (c) 2018 Bryce Guinta <bryce.paul.guinta@gmail.com>\n# Copyright (c) 2019 Nick Drozd <nicholasdrozd@gmail.com>\n# Copyright (c) 2020-2021 hippo91 <guillaume.peillex@gmail.com>\n# Copyright (c) 2021 Pierre Sassoulas <pierre.sassoulas@gmail.com>\n# Copyright (c) 2021 Daniël van Noord <13665637+DanielNoord@users.noreply.github.com>\n# Copyright (c) 2021 Marc Mueller <30130371+cdce8p@users.noreply.github.com>\n# Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n# For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n\"\"\"Python Abstract Syntax Tree New Generation\nThe aim of this module is to provide a common base representation of\npython source code for projects such as pychecker, pyreverse,\npylint... Well, actually the development of this library is essentially\ngoverned by pylint's needs.\nIt extends class defined in the python's _ast module with some\nadditional methods and attributes. Instance attributes are added by a\nbuilder object, which can either generate extended ast (let's call\nthem astroid ;) by visiting an existent ast tree or by inspecting living\nobject. Methods are added by monkey patching ast classes.\nMain modules are:\n* nodes and scoped_nodes for more information about methods and\n  attributes added to different node classes\n* the manager contains a high level object to get astroid trees from\n  source files and living objects. It maintains a cache of previously\n  constructed tree for quick access\n* builder contains the class responsible to build astroid trees\n\"\"\"\nfrom importlib import import_module\nfrom pathlib import Path\n# isort: off\n# We have an isort: off on '__version__' because the packaging need to access\n# the version before the dependencies are installed (in particular 'wrapt'\n# that is imported in astroid.inference)\nfrom astroid.__pkginfo__ import __version__, version\nfrom astroid.nodes import node_classes, scoped_nodes\n# isort: on\nfrom astroid import inference, raw_building\nfrom astroid.astroid_manager import MANAGER\nfrom astroid.bases import BaseInstance, BoundMethod, Instance, UnboundMethod\nfrom astroid.brain.helpers import register_module_extender\nfrom astroid.builder import extract_node, parse\nfrom astroid.const import Context, Del, Load, Store\nfrom astroid.exceptions import *\nfrom astroid.inference_tip import _inference_tip_cached, inference_tip\nfrom astroid.objects import ExceptionInstance\n# isort: off\n# It's impossible to import from astroid.nodes with a wildcard, because\n# there is a cyclic import that prevent creating an __all__ in astroid/nodes\n# and we need astroid/scoped_nodes and astroid/node_classes to work. So\n# importing with a wildcard would clash with astroid/nodes/scoped_nodes\n# and astroid/nodes/node_classes.\nfrom astroid.nodes import (  # pylint: disable=redefined-builtin (Ellipsis)\n    CONST_CLS,\n    AnnAssign,\n    Arguments,\n    Assert,\n    Assign,\n    AssignAttr,\n    AssignName,\n    AsyncFor,\n    AsyncFunctionDef,\n    AsyncWith,\n    Attribute,\n    AugAssign,\n    Await,\n    BinOp,\n    BoolOp,\n    Break,\n    Call,\n    ClassDef,\n    Compare,\n    Comprehension,\n    ComprehensionScope,\n    Const,\n    Continue,\n    Decorators,\n    DelAttr,\n    Delete,\n    DelName,\n    Dict,\n    DictComp,\n    DictUnpack,\n    Ellipsis,\n    EmptyNode,\n    EvaluatedObject,\n    ExceptHandler,\n    Expr,\n    ExtSlice,\n    For,\n    FormattedValue,\n    FunctionDef,\n    GeneratorExp,\n    Global,\n    If,\n    IfExp,\n    Import,\n    ImportFrom,\n    Index,\n    JoinedStr,\n    Keyword,\n    Lambda,\n    List,\n    ListComp,\n    Match,\n    MatchAs,\n    MatchCase,\n    MatchClass,\n    MatchMapping,\n    MatchOr,\n    MatchSequence,\n    MatchSingleton,\n    MatchStar,\n    MatchValue,\n    Module,\n    Name,\n    NamedExpr,\n    NodeNG,\n    Nonlocal,\n    Pass,\n    Raise,\n    Return,\n    Set,\n    SetComp,\n    Slice,\n    Starred,\n    Subscript,\n    TryExcept,\n    TryFinally,\n    Tuple,\n    UnaryOp,\n    Unknown,\n    While,\n    With,\n    Yield,\n    YieldFrom,\n    are_exclusive,\n    builtin_lookup,\n    unpack_infer,\n    function_to_method,\n)\n# isort: on\nfrom astroid.util import Uninferable\n# load brain plugins\nASTROID_INSTALL_DIRECTORY = Path(__file__).parent\nBRAIN_MODULES_DIRECTORY = ASTROID_INSTALL_DIRECTORY / \"brain\"\nfor module in BRAIN_MODULES_DIRECTORY.iterdir():\n    if module.suffix == \".py\":\n        import_module(f\"astroid.brain.{module.stem}\")",
        "file_path": "astroid/__init__.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 5,
        "score": 0.5745179653167725,
        "content": "# Copyright (c) 2006-2011, 2013-2014 LOGILAB S.A. (Paris, FRANCE) <contact@logilab.fr>\n# Copyright (c) 2013 Phil Schaf <flying-sheep@web.de>\n# Copyright (c) 2014-2020 Claudiu Popa <pcmanticore@gmail.com>\n# Copyright (c) 2014-2015 Google, Inc.\n# Copyright (c) 2014 Alexander Presnyakov <flagist0@gmail.com>\n# Copyright (c) 2015-2016 Ceridwen <ceridwenv@gmail.com>\n# Copyright (c) 2016 Derek Gustafson <degustaf@gmail.com>\n# Copyright (c) 2017 Łukasz Rogalski <rogalski.91@gmail.com>\n# Copyright (c) 2018 Anthony Sottile <asottile@umich.edu>\n# Copyright (c) 2020-2021 hippo91 <guillaume.peillex@gmail.com>\n# Copyright (c) 2021 Pierre Sassoulas <pierre.sassoulas@gmail.com>\n# Copyright (c) 2021 Tushar Sadhwani <86737547+tushar-deepsource@users.noreply.github.com>\n# Copyright (c) 2021 Daniël van Noord <13665637+DanielNoord@users.noreply.github.com>\n# Copyright (c) 2021 Gregory P. Smith <greg@krypto.org>\n# Copyright (c) 2021 Kian Meng, Ang <kianmeng.ang@gmail.com>\n# Copyright (c) 2021 Marc Mueller <30130371+cdce8p@users.noreply.github.com>\n# Copyright (c) 2021 Andrew Haigh <hello@nelf.in>\n# Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n# For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n\"\"\"The AstroidBuilder makes astroid from living object and / or from _ast\nThe builder is not thread safe and can't be used to parse different sources\nat the same time.\n\"\"\"\nimport os\nimport textwrap\nimport types\nfrom tokenize import detect_encoding\nfrom typing import List, Optional, Union\nfrom astroid import bases, modutils, nodes, raw_building, rebuilder, util\nfrom astroid._ast import get_parser_module\nfrom astroid.exceptions import AstroidBuildingError, AstroidSyntaxError, InferenceError\nfrom astroid.manager import AstroidManager\nfrom astroid.nodes.node_classes import NodeNG\nobjects = util.lazy_import(\"objects\")\n# The name of the transient function that is used to\n# wrap expressions to be extracted when calling\n# extract_node.\n_TRANSIENT_FUNCTION = \"__\"\n# The comment used to select a statement to be extracted\n# when calling extract_node.\n_STATEMENT_SELECTOR = \"#@\"\nMISPLACED_TYPE_ANNOTATION_ERROR = \"misplaced type annotation\"\ndef open_source_file(filename):\n    # pylint: disable=consider-using-with\n    with open(filename, \"rb\") as byte_stream:\n        encoding = detect_encoding(byte_stream.readline)[0]\n    stream = open(filename, newline=None, encoding=encoding)\n    data = stream.read()\n    return stream, encoding, data\ndef _can_assign_attr(node, attrname):\n    try:\n        slots = node.slots()\n    except NotImplementedError:\n        pass\n    else:\n        if slots and attrname not in {slot.value for slot in slots}:\n            return False\n    return node.qname() != \"builtins.object\"\nclass AstroidBuilder(raw_building.InspectBuilder):\n    \"\"\"Class for building an astroid tree from source code or from a live module.\n    The param *manager* specifies the manager class which should be used.\n    If no manager is given, then the default one will be used. The\n    param *apply_transforms* determines if the transforms should be\n    applied after the tree was built from source or from a live object,\n    by default being True.\n    \"\"\"\n    # pylint: disable=redefined-outer-name\n    def __init__(self, manager=None, apply_transforms=True):\n        super().__init__(manager)\n        self._apply_transforms = apply_transforms\n    def module_build(\n        self, module: types.ModuleType, modname: Optional[str] = None\n    ) -> nodes.Module:\n        \"\"\"Build an astroid from a living module instance.\"\"\"\n        node = None\n        path = getattr(module, \"__file__\", None)\n        loader = getattr(module, \"__loader__\", None)\n        # Prefer the loader to get the source rather than assuming we have a\n        # filesystem to read the source file from ourselves.\n        if loader:\n            modname = modname or module.__name__\n            source = loader.get_source(modname)\n            if source:\n                node = self.string_build(source, modname, path=path)\n        if node is None and path is not None:\n            path_, ext = os.path.splitext(modutils._path_from_filename(path))\n            if ext in {\".py\", \".pyc\", \".pyo\"} and os.path.exists(path_ + \".py\"):\n                node = self.file_build(path_ + \".py\", modname)\n        if node is None:\n            # this is a built-in module\n            # get a partial representation by introspection\n            node = self.inspect_build(module, modname=modname, path=path)\n            if self._apply_transforms:\n                # We have to handle transformation by ourselves since the\n                # rebuilder isn't called for builtin nodes\n                node = self._manager.visit_transforms(node)\n        return node\n    def file_build(self, path, modname=None):\n        \"\"\"Build astroid from a source code file (i.e. from an ast)\n        *path* is expected to be a python source file\n        \"\"\"\n        try:\n            stream, encoding, data = open_source_file(path)\n        except OSError as exc:\n            raise AstroidBuildingError(\n                \"Unable to load file {path}:\\n{error}\",\n                modname=modname,\n                path=path,\n                error=exc,\n            ) from exc\n        except (SyntaxError, LookupError) as exc:\n            raise AstroidSyntaxError(\n                \"Python 3 encoding specification error or unknown encoding:\\n\"\n                \"{error}\",\n                modname=modname,\n                path=path,\n                error=exc,\n            ) from exc\n        except UnicodeError as exc:  # wrong encoding\n            # detect_encoding returns utf-8 if no encoding specified\n            raise AstroidBuildingError(\n                \"Wrong or no encoding specified for {filename}.\", filename=path\n            ) from exc\n        with stream:\n            # get module name if necessary\n            if modname is None:\n                try:\n                    modname = \".\".join(modutils.modpath_from_file(path))\n                except ImportError:\n                    modname = os.path.splitext(os.path.basename(path))[0]\n            # build astroid representation\n            module = self._data_build(data, modname, path)\n            return self._post_build(module, encoding)\n    def string_build(self, data, modname=\"\", path=None):\n        \"\"\"Build astroid from source code string.\"\"\"\n        module = self._data_build(data, modname, path)\n        module.file_bytes = data.encode(\"utf-8\")\n        return self._post_build(module, \"utf-8\")\n    def _post_build(self, module, encoding):\n        \"\"\"Handles encoding and delayed nodes after a module has been built\"\"\"\n        module.file_encoding = encoding\n        self._manager.cache_module(module)\n        # post tree building steps after we stored the module in the cache:\n        for from_node in module._import_from_nodes:\n            if from_node.modname == \"__future__\":\n                for symbol, _ in from_node.names:\n                    module.future_imports.add(symbol)\n            self.add_from_names_to_locals(from_node)\n        # handle delayed assattr nodes\n        for delayed in module._delayed_assattr:\n            self.delayed_assattr(delayed)\n        # Visit the transforms\n        if self._apply_transforms:\n            module = self._manager.visit_transforms(module)\n        return module\n    def _data_build(self, data, modname, path):\n        \"\"\"Build tree node from data and add some information\"\"\"\n        try:\n            node, parser_module = _parse_string(data, type_comments=True)\n        except (TypeError, ValueError, SyntaxError) as exc:\n            raise AstroidSyntaxError(\n                \"Parsing Python code failed:\\n{error}\",\n                source=data,\n                modname=modname,\n                path=path,\n                error=exc,\n            ) from exc\n        if path is not None:\n            node_file = os.path.abspath(path)\n        else:\n            node_file = \"<?>\"\n        if modname.endswith(\".__init__\"):\n            modname = modname[:-9]\n            package = True\n        else:\n            package = (\n                path is not None\n                and os.path.splitext(os.path.basename(path))[0] == \"__init__\"\n            )\n        builder = rebuilder.TreeRebuilder(self._manager, parser_module)\n        module = builder.visit_module(node, modname, node_file, package)\n        module._import_from_nodes = builder._import_from_nodes\n        module._delayed_assattr = builder._delayed_assattr\n        return module\n    def add_from_names_to_locals(self, node):\n        \"\"\"Store imported names to the locals\n        Resort the locals if coming from a delayed node\n        \"\"\"\n        def _key_func(node):\n            return node.fromlineno\n        def sort_locals(my_list):\n            my_list.sort(key=_key_func)\n        for (name, asname) in node.names:\n            if name == \"*\":\n                try:\n                    imported = node.do_import_module()\n                except AstroidBuildingError:\n                    continue\n                for name in imported.public_names():\n                    node.parent.set_local(name, node)\n                    sort_locals(node.parent.scope().locals[name])\n            else:\n                node.parent.set_local(asname or name, node)\n                sort_locals(node.parent.scope().locals[asname or name])\n    def delayed_assattr(self, node):\n        \"\"\"Visit a AssAttr node\n        This adds name to locals and handle members definition.\n        \"\"\"\n        try:\n            frame = node.frame(future=True)\n            for inferred in node.expr.infer():\n                if inferred is util.Uninferable:\n                    continue\n                try:\n                    cls = inferred.__class__\n                    if cls is bases.Instance or cls is objects.ExceptionInstance:\n                        inferred = inferred._proxied\n                        iattrs = inferred.instance_attrs\n                        if not _can_assign_attr(inferred, node.attrname):\n                            continue\n                    elif isinstance(inferred, bases.Instance):\n                        # Const, Tuple or other containers that inherit from\n                        # `Instance`\n                        continue\n                    elif inferred.is_function:\n                        iattrs = inferred.instance_attrs\n                    else:\n                        iattrs = inferred.locals\n                except AttributeError:\n                    # XXX log error\n                    continue\n                values = iattrs.setdefault(node.attrname, [])\n                if node in values:\n                    continue\n                # get assign in __init__ first XXX useful ?\n                if (\n                    frame.name == \"__init__\"\n                    and values\n                    and values[0].frame(future=True).name != \"__init__\"\n                ):\n                    values.insert(0, node)\n                else:\n                    values.append(node)\n        except InferenceError:\n            pass\ndef build_namespace_package_module(name: str, path: List[str]) -> nodes.Module:\n    return nodes.Module(name, doc=\"\", path=path, package=True)\ndef parse(code, module_name=\"\", path=None, apply_transforms=True):\n    \"\"\"Parses a source string in order to obtain an astroid AST from it\n    :param str code: The code for the module.\n    :param str module_name: The name for the module, if any\n    :param str path: The path for the module\n    :param bool apply_transforms:\n        Apply the transforms for the give code. Use it if you\n        don't want the default transforms to be applied.\n    \"\"\"\n    code = textwrap.dedent(code)\n    builder = AstroidBuilder(\n        manager=AstroidManager(), apply_transforms=apply_transforms\n    )\n    return builder.string_build(code, modname=module_name, path=path)\ndef _extract_expressions(node):\n    \"\"\"Find expressions in a call to _TRANSIENT_FUNCTION and extract them.\n    The function walks the AST recursively to search for expressions that\n    are wrapped into a call to _TRANSIENT_FUNCTION. If it finds such an\n    expression, it completely removes the function call node from the tree,\n    replacing it by the wrapped expression inside the parent.\n    :param node: An astroid node.\n    :type node:  astroid.bases.NodeNG\n    :yields: The sequence of wrapped expressions on the modified tree\n    expression can be found.\n    \"\"\"\n    if (\n        isinstance(node, nodes.Call)\n        and isinstance(node.func, nodes.Name)\n        and node.func.name == _TRANSIENT_FUNCTION\n    ):\n        real_expr = node.args[0]\n        real_expr.parent = node.parent\n        # Search for node in all _astng_fields (the fields checked when\n        # get_children is called) of its parent. Some of those fields may\n        # be lists or tuples, in which case the elements need to be checked.\n        # When we find it, replace it by real_expr, so that the AST looks\n        # like no call to _TRANSIENT_FUNCTION ever took place.\n        for name in node.parent._astroid_fields:\n            child = getattr(node.parent, name)\n            if isinstance(child, (list, tuple)):\n                for idx, compound_child in enumerate(child):\n                    if compound_child is node:\n                        child[idx] = real_expr\n            elif child is node:\n                setattr(node.parent, name, real_expr)\n        yield real_expr\n    else:\n        for child in node.get_children():\n            yield from _extract_expressions(child)\ndef _find_statement_by_line(node, line):\n    \"\"\"Extracts the statement on a specific line from an AST.\n    If the line number of node matches line, it will be returned;\n    otherwise its children are iterated and the function is called\n    recursively.\n    :param node: An astroid node.\n    :type node: astroid.bases.NodeNG\n    :param line: The line number of the statement to extract.\n    :type line: int\n    :returns: The statement on the line, or None if no statement for the line\n      can be found.\n    :rtype:  astroid.bases.NodeNG or None\n    \"\"\"\n    if isinstance(node, (nodes.ClassDef, nodes.FunctionDef, nodes.MatchCase)):\n        # This is an inaccuracy in the AST: the nodes that can be\n        # decorated do not carry explicit information on which line\n        # the actual definition (class/def), but .fromline seems to\n        # be close enough.\n        node_line = node.fromlineno\n    else:\n        node_line = node.lineno\n    if node_line == line:\n        return node\n    for child in node.get_children():\n        result = _find_statement_by_line(child, line)\n        if result:\n            return result\n    return None\ndef extract_node(code: str, module_name: str = \"\") -> Union[NodeNG, List[NodeNG]]:\n    \"\"\"Parses some Python code as a module and extracts a designated AST node.\n    Statements:\n     To extract one or more statement nodes, append #@ to the end of the line\n     Examples:\n       >>> def x():\n       >>>   def y():\n       >>>     return 1 #@\n       The return statement will be extracted.\n       >>> class X(object):\n       >>>   def meth(self): #@\n       >>>     pass\n      The function object 'meth' will be extracted.\n    Expressions:\n     To extract arbitrary expressions, surround them with the fake\n     function call __(...). After parsing, the surrounded expression\n     will be returned and the whole AST (accessible via the returned\n     node's parent attribute) will look like the function call was\n     never there in the first place.\n     Examples:\n       >>> a = __(1)\n       The const node will be extracted.\n       >>> def x(d=__(foo.bar)): pass\n       The node containing the default argument will be extracted.\n       >>> def foo(a, b):\n       >>>   return 0 < __(len(a)) < b\n       The node containing the function call 'len' will be extracted.\n    If no statements or expressions are selected, the last toplevel\n    statement will be returned.\n    If the selected statement is a discard statement, (i.e. an expression\n    turned into a statement), the wrapped expression is returned instead.\n    For convenience, singleton lists are unpacked.\n    :param str code: A piece of Python code that is parsed as\n    a module. Will be passed through textwrap.dedent first.\n    :param str module_name: The name of the module.\n    :returns: The designated node from the parse tree, or a list of nodes.\n    \"\"\"\n    def _extract(node):\n        if isinstance(node, nodes.Expr):\n            return node.value\n        return node\n    requested_lines = []\n    for idx, line in enumerate(code.splitlines()):\n        if line.strip().endswith(_STATEMENT_SELECTOR):\n            requested_lines.append(idx + 1)\n    tree = parse(code, module_name=module_name)\n    if not tree.body:\n        raise ValueError(\"Empty tree, cannot extract from it\")\n    extracted = []\n    if requested_lines:\n        extracted = [_find_statement_by_line(tree, line) for line in requested_lines]\n    # Modifies the tree.\n    extracted.extend(_extract_expressions(tree))\n    if not extracted:\n        extracted.append(tree.body[-1])\n    extracted = [_extract(node) for node in extracted]\n    if len(extracted) == 1:\n        return extracted[0]\n    return extracted\ndef _parse_string(data, type_comments=True):\n    parser_module = get_parser_module(type_comments=type_comments)\n    try:\n        parsed = parser_module.parse(data + \"\\n\", type_comments=type_comments)\n    except SyntaxError as exc:\n        # If the type annotations are misplaced for some reason, we do not want\n        # to fail the entire parsing of the file, so we need to retry the parsing without\n        # type comment support.\n        if exc.args[0] != MISPLACED_TYPE_ANNOTATION_ERROR or not type_comments:\n            raise\n        parser_module = get_parser_module(type_comments=False)\n        parsed = parser_module.parse(data + \"\\n\", type_comments=False)\n    return parsed, parser_module",
        "file_path": "astroid/builder.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 6,
        "score": 0.5671963691711426,
        "content": "# Copyright (c) 2014-2016, 2018, 2020 Claudiu Popa <pcmanticore@gmail.com>\n# Copyright (c) 2014 Jeff Quast <contact@jeffquast.com>\n# Copyright (c) 2014 Google, Inc.\n# Copyright (c) 2016 Florian Bruhin <me@the-compiler.org>\n# Copyright (c) 2016 Ceridwen <ceridwenv@gmail.com>\n# Copyright (c) 2020-2021 hippo91 <guillaume.peillex@gmail.com>\n# Copyright (c) 2021 Pierre Sassoulas <pierre.sassoulas@gmail.com>\n# Copyright (c) 2021 Marc Mueller <30130371+cdce8p@users.noreply.github.com>\n# Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n# For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n\"\"\"Astroid hooks for pytest.\"\"\"\nfrom astroid.brain.helpers import register_module_extender\nfrom astroid.builder import AstroidBuilder\nfrom astroid.manager import AstroidManager\ndef pytest_transform():\n    return AstroidBuilder(AstroidManager()).string_build(\n        \"\"\"\ntry:\n    import _pytest.mark\n    import _pytest.recwarn\n    import _pytest.runner\n    import _pytest.python\n    import _pytest.skipping\n    import _pytest.assertion\nexcept ImportError:\n    pass\nelse:\n    deprecated_call = _pytest.recwarn.deprecated_call\n    warns = _pytest.recwarn.warns\n    exit = _pytest.runner.exit\n    fail = _pytest.runner.fail\n    skip = _pytest.runner.skip\n    importorskip = _pytest.runner.importorskip\n    xfail = _pytest.skipping.xfail\n    mark = _pytest.mark.MarkGenerator()\n    raises = _pytest.python.raises\n    # New in pytest 3.0\n    try:\n        approx = _pytest.python.approx\n        register_assert_rewrite = _pytest.assertion.register_assert_rewrite\n    except AttributeError:\n        pass\n# Moved in pytest 3.0\ntry:\n    import _pytest.freeze_support\n    freeze_includes = _pytest.freeze_support.freeze_includes\nexcept ImportError:\n    try:\n        import _pytest.genscript\n        freeze_includes = _pytest.genscript.freeze_includes\n    except ImportError:\n        pass\ntry:\n    import _pytest.debugging\n    set_trace = _pytest.debugging.pytestPDB().set_trace\nexcept ImportError:\n    try:\n        import _pytest.pdb\n        set_trace = _pytest.pdb.pytestPDB().set_trace\n    except ImportError:\n        pass\ntry:\n    import _pytest.fixtures\n    fixture = _pytest.fixtures.fixture\n    yield_fixture = _pytest.fixtures.yield_fixture\nexcept ImportError:\n    try:\n        import _pytest.python\n        fixture = _pytest.python.fixture\n        yield_fixture = _pytest.python.yield_fixture\n    except ImportError:\n        pass\n\"\"\"\n    )\nregister_module_extender(AstroidManager(), \"pytest\", pytest_transform)\nregister_module_extender(AstroidManager(), \"py.test\", pytest_transform)",
        "file_path": "astroid/brain/brain_pytest.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 7,
        "score": 0.5410047173500061,
        "content": "# Copyright (c) 2015-2018 Claudiu Popa <pcmanticore@gmail.com>\n# Copyright (c) 2015-2016 Ceridwen <ceridwenv@gmail.com>\n# Copyright (c) 2018 Bryce Guinta <bryce.paul.guinta@gmail.com>\n# Copyright (c) 2018 Nick Drozd <nicholasdrozd@gmail.com>\n# Copyright (c) 2020-2021 hippo91 <guillaume.peillex@gmail.com>\n# Copyright (c) 2020 Bryce Guinta <bryce.guinta@protonmail.com>\n# Copyright (c) 2021 Pierre Sassoulas <pierre.sassoulas@gmail.com>\n# Copyright (c) 2021 Marc Mueller <30130371+cdce8p@users.noreply.github.com>\n# Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n# For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\nimport importlib\nimport warnings\nimport lazy_object_proxy\ndef lazy_descriptor(obj):\n    class DescriptorProxy(lazy_object_proxy.Proxy):\n        def __get__(self, instance, owner=None):\n            return self.__class__.__get__(self, instance)\n    return DescriptorProxy(obj)\ndef lazy_import(module_name):\n    return lazy_object_proxy.Proxy(\n        lambda: importlib.import_module(\".\" + module_name, \"astroid\")\n    )\n@object.__new__\nclass Uninferable:\n    \"\"\"Special inference object, which is returned when inference fails.\"\"\"\n    def __repr__(self):\n        return \"Uninferable\"\n    __str__ = __repr__\n    def __getattribute__(self, name):\n        if name == \"next\":\n            raise AttributeError(\"next method should not be called\")\n        if name.startswith(\"__\") and name.endswith(\"__\"):\n            return object.__getattribute__(self, name)\n        if name == \"accept\":\n            return object.__getattribute__(self, name)\n        return self\n    def __call__(self, *args, **kwargs):\n        return self\n    def __bool__(self):\n        return False\n    __nonzero__ = __bool__\n    def accept(self, visitor):\n        return visitor.visit_uninferable(self)\nclass BadOperationMessage:\n    \"\"\"Object which describes a TypeError occurred somewhere in the inference chain\n    This is not an exception, but a container object which holds the types and\n    the error which occurred.\n    \"\"\"\nclass BadUnaryOperationMessage(BadOperationMessage):\n    \"\"\"Object which describes operational failures on UnaryOps.\"\"\"\n    def __init__(self, operand, op, error):\n        self.operand = operand\n        self.op = op\n        self.error = error\n    @property\n    def _object_type_helper(self):\n        helpers = lazy_import(\"helpers\")\n        return helpers.object_type\n    def _object_type(self, obj):\n        objtype = self._object_type_helper(obj)\n        if objtype is Uninferable:\n            return None\n        return objtype\n    def __str__(self):\n        if hasattr(self.operand, \"name\"):\n            operand_type = self.operand.name\n        else:\n            object_type = self._object_type(self.operand)\n            if hasattr(object_type, \"name\"):\n                operand_type = object_type.name\n            else:\n                # Just fallback to as_string\n                operand_type = object_type.as_string()\n        msg = \"bad operand type for unary {}: {}\"\n        return msg.format(self.op, operand_type)\nclass BadBinaryOperationMessage(BadOperationMessage):\n    \"\"\"Object which describes type errors for BinOps.\"\"\"\n    def __init__(self, left_type, op, right_type):\n        self.left_type = left_type\n        self.right_type = right_type\n        self.op = op\n    def __str__(self):\n        msg = \"unsupported operand type(s) for {}: {!r} and {!r}\"\n        return msg.format(self.op, self.left_type.name, self.right_type.name)\ndef _instancecheck(cls, other):\n    wrapped = cls.__wrapped__\n    other_cls = other.__class__\n    is_instance_of = wrapped is other_cls or issubclass(other_cls, wrapped)\n    warnings.warn(\n        \"%r is deprecated and slated for removal in astroid \"\n        \"2.0, use %r instead\" % (cls.__class__.__name__, wrapped.__name__),\n        PendingDeprecationWarning,\n        stacklevel=2,\n    )\n    return is_instance_of\ndef proxy_alias(alias_name, node_type):\n    \"\"\"Get a Proxy from the given name to the given node type.\"\"\"\n    proxy = type(\n        alias_name,\n        (lazy_object_proxy.Proxy,),\n        {\n            \"__class__\": object.__dict__[\"__class__\"],\n            \"__instancecheck__\": _instancecheck,\n        },\n    )\n    return proxy(lambda: node_type)",
        "file_path": "astroid/util.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 8,
        "score": 0.5337798595428467,
        "content": "\"\"\"Astroid hooks for unittest module\"\"\"\nfrom astroid.brain.helpers import register_module_extender\nfrom astroid.builder import parse\nfrom astroid.const import PY38_PLUS\nfrom astroid.manager import AstroidManager\ndef IsolatedAsyncioTestCaseImport():\n    \"\"\"\n    In the unittest package, the IsolatedAsyncioTestCase class is imported lazily, i.e only\n    when the __getattr__ method of the unittest module is called with 'IsolatedAsyncioTestCase' as\n    argument. Thus the IsolatedAsyncioTestCase is not imported statically (during import time).\n    This function mocks a classical static import of the IsolatedAsyncioTestCase.\n    (see https://github.com/PyCQA/pylint/issues/4060)\n    \"\"\"\n    return parse(\n        \"\"\"\n    from .async_case import IsolatedAsyncioTestCase\n    \"\"\"\n    )\nif PY38_PLUS:\n    register_module_extender(\n        AstroidManager(), \"unittest\", IsolatedAsyncioTestCaseImport\n    )",
        "file_path": "astroid/brain/brain_unittest.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 9,
        "score": 0.5255482196807861,
        "content": "#\n# Astroid documentation build configuration file, created by\n# sphinx-quickstart on Wed Jun 26 15:00:40 2013.\n#\n# This file is execfile()d with the current directory set to its containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\nimport os\nimport sys\nfrom datetime import datetime\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\nsys.path.insert(0, os.path.abspath(\"../../\"))\n# -- General configuration -----------------------------------------------------\n# If your documentation needs a minimal Sphinx version, state it here.\n# needs_sphinx = '1.0'\n# Add any Sphinx extension module names here, as strings. They can be extensions\n# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.\nextensions = [\n    \"sphinx.ext.autodoc\",\n    \"sphinx.ext.autosummary\",\n    \"sphinx.ext.doctest\",\n    \"sphinx.ext.intersphinx\",\n    \"sphinx.ext.todo\",\n    \"sphinx.ext.viewcode\",\n    \"sphinx.ext.napoleon\",\n]\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\"_templates\"]\n# The suffix of source filenames.\nsource_suffix = \".rst\"\n# The encoding of source files.\n# source_encoding = 'utf-8-sig'\n# The master toctree document.\nmaster_doc = \"index\"\n# General information about the project.\nproject = \"Astroid\"\ncurrent_year = datetime.utcnow().year\ncopyright = f\"2003-{current_year}, Logilab, PyCQA and contributors\"\n# The version info for the project you're documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nfrom astroid.__pkginfo__ import __version__\n# The full version, including alpha/beta/rc tags.\nrelease = __version__\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n# language = None\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n# today = ''\n# Else, today_fmt is used as the format for a strftime call.\n# today_fmt = '%B %d, %Y'\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\nexclude_patterns = [\"_build\"]\n# The reST default role (used for this markup: `text`) to use for all documents.\n# default_role = None\n# If true, '()' will be appended to :func: etc. cross-reference text.\nadd_function_parentheses = True\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n# add_module_names = True\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n# show_authors = False\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \"sphinx\"\n# A list of ignored prefixes for module index sorting.\n# modindex_common_prefix = []\n# -- Customization --\nprimary_domain = \"py\"\ntodo_include_todos = True\n# -- Options for HTML output ---------------------------------------------------\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\nhtml_theme = \"nature\"\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n# html_theme_options = {}\n# Add any paths that contain custom themes here, relative to this directory.\n# html_theme_path = []\n# The name for this set of Sphinx documents.  If None, it defaults to\n# \"<project> v<release> documentation\".\n# html_title = None\n# A shorter title for the navigation bar.  Default is the same as html_title.\n# html_short_title = None\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\n# html_logo = None\n# The name of an image file (within the static path) to use as favicon of the\n# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\n# html_favicon = None\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = [\"media\"]\n# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,\n# using the given strftime format.\n# html_last_updated_fmt = '%b %d, %Y'\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n# html_use_smartypants = True\n# Custom sidebar templates, maps document names to template names.\n# html_sidebars = {}\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n# html_additional_pages = {}\n# If false, no module index is generated.\n# html_domain_indices = True\n# If false, no index is generated.\n# html_use_index = True\n# If true, the index is split into individual pages for each letter.\n# html_split_index = False\n# If true, links to the reST sources are added to the pages.\n# html_show_sourcelink = True\n# If true, \"Created using Sphinx\" is shown in the HTML footer. Default is True.\n# html_show_sphinx = True\n# If true, \"(C) Copyright ...\" is shown in the HTML footer. Default is True.\n# html_show_copyright = True\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n# html_use_opensearch = ''\n# This is the file name suffix for HTML files (e.g. \".xhtml\").\n# html_file_suffix = None\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \"Pylintdoc\"\n# -- Options for LaTeX output --------------------------------------------------\n# The paper size ('letter' or 'a4').\n# latex_paper_size = 'letter'\n# The font size ('10pt', '11pt' or '12pt').\n# latex_font_size = '10pt'\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title, author, documentclass [howto/manual]).\nlatex_documents = [\n    (\n        \"index\",\n        \"Astroid.tex\",\n        \"Astroid Documentation\",\n        \"Logilab, PyCQA and contributors\",\n        \"manual\",\n    ),\n]\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n# latex_logo = None\n# For \"manual\" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n# latex_use_parts = False\n# If true, show page references after internal links.\n# latex_show_pagerefs = False\n# If true, show URL addresses after external links.\n# latex_show_urls = False\n# Additional stuff for the LaTeX preamble.\n# latex_preamble = ''\n# Documents to append as an appendix to all manuals.\n# latex_appendices = []\n# If false, no module index is generated.\n# latex_domain_indices = True\n# -- Options for manual page output --------------------------------------------\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (\n        \"index\",\n        \"astroid\",\n        \"Astroid Documentation\",\n        [\"Logilab, PyCQA and contributors\"],\n        1,\n    )\n]\nautodoc_default_options = {\n    \"members\": True,\n    \"undoc-members\": True,\n    \"show-inheritance\": True,\n}\nautoclass_content = \"both\"\nautodoc_member_order = \"groupwise\"\nautodoc_typehints = \"description\"\nintersphinx_mapping = {\n    \"green_tree_snakes\": (\n        \"http://greentreesnakes.readthedocs.io/en/latest/\",\n        \"ast_objects.inv\",\n    ),\n}",
        "file_path": "doc/conf.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 10,
        "score": 0.5242822170257568,
        "content": "# Copyright (c) 2014-2016, 2018, 2020 Claudiu Popa <pcmanticore@gmail.com>\n# Copyright (c) 2015-2016 Ceridwen <ceridwenv@gmail.com>\n# Copyright (c) 2018 Bryce Guinta <bryce.paul.guinta@gmail.com>\n# Copyright (c) 2020-2021 hippo91 <guillaume.peillex@gmail.com>\n# Copyright (c) 2020 Ram Rachum <ram@rachum.com>\n# Copyright (c) 2021 Pierre Sassoulas <pierre.sassoulas@gmail.com>\n# Copyright (c) 2021 Marc Mueller <30130371+cdce8p@users.noreply.github.com>\n# Copyright (c) 2021 Artsiom Kaval <lezeroq@gmail.com>\n# Copyright (c) 2021 Francis Charette Migneault <francis.charette.migneault@gmail.com>\n# Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n# For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n\"\"\"Astroid hooks for six module.\"\"\"\nfrom textwrap import dedent\nfrom astroid import nodes\nfrom astroid.brain.helpers import register_module_extender\nfrom astroid.builder import AstroidBuilder\nfrom astroid.exceptions import (\n    AstroidBuildingError,\n    AttributeInferenceError,\n    InferenceError,\n)\nfrom astroid.manager import AstroidManager\nSIX_ADD_METACLASS = \"six.add_metaclass\"\nSIX_WITH_METACLASS = \"six.with_metaclass\"\ndef default_predicate(line):\n    return line.strip()\ndef _indent(text, prefix, predicate=default_predicate):\n    \"\"\"Adds 'prefix' to the beginning of selected lines in 'text'.\n    If 'predicate' is provided, 'prefix' will only be added to the lines\n    where 'predicate(line)' is True. If 'predicate' is not provided,\n    it will default to adding 'prefix' to all non-empty lines that do not\n    consist solely of whitespace characters.\n    \"\"\"\n    def prefixed_lines():\n        for line in text.splitlines(True):\n            yield prefix + line if predicate(line) else line\n    return \"\".join(prefixed_lines())\n_IMPORTS = \"\"\"\nimport _io\ncStringIO = _io.StringIO\nfilter = filter\nfrom itertools import filterfalse\ninput = input\nfrom sys import intern\nmap = map\nrange = range\nfrom importlib import reload\nreload_module = lambda module: reload(module)\nfrom functools import reduce\nfrom shlex import quote as shlex_quote\nfrom io import StringIO\nfrom collections import UserDict, UserList, UserString\nxrange = range\nzip = zip\nfrom itertools import zip_longest\nimport builtins\nimport configparser\nimport copyreg\nimport _dummy_thread\nimport http.cookiejar as http_cookiejar\nimport http.cookies as http_cookies\nimport html.entities as html_entities\nimport html.parser as html_parser\nimport http.client as http_client\nimport http.server as http_server\nBaseHTTPServer = CGIHTTPServer = SimpleHTTPServer = http.server\nimport pickle as cPickle\nimport queue\nimport reprlib\nimport socketserver\nimport _thread\nimport winreg\nimport xmlrpc.server as xmlrpc_server\nimport xmlrpc.client as xmlrpc_client\nimport urllib.robotparser as urllib_robotparser\nimport email.mime.multipart as email_mime_multipart\nimport email.mime.nonmultipart as email_mime_nonmultipart\nimport email.mime.text as email_mime_text\nimport email.mime.base as email_mime_base\nimport urllib.parse as urllib_parse\nimport urllib.error as urllib_error\nimport tkinter\nimport tkinter.dialog as tkinter_dialog\nimport tkinter.filedialog as tkinter_filedialog\nimport tkinter.scrolledtext as tkinter_scrolledtext\nimport tkinter.simpledialog as tkinder_simpledialog\nimport tkinter.tix as tkinter_tix\nimport tkinter.ttk as tkinter_ttk\nimport tkinter.constants as tkinter_constants\nimport tkinter.dnd as tkinter_dnd\nimport tkinter.colorchooser as tkinter_colorchooser\nimport tkinter.commondialog as tkinter_commondialog\nimport tkinter.filedialog as tkinter_tkfiledialog\nimport tkinter.font as tkinter_font\nimport tkinter.messagebox as tkinter_messagebox\nimport urllib\nimport urllib.request as urllib_request\nimport urllib.robotparser as urllib_robotparser\nimport urllib.parse as urllib_parse\nimport urllib.error as urllib_error\n\"\"\"\ndef six_moves_transform():\n    code = dedent(\n        \"\"\"\n    class Moves(object):\n    {}\n    moves = Moves()\n    \"\"\"\n    ).format(_indent(_IMPORTS, \"    \"))\n    module = AstroidBuilder(AstroidManager()).string_build(code)\n    module.name = \"six.moves\"\n    return module\ndef _six_fail_hook(modname):\n    \"\"\"Fix six.moves imports due to the dynamic nature of this\n    class.\n    Construct a pseudo-module which contains all the necessary imports\n    for six\n    :param modname: Name of failed module\n    :type modname: str\n    :return: An astroid module\n    :rtype: nodes.Module\n    \"\"\"\n    attribute_of = modname != \"six.moves\" and modname.startswith(\"six.moves\")\n    if modname != \"six.moves\" and not attribute_of:\n        raise AstroidBuildingError(modname=modname)\n    module = AstroidBuilder(AstroidManager()).string_build(_IMPORTS)\n    module.name = \"six.moves\"\n    if attribute_of:\n        # Facilitate import of submodules in Moves\n        start_index = len(module.name)\n        attribute = modname[start_index:].lstrip(\".\").replace(\".\", \"_\")\n        try:\n            import_attr = module.getattr(attribute)[0]\n        except AttributeInferenceError as exc:\n            raise AstroidBuildingError(modname=modname) from exc\n        if isinstance(import_attr, nodes.Import):\n            submodule = AstroidManager().ast_from_module_name(import_attr.names[0][0])\n            return submodule\n    # Let dummy submodule imports pass through\n    # This will cause an Uninferable result, which is okay\n    return module\ndef _looks_like_decorated_with_six_add_metaclass(node):\n    if not node.decorators:\n        return False\n    for decorator in node.decorators.nodes:\n        if not isinstance(decorator, nodes.Call):\n            continue\n        if decorator.func.as_string() == SIX_ADD_METACLASS:\n            return True\n    return False\ndef transform_six_add_metaclass(node):  # pylint: disable=inconsistent-return-statements\n    \"\"\"Check if the given class node is decorated with *six.add_metaclass*\n    If so, inject its argument as the metaclass of the underlying class.\n    \"\"\"\n    if not node.decorators:\n        return\n    for decorator in node.decorators.nodes:\n        if not isinstance(decorator, nodes.Call):\n            continue\n        try:\n            func = next(decorator.func.infer())\n        except (InferenceError, StopIteration):\n            continue\n        if func.qname() == SIX_ADD_METACLASS and decorator.args:\n            metaclass = decorator.args[0]\n            node._metaclass = metaclass\n            return node\n    return\ndef _looks_like_nested_from_six_with_metaclass(node):\n    if len(node.bases) != 1:\n        return False\n    base = node.bases[0]\n    if not isinstance(base, nodes.Call):\n        return False\n    try:\n        if hasattr(base.func, \"expr\"):\n            # format when explicit 'six.with_metaclass' is used\n            mod = base.func.expr.name\n            func = base.func.attrname\n            func = f\"{mod}.{func}\"\n        else:\n            # format when 'with_metaclass' is used directly (local import from six)\n            # check reference module to avoid 'with_metaclass' name clashes\n            mod = base.parent.parent\n            import_from = mod.locals[\"with_metaclass\"][0]\n            func = f\"{import_from.modname}.{base.func.name}\"\n    except (AttributeError, KeyError, IndexError):\n        return False\n    return func == SIX_WITH_METACLASS\ndef transform_six_with_metaclass(node):\n    \"\"\"Check if the given class node is defined with *six.with_metaclass*\n    If so, inject its argument as the metaclass of the underlying class.\n    \"\"\"\n    call = node.bases[0]\n    node._metaclass = call.args[0]\n    return node\nregister_module_extender(AstroidManager(), \"six\", six_moves_transform)\nregister_module_extender(\n    AstroidManager(), \"requests.packages.urllib3.packages.six\", six_moves_transform\n)\nAstroidManager().register_failed_import_hook(_six_fail_hook)\nAstroidManager().register_transform(\n    nodes.ClassDef,\n    transform_six_add_metaclass,\n    _looks_like_decorated_with_six_add_metaclass,\n)\nAstroidManager().register_transform(\n    nodes.ClassDef,\n    transform_six_with_metaclass,\n    _looks_like_nested_from_six_with_metaclass,\n)",
        "file_path": "astroid/brain/brain_six.py",
        "chunk_index": 0,
        "metadata": {}
      }
    ]
  },
  "pylint-dev__astroid-1196": {
    "query": "getitem does not infer the actual unpacked value\nWhen trying to call `Dict.getitem()` on a context where we have a dict unpacking of anything beside a real dict, astroid currently raises an `AttributeError: 'getitem'`, which has 2 problems:\r\n\r\n- The object might be a reference against something constant, this pattern is usually seen when we have different sets of dicts that extend each other, and all of their values are inferrable. \r\n- We can have something that is uninferable, but in that case instead of an `AttributeError` I think it makes sense to raise the usual `AstroidIndexError` which is supposed to be already handled by the downstream.\r\n\r\n\r\nHere is a short reproducer;\r\n\r\n```py\r\nfrom astroid import parse\r\n\r\n\r\nsource = \"\"\"\r\nX = {\r\n    'A': 'B'\r\n}\r\n\r\nY = {\r\n    **X\r\n}\r\n\r\nKEY = 'A'\r\n\"\"\"\r\n\r\ntree = parse(source)\r\n\r\nfirst_dict = tree.body[0].value\r\nsecond_dict = tree.body[1].value\r\nkey = tree.body[2].value\r\n\r\nprint(f'{first_dict.getitem(key).value = }')\r\nprint(f'{second_dict.getitem(key).value = }')\r\n\r\n\r\n```\r\n\r\nThe current output;\r\n\r\n```\r\n $ python t1.py                                                                                                 3ms\r\nfirst_dict.getitem(key).value = 'B'\r\nTraceback (most recent call last):\r\n  File \"/home/isidentical/projects/astroid/t1.py\", line 23, in <module>\r\n    print(f'{second_dict.getitem(key).value = }')\r\n  File \"/home/isidentical/projects/astroid/astroid/nodes/node_classes.py\", line 2254, in getitem\r\n    return value.getitem(index, context)\r\nAttributeError: 'Name' object has no attribute 'getitem'\r\n```\r\n\r\nExpeceted output;\r\n```\r\n $ python t1.py                                                                                                 4ms\r\nfirst_dict.getitem(key).value = 'B'\r\nsecond_dict.getitem(key).value = 'B'\r\n\r\n```\r\n\n",
    "method": "sliding",
    "top_k": 10,
    "results": [
      {
        "rank": 1,
        "score": 0.6049461364746094,
        "content": "# Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n# For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n# Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\nfrom __future__ import annotations\nfrom astroid import nodes\nfrom astroid.bases import Instance\nfrom astroid.context import CallContext, InferenceContext\nfrom astroid.exceptions import InferenceError, NoDefault\nfrom astroid.util import Uninferable\nclass CallSite:\n    \"\"\"Class for understanding arguments passed into a call site\n    It needs a call context, which contains the arguments and the\n    keyword arguments that were passed into a given call site.\n    In order to infer what an argument represents, call :meth:`infer_argument`\n    with the corresponding function node and the argument name.\n    :param callcontext:\n        An instance of :class:`astroid.context.CallContext`, that holds\n        the arguments for the call site.\n    :param argument_context_map:\n        Additional contexts per node, passed in from :attr:`astroid.context.Context.extra_context`\n    :param context:\n        An instance of :class:`astroid.context.Context`.\n    \"\"\"\n    def __init__(\n        self, callcontext: CallContext, argument_context_map=None, context=None\n    ):\n        if argument_context_map is None:\n            argument_context_map = {}\n        self.argument_context_map = argument_context_map\n        args = callcontext.args\n        keywords = callcontext.keywords\n        self.duplicated_keywords: set[str] = set()\n        self._unpacked_args = self._unpack_args(args, context=context)\n        self._unpacked_kwargs = self._unpack_keywords(keywords, context=context)\n        self.positional_arguments = [\n            arg for arg in self._unpacked_args if arg is not Uninferable\n        ]\n        self.keyword_arguments = {\n            key: value\n            for key, value in self._unpacked_kwargs.items()\n            if value is not Uninferable\n        }\n    @classmethod\n    def from_call(cls, call_node, context: InferenceContext | None = None):\n        \"\"\"Get a CallSite object from the given Call node.\n        context will be used to force a single inference path.\n        \"\"\"\n        # Determine the callcontext from the given `context` object if any.\n        context = context or InferenceContext()\n        callcontext = CallContext(call_node.args, call_node.keywords)\n        return cls(callcontext, context=context)\n    def has_invalid_arguments(self):\n        \"\"\"Check if in the current CallSite were passed *invalid* arguments\n        This can mean multiple things. For instance, if an unpacking\n        of an invalid object was passed, then this method will return True.\n        Other cases can be when the arguments can't be inferred by astroid,\n        for example, by passing objects which aren't known statically.\n        \"\"\"\n        return len(self.positional_arguments) != len(self._unpacked_args)\n    def has_invalid_keywords(self):\n        \"\"\"Check if in the current CallSite were passed *invalid* keyword arguments\n        For instance, unpacking a dictionary with integer keys is invalid\n        (**{1:2}), because the keys must be strings, which will make this\n        method to return True. Other cases where this might return True if\n        objects which can't be inferred were passed.\n        \"\"\"\n        return len(self.keyword_arguments) != len(self._unpacked_kwargs)\n    def _unpack_keywords(self, keywords, context=None):\n        values = {}\n        context = context or InferenceContext()\n        context.extra_context = self.argument_context_map\n        for name, value in keywords:\n            if name is None:\n                # Then it's an unpacking operation (**)\n                try:\n                    inferred = next(value.infer(context=context))\n                except InferenceError:\n                    values[name] = Uninferable\n                    continue\n                except StopIteration:\n                    continue\n                if not isinstance(inferred, nodes.Dict):\n                    # Not something we can work with.\n                    values[name] = Uninferable\n                    continue\n                for dict_key, dict_value in inferred.items:\n                    try:\n                        dict_key = next(dict_key.infer(context=context))\n                    except InferenceError:\n                        values[name] = Uninferable\n                        continue\n                    except StopIteration:\n                        continue\n                    if not isinstance(dict_key, nodes.Const):\n                        values[name] = Uninferable\n                        continue\n                    if not isinstance(dict_key.value, str):\n                        values[name] = Uninferable\n                        continue\n                    if dict_key.value in values:\n                        # The name is already in the dictionary\n                        values[dict_key.value] = Uninferable\n                        self.duplicated_keywords.add(dict_key.value)\n                        continue\n                    values[dict_key.value] = dict_value\n            else:\n                values[name] = value\n        return values\n    def _unpack_args(self, args, context=None):\n        values = []\n        context = context or InferenceContext()\n        context.extra_context = self.argument_context_map\n        for arg in args:\n            if isinstance(arg, nodes.Starred):\n                try:\n                    inferred = next(arg.value.infer(context=context))\n                except InferenceError:\n                    values.append(Uninferable)\n                    continue\n                except StopIteration:\n                    continue\n                if inferred is Uninferable:\n                    values.append(Uninferable)\n                    continue\n                if not hasattr(inferred, \"elts\"):\n                    values.append(Uninferable)\n                    continue\n                values.extend(inferred.elts)\n            else:\n                values.append(arg)\n        return values\n    def infer_argument(self, funcnode, name, context):\n        \"\"\"infer a function argument value according to the call context\n        Arguments:\n            funcnode: The function being called.\n            name: The name of the argument whose value is being inferred.\n            context: Inference context object\n        \"\"\"\n        if name in self.duplicated_keywords:\n            raise InferenceError(\n                \"The arguments passed to {func!r} have duplicate keywords.\",\n                call_site=self,\n                func=funcnode,\n                arg=name,\n                context=context,\n            )\n        # Look into the keywords first, maybe it's already there.\n        try:\n            return self.keyword_arguments[name].infer(context)\n        except KeyError:\n            pass\n        # Too many arguments given and no variable arguments.\n        if len(self.positional_arguments) > len(funcnode.args.args):\n            if not funcnode.args.vararg and not funcnode.args.posonlyargs:\n                raise InferenceError(\n                    \"Too many positional arguments \"\n                    \"passed to {func!r} that does \"\n                    \"not have *args.\",\n                    call_site=self,\n                    func=funcnode,\n                    arg=name,\n                    context=context,\n                )\n        positional = self.positional_arguments[: len(funcnode.args.args)]\n        vararg = self.positional_arguments[len(funcnode.args.args) :]\n        argindex = funcnode.args.find_argname(name)[0]\n        kwonlyargs = {arg.name for arg in funcnode.args.kwonlyargs}\n        kwargs = {\n            key: value\n            for key, value in self.keyword_arguments.items()\n            if key not in kwonlyargs\n        }\n        # If there are too few positionals compared to\n        # what the function expects to receive, check to see\n        # if the missing positional arguments were passed\n        # as keyword arguments and if so, place them into the\n        # positional args list.\n        if len(positional) < len(funcnode.args.args):\n            for func_arg in funcnode.args.args:\n                if func_arg.name in kwargs:\n                    arg = kwargs.pop(func_arg.name)\n                    positional.append(arg)\n        if argindex is not None:\n            boundnode = getattr(context, \"boundnode\", None)\n            # 2. first argument of instance/class method\n            if argindex == 0 and funcnode.type in {\"method\", \"classmethod\"}:\n                # context.boundnode is None when an instance method is called with\n                # the class, e.g. MyClass.method(obj, ...). In this case, self\n                # is the first argument.\n                if boundnode is None and funcnode.type == \"method\" and positional:\n                    return positional[0].infer(context=context)\n                if boundnode is None:\n                    # XXX can do better ?\n                    boundnode = funcnode.parent.frame(future=True)\n                if isinstance(boundnode, nodes.ClassDef):\n                    # Verify that we're accessing a method\n                    # of the metaclass through a class, as in\n                    # `cls.metaclass_method`. In this case, the\n                    # first argument is always the class.\n                    method_scope = funcnode.parent.scope()\n                    if method_scope is boundnode.metaclass():\n                        return iter((boundnode,))\n                if funcnode.type == \"method\":\n                    if not isinstance(boundnode, Instance):\n                        boundnode = boundnode.instantiate_class()\n                    return iter((boundnode,))\n                if funcnode.type == \"classmethod\":\n                    return iter((boundnode,))\n            # if we have a method, extract one position\n            # from the index, so we'll take in account\n            # the extra parameter represented by `self` or `cls`\n            if funcnode.type in {\"method\", \"classmethod\"} and boundnode:\n                argindex -= 1\n            # 2. search arg index\n            try:\n                return self.positional_arguments[argindex].infer(context)\n            except IndexError:\n                pass\n        if funcnode.args.kwarg == name:\n            # It wants all the keywords that were passed into\n            # the call site.\n            if self.has_invalid_keywords():\n                raise InferenceError(\n                    \"Inference failed to find values for all keyword arguments \"\n                    \"to {func!r}: {unpacked_kwargs!r} doesn't correspond to \"\n                    \"{keyword_arguments!r}.\",\n                    keyword_arguments=self.keyword_arguments,\n                    unpacked_kwargs=self._unpacked_kwargs,\n                    call_site=self,\n                    func=funcnode,\n                    arg=name,\n                    context=context,\n                )\n            kwarg = nodes.Dict(\n                lineno=funcnode.args.lineno,\n                col_offset=funcnode.args.col_offset,\n                parent=funcnode.args,\n            )\n            kwarg.postinit(\n                [(nodes.const_factory(key), value) for key, value in kwargs.items()]\n            )\n            return iter((kwarg,))\n        if funcnode.args.vararg == name:\n            # It wants all the args that were passed into\n            # the call site.\n            if self.has_invalid_arguments():\n                raise InferenceError(\n                    \"Inference failed to find values for all positional \"\n                    \"arguments to {func!r}: {unpacked_args!r} doesn't \"\n                    \"correspond to {positional_arguments!r}.\",\n                    positional_arguments=self.positional_arguments,\n                    unpacked_args=self._unpacked_args,\n                    call_site=self,\n                    func=funcnode,\n                    arg=name,\n                    context=context,\n                )\n            args = nodes.Tuple(\n                lineno=funcnode.args.lineno,\n                col_offset=funcnode.args.col_offset,\n                parent=funcnode.args,\n            )\n            args.postinit(vararg)\n            return iter((args,))\n        # Check if it's a default parameter.\n        try:\n            return funcnode.args.default_value(name).infer(context)\n        except NoDefault:\n            pass\n        raise InferenceError(\n            \"No value found for argument {arg} to {func!r}\",\n            call_site=self,\n            func=funcnode,\n            arg=name,\n            context=context,\n        )",
        "file_path": "astroid/arguments.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 2,
        "score": 0.6014056205749512,
        "content": "# Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n# For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n# Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\nfrom astroid.brain.helpers import register_module_extender\nfrom astroid.builder import extract_node, parse\nfrom astroid.const import PY39_PLUS\nfrom astroid.exceptions import AttributeInferenceError\nfrom astroid.manager import AstroidManager\nfrom astroid.nodes.scoped_nodes import ClassDef\ndef _collections_transform():\n    return parse(\n        \"\"\"\n    class defaultdict(dict):\n        default_factory = None\n        def __missing__(self, key): pass\n        def __getitem__(self, key): return default_factory\n    \"\"\"\n        + _deque_mock()\n        + _ordered_dict_mock()\n    )\ndef _deque_mock():\n    base_deque_class = \"\"\"\n    class deque(object):\n        maxlen = 0\n        def __init__(self, iterable=None, maxlen=None):\n            self.iterable = iterable or []\n        def append(self, x): pass\n        def appendleft(self, x): pass\n        def clear(self): pass\n        def count(self, x): return 0\n        def extend(self, iterable): pass\n        def extendleft(self, iterable): pass\n        def pop(self): return self.iterable[0]\n        def popleft(self): return self.iterable[0]\n        def remove(self, value): pass\n        def reverse(self): return reversed(self.iterable)\n        def rotate(self, n=1): return self\n        def __iter__(self): return self\n        def __reversed__(self): return self.iterable[::-1]\n        def __getitem__(self, index): return self.iterable[index]\n        def __setitem__(self, index, value): pass\n        def __delitem__(self, index): pass\n        def __bool__(self): return bool(self.iterable)\n        def __nonzero__(self): return bool(self.iterable)\n        def __contains__(self, o): return o in self.iterable\n        def __len__(self): return len(self.iterable)\n        def __copy__(self): return deque(self.iterable)\n        def copy(self): return deque(self.iterable)\n        def index(self, x, start=0, end=0): return 0\n        def insert(self, i, x): pass\n        def __add__(self, other): pass\n        def __iadd__(self, other): pass\n        def __mul__(self, other): pass\n        def __imul__(self, other): pass\n        def __rmul__(self, other): pass\"\"\"\n    if PY39_PLUS:\n        base_deque_class += \"\"\"\n        @classmethod\n        def __class_getitem__(self, item): return cls\"\"\"\n    return base_deque_class\ndef _ordered_dict_mock():\n    base_ordered_dict_class = \"\"\"\n    class OrderedDict(dict):\n        def __reversed__(self): return self[::-1]\n        def move_to_end(self, key, last=False): pass\"\"\"\n    if PY39_PLUS:\n        base_ordered_dict_class += \"\"\"\n        @classmethod\n        def __class_getitem__(cls, item): return cls\"\"\"\n    return base_ordered_dict_class\nregister_module_extender(AstroidManager(), \"collections\", _collections_transform)\ndef _looks_like_subscriptable(node: ClassDef) -> bool:\n    \"\"\"\n    Returns True if the node corresponds to a ClassDef of the Collections.abc module that\n    supports subscripting\n    :param node: ClassDef node\n    \"\"\"\n    if node.qname().startswith(\"_collections\") or node.qname().startswith(\n        \"collections\"\n    ):\n        try:\n            node.getattr(\"__class_getitem__\")\n            return True\n        except AttributeInferenceError:\n            pass\n    return False\nCLASS_GET_ITEM_TEMPLATE = \"\"\"\n@classmethod\ndef __class_getitem__(cls, item):\n    return cls\n\"\"\"\ndef easy_class_getitem_inference(node, context=None):\n    # Here __class_getitem__ exists but is quite a mess to infer thus\n    # put an easy inference tip\n    func_to_add = extract_node(CLASS_GET_ITEM_TEMPLATE)\n    node.locals[\"__class_getitem__\"] = [func_to_add]\nif PY39_PLUS:\n    # Starting with Python39 some objects of the collection module are subscriptable\n    # thanks to the __class_getitem__ method but the way it is implemented in\n    # _collection_abc makes it difficult to infer. (We would have to handle AssignName inference in the\n    # getitem method of the ClassDef class) Instead we put here a mock of the __class_getitem__ method\n    AstroidManager().register_transform(\n        ClassDef, easy_class_getitem_inference, _looks_like_subscriptable\n    )",
        "file_path": "astroid/brain/brain_collections.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 3,
        "score": 0.5894826650619507,
        "content": "    result will be an Uninferable object. Otherwise, the normal attribute\n    lookup will be done.\n    \"\"\"\n    obj, attr = _infer_getattr_args(node, context)\n    if (\n        obj is util.Uninferable\n        or attr is util.Uninferable\n        or not hasattr(obj, \"igetattr\")\n    ):\n        return util.Uninferable\n    try:\n        return next(obj.igetattr(attr, context=context))\n    except (StopIteration, InferenceError, AttributeInferenceError):\n        if len(node.args) == 3:\n            # Try to infer the default and return it instead.\n            try:\n                return next(node.args[2].infer(context=context))\n            except (StopIteration, InferenceError) as exc:\n                raise UseInferenceDefault from exc\n    raise UseInferenceDefault\ndef infer_hasattr(node, context=None):\n    \"\"\"Understand hasattr calls\n    This always guarantees three possible outcomes for calling\n    hasattr: Const(False) when we are sure that the object\n    doesn't have the intended attribute, Const(True) when\n    we know that the object has the attribute and Uninferable\n    when we are unsure of the outcome of the function call.\n    \"\"\"\n    try:\n        obj, attr = _infer_getattr_args(node, context)\n        if (\n            obj is util.Uninferable\n            or attr is util.Uninferable\n            or not hasattr(obj, \"getattr\")\n        ):\n            return util.Uninferable\n        obj.getattr(attr, context=context)\n    except UseInferenceDefault:\n        # Can't infer something from this function call.\n        return util.Uninferable\n    except AttributeInferenceError:\n        # Doesn't have it.\n        return nodes.Const(False)\n    return nodes.Const(True)\ndef infer_callable(node, context=None):\n    \"\"\"Understand callable calls\n    This follows Python's semantics, where an object\n    is callable if it provides an attribute __call__,\n    even though that attribute is something which can't be\n    called.\n    \"\"\"\n    if len(node.args) != 1:\n        # Invalid callable call.\n        raise UseInferenceDefault\n    argument = node.args[0]\n    try:\n        inferred = next(argument.infer(context=context))\n    except (InferenceError, StopIteration):\n        return util.Uninferable\n    if inferred is util.Uninferable:\n        return util.Uninferable\n    return nodes.Const(inferred.callable())\ndef infer_property(\n    node: nodes.Call, context: InferenceContext | None = None\n) -> objects.Property:\n    \"\"\"Understand `property` class\n    This only infers the output of `property`\n    call, not the arguments themselves.\n    \"\"\"\n    if len(node.args) < 1:\n        # Invalid property call.\n        raise UseInferenceDefault\n    getter = node.args[0]\n    try:\n        inferred = next(getter.infer(context=context))\n    except (InferenceError, StopIteration) as exc:\n        raise UseInferenceDefault from exc\n    if not isinstance(inferred, (nodes.FunctionDef, nodes.Lambda)):\n        raise UseInferenceDefault\n    prop_func = objects.Property(\n        function=inferred,\n        name=inferred.name,\n        lineno=node.lineno,\n        parent=node,\n        col_offset=node.col_offset,\n    )\n    prop_func.postinit(\n        body=[],\n        args=inferred.args,\n        doc_node=getattr(inferred, \"doc_node\", None),\n    )\n    return prop_func\ndef infer_bool(node, context=None):\n    \"\"\"Understand bool calls.\"\"\"\n    if len(node.args) > 1:\n        # Invalid bool call.\n        raise UseInferenceDefault\n    if not node.args:\n        return nodes.Const(False)\n    argument = node.args[0]\n    try:\n        inferred = next(argument.infer(context=context))\n    except (InferenceError, StopIteration):\n        return util.Uninferable\n    if inferred is util.Uninferable:\n        return util.Uninferable\n    bool_value = inferred.bool_value(context=context)\n    if bool_value is util.Uninferable:\n        return util.Uninferable\n    return nodes.Const(bool_value)\ndef infer_type(node, context=None):\n    \"\"\"Understand the one-argument form of *type*.\"\"\"\n    if len(node.args) != 1:\n        raise UseInferenceDefault\n    return helpers.object_type(node.args[0], context)\ndef infer_slice(node, context=None):\n    \"\"\"Understand `slice` calls.\"\"\"\n    args = node.args\n    if not 0 < len(args) <= 3:\n        raise UseInferenceDefault\n    infer_func = partial(helpers.safe_infer, context=context)\n    args = [infer_func(arg) for arg in args]\n    for arg in args:\n        if not arg or arg is util.Uninferable:\n            raise UseInferenceDefault\n        if not isinstance(arg, nodes.Const):\n            raise UseInferenceDefault\n        if not isinstance(arg.value, (type(None), int)):\n            raise UseInferenceDefault\n    if len(args) < 3:\n        # Make sure we have 3 arguments.\n        args.extend([None] * (3 - len(args)))\n    slice_node = nodes.Slice(\n        lineno=node.lineno, col_offset=node.col_offset, parent=node.parent\n    )\n    slice_node.postinit(*args)\n    return slice_node\ndef _infer_object__new__decorator(node, context=None):\n    # Instantiate class immediately\n    # since that's what @object.__new__ does\n    return iter((node.instantiate_class(),))\ndef _infer_object__new__decorator_check(node):\n    \"\"\"Predicate before inference_tip\n    Check if the given ClassDef has an @object.__new__ decorator\n    \"\"\"\n    if not node.decorators:\n        return False\n    for decorator in node.decorators.nodes:\n        if isinstance(decorator, nodes.Attribute):\n            if decorator.as_string() == OBJECT_DUNDER_NEW:\n                return True\n    return False\ndef infer_issubclass(callnode, context=None):\n    \"\"\"Infer issubclass() calls\n    :param nodes.Call callnode: an `issubclass` call\n    :param InferenceContext context: the context for the inference\n    :rtype nodes.Const: Boolean Const value of the `issubclass` call\n    :raises UseInferenceDefault: If the node cannot be inferred\n    \"\"\"\n    call = arguments.CallSite.from_call(callnode, context=context)\n    if call.keyword_arguments:\n        # issubclass doesn't support keyword arguments\n        raise UseInferenceDefault(\"TypeError: issubclass() takes no keyword arguments\")\n    if len(call.positional_arguments) != 2:\n        raise UseInferenceDefault(\n            f\"Expected two arguments, got {len(call.positional_arguments)}\"\n        )\n    # The left hand argument is the obj to be checked\n    obj_node, class_or_tuple_node = call.positional_arguments\n    try:\n        obj_type = next(obj_node.infer(context=context))\n    except (InferenceError, StopIteration) as exc:\n        raise UseInferenceDefault from exc\n    if not isinstance(obj_type, nodes.ClassDef):\n        raise UseInferenceDefault(\"TypeError: arg 1 must be class\")\n    # The right hand argument is the class(es) that the given\n    # object is to be checked against.\n    try:\n        class_container = _class_or_tuple_to_container(\n            class_or_tuple_node, context=context\n        )\n    except InferenceError as exc:\n        raise UseInferenceDefault from exc\n    try:\n        issubclass_bool = helpers.object_issubclass(obj_type, class_container, context)\n    except AstroidTypeError as exc:\n        raise UseInferenceDefault(\"TypeError: \" + str(exc)) from exc\n    except MroError as exc:\n        raise UseInferenceDefault from exc\n    return nodes.Const(issubclass_bool)\ndef infer_isinstance(callnode, context=None):\n    \"\"\"Infer isinstance calls\n    :param nodes.Call callnode: an isinstance call\n    :param InferenceContext context: context for call\n        (currently unused but is a common interface for inference)\n    :rtype nodes.Const: Boolean Const value of isinstance call\n    :raises UseInferenceDefault: If the node cannot be inferred\n    \"\"\"\n    call = arguments.CallSite.from_call(callnode, context=context)\n    if call.keyword_arguments:\n        # isinstance doesn't support keyword arguments\n        raise UseInferenceDefault(\"TypeError: isinstance() takes no keyword arguments\")\n    if len(call.positional_arguments) != 2:\n        raise UseInferenceDefault(\n            f\"Expected two arguments, got {len(call.positional_arguments)}\"\n        )\n    # The left hand argument is the obj to be checked\n    obj_node, class_or_tuple_node = call.positional_arguments\n    # The right hand argument is the class(es) that the given\n    # obj is to be check is an instance of\n    try:\n        class_container = _class_or_tuple_to_container(\n            class_or_tuple_node, context=context\n        )\n    except InferenceError as exc:\n        raise UseInferenceDefault from exc\n    try:\n        isinstance_bool = helpers.object_isinstance(obj_node, class_container, context)\n    except AstroidTypeError as exc:\n        raise UseInferenceDefault(\"TypeError: \" + str(exc)) from exc\n    except MroError as exc:\n        raise UseInferenceDefault from exc\n    if isinstance_bool is util.Uninferable:\n        raise UseInferenceDefault\n    return nodes.Const(isinstance_bool)\ndef _class_or_tuple_to_container(node, context=None):\n    # Move inferences results into container\n    # to simplify later logic\n    # raises InferenceError if any of the inferences fall through\n    try:\n        node_infer = next(node.infer(context=context))\n    except StopIteration as e:\n        raise InferenceError(node=node, context=context) from e\n    # arg2 MUST be a type or a TUPLE of types\n    # for isinstance\n    if isinstance(node_infer, nodes.Tuple):\n        try:\n            class_container = [\n                next(node.infer(context=context)) for node in node_infer.elts\n            ]\n        except StopIteration as e:\n            raise InferenceError(node=node, context=context) from e\n        class_container = [\n            klass_node for klass_node in class_container if klass_node is not None\n        ]\n    else:\n        class_container = [node_infer]\n    return class_container\ndef infer_len(node, context=None):\n    \"\"\"Infer length calls\n    :param nodes.Call node: len call to infer\n    :param context.InferenceContext: node context\n    :rtype nodes.Const: a Const node with the inferred length, if possible\n    \"\"\"\n    call = arguments.CallSite.from_call(node, context=context)\n    if call.keyword_arguments:\n        raise UseInferenceDefault(\"TypeError: len() must take no keyword arguments\")\n    if len(call.positional_arguments) != 1:\n        raise UseInferenceDefault(\n            \"TypeError: len() must take exactly one argument \"\n            \"({len}) given\".format(len=len(call.positional_arguments))\n        )\n    [argument_node] = call.positional_arguments\n    try:\n        return nodes.Const(helpers.object_len(argument_node, context=context))\n    except (AstroidTypeError, InferenceError) as exc:\n        raise UseInferenceDefault(str(exc)) from exc\ndef infer_str(node, context=None):\n    \"\"\"Infer str() calls\n    :param nodes.Call node: str() call to infer\n    :param context.InferenceContext: node context\n    :rtype nodes.Const: a Const containing an empty string\n    \"\"\"\n    call = arguments.CallSite.from_call(node, context=context)\n    if call.keyword_arguments:\n        raise UseInferenceDefault(\"TypeError: str() must take no keyword arguments\")\n    try:\n        return nodes.Const(\"\")\n    except (AstroidTypeError, InferenceError) as exc:\n        raise UseInferenceDefault(str(exc)) from exc\ndef infer_int(node, context=None):\n    \"\"\"Infer int() calls\n    :param nodes.Call node: int() call to infer\n    :param context.InferenceContext: node context\n    :rtype nodes.Const: a Const containing the integer value of the int() call\n    \"\"\"\n    call = arguments.CallSite.from_call(node, context=context)\n    if call.keyword_arguments:\n        raise UseInferenceDefault(\"TypeError: int() must take no keyword arguments\")\n    if call.positional_arguments:\n        try:\n            first_value = next(call.positional_arguments[0].infer(context=context))\n        except (InferenceError, StopIteration) as exc:\n            raise UseInferenceDefault(str(exc)) from exc\n        if first_value is util.Uninferable:\n            raise UseInferenceDefault\n        if isinstance(first_value, nodes.Const) and isinstance(\n            first_value.value, (int, str)\n        ):\n            try:\n                actual_value = int(first_value.value)\n            except ValueError:\n                return nodes.Const(0)\n            return nodes.Const(actual_value)\n    return nodes.Const(0)\ndef infer_dict_fromkeys(node, context=None):\n    \"\"\"Infer dict.fromkeys\n    :param nodes.Call node: dict.fromkeys() call to infer\n    :param context.InferenceContext context: node context\n    :rtype nodes.Dict:\n        a Dictionary containing the values that astroid was able to infer.\n        In case the inference failed for any reason, an empty dictionary\n        will be inferred instead.\n    \"\"\"\n    def _build_dict_with_elements(elements):\n        new_node = nodes.Dict(\n            col_offset=node.col_offset, lineno=node.lineno, parent=node.parent\n        )\n        new_node.postinit(elements)\n        return new_node\n    call = arguments.CallSite.from_call(node, context=context)\n    if call.keyword_arguments:\n        raise UseInferenceDefault(\"TypeError: int() must take no keyword arguments\")\n    if len(call.positional_arguments) not in {1, 2}:\n        raise UseInferenceDefault(\n            \"TypeError: Needs between 1 and 2 positional arguments\"\n        )\n    default = nodes.Const(None)\n    values = call.positional_arguments[0]\n    try:\n        inferred_values = next(values.infer(context=context))\n    except (InferenceError, StopIteration):\n        return _build_dict_with_elements([])\n    if inferred_values is util.Uninferable:\n        return _build_dict_with_elements([])\n    # Limit to a couple of potential values, as this can become pretty complicated\n    accepted_iterable_elements = (nodes.Const,)\n    if isinstance(inferred_values, (nodes.List, nodes.Set, nodes.Tuple)):\n        elements = inferred_values.elts\n        for element in elements:\n            if not isinstance(element, accepted_iterable_elements):\n                # Fallback to an empty dict\n                return _build_dict_with_elements([])\n        elements_with_value = [(element, default) for element in elements]\n        return _build_dict_with_elements(elements_with_value)\n    if isinstance(inferred_values, nodes.Const) and isinstance(\n        inferred_values.value, (str, bytes)\n    ):\n        elements = [\n            (nodes.Const(element), default) for element in inferred_values.value\n        ]\n        return _build_dict_with_elements(elements)\n    if isinstance(inferred_values, nodes.Dict):\n        keys = inferred_values.itered()\n        for key in keys:\n            if not isinstance(key, accepted_iterable_elements):\n                # Fallback to an empty dict\n                return _build_dict_with_elements([])\n        elements_with_value = [(element, default) for element in keys]\n        return _build_dict_with_elements(elements_with_value)\n    # Fallback to an empty dictionary\n    return _build_dict_with_elements([])\ndef _infer_copy_method(\n    node: nodes.Call, context: InferenceContext | None = None\n) -> Iterator[nodes.NodeNG]:\n    assert isinstance(node.func, nodes.Attribute)\n    inferred_orig, inferred_copy = itertools.tee(node.func.expr.infer(context=context))\n    if all(\n        isinstance(\n            inferred_node, (nodes.Dict, nodes.List, nodes.Set, objects.FrozenSet)\n        )\n        for inferred_node in inferred_orig\n    ):\n        return inferred_copy\n    raise UseInferenceDefault()\n# Builtins inference\nregister_builtin_transform(infer_bool, \"bool\")\nregister_builtin_transform(infer_super, \"super\")\nregister_builtin_transform(infer_callable, \"callable\")\nregister_builtin_transform(infer_property, \"property\")\nregister_builtin_transform(infer_getattr, \"getattr\")\nregister_builtin_transform(infer_hasattr, \"hasattr\")\nregister_builtin_transform(infer_tuple, \"tuple\")\nregister_builtin_transform(infer_set, \"set\")\nregister_builtin_transform(infer_list, \"list\")\nregister_builtin_transform(infer_dict, \"dict\")\nregister_builtin_transform(infer_frozenset, \"frozenset\")\nregister_builtin_transform(infer_type, \"type\")\nregister_builtin_transform(infer_slice, \"slice\")\nregister_builtin_transform(infer_isinstance, \"isinstance\")\nregister_builtin_transform(infer_issubclass, \"issubclass\")\nregister_builtin_transform(infer_len, \"len\")\nregister_builtin_transform(infer_str, \"str\")\nregister_builtin_transform(infer_int, \"int\")\nregister_builtin_transform(infer_dict_fromkeys, \"dict.fromkeys\")\n# Infer object.__new__ calls\nAstroidManager().register_transform(\n    nodes.ClassDef,\n    inference_tip(_infer_object__new__decorator),\n    _infer_object__new__decorator_check,\n)\nAstroidManager().register_transform(\n    nodes.Call,\n    inference_tip(_infer_copy_method),\n    lambda node: isinstance(node.func, nodes.Attribute)\n    and node.func.attrname == \"copy\",\n)",
        "file_path": "astroid/brain/brain_builtin_inference.py",
        "chunk_index": 1,
        "metadata": {}
      },
      {
        "rank": 4,
        "score": 0.5882864594459534,
        "content": "# Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n# For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n# Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n\"\"\"this module contains a set of functions to handle inference on astroid trees\n\"\"\"\nfrom __future__ import annotations\nimport ast\nimport functools\nimport itertools\nimport operator\nfrom collections.abc import Callable, Generator, Iterable, Iterator\nfrom typing import TYPE_CHECKING, Any, TypeVar\nfrom astroid import bases, decorators, helpers, nodes, protocols, util\nfrom astroid.context import (\n    CallContext,\n    InferenceContext,\n    bind_context_to_node,\n    copy_context,\n)\nfrom astroid.exceptions import (\n    AstroidBuildingError,\n    AstroidError,\n    AstroidIndexError,\n    AstroidTypeError,\n    AttributeInferenceError,\n    InferenceError,\n    NameInferenceError,\n    _NonDeducibleTypeHierarchy,\n)\nfrom astroid.interpreter import dunder_lookup\nfrom astroid.manager import AstroidManager\nfrom astroid.typing import InferenceErrorInfo\nif TYPE_CHECKING:\n    from astroid.objects import Property\n# Prevents circular imports\nobjects = util.lazy_import(\"objects\")\n_FunctionDefT = TypeVar(\"_FunctionDefT\", bound=nodes.FunctionDef)\n# .infer method ###############################################################\ndef infer_end(self, context=None):\n    \"\"\"Inference's end for nodes that yield themselves on inference\n    These are objects for which inference does not have any semantic,\n    such as Module or Consts.\n    \"\"\"\n    yield self\n# We add ignores to all these assignments in this file\n# See https://github.com/python/mypy/issues/2427\nnodes.Module._infer = infer_end  # type: ignore[assignment]\nnodes.ClassDef._infer = infer_end  # type: ignore[assignment]\nnodes.Lambda._infer = infer_end  # type: ignore[assignment]\nnodes.Const._infer = infer_end  # type: ignore[assignment]\nnodes.Slice._infer = infer_end  # type: ignore[assignment]\ndef _infer_sequence_helper(node, context=None):\n    \"\"\"Infer all values based on _BaseContainer.elts\"\"\"\n    values = []\n    for elt in node.elts:\n        if isinstance(elt, nodes.Starred):\n            starred = helpers.safe_infer(elt.value, context)\n            if not starred:\n                raise InferenceError(node=node, context=context)\n            if not hasattr(starred, \"elts\"):\n                raise InferenceError(node=node, context=context)\n            values.extend(_infer_sequence_helper(starred))\n        elif isinstance(elt, nodes.NamedExpr):\n            value = helpers.safe_infer(elt.value, context)\n            if not value:\n                raise InferenceError(node=node, context=context)\n            values.append(value)\n        else:\n            values.append(elt)\n    return values\n@decorators.raise_if_nothing_inferred\ndef infer_sequence(self, context=None):\n    has_starred_named_expr = any(\n        isinstance(e, (nodes.Starred, nodes.NamedExpr)) for e in self.elts\n    )\n    if has_starred_named_expr:\n        values = _infer_sequence_helper(self, context)\n        new_seq = type(self)(\n            lineno=self.lineno, col_offset=self.col_offset, parent=self.parent\n        )\n        new_seq.postinit(values)\n        yield new_seq\n    else:\n        yield self\nnodes.List._infer = infer_sequence  # type: ignore[assignment]\nnodes.Tuple._infer = infer_sequence  # type: ignore[assignment]\nnodes.Set._infer = infer_sequence  # type: ignore[assignment]\ndef infer_map(self, context=None):\n    if not any(isinstance(k, nodes.DictUnpack) for k, _ in self.items):\n        yield self\n    else:\n        items = _infer_map(self, context)\n        new_seq = type(self)(self.lineno, self.col_offset, self.parent)\n        new_seq.postinit(list(items.items()))\n        yield new_seq\ndef _update_with_replacement(lhs_dict, rhs_dict):\n    \"\"\"Delete nodes that equate to duplicate keys\n    Since an astroid node doesn't 'equal' another node with the same value,\n    this function uses the as_string method to make sure duplicate keys\n    don't get through\n    Note that both the key and the value are astroid nodes\n    Fixes issue with DictUnpack causing duplicte keys\n    in inferred Dict items\n    :param dict(nodes.NodeNG, nodes.NodeNG) lhs_dict: Dictionary to 'merge' nodes into\n    :param dict(nodes.NodeNG, nodes.NodeNG) rhs_dict: Dictionary with nodes to pull from\n    :return dict(nodes.NodeNG, nodes.NodeNG): merged dictionary of nodes\n    \"\"\"\n    combined_dict = itertools.chain(lhs_dict.items(), rhs_dict.items())\n    # Overwrite keys which have the same string values\n    string_map = {key.as_string(): (key, value) for key, value in combined_dict}\n    # Return to dictionary\n    return dict(string_map.values())\ndef _infer_map(node, context):\n    \"\"\"Infer all values based on Dict.items\"\"\"\n    values = {}\n    for name, value in node.items:\n        if isinstance(name, nodes.DictUnpack):\n            double_starred = helpers.safe_infer(value, context)\n            if not double_starred:\n                raise InferenceError\n            if not isinstance(double_starred, nodes.Dict):\n                raise InferenceError(node=node, context=context)\n            unpack_items = _infer_map(double_starred, context)\n            values = _update_with_replacement(values, unpack_items)\n        else:\n            key = helpers.safe_infer(name, context=context)\n            value = helpers.safe_infer(value, context=context)\n            if any(not elem for elem in (key, value)):\n                raise InferenceError(node=node, context=context)\n            values = _update_with_replacement(values, {key: value})\n    return values\nnodes.Dict._infer = infer_map  # type: ignore[assignment]\ndef _higher_function_scope(node):\n    \"\"\"Search for the first function which encloses the given\n    scope. This can be used for looking up in that function's\n    scope, in case looking up in a lower scope for a particular\n    name fails.\n    :param node: A scope node.\n    :returns:\n        ``None``, if no parent function scope was found,\n        otherwise an instance of :class:`astroid.nodes.scoped_nodes.Function`,\n        which encloses the given node.\n    \"\"\"\n    current = node\n    while current.parent and not isinstance(current.parent, nodes.FunctionDef):\n        current = current.parent\n    if current and current.parent:\n        return current.parent\n    return None\ndef infer_name(self, context=None):\n    \"\"\"infer a Name: use name lookup rules\"\"\"\n    frame, stmts = self.lookup(self.name)\n    if not stmts:\n        # Try to see if the name is enclosed in a nested function\n        # and use the higher (first function) scope for searching.\n        parent_function = _higher_function_scope(self.scope())\n        if parent_function:\n            _, stmts = parent_function.lookup(self.name)\n        if not stmts:\n            raise NameInferenceError(\n                name=self.name, scope=self.scope(), context=context\n            )\n    context = copy_context(context)\n    context.lookupname = self.name\n    return bases._infer_stmts(stmts, context, frame)\n# pylint: disable=no-value-for-parameter\nnodes.Name._infer = decorators.raise_if_nothing_inferred(\n    decorators.path_wrapper(infer_name)\n)\nnodes.AssignName.infer_lhs = infer_name  # won't work with a path wrapper\n@decorators.raise_if_nothing_inferred\n@decorators.path_wrapper\ndef infer_call(self, context=None):\n    \"\"\"infer a Call node by trying to guess what the function returns\"\"\"\n    callcontext = copy_context(context)\n    callcontext.boundnode = None\n    if context is not None:\n        callcontext.extra_context = _populate_context_lookup(self, context.clone())\n    for callee in self.func.infer(context):\n        if callee is util.Uninferable:\n            yield callee\n            continue\n        try:\n            if hasattr(callee, \"infer_call_result\"):\n                callcontext.callcontext = CallContext(\n                    args=self.args, keywords=self.keywords, callee=callee\n                )\n                yield from callee.infer_call_result(caller=self, context=callcontext)\n        except InferenceError:\n            continue\n    return dict(node=self, context=context)\nnodes.Call._infer = infer_call  # type: ignore[assignment]\n@decorators.raise_if_nothing_inferred\n@decorators.path_wrapper\ndef infer_import(self, context=None, asname=True):\n    \"\"\"infer an Import node: return the imported module/object\"\"\"\n    name = context.lookupname\n    if name is None:\n        raise InferenceError(node=self, context=context)\n    try:\n        if asname:\n            yield self.do_import_module(self.real_name(name))\n        else:\n            yield self.do_import_module(name)\n    except AstroidBuildingError as exc:\n        raise InferenceError(node=self, context=context) from exc\nnodes.Import._infer = infer_import\n@decorators.raise_if_nothing_inferred\n@decorators.path_wrapper\ndef infer_import_from(self, context=None, asname=True):\n    \"\"\"infer a ImportFrom node: return the imported module/object\"\"\"\n    name = context.lookupname\n    if name is None:\n        raise InferenceError(node=self, context=context)\n    if asname:\n        try:\n            name = self.real_name(name)\n        except AttributeInferenceError as exc:\n            # See https://github.com/PyCQA/pylint/issues/4692\n            raise InferenceError(node=self, context=context) from exc\n    try:\n        module = self.do_import_module()\n    except AstroidBuildingError as exc:\n        raise InferenceError(node=self, context=context) from exc\n    try:\n        context = copy_context(context)\n        context.lookupname = name\n        stmts = module.getattr(name, ignore_locals=module is self.root())\n        return bases._infer_stmts(stmts, context)\n    except AttributeInferenceError as error:\n        raise InferenceError(\n            str(error), target=self, attribute=name, context=context\n        ) from error\nnodes.ImportFrom._infer = infer_import_from  # type: ignore[assignment]\ndef infer_attribute(self, context=None):\n    \"\"\"infer an Attribute node by using getattr on the associated object\"\"\"\n    for owner in self.expr.infer(context):\n        if owner is util.Uninferable:\n            yield owner\n            continue\n        if not context:\n            context = InferenceContext()\n        else:\n            context = copy_context(context)\n        old_boundnode = context.boundnode\n        try:\n            context.boundnode = owner\n            yield from owner.igetattr(self.attrname, context)\n        except (\n            AttributeInferenceError,\n            InferenceError,\n            AttributeError,\n        ):\n            pass\n        finally:\n            context.boundnode = old_boundnode\n    return dict(node=self, context=context)\nnodes.Attribute._infer = decorators.raise_if_nothing_inferred(\n    decorators.path_wrapper(infer_attribute)\n)\n# won't work with a path wrapper\nnodes.AssignAttr.infer_lhs = decorators.raise_if_nothing_inferred(infer_attribute)\n@decorators.raise_if_nothing_inferred\n@decorators.path_wrapper\ndef infer_global(self, context=None):\n    if context.lookupname is None:\n        raise InferenceError(node=self, context=context)\n    try:\n        return bases._infer_stmts(self.root().getattr(context.lookupname), context)\n    except AttributeInferenceError as error:\n        raise InferenceError(\n            str(error), target=self, attribute=context.lookupname, context=context\n        ) from error\nnodes.Global._infer = infer_global  # type: ignore[assignment]\n_SUBSCRIPT_SENTINEL = object()\ndef infer_subscript(self, context=None):\n    \"\"\"Inference for subscripts\n    We're understanding if the index is a Const\n    or a slice, passing the result of inference\n    to the value's `getitem` method, which should\n    handle each supported index type accordingly.\n    \"\"\"\n    found_one = False\n    for value in self.value.infer(context):\n        if value is util.Uninferable:\n            yield util.Uninferable\n            return None\n        for index in self.slice.infer(context):\n            if index is util.Uninferable:\n                yield util.Uninferable\n                return None\n            # Try to deduce the index value.\n            index_value = _SUBSCRIPT_SENTINEL\n            if value.__class__ == bases.Instance:\n                index_value = index\n            elif index.__class__ == bases.Instance:\n                instance_as_index = helpers.class_instance_as_index(index)\n                if instance_as_index:\n                    index_value = instance_as_index\n            else:\n                index_value = index\n            if index_value is _SUBSCRIPT_SENTINEL:\n                raise InferenceError(node=self, context=context)\n            try:\n                assigned = value.getitem(index_value, context)\n            except (\n                AstroidTypeError,\n                AstroidIndexError,\n                AttributeInferenceError,\n                AttributeError,\n            ) as exc:\n                raise InferenceError(node=self, context=context) from exc\n            # Prevent inferring if the inferred subscript\n            # is the same as the original subscripted object.\n            if self is assigned or assigned is util.Uninferable:\n                yield util.Uninferable\n                return None\n            yield from assigned.infer(context)\n            found_one = True\n    if found_one:\n        return dict(node=self, context=context)\n    return None\nnodes.Subscript._infer = decorators.raise_if_nothing_inferred(  # type: ignore[assignment]\n    decorators.path_wrapper(infer_subscript)\n)\nnodes.Subscript.infer_lhs = decorators.raise_if_nothing_inferred(infer_subscript)\n@decorators.raise_if_nothing_inferred\n@decorators.path_wrapper\ndef _infer_boolop(self, context=None):\n    \"\"\"Infer a boolean operation (and / or / not).\n    The function will calculate the boolean operation\n    for all pairs generated through inference for each component\n    node.\n    \"\"\"\n    values = self.values\n    if self.op == \"or\":\n        predicate = operator.truth\n    else:\n        predicate = operator.not_\n    try:\n        values = [value.infer(context=context) for value in values]\n    except InferenceError:\n        yield util.Uninferable\n        return None\n    for pair in itertools.product(*values):\n        if any(item is util.Uninferable for item in pair):\n            # Can't infer the final result, just yield Uninferable.\n            yield util.Uninferable\n            continue\n        bool_values = [item.bool_value() for item in pair]\n        if any(item is util.Uninferable for item in bool_values):\n            # Can't infer the final result, just yield Uninferable.\n            yield util.Uninferable\n            continue\n        # Since the boolean operations are short circuited operations,\n        # this code yields the first value for which the predicate is True\n        # and if no value respected the predicate, then the last value will\n        # be returned (or Uninferable if there was no last value).\n        # This is conforming to the semantics of `and` and `or`:\n        #   1 and 0 -> 1\n        #   0 and 1 -> 0\n        #   1 or 0 -> 1\n        #   0 or 1 -> 1\n        value = util.Uninferable\n        for value, bool_value in zip(pair, bool_values):\n            if predicate(bool_value):\n                yield value\n                break\n        else:\n            yield value\n    return dict(node=self, context=context)\nnodes.BoolOp._infer = _infer_boolop\n# UnaryOp, BinOp and AugAssign inferences\ndef _filter_operation_errors(self, infer_callable, context, error):\n    for result in infer_callable(self, context):\n        if isinstance(result, error):\n            # For the sake of .infer(), we don't care about operation\n            # errors, which is the job of pylint. So return something\n            # which shows that we can't infer the result.\n            yield util.Uninferable\n        else:\n            yield result\ndef _infer_unaryop(self, context=None):\n    \"\"\"Infer what an UnaryOp should return when evaluated.\"\"\"\n    for operand in self.operand.infer(context):\n        try:\n            yield operand.infer_unary_op(self.op)\n        except TypeError as exc:\n            # The operand doesn't support this operation.\n            yield util.BadUnaryOperationMessage(operand, self.op, exc)\n        except AttributeError as exc:\n            meth = protocols.UNARY_OP_METHOD[self.op]\n            if meth is None:\n                # `not node`. Determine node's boolean\n                # value and negate its result, unless it is\n                # Uninferable, which will be returned as is.\n                bool_value = operand.bool_value()\n                if bool_value is not util.Uninferable:\n                    yield nodes.const_factory(not bool_value)\n                else:\n                    yield util.Uninferable\n            else:\n                if not isinstance(operand, (bases.Instance, nodes.ClassDef)):\n                    # The operation was used on something which\n                    # doesn't support it.\n                    yield util.BadUnaryOperationMessage(operand, self.op, exc)\n                    continue\n                try:\n                    try:\n                        methods = dunder_lookup.lookup(operand, meth)\n                    except AttributeInferenceError:\n                        yield util.BadUnaryOperationMessage(operand, self.op, exc)\n                        continue\n                    meth = methods[0]\n                    inferred = next(meth.infer(context=context), None)\n                    if inferred is util.Uninferable or not inferred.callable():\n                        continue\n                    context = copy_context(context)\n                    context.boundnode = operand\n                    context.callcontext = CallContext(args=[], callee=inferred)\n                    call_results = inferred.infer_call_result(self, context=context)\n                    result = next(call_results, None)\n                    if result is None:\n                        # Failed to infer, return the same type.\n                        yield operand\n                    else:\n                        yield result\n                except AttributeInferenceError as inner_exc:\n                    # The unary operation special method was not found.\n                    yield util.BadUnaryOperationMessage(operand, self.op, inner_exc)\n                except InferenceError:\n                    yield util.Uninferable\n@decorators.raise_if_nothing_inferred\n@decorators.path_wrapper\ndef infer_unaryop(self, context=None):\n    \"\"\"Infer what an UnaryOp should return when evaluated.\"\"\"\n    yield from _filter_operation_errors(\n        self, _infer_unaryop, context, util.BadUnaryOperationMessage\n    )\n    return dict(node=self, context=context)\nnodes.UnaryOp._infer_unaryop = _infer_unaryop\nnodes.UnaryOp._infer = infer_unaryop\ndef _is_not_implemented(const):\n    \"\"\"Check if the given const node is NotImplemented.\"\"\"\n    return isinstance(const, nodes.Const) and const.value is NotImplemented\ndef _invoke_binop_inference(instance, opnode, op, other, context, method_name):\n    \"\"\"Invoke binary operation inference on the given instance.\"\"\"\n    methods = dunder_lookup.lookup(instance, method_name)\n    context = bind_context_to_node(context, instance)\n    method = methods[0]\n    context.callcontext.callee = method\n    try:\n        inferred = next(method.infer(context=context))\n    except StopIteration as e:\n        raise InferenceError(node=method, context=context) from e\n    if inferred is util.Uninferable:\n        raise InferenceError\n    return instance.infer_binary_op(opnode, op, other, context, inferred)\ndef _aug_op(instance, opnode, op, other, context, reverse=False):\n    \"\"\"Get an inference callable for an augmented binary operation.\"\"\"\n    method_name = protocols.AUGMENTED_OP_METHOD[op]\n    return functools.partial(\n        _invoke_binop_inference,\n        instance=instance,\n        op=op,\n        opnode=opnode,\n        other=other,\n        context=context,\n        method_name=method_name,\n    )\ndef _bin_op(instance, opnode, op, other, context, reverse=False):\n    \"\"\"Get an inference callable for a normal binary operation.\n    If *reverse* is True, then the reflected method will be used instead.\n    \"\"\"\n    if reverse:\n        method_name = protocols.REFLECTED_BIN_OP_METHOD[op]\n    else:\n        method_name = protocols.BIN_OP_METHOD[op]\n    return functools.partial(\n        _invoke_binop_inference,\n        instance=instance,\n        op=op,\n        opnode=opnode,\n        other=other,\n        context=context,\n        method_name=method_name,\n    )\ndef _get_binop_contexts(context, left, right):\n    \"\"\"Get contexts for binary operations.\n    This will return two inference contexts, the first one\n    for x.__op__(y), the other one for y.__rop__(x), where\n    only the arguments are inversed.\n    \"\"\"\n    # The order is important, since the first one should be\n    # left.__op__(right).\n    for arg in (right, left):\n        new_context = context.clone()\n        new_context.callcontext = CallContext(args=[arg])\n        new_context.boundnode = None\n        yield new_context",
        "file_path": "astroid/inference.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 5,
        "score": 0.5507186651229858,
        "content": "# Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n# For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n# Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n\"\"\"Module for some node classes. More nodes in scoped_nodes.py\"\"\"\nfrom __future__ import annotations\nimport abc\nimport itertools\nimport sys\nimport typing\nimport warnings\nfrom collections.abc import Generator, Iterator\nfrom functools import lru_cache\nfrom typing import TYPE_CHECKING, Any, Callable, ClassVar, Optional, TypeVar, Union\nfrom astroid import decorators, mixins, util\nfrom astroid.bases import Instance, _infer_stmts\nfrom astroid.const import Context\nfrom astroid.context import InferenceContext\nfrom astroid.exceptions import (\n    AstroidIndexError,\n    AstroidTypeError,\n    InferenceError,\n    NoDefault,\n    ParentMissingError,\n)\nfrom astroid.manager import AstroidManager\nfrom astroid.nodes.const import OP_PRECEDENCE\nfrom astroid.nodes.node_ng import NodeNG\nif sys.version_info >= (3, 8):\n    from typing import Literal\nelse:\n    from typing_extensions import Literal\nif TYPE_CHECKING:\n    from astroid import nodes\n    from astroid.nodes import LocalsDictNodeNG\nif sys.version_info >= (3, 8):\n    from functools import cached_property\nelse:\n    from astroid.decorators import cachedproperty as cached_property\ndef _is_const(value):\n    return isinstance(value, tuple(CONST_CLS))\n_NodesT = TypeVar(\"_NodesT\", bound=NodeNG)\nAssignedStmtsPossibleNode = Union[\"List\", \"Tuple\", \"AssignName\", \"AssignAttr\", None]\nAssignedStmtsCall = Callable[\n    [\n        _NodesT,\n        AssignedStmtsPossibleNode,\n        Optional[InferenceContext],\n        Optional[typing.List[int]],\n    ],\n    Any,\n]\n@decorators.raise_if_nothing_inferred\ndef unpack_infer(stmt, context=None):\n    \"\"\"recursively generate nodes inferred by the given statement.\n    If the inferred value is a list or a tuple, recurse on the elements\n    \"\"\"\n    if isinstance(stmt, (List, Tuple)):\n        for elt in stmt.elts:\n            if elt is util.Uninferable:\n                yield elt\n                continue\n            yield from unpack_infer(elt, context)\n        return dict(node=stmt, context=context)\n    # if inferred is a final node, return it and stop\n    inferred = next(stmt.infer(context), util.Uninferable)\n    if inferred is stmt:\n        yield inferred\n        return dict(node=stmt, context=context)\n    # else, infer recursively, except Uninferable object that should be returned as is\n    for inferred in stmt.infer(context):\n        if inferred is util.Uninferable:\n            yield inferred\n        else:\n            yield from unpack_infer(inferred, context)\n    return dict(node=stmt, context=context)\ndef are_exclusive(stmt1, stmt2, exceptions: list[str] | None = None) -> bool:\n    \"\"\"return true if the two given statements are mutually exclusive\n    `exceptions` may be a list of exception names. If specified, discard If\n    branches and check one of the statement is in an exception handler catching\n    one of the given exceptions.\n    algorithm :\n     1) index stmt1's parents\n     2) climb among stmt2's parents until we find a common parent\n     3) if the common parent is a If or TryExcept statement, look if nodes are\n        in exclusive branches\n    \"\"\"\n    # index stmt1's parents\n    stmt1_parents = {}\n    children = {}\n    previous = stmt1\n    for node in stmt1.node_ancestors():\n        stmt1_parents[node] = 1\n        children[node] = previous\n        previous = node\n    # climb among stmt2's parents until we find a common parent\n    previous = stmt2\n    for node in stmt2.node_ancestors():\n        if node in stmt1_parents:\n            # if the common parent is a If or TryExcept statement, look if\n            # nodes are in exclusive branches\n            if isinstance(node, If) and exceptions is None:\n                if (\n                    node.locate_child(previous)[1]\n                    is not node.locate_child(children[node])[1]\n                ):\n                    return True\n            elif isinstance(node, TryExcept):\n                c2attr, c2node = node.locate_child(previous)\n                c1attr, c1node = node.locate_child(children[node])\n                if c1node is not c2node:\n                    first_in_body_caught_by_handlers = (\n                        c2attr == \"handlers\"\n                        and c1attr == \"body\"\n                        and previous.catch(exceptions)\n                    )\n                    second_in_body_caught_by_handlers = (\n                        c2attr == \"body\"\n                        and c1attr == \"handlers\"\n                        and children[node].catch(exceptions)\n                    )\n                    first_in_else_other_in_handlers = (\n                        c2attr == \"handlers\" and c1attr == \"orelse\"\n                    )\n                    second_in_else_other_in_handlers = (\n                        c2attr == \"orelse\" and c1attr == \"handlers\"\n                    )\n                    if any(\n                        (\n                            first_in_body_caught_by_handlers,\n                            second_in_body_caught_by_handlers,\n                            first_in_else_other_in_handlers,\n                            second_in_else_other_in_handlers,\n                        )\n                    ):\n                        return True\n                elif c2attr == \"handlers\" and c1attr == \"handlers\":\n                    return previous is not children[node]\n            return False\n        previous = node\n    return False\n# getitem() helpers.\n_SLICE_SENTINEL = object()\ndef _slice_value(index, context=None):\n    \"\"\"Get the value of the given slice index.\"\"\"\n    if isinstance(index, Const):\n        if isinstance(index.value, (int, type(None))):\n            return index.value\n    elif index is None:\n        return None\n    else:\n        # Try to infer what the index actually is.\n        # Since we can't return all the possible values,\n        # we'll stop at the first possible value.\n        try:\n            inferred = next(index.infer(context=context))\n        except (InferenceError, StopIteration):\n            pass\n        else:\n            if isinstance(inferred, Const):\n                if isinstance(inferred.value, (int, type(None))):\n                    return inferred.value\n    # Use a sentinel, because None can be a valid\n    # value that this function can return,\n    # as it is the case for unspecified bounds.\n    return _SLICE_SENTINEL\ndef _infer_slice(node, context=None):\n    lower = _slice_value(node.lower, context)\n    upper = _slice_value(node.upper, context)\n    step = _slice_value(node.step, context)\n    if all(elem is not _SLICE_SENTINEL for elem in (lower, upper, step)):\n        return slice(lower, upper, step)\n    raise AstroidTypeError(\n        message=\"Could not infer slice used in subscript\",\n        node=node,\n        index=node.parent,\n        context=context,\n    )\ndef _container_getitem(instance, elts, index, context=None):\n    \"\"\"Get a slice or an item, using the given *index*, for the given sequence.\"\"\"\n    try:\n        if isinstance(index, Slice):\n            index_slice = _infer_slice(index, context=context)\n            new_cls = instance.__class__()\n            new_cls.elts = elts[index_slice]\n            new_cls.parent = instance.parent\n            return new_cls\n        if isinstance(index, Const):\n            return elts[index.value]\n    except IndexError as exc:\n        raise AstroidIndexError(\n            message=\"Index {index!s} out of range\",\n            node=instance,\n            index=index,\n            context=context,\n        ) from exc\n    except TypeError as exc:\n        raise AstroidTypeError(\n            message=\"Type error {error!r}\", node=instance, index=index, context=context\n        ) from exc\n    raise AstroidTypeError(f\"Could not use {index} as subscript index\")\nclass Statement(NodeNG):\n    \"\"\"Statement node adding a few attributes\"\"\"\n    is_statement = True\n    \"\"\"Whether this node indicates a statement.\"\"\"\n    def next_sibling(self):\n        \"\"\"The next sibling statement node.\n        :returns: The next sibling statement node.\n        :rtype: NodeNG or None\n        \"\"\"\n        stmts = self.parent.child_sequence(self)\n        index = stmts.index(self)\n        try:\n            return stmts[index + 1]\n        except IndexError:\n            return None\n    def previous_sibling(self):\n        \"\"\"The previous sibling statement.\n        :returns: The previous sibling statement node.\n        :rtype: NodeNG or None\n        \"\"\"\n        stmts = self.parent.child_sequence(self)\n        index = stmts.index(self)\n        if index >= 1:\n            return stmts[index - 1]\n        return None\nclass BaseContainer(\n    mixins.ParentAssignTypeMixin, NodeNG, Instance, metaclass=abc.ABCMeta\n):\n    \"\"\"Base class for Set, FrozenSet, Tuple and List.\"\"\"\n    _astroid_fields = (\"elts\",)\n    def __init__(\n        self,\n        lineno: int | None = None,\n        col_offset: int | None = None,\n        parent: NodeNG | None = None,\n        *,\n        end_lineno: int | None = None,\n        end_col_offset: int | None = None,\n    ) -> None:\n        \"\"\"\n        :param lineno: The line that this node appears on in the source code.\n        :param col_offset: The column that this node appears on in the\n            source code.\n        :param parent: The parent node in the syntax tree.\n        :param end_lineno: The last line this node appears on in the source code.\n        :param end_col_offset: The end column this node appears on in the\n            source code. Note: This is after the last symbol.\n        \"\"\"\n        self.elts: list[NodeNG] = []\n        \"\"\"The elements in the node.\"\"\"\n        super().__init__(\n            lineno=lineno,\n            col_offset=col_offset,\n            end_lineno=end_lineno,\n            end_col_offset=end_col_offset,\n            parent=parent,\n        )\n    def postinit(self, elts: list[NodeNG]) -> None:\n        \"\"\"Do some setup after initialisation.\n        :param elts: The list of elements the that node contains.\n        \"\"\"\n        self.elts = elts\n    @classmethod\n    def from_elements(cls, elts=None):\n        \"\"\"Create a node of this type from the given list of elements.\n        :param elts: The list of elements that the node should contain.\n        :type elts: list(NodeNG)\n        :returns: A new node containing the given elements.\n        :rtype: NodeNG\n        \"\"\"\n        node = cls()\n        if elts is None:\n            node.elts = []\n        else:\n            node.elts = [const_factory(e) if _is_const(e) else e for e in elts]\n        return node\n    def itered(self):\n        \"\"\"An iterator over the elements this node contains.\n        :returns: The contents of this node.\n        :rtype: iterable(NodeNG)\n        \"\"\"\n        return self.elts\n    def bool_value(self, context=None):\n        \"\"\"Determine the boolean value of this node.\n        :returns: The boolean value of this node.\n        :rtype: bool or Uninferable\n        \"\"\"\n        return bool(self.elts)\n    @abc.abstractmethod\n    def pytype(self):\n        \"\"\"Get the name of the type that this node represents.\n        :returns: The name of the type.\n        :rtype: str\n        \"\"\"\n    def get_children(self):\n        yield from self.elts\nclass LookupMixIn:\n    \"\"\"Mixin to look up a name in the right scope.\"\"\"\n    @lru_cache()  # noqa\n    def lookup(self, name: str) -> tuple[str, list[NodeNG]]:\n        \"\"\"Lookup where the given variable is assigned.\n        The lookup starts from self's scope. If self is not a frame itself\n        and the name is found in the inner frame locals, statements will be\n        filtered to remove ignorable statements according to self's location.\n        :param name: The name of the variable to find assignments for.\n        :returns: The scope node and the list of assignments associated to the\n            given name according to the scope where it has been found (locals,\n            globals or builtin).\n        \"\"\"\n        return self.scope().scope_lookup(self, name)\n    def ilookup(self, name):\n        \"\"\"Lookup the inferred values of the given variable.\n        :param name: The variable name to find values for.\n        :type name: str\n        :returns: The inferred values of the statements returned from\n            :meth:`lookup`.\n        :rtype: iterable\n        \"\"\"\n        frame, stmts = self.lookup(name)\n        context = InferenceContext()\n        return _infer_stmts(stmts, context, frame)\n# Name classes\nclass AssignName(\n    mixins.NoChildrenMixin, LookupMixIn, mixins.ParentAssignTypeMixin, NodeNG\n):\n    \"\"\"Variation of :class:`ast.Assign` representing assignment to a name.\n    An :class:`AssignName` is the name of something that is assigned to.\n    This includes variables defined in a function signature or in a loop.\n    >>> import astroid\n    >>> node = astroid.extract_node('variable = range(10)')\n    >>> node\n    <Assign l.1 at 0x7effe1db8550>\n    >>> list(node.get_children())\n    [<AssignName.variable l.1 at 0x7effe1db8748>, <Call l.1 at 0x7effe1db8630>]\n    >>> list(node.get_children())[0].as_string()\n    'variable'\n    \"\"\"\n    _other_fields = (\"name\",)\n    @decorators.deprecate_default_argument_values(name=\"str\")\n    def __init__(\n        self,\n        name: str | None = None,\n        lineno: int | None = None,\n        col_offset: int | None = None,\n        parent: NodeNG | None = None,\n        *,\n        end_lineno: int | None = None,\n        end_col_offset: int | None = None,\n    ) -> None:\n        \"\"\"\n        :param name: The name that is assigned to.\n        :param lineno: The line that this node appears on in the source code.\n        :param col_offset: The column that this node appears on in the\n            source code.\n        :param parent: The parent node in the syntax tree.\n        :param end_lineno: The last line this node appears on in the source code.\n        :param end_col_offset: The end column this node appears on in the\n            source code. Note: This is after the last symbol.\n        \"\"\"\n        self.name: str | None = name\n        \"\"\"The name that is assigned to.\"\"\"\n        super().__init__(\n            lineno=lineno,\n            col_offset=col_offset,\n            end_lineno=end_lineno,\n            end_col_offset=end_col_offset,\n            parent=parent,\n        )\n    assigned_stmts: ClassVar[AssignedStmtsCall[AssignName]]\n    \"\"\"Returns the assigned statement (non inferred) according to the assignment type.\n    See astroid/protocols.py for actual implementation.\n    \"\"\"\nclass DelName(\n    mixins.NoChildrenMixin, LookupMixIn, mixins.ParentAssignTypeMixin, NodeNG\n):\n    \"\"\"Variation of :class:`ast.Delete` representing deletion of a name.\n    A :class:`DelName` is the name of something that is deleted.\n    >>> import astroid\n    >>> node = astroid.extract_node(\"del variable #@\")\n    >>> list(node.get_children())\n    [<DelName.variable l.1 at 0x7effe1da4d30>]\n    >>> list(node.get_children())[0].as_string()\n    'variable'\n    \"\"\"\n    _other_fields = (\"name\",)\n    @decorators.deprecate_default_argument_values(name=\"str\")\n    def __init__(\n        self,\n        name: str | None = None,\n        lineno: int | None = None,\n        col_offset: int | None = None,\n        parent: NodeNG | None = None,\n        *,\n        end_lineno: int | None = None,\n        end_col_offset: int | None = None,\n    ) -> None:\n        \"\"\"\n        :param name: The name that is being deleted.\n        :param lineno: The line that this node appears on in the source code.\n        :param col_offset: The column that this node appears on in the\n            source code.\n        :param parent: The parent node in the syntax tree.\n        :param end_lineno: The last line this node appears on in the source code.\n        :param end_col_offset: The end column this node appears on in the\n            source code. Note: This is after the last symbol.\n        \"\"\"\n        self.name: str | None = name\n        \"\"\"The name that is being deleted.\"\"\"\n        super().__init__(\n            lineno=lineno,\n            col_offset=col_offset,\n            end_lineno=end_lineno,\n            end_col_offset=end_col_offset,\n            parent=parent,\n        )\nclass Name(mixins.NoChildrenMixin, LookupMixIn, NodeNG):\n    \"\"\"Class representing an :class:`ast.Name` node.\n    A :class:`Name` node is something that is named, but not covered by\n    :class:`AssignName` or :class:`DelName`.\n    >>> import astroid\n    >>> node = astroid.extract_node('range(10)')\n    >>> node\n    <Call l.1 at 0x7effe1db8710>\n    >>> list(node.get_children())\n    [<Name.range l.1 at 0x7effe1db86a0>, <Const.int l.1 at 0x7effe1db8518>]\n    >>> list(node.get_children())[0].as_string()\n    'range'\n    \"\"\"\n    _other_fields = (\"name\",)\n    @decorators.deprecate_default_argument_values(name=\"str\")\n    def __init__(\n        self,\n        name: str | None = None,\n        lineno: int | None = None,\n        col_offset: int | None = None,\n        parent: NodeNG | None = None,\n        *,\n        end_lineno: int | None = None,\n        end_col_offset: int | None = None,\n    ) -> None:\n        \"\"\"\n        :param name: The name that this node refers to.\n        :param lineno: The line that this node appears on in the source code.\n        :param col_offset: The column that this node appears on in the\n            source code.\n        :param parent: The parent node in the syntax tree.\n        :param end_lineno: The last line this node appears on in the source code.\n        :param end_col_offset: The end column this node appears on in the\n            source code. Note: This is after the last symbol.\n        \"\"\"\n        self.name: str | None = name\n        \"\"\"The name that this node refers to.\"\"\"\n        super().__init__(\n            lineno=lineno,\n            col_offset=col_offset,\n            end_lineno=end_lineno,\n            end_col_offset=end_col_offset,\n            parent=parent,\n        )\n    def _get_name_nodes(self):\n        yield self\n        for child_node in self.get_children():\n            yield from child_node._get_name_nodes()\nclass Arguments(mixins.AssignTypeMixin, NodeNG):\n    \"\"\"Class representing an :class:`ast.arguments` node.\n    An :class:`Arguments` node represents that arguments in a\n    function definition.\n    >>> import astroid\n    >>> node = astroid.extract_node('def foo(bar): pass')\n    >>> node\n    <FunctionDef.foo l.1 at 0x7effe1db8198>\n    >>> node.args\n    <Arguments l.1 at 0x7effe1db82e8>\n    \"\"\"\n    # Python 3.4+ uses a different approach regarding annotations,\n    # each argument is a new class, _ast.arg, which exposes an\n    # 'annotation' attribute. In astroid though, arguments are exposed\n    # as is in the Arguments node and the only way to expose annotations\n    # is by using something similar with Python 3.3:\n    #  - we expose 'varargannotation' and 'kwargannotation' of annotations\n    #    of varargs and kwargs.\n    #  - we expose 'annotation', a list with annotations for\n    #    for each normal argument. If an argument doesn't have an\n    #    annotation, its value will be None.\n    _astroid_fields = (\n        \"args\",\n        \"defaults\",\n        \"kwonlyargs\",\n        \"posonlyargs\",\n        \"posonlyargs_annotations\",\n        \"kw_defaults\",\n        \"annotations\",\n        \"varargannotation\",\n        \"kwargannotation\",\n        \"kwonlyargs_annotations\",\n        \"type_comment_args\",\n        \"type_comment_kwonlyargs\",\n        \"type_comment_posonlyargs\",\n    )\n    _other_fields = (\"vararg\", \"kwarg\")",
        "file_path": "astroid/nodes/node_classes.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 6,
        "score": 0.5503138303756714,
        "content": "# Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n# For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n# Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n\"\"\"\nInference objects are a way to represent composite AST nodes,\nwhich are used only as inference results, so they can't be found in the\noriginal AST tree. For instance, inferring the following frozenset use,\nleads to an inferred FrozenSet:\n    Call(func=Name('frozenset'), args=Tuple(...))\n\"\"\"\nfrom __future__ import annotations\nimport sys\nfrom collections.abc import Iterator\nfrom typing import TypeVar\nfrom astroid import bases, decorators, util\nfrom astroid.context import InferenceContext\nfrom astroid.exceptions import (\n    AttributeInferenceError,\n    InferenceError,\n    MroError,\n    SuperError,\n)\nfrom astroid.manager import AstroidManager\nfrom astroid.nodes import node_classes, scoped_nodes\nobjectmodel = util.lazy_import(\"interpreter.objectmodel\")\nif sys.version_info >= (3, 8):\n    from functools import cached_property\nelse:\n    from astroid.decorators import cachedproperty as cached_property\n_T = TypeVar(\"_T\")\nclass FrozenSet(node_classes.BaseContainer):\n    \"\"\"class representing a FrozenSet composite node\"\"\"\n    def pytype(self):\n        return \"builtins.frozenset\"\n    def _infer(self, context=None):\n        yield self\n    @cached_property\n    def _proxied(self):  # pylint: disable=method-hidden\n        ast_builtins = AstroidManager().builtins_module\n        return ast_builtins.getattr(\"frozenset\")[0]\nclass Super(node_classes.NodeNG):\n    \"\"\"Proxy class over a super call.\n    This class offers almost the same behaviour as Python's super,\n    which is MRO lookups for retrieving attributes from the parents.\n    The *mro_pointer* is the place in the MRO from where we should\n    start looking, not counting it. *mro_type* is the object which\n    provides the MRO, it can be both a type or an instance.\n    *self_class* is the class where the super call is, while\n    *scope* is the function where the super call is.\n    \"\"\"\n    # pylint: disable=unnecessary-lambda\n    special_attributes = util.lazy_descriptor(lambda: objectmodel.SuperModel())\n    def __init__(self, mro_pointer, mro_type, self_class, scope):\n        self.type = mro_type\n        self.mro_pointer = mro_pointer\n        self._class_based = False\n        self._self_class = self_class\n        self._scope = scope\n        super().__init__()\n    def _infer(self, context=None):\n        yield self\n    def super_mro(self):\n        \"\"\"Get the MRO which will be used to lookup attributes in this super.\"\"\"\n        if not isinstance(self.mro_pointer, scoped_nodes.ClassDef):\n            raise SuperError(\n                \"The first argument to super must be a subtype of \"\n                \"type, not {mro_pointer}.\",\n                super_=self,\n            )\n        if isinstance(self.type, scoped_nodes.ClassDef):\n            # `super(type, type)`, most likely in a class method.\n            self._class_based = True\n            mro_type = self.type\n        else:\n            mro_type = getattr(self.type, \"_proxied\", None)\n            if not isinstance(mro_type, (bases.Instance, scoped_nodes.ClassDef)):\n                raise SuperError(\n                    \"The second argument to super must be an \"\n                    \"instance or subtype of type, not {type}.\",\n                    super_=self,\n                )\n        if not mro_type.newstyle:\n            raise SuperError(\"Unable to call super on old-style classes.\", super_=self)\n        mro = mro_type.mro()\n        if self.mro_pointer not in mro:\n            raise SuperError(\n                \"The second argument to super must be an \"\n                \"instance or subtype of type, not {type}.\",\n                super_=self,\n            )\n        index = mro.index(self.mro_pointer)\n        return mro[index + 1 :]\n    @cached_property\n    def _proxied(self):\n        ast_builtins = AstroidManager().builtins_module\n        return ast_builtins.getattr(\"super\")[0]\n    def pytype(self):\n        return \"builtins.super\"\n    def display_type(self):\n        return \"Super of\"\n    @property\n    def name(self):\n        \"\"\"Get the name of the MRO pointer.\"\"\"\n        return self.mro_pointer.name\n    def qname(self):\n        return \"super\"\n    def igetattr(self, name, context=None):\n        \"\"\"Retrieve the inferred values of the given attribute name.\"\"\"\n        if name in self.special_attributes:\n            yield self.special_attributes.lookup(name)\n            return\n        try:\n            mro = self.super_mro()\n        # Don't let invalid MROs or invalid super calls\n        # leak out as is from this function.\n        except SuperError as exc:\n            raise AttributeInferenceError(\n                (\n                    \"Lookup for {name} on {target!r} because super call {super!r} \"\n                    \"is invalid.\"\n                ),\n                target=self,\n                attribute=name,\n                context=context,\n                super_=exc.super_,\n            ) from exc\n        except MroError as exc:\n            raise AttributeInferenceError(\n                (\n                    \"Lookup for {name} on {target!r} failed because {cls!r} has an \"\n                    \"invalid MRO.\"\n                ),\n                target=self,\n                attribute=name,\n                context=context,\n                mros=exc.mros,\n                cls=exc.cls,\n            ) from exc\n        found = False\n        for cls in mro:\n            if name not in cls.locals:\n                continue\n            found = True\n            for inferred in bases._infer_stmts([cls[name]], context, frame=self):\n                if not isinstance(inferred, scoped_nodes.FunctionDef):\n                    yield inferred\n                    continue\n                # We can obtain different descriptors from a super depending\n                # on what we are accessing and where the super call is.\n                if inferred.type == \"classmethod\":\n                    yield bases.BoundMethod(inferred, cls)\n                elif self._scope.type == \"classmethod\" and inferred.type == \"method\":\n                    yield inferred\n                elif self._class_based or inferred.type == \"staticmethod\":\n                    yield inferred\n                elif isinstance(inferred, Property):\n                    function = inferred.function\n                    try:\n                        yield from function.infer_call_result(\n                            caller=self, context=context\n                        )\n                    except InferenceError:\n                        yield util.Uninferable\n                elif bases._is_property(inferred):\n                    # TODO: support other descriptors as well.\n                    try:\n                        yield from inferred.infer_call_result(self, context)\n                    except InferenceError:\n                        yield util.Uninferable\n                else:\n                    yield bases.BoundMethod(inferred, cls)\n        if not found:\n            raise AttributeInferenceError(target=self, attribute=name, context=context)\n    def getattr(self, name, context=None):\n        return list(self.igetattr(name, context=context))\nclass ExceptionInstance(bases.Instance):\n    \"\"\"Class for instances of exceptions\n    It has special treatment for some of the exceptions's attributes,\n    which are transformed at runtime into certain concrete objects, such as\n    the case of .args.\n    \"\"\"\n    @cached_property\n    def special_attributes(self):\n        qname = self.qname()\n        instance = objectmodel.BUILTIN_EXCEPTIONS.get(\n            qname, objectmodel.ExceptionInstanceModel\n        )\n        return instance()(self)\nclass DictInstance(bases.Instance):\n    \"\"\"Special kind of instances for dictionaries\n    This instance knows the underlying object model of the dictionaries, which means\n    that methods such as .values or .items can be properly inferred.\n    \"\"\"\n    # pylint: disable=unnecessary-lambda\n    special_attributes = util.lazy_descriptor(lambda: objectmodel.DictModel())\n# Custom objects tailored for dictionaries, which are used to\n# disambiguate between the types of Python 2 dict's method returns\n# and Python 3 (where they return set like objects).\nclass DictItems(bases.Proxy):\n    __str__ = node_classes.NodeNG.__str__\n    __repr__ = node_classes.NodeNG.__repr__\nclass DictKeys(bases.Proxy):\n    __str__ = node_classes.NodeNG.__str__\n    __repr__ = node_classes.NodeNG.__repr__\nclass DictValues(bases.Proxy):\n    __str__ = node_classes.NodeNG.__str__\n    __repr__ = node_classes.NodeNG.__repr__\nclass PartialFunction(scoped_nodes.FunctionDef):\n    \"\"\"A class representing partial function obtained via functools.partial\"\"\"\n    @decorators.deprecate_arguments(doc=\"Use the postinit arg 'doc_node' instead\")\n    def __init__(\n        self, call, name=None, doc=None, lineno=None, col_offset=None, parent=None\n    ):\n        # TODO: Pass end_lineno and end_col_offset as well\n        super().__init__(name, lineno=lineno, col_offset=col_offset, parent=None)\n        # Assigned directly to prevent triggering the DeprecationWarning.\n        self._doc = doc\n        # A typical FunctionDef automatically adds its name to the parent scope,\n        # but a partial should not, so defer setting parent until after init\n        self.parent = parent\n        self.filled_args = call.positional_arguments[1:]\n        self.filled_keywords = call.keyword_arguments\n        wrapped_function = call.positional_arguments[0]\n        inferred_wrapped_function = next(wrapped_function.infer())\n        if isinstance(inferred_wrapped_function, PartialFunction):\n            self.filled_args = inferred_wrapped_function.filled_args + self.filled_args\n            self.filled_keywords = {\n                **inferred_wrapped_function.filled_keywords,\n                **self.filled_keywords,\n            }\n        self.filled_positionals = len(self.filled_args)\n    def infer_call_result(self, caller=None, context=None):\n        if context:\n            current_passed_keywords = {\n                keyword for (keyword, _) in context.callcontext.keywords\n            }\n            for keyword, value in self.filled_keywords.items():\n                if keyword not in current_passed_keywords:\n                    context.callcontext.keywords.append((keyword, value))\n            call_context_args = context.callcontext.args or []\n            context.callcontext.args = self.filled_args + call_context_args\n        return super().infer_call_result(caller=caller, context=context)\n    def qname(self):\n        return self.__class__.__name__\n# TODO: Hack to solve the circular import problem between node_classes and objects\n# This is not needed in 2.0, which has a cleaner design overall\nnode_classes.Dict.__bases__ = (node_classes.NodeNG, DictInstance)\nclass Property(scoped_nodes.FunctionDef):\n    \"\"\"Class representing a Python property\"\"\"\n    @decorators.deprecate_arguments(doc=\"Use the postinit arg 'doc_node' instead\")\n    def __init__(\n        self, function, name=None, doc=None, lineno=None, col_offset=None, parent=None\n    ):\n        self.function = function\n        super().__init__(name, lineno=lineno, col_offset=col_offset, parent=parent)\n        # Assigned directly to prevent triggering the DeprecationWarning.\n        self._doc = doc\n    # pylint: disable=unnecessary-lambda\n    special_attributes = util.lazy_descriptor(lambda: objectmodel.PropertyModel())\n    type = \"property\"\n    def pytype(self):\n        return \"builtins.property\"\n    def infer_call_result(self, caller=None, context=None):\n        raise InferenceError(\"Properties are not callable\")\n    def _infer(self: _T, context: InferenceContext | None = None) -> Iterator[_T]:\n        yield self",
        "file_path": "astroid/objects.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 7,
        "score": 0.5498518943786621,
        "content": "# Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n# For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n# Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n\"\"\"Astroid hooks for various builtins.\"\"\"\nfrom __future__ import annotations\nimport itertools\nfrom collections.abc import Iterator\nfrom functools import partial\nfrom astroid import arguments, helpers, inference_tip, nodes, objects, util\nfrom astroid.builder import AstroidBuilder\nfrom astroid.context import InferenceContext\nfrom astroid.exceptions import (\n    AstroidTypeError,\n    AttributeInferenceError,\n    InferenceError,\n    MroError,\n    UseInferenceDefault,\n)\nfrom astroid.manager import AstroidManager\nfrom astroid.nodes import scoped_nodes\nOBJECT_DUNDER_NEW = \"object.__new__\"\nSTR_CLASS = \"\"\"\nclass whatever(object):\n    def join(self, iterable):\n        return {rvalue}\n    def replace(self, old, new, count=None):\n        return {rvalue}\n    def format(self, *args, **kwargs):\n        return {rvalue}\n    def encode(self, encoding='ascii', errors=None):\n        return b''\n    def decode(self, encoding='ascii', errors=None):\n        return u''\n    def capitalize(self):\n        return {rvalue}\n    def title(self):\n        return {rvalue}\n    def lower(self):\n        return {rvalue}\n    def upper(self):\n        return {rvalue}\n    def swapcase(self):\n        return {rvalue}\n    def index(self, sub, start=None, end=None):\n        return 0\n    def find(self, sub, start=None, end=None):\n        return 0\n    def count(self, sub, start=None, end=None):\n        return 0\n    def strip(self, chars=None):\n        return {rvalue}\n    def lstrip(self, chars=None):\n        return {rvalue}\n    def rstrip(self, chars=None):\n        return {rvalue}\n    def rjust(self, width, fillchar=None):\n        return {rvalue}\n    def center(self, width, fillchar=None):\n        return {rvalue}\n    def ljust(self, width, fillchar=None):\n        return {rvalue}\n\"\"\"\nBYTES_CLASS = \"\"\"\nclass whatever(object):\n    def join(self, iterable):\n        return {rvalue}\n    def replace(self, old, new, count=None):\n        return {rvalue}\n    def decode(self, encoding='ascii', errors=None):\n        return u''\n    def capitalize(self):\n        return {rvalue}\n    def title(self):\n        return {rvalue}\n    def lower(self):\n        return {rvalue}\n    def upper(self):\n        return {rvalue}\n    def swapcase(self):\n        return {rvalue}\n    def index(self, sub, start=None, end=None):\n        return 0\n    def find(self, sub, start=None, end=None):\n        return 0\n    def count(self, sub, start=None, end=None):\n        return 0\n    def strip(self, chars=None):\n        return {rvalue}\n    def lstrip(self, chars=None):\n        return {rvalue}\n    def rstrip(self, chars=None):\n        return {rvalue}\n    def rjust(self, width, fillchar=None):\n        return {rvalue}\n    def center(self, width, fillchar=None):\n        return {rvalue}\n    def ljust(self, width, fillchar=None):\n        return {rvalue}\n\"\"\"\ndef _extend_string_class(class_node, code, rvalue):\n    \"\"\"function to extend builtin str/unicode class\"\"\"\n    code = code.format(rvalue=rvalue)\n    fake = AstroidBuilder(AstroidManager()).string_build(code)[\"whatever\"]\n    for method in fake.mymethods():\n        method.parent = class_node\n        method.lineno = None\n        method.col_offset = None\n        if \"__class__\" in method.locals:\n            method.locals[\"__class__\"] = [class_node]\n        class_node.locals[method.name] = [method]\n        method.parent = class_node\ndef _extend_builtins(class_transforms):\n    builtin_ast = AstroidManager().builtins_module\n    for class_name, transform in class_transforms.items():\n        transform(builtin_ast[class_name])\n_extend_builtins(\n    {\n        \"bytes\": partial(_extend_string_class, code=BYTES_CLASS, rvalue=\"b''\"),\n        \"str\": partial(_extend_string_class, code=STR_CLASS, rvalue=\"''\"),\n    }\n)\ndef _builtin_filter_predicate(node, builtin_name):\n    if (\n        builtin_name == \"type\"\n        and node.root().name == \"re\"\n        and isinstance(node.func, nodes.Name)\n        and node.func.name == \"type\"\n        and isinstance(node.parent, nodes.Assign)\n        and len(node.parent.targets) == 1\n        and isinstance(node.parent.targets[0], nodes.AssignName)\n        and node.parent.targets[0].name in {\"Pattern\", \"Match\"}\n    ):\n        # Handle re.Pattern and re.Match in brain_re\n        # Match these patterns from stdlib/re.py\n        # ```py\n        # Pattern = type(...)\n        # Match = type(...)\n        # ```\n        return False\n    if isinstance(node.func, nodes.Name) and node.func.name == builtin_name:\n        return True\n    if isinstance(node.func, nodes.Attribute):\n        return (\n            node.func.attrname == \"fromkeys\"\n            and isinstance(node.func.expr, nodes.Name)\n            and node.func.expr.name == \"dict\"\n        )\n    return False\ndef register_builtin_transform(transform, builtin_name):\n    \"\"\"Register a new transform function for the given *builtin_name*.\n    The transform function must accept two parameters, a node and\n    an optional context.\n    \"\"\"\n    def _transform_wrapper(node, context=None):\n        result = transform(node, context=context)\n        if result:\n            if not result.parent:\n                # Let the transformation function determine\n                # the parent for its result. Otherwise,\n                # we set it to be the node we transformed from.\n                result.parent = node\n            if result.lineno is None:\n                result.lineno = node.lineno\n            # Can be a 'Module' see https://github.com/PyCQA/pylint/issues/4671\n            # We don't have a regression test on this one: tread carefully\n            if hasattr(result, \"col_offset\") and result.col_offset is None:\n                result.col_offset = node.col_offset\n        return iter([result])\n    AstroidManager().register_transform(\n        nodes.Call,\n        inference_tip(_transform_wrapper),\n        partial(_builtin_filter_predicate, builtin_name=builtin_name),\n    )\ndef _container_generic_inference(node, context, node_type, transform):\n    args = node.args\n    if not args:\n        return node_type()\n    if len(node.args) > 1:\n        raise UseInferenceDefault()\n    (arg,) = args\n    transformed = transform(arg)\n    if not transformed:\n        try:\n            inferred = next(arg.infer(context=context))\n        except (InferenceError, StopIteration) as exc:\n            raise UseInferenceDefault from exc\n        if inferred is util.Uninferable:\n            raise UseInferenceDefault\n        transformed = transform(inferred)\n    if not transformed or transformed is util.Uninferable:\n        raise UseInferenceDefault\n    return transformed\ndef _container_generic_transform(  # pylint: disable=inconsistent-return-statements\n    arg, context, klass, iterables, build_elts\n):\n    if isinstance(arg, klass):\n        return arg\n    if isinstance(arg, iterables):\n        if all(isinstance(elt, nodes.Const) for elt in arg.elts):\n            elts = [elt.value for elt in arg.elts]\n        else:\n            # TODO: Does not handle deduplication for sets.\n            elts = []\n            for element in arg.elts:\n                if not element:\n                    continue\n                inferred = helpers.safe_infer(element, context=context)\n                if inferred:\n                    evaluated_object = nodes.EvaluatedObject(\n                        original=element, value=inferred\n                    )\n                    elts.append(evaluated_object)\n    elif isinstance(arg, nodes.Dict):\n        # Dicts need to have consts as strings already.\n        if not all(isinstance(elt[0], nodes.Const) for elt in arg.items):\n            raise UseInferenceDefault()\n        elts = [item[0].value for item in arg.items]\n    elif isinstance(arg, nodes.Const) and isinstance(arg.value, (str, bytes)):\n        elts = arg.value\n    else:\n        return\n    return klass.from_elements(elts=build_elts(elts))\ndef _infer_builtin_container(\n    node, context, klass=None, iterables=None, build_elts=None\n):\n    transform_func = partial(\n        _container_generic_transform,\n        context=context,\n        klass=klass,\n        iterables=iterables,\n        build_elts=build_elts,\n    )\n    return _container_generic_inference(node, context, klass, transform_func)\n# pylint: disable=invalid-name\ninfer_tuple = partial(\n    _infer_builtin_container,\n    klass=nodes.Tuple,\n    iterables=(\n        nodes.List,\n        nodes.Set,\n        objects.FrozenSet,\n        objects.DictItems,\n        objects.DictKeys,\n        objects.DictValues,\n    ),\n    build_elts=tuple,\n)\ninfer_list = partial(\n    _infer_builtin_container,\n    klass=nodes.List,\n    iterables=(\n        nodes.Tuple,\n        nodes.Set,\n        objects.FrozenSet,\n        objects.DictItems,\n        objects.DictKeys,\n        objects.DictValues,\n    ),\n    build_elts=list,\n)\ninfer_set = partial(\n    _infer_builtin_container,\n    klass=nodes.Set,\n    iterables=(nodes.List, nodes.Tuple, objects.FrozenSet, objects.DictKeys),\n    build_elts=set,\n)\ninfer_frozenset = partial(\n    _infer_builtin_container,\n    klass=objects.FrozenSet,\n    iterables=(nodes.List, nodes.Tuple, nodes.Set, objects.FrozenSet, objects.DictKeys),\n    build_elts=frozenset,\n)\ndef _get_elts(arg, context):\n    def is_iterable(n):\n        return isinstance(n, (nodes.List, nodes.Tuple, nodes.Set))\n    try:\n        inferred = next(arg.infer(context))\n    except (InferenceError, StopIteration) as exc:\n        raise UseInferenceDefault from exc\n    if isinstance(inferred, nodes.Dict):\n        items = inferred.items\n    elif is_iterable(inferred):\n        items = []\n        for elt in inferred.elts:\n            # If an item is not a pair of two items,\n            # then fallback to the default inference.\n            # Also, take in consideration only hashable items,\n            # tuples and consts. We are choosing Names as well.\n            if not is_iterable(elt):\n                raise UseInferenceDefault()\n            if len(elt.elts) != 2:\n                raise UseInferenceDefault()\n            if not isinstance(elt.elts[0], (nodes.Tuple, nodes.Const, nodes.Name)):\n                raise UseInferenceDefault()\n            items.append(tuple(elt.elts))\n    else:\n        raise UseInferenceDefault()\n    return items\ndef infer_dict(node, context=None):\n    \"\"\"Try to infer a dict call to a Dict node.\n    The function treats the following cases:\n        * dict()\n        * dict(mapping)\n        * dict(iterable)\n        * dict(iterable, **kwargs)\n        * dict(mapping, **kwargs)\n        * dict(**kwargs)\n    If a case can't be inferred, we'll fallback to default inference.\n    \"\"\"\n    call = arguments.CallSite.from_call(node, context=context)\n    if call.has_invalid_arguments() or call.has_invalid_keywords():\n        raise UseInferenceDefault\n    args = call.positional_arguments\n    kwargs = list(call.keyword_arguments.items())\n    if not args and not kwargs:\n        # dict()\n        return nodes.Dict()\n    if kwargs and not args:\n        # dict(a=1, b=2, c=4)\n        items = [(nodes.Const(key), value) for key, value in kwargs]\n    elif len(args) == 1 and kwargs:\n        # dict(some_iterable, b=2, c=4)\n        elts = _get_elts(args[0], context)\n        keys = [(nodes.Const(key), value) for key, value in kwargs]\n        items = elts + keys\n    elif len(args) == 1:\n        items = _get_elts(args[0], context)\n    else:\n        raise UseInferenceDefault()\n    value = nodes.Dict(\n        col_offset=node.col_offset, lineno=node.lineno, parent=node.parent\n    )\n    value.postinit(items)\n    return value\ndef infer_super(node, context=None):\n    \"\"\"Understand super calls.\n    There are some restrictions for what can be understood:\n        * unbounded super (one argument form) is not understood.\n        * if the super call is not inside a function (classmethod or method),\n          then the default inference will be used.\n        * if the super arguments can't be inferred, the default inference\n          will be used.\n    \"\"\"\n    if len(node.args) == 1:\n        # Ignore unbounded super.\n        raise UseInferenceDefault\n    scope = node.scope()\n    if not isinstance(scope, nodes.FunctionDef):\n        # Ignore non-method uses of super.\n        raise UseInferenceDefault\n    if scope.type not in (\"classmethod\", \"method\"):\n        # Not interested in staticmethods.\n        raise UseInferenceDefault\n    cls = scoped_nodes.get_wrapping_class(scope)\n    if not node.args:\n        mro_pointer = cls\n        # In we are in a classmethod, the interpreter will fill\n        # automatically the class as the second argument, not an instance.\n        if scope.type == \"classmethod\":\n            mro_type = cls\n        else:\n            mro_type = cls.instantiate_class()\n    else:\n        try:\n            mro_pointer = next(node.args[0].infer(context=context))\n        except (InferenceError, StopIteration) as exc:\n            raise UseInferenceDefault from exc\n        try:\n            mro_type = next(node.args[1].infer(context=context))\n        except (InferenceError, StopIteration) as exc:\n            raise UseInferenceDefault from exc\n    if mro_pointer is util.Uninferable or mro_type is util.Uninferable:\n        # No way we could understand this.\n        raise UseInferenceDefault\n    super_obj = objects.Super(\n        mro_pointer=mro_pointer, mro_type=mro_type, self_class=cls, scope=scope\n    )\n    super_obj.parent = node\n    return super_obj\ndef _infer_getattr_args(node, context):\n    if len(node.args) not in (2, 3):\n        # Not a valid getattr call.\n        raise UseInferenceDefault\n    try:\n        obj = next(node.args[0].infer(context=context))\n        attr = next(node.args[1].infer(context=context))\n    except (InferenceError, StopIteration) as exc:\n        raise UseInferenceDefault from exc\n    if obj is util.Uninferable or attr is util.Uninferable:\n        # If one of the arguments is something we can't infer,\n        # then also make the result of the getattr call something\n        # which is unknown.\n        return util.Uninferable, util.Uninferable\n    is_string = isinstance(attr, nodes.Const) and isinstance(attr.value, str)\n    if not is_string:\n        raise UseInferenceDefault\n    return obj, attr.value\ndef infer_getattr(node, context=None):\n    \"\"\"Understand getattr calls\n    If one of the arguments is an Uninferable object, then the\n    result will be an Uninferable object. Otherwise, the normal attribute\n    lookup will be done.\n    \"\"\"\n    obj, attr = _infer_getattr_args(node, context)\n    if (\n        obj is util.Uninferable\n        or attr is util.Uninferable\n        or not hasattr(obj, \"igetattr\")\n    ):\n        return util.Uninferable\n    try:\n        return next(obj.igetattr(attr, context=context))\n    except (StopIteration, InferenceError, AttributeInferenceError):\n        if len(node.args) == 3:\n            # Try to infer the default and return it instead.\n            try:\n                return next(node.args[2].infer(context=context))\n            except (StopIteration, InferenceError) as exc:\n                raise UseInferenceDefault from exc\n    raise UseInferenceDefault\ndef infer_hasattr(node, context=None):\n    \"\"\"Understand hasattr calls\n    This always guarantees three possible outcomes for calling\n    hasattr: Const(False) when we are sure that the object\n    doesn't have the intended attribute, Const(True) when\n    we know that the object has the attribute and Uninferable\n    when we are unsure of the outcome of the function call.\n    \"\"\"\n    try:\n        obj, attr = _infer_getattr_args(node, context)\n        if (\n            obj is util.Uninferable\n            or attr is util.Uninferable\n            or not hasattr(obj, \"getattr\")\n        ):\n            return util.Uninferable\n        obj.getattr(attr, context=context)\n    except UseInferenceDefault:\n        # Can't infer something from this function call.\n        return util.Uninferable\n    except AttributeInferenceError:\n        # Doesn't have it.\n        return nodes.Const(False)\n    return nodes.Const(True)\ndef infer_callable(node, context=None):\n    \"\"\"Understand callable calls\n    This follows Python's semantics, where an object\n    is callable if it provides an attribute __call__,\n    even though that attribute is something which can't be\n    called.\n    \"\"\"\n    if len(node.args) != 1:\n        # Invalid callable call.\n        raise UseInferenceDefault\n    argument = node.args[0]\n    try:\n        inferred = next(argument.infer(context=context))\n    except (InferenceError, StopIteration):\n        return util.Uninferable\n    if inferred is util.Uninferable:\n        return util.Uninferable\n    return nodes.Const(inferred.callable())\ndef infer_property(\n    node: nodes.Call, context: InferenceContext | None = None\n) -> objects.Property:\n    \"\"\"Understand `property` class\n    This only infers the output of `property`\n    call, not the arguments themselves.\n    \"\"\"\n    if len(node.args) < 1:\n        # Invalid property call.\n        raise UseInferenceDefault\n    getter = node.args[0]\n    try:\n        inferred = next(getter.infer(context=context))\n    except (InferenceError, StopIteration) as exc:\n        raise UseInferenceDefault from exc\n    if not isinstance(inferred, (nodes.FunctionDef, nodes.Lambda)):\n        raise UseInferenceDefault\n    prop_func = objects.Property(\n        function=inferred,\n        name=inferred.name,\n        lineno=node.lineno,\n        parent=node,\n        col_offset=node.col_offset,\n    )\n    prop_func.postinit(\n        body=[],\n        args=inferred.args,\n        doc_node=getattr(inferred, \"doc_node\", None),\n    )\n    return prop_func\ndef infer_bool(node, context=None):\n    \"\"\"Understand bool calls.\"\"\"\n    if len(node.args) > 1:\n        # Invalid bool call.\n        raise UseInferenceDefault\n    if not node.args:\n        return nodes.Const(False)\n    argument = node.args[0]",
        "file_path": "astroid/brain/brain_builtin_inference.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 8,
        "score": 0.5424837470054626,
        "content": "# Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n# For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n# Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\nimport importlib\nimport warnings\nimport lazy_object_proxy\ndef lazy_descriptor(obj):\n    class DescriptorProxy(lazy_object_proxy.Proxy):\n        def __get__(self, instance, owner=None):\n            return self.__class__.__get__(self, instance)\n    return DescriptorProxy(obj)\ndef lazy_import(module_name):\n    return lazy_object_proxy.Proxy(\n        lambda: importlib.import_module(\".\" + module_name, \"astroid\")\n    )\n@object.__new__\nclass Uninferable:\n    \"\"\"Special inference object, which is returned when inference fails.\"\"\"\n    def __repr__(self):\n        return \"Uninferable\"\n    __str__ = __repr__\n    def __getattribute__(self, name):\n        if name == \"next\":\n            raise AttributeError(\"next method should not be called\")\n        if name.startswith(\"__\") and name.endswith(\"__\"):\n            return object.__getattribute__(self, name)\n        if name == \"accept\":\n            return object.__getattribute__(self, name)\n        return self\n    def __call__(self, *args, **kwargs):\n        return self\n    def __bool__(self):\n        return False\n    __nonzero__ = __bool__\n    def accept(self, visitor):\n        return visitor.visit_uninferable(self)\nclass BadOperationMessage:\n    \"\"\"Object which describes a TypeError occurred somewhere in the inference chain\n    This is not an exception, but a container object which holds the types and\n    the error which occurred.\n    \"\"\"\nclass BadUnaryOperationMessage(BadOperationMessage):\n    \"\"\"Object which describes operational failures on UnaryOps.\"\"\"\n    def __init__(self, operand, op, error):\n        self.operand = operand\n        self.op = op\n        self.error = error\n    @property\n    def _object_type_helper(self):\n        helpers = lazy_import(\"helpers\")\n        return helpers.object_type\n    def _object_type(self, obj):\n        objtype = self._object_type_helper(obj)\n        if objtype is Uninferable:\n            return None\n        return objtype\n    def __str__(self):\n        if hasattr(self.operand, \"name\"):\n            operand_type = self.operand.name\n        else:\n            object_type = self._object_type(self.operand)\n            if hasattr(object_type, \"name\"):\n                operand_type = object_type.name\n            else:\n                # Just fallback to as_string\n                operand_type = object_type.as_string()\n        msg = \"bad operand type for unary {}: {}\"\n        return msg.format(self.op, operand_type)\nclass BadBinaryOperationMessage(BadOperationMessage):\n    \"\"\"Object which describes type errors for BinOps.\"\"\"\n    def __init__(self, left_type, op, right_type):\n        self.left_type = left_type\n        self.right_type = right_type\n        self.op = op\n    def __str__(self):\n        msg = \"unsupported operand type(s) for {}: {!r} and {!r}\"\n        return msg.format(self.op, self.left_type.name, self.right_type.name)\ndef _instancecheck(cls, other):\n    wrapped = cls.__wrapped__\n    other_cls = other.__class__\n    is_instance_of = wrapped is other_cls or issubclass(other_cls, wrapped)\n    warnings.warn(\n        \"%r is deprecated and slated for removal in astroid \"\n        \"2.0, use %r instead\" % (cls.__class__.__name__, wrapped.__name__),\n        PendingDeprecationWarning,\n        stacklevel=2,\n    )\n    return is_instance_of\ndef proxy_alias(alias_name, node_type):\n    \"\"\"Get a Proxy from the given name to the given node type.\"\"\"\n    proxy = type(\n        alias_name,\n        (lazy_object_proxy.Proxy,),\n        {\n            \"__class__\": object.__dict__[\"__class__\"],\n            \"__instancecheck__\": _instancecheck,\n        },\n    )\n    return proxy(lambda: node_type)\ndef check_warnings_filter() -> bool:\n    \"\"\"Return True if any other than the default DeprecationWarning filter is enabled.\n    https://docs.python.org/3/library/warnings.html#default-warning-filter\n    \"\"\"\n    return any(\n        issubclass(DeprecationWarning, filter[2])\n        and filter[0] != \"ignore\"\n        and filter[3] != \"__main__\"\n        for filter in warnings.filters\n    )",
        "file_path": "astroid/util.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 9,
        "score": 0.5416665077209473,
        "content": "                    yield util.Uninferable\n            else:\n                if not isinstance(operand, (bases.Instance, nodes.ClassDef)):\n                    # The operation was used on something which\n                    # doesn't support it.\n                    yield util.BadUnaryOperationMessage(operand, self.op, exc)\n                    continue\n                try:\n                    try:\n                        methods = dunder_lookup.lookup(operand, meth)\n                    except AttributeInferenceError:\n                        yield util.BadUnaryOperationMessage(operand, self.op, exc)\n                        continue\n                    meth = methods[0]\n                    inferred = next(meth.infer(context=context), None)\n                    if inferred is util.Uninferable or not inferred.callable():\n                        continue\n                    context = copy_context(context)\n                    context.boundnode = operand\n                    context.callcontext = CallContext(args=[], callee=inferred)\n                    call_results = inferred.infer_call_result(self, context=context)\n                    result = next(call_results, None)\n                    if result is None:\n                        # Failed to infer, return the same type.\n                        yield operand\n                    else:\n                        yield result\n                except AttributeInferenceError as inner_exc:\n                    # The unary operation special method was not found.\n                    yield util.BadUnaryOperationMessage(operand, self.op, inner_exc)\n                except InferenceError:\n                    yield util.Uninferable\n@decorators.raise_if_nothing_inferred\n@decorators.path_wrapper\ndef infer_unaryop(self, context=None):\n    \"\"\"Infer what an UnaryOp should return when evaluated.\"\"\"\n    yield from _filter_operation_errors(\n        self, _infer_unaryop, context, util.BadUnaryOperationMessage\n    )\n    return dict(node=self, context=context)\nnodes.UnaryOp._infer_unaryop = _infer_unaryop\nnodes.UnaryOp._infer = infer_unaryop\ndef _is_not_implemented(const):\n    \"\"\"Check if the given const node is NotImplemented.\"\"\"\n    return isinstance(const, nodes.Const) and const.value is NotImplemented\ndef _invoke_binop_inference(instance, opnode, op, other, context, method_name):\n    \"\"\"Invoke binary operation inference on the given instance.\"\"\"\n    methods = dunder_lookup.lookup(instance, method_name)\n    context = bind_context_to_node(context, instance)\n    method = methods[0]\n    context.callcontext.callee = method\n    try:\n        inferred = next(method.infer(context=context))\n    except StopIteration as e:\n        raise InferenceError(node=method, context=context) from e\n    if inferred is util.Uninferable:\n        raise InferenceError\n    return instance.infer_binary_op(opnode, op, other, context, inferred)\ndef _aug_op(instance, opnode, op, other, context, reverse=False):\n    \"\"\"Get an inference callable for an augmented binary operation.\"\"\"\n    method_name = protocols.AUGMENTED_OP_METHOD[op]\n    return functools.partial(\n        _invoke_binop_inference,\n        instance=instance,\n        op=op,\n        opnode=opnode,\n        other=other,\n        context=context,\n        method_name=method_name,\n    )\ndef _bin_op(instance, opnode, op, other, context, reverse=False):\n    \"\"\"Get an inference callable for a normal binary operation.\n    If *reverse* is True, then the reflected method will be used instead.\n    \"\"\"\n    if reverse:\n        method_name = protocols.REFLECTED_BIN_OP_METHOD[op]\n    else:\n        method_name = protocols.BIN_OP_METHOD[op]\n    return functools.partial(\n        _invoke_binop_inference,\n        instance=instance,\n        op=op,\n        opnode=opnode,\n        other=other,\n        context=context,\n        method_name=method_name,\n    )\ndef _get_binop_contexts(context, left, right):\n    \"\"\"Get contexts for binary operations.\n    This will return two inference contexts, the first one\n    for x.__op__(y), the other one for y.__rop__(x), where\n    only the arguments are inversed.\n    \"\"\"\n    # The order is important, since the first one should be\n    # left.__op__(right).\n    for arg in (right, left):\n        new_context = context.clone()\n        new_context.callcontext = CallContext(args=[arg])\n        new_context.boundnode = None\n        yield new_context\ndef _same_type(type1, type2):\n    \"\"\"Check if type1 is the same as type2.\"\"\"\n    return type1.qname() == type2.qname()\ndef _get_binop_flow(\n    left, left_type, binary_opnode, right, right_type, context, reverse_context\n):\n    \"\"\"Get the flow for binary operations.\n    The rules are a bit messy:\n        * if left and right have the same type, then only one\n          method will be called, left.__op__(right)\n        * if left and right are unrelated typewise, then first\n          left.__op__(right) is tried and if this does not exist\n          or returns NotImplemented, then right.__rop__(left) is tried.\n        * if left is a subtype of right, then only left.__op__(right)\n          is tried.\n        * if left is a supertype of right, then right.__rop__(left)\n          is first tried and then left.__op__(right)\n    \"\"\"\n    op = binary_opnode.op\n    if _same_type(left_type, right_type):\n        methods = [_bin_op(left, binary_opnode, op, right, context)]\n    elif helpers.is_subtype(left_type, right_type):\n        methods = [_bin_op(left, binary_opnode, op, right, context)]\n    elif helpers.is_supertype(left_type, right_type):\n        methods = [\n            _bin_op(right, binary_opnode, op, left, reverse_context, reverse=True),\n            _bin_op(left, binary_opnode, op, right, context),\n        ]\n    else:\n        methods = [\n            _bin_op(left, binary_opnode, op, right, context),\n            _bin_op(right, binary_opnode, op, left, reverse_context, reverse=True),\n        ]\n    return methods\ndef _get_aug_flow(\n    left, left_type, aug_opnode, right, right_type, context, reverse_context\n):\n    \"\"\"Get the flow for augmented binary operations.\n    The rules are a bit messy:\n        * if left and right have the same type, then left.__augop__(right)\n          is first tried and then left.__op__(right).\n        * if left and right are unrelated typewise, then\n          left.__augop__(right) is tried, then left.__op__(right)\n          is tried and then right.__rop__(left) is tried.\n        * if left is a subtype of right, then left.__augop__(right)\n          is tried and then left.__op__(right).\n        * if left is a supertype of right, then left.__augop__(right)\n          is tried, then right.__rop__(left) and then\n          left.__op__(right)\n    \"\"\"\n    bin_op = aug_opnode.op.strip(\"=\")\n    aug_op = aug_opnode.op\n    if _same_type(left_type, right_type):\n        methods = [\n            _aug_op(left, aug_opnode, aug_op, right, context),\n            _bin_op(left, aug_opnode, bin_op, right, context),\n        ]\n    elif helpers.is_subtype(left_type, right_type):\n        methods = [\n            _aug_op(left, aug_opnode, aug_op, right, context),\n            _bin_op(left, aug_opnode, bin_op, right, context),\n        ]\n    elif helpers.is_supertype(left_type, right_type):\n        methods = [\n            _aug_op(left, aug_opnode, aug_op, right, context),\n            _bin_op(right, aug_opnode, bin_op, left, reverse_context, reverse=True),\n            _bin_op(left, aug_opnode, bin_op, right, context),\n        ]\n    else:\n        methods = [\n            _aug_op(left, aug_opnode, aug_op, right, context),\n            _bin_op(left, aug_opnode, bin_op, right, context),\n            _bin_op(right, aug_opnode, bin_op, left, reverse_context, reverse=True),\n        ]\n    return methods\ndef _infer_binary_operation(left, right, binary_opnode, context, flow_factory):\n    \"\"\"Infer a binary operation between a left operand and a right operand\n    This is used by both normal binary operations and augmented binary\n    operations, the only difference is the flow factory used.\n    \"\"\"\n    context, reverse_context = _get_binop_contexts(context, left, right)\n    left_type = helpers.object_type(left)\n    right_type = helpers.object_type(right)\n    methods = flow_factory(\n        left, left_type, binary_opnode, right, right_type, context, reverse_context\n    )\n    for method in methods:\n        try:\n            results = list(method())\n        except AttributeError:\n            continue\n        except AttributeInferenceError:\n            continue\n        except InferenceError:\n            yield util.Uninferable\n            return\n        else:\n            if any(result is util.Uninferable for result in results):\n                yield util.Uninferable\n                return\n            if all(map(_is_not_implemented, results)):\n                continue\n            not_implemented = sum(\n                1 for result in results if _is_not_implemented(result)\n            )\n            if not_implemented and not_implemented != len(results):\n                # Can't infer yet what this is.\n                yield util.Uninferable\n                return\n            yield from results\n            return\n    # The operation doesn't seem to be supported so let the caller know about it\n    yield util.BadBinaryOperationMessage(left_type, binary_opnode.op, right_type)\ndef _infer_binop(self, context):\n    \"\"\"Binary operation inference logic.\"\"\"\n    left = self.left\n    right = self.right\n    # we use two separate contexts for evaluating lhs and rhs because\n    # 1. evaluating lhs may leave some undesired entries in context.path\n    #    which may not let us infer right value of rhs\n    context = context or InferenceContext()\n    lhs_context = copy_context(context)\n    rhs_context = copy_context(context)\n    lhs_iter = left.infer(context=lhs_context)\n    rhs_iter = right.infer(context=rhs_context)\n    for lhs, rhs in itertools.product(lhs_iter, rhs_iter):\n        if any(value is util.Uninferable for value in (rhs, lhs)):\n            # Don't know how to process this.\n            yield util.Uninferable\n            return\n        try:\n            yield from _infer_binary_operation(lhs, rhs, self, context, _get_binop_flow)\n        except _NonDeducibleTypeHierarchy:\n            yield util.Uninferable\n@decorators.yes_if_nothing_inferred\n@decorators.path_wrapper\ndef infer_binop(self, context=None):\n    return _filter_operation_errors(\n        self, _infer_binop, context, util.BadBinaryOperationMessage\n    )\nnodes.BinOp._infer_binop = _infer_binop\nnodes.BinOp._infer = infer_binop\nCOMPARE_OPS: dict[str, Callable[[Any, Any], bool]] = {\n    \"==\": operator.eq,\n    \"!=\": operator.ne,\n    \"<\": operator.lt,\n    \"<=\": operator.le,\n    \">\": operator.gt,\n    \">=\": operator.ge,\n    \"in\": lambda a, b: a in b,\n    \"not in\": lambda a, b: a not in b,\n}\nUNINFERABLE_OPS = {\n    \"is\",\n    \"is not\",\n}\ndef _to_literal(node: nodes.NodeNG) -> Any:\n    # Can raise SyntaxError or ValueError from ast.literal_eval\n    # Can raise AttributeError from node.as_string() as not all nodes have a visitor\n    # Is this the stupidest idea or the simplest idea?\n    return ast.literal_eval(node.as_string())\ndef _do_compare(\n    left_iter: Iterable[nodes.NodeNG], op: str, right_iter: Iterable[nodes.NodeNG]\n) -> bool | type[util.Uninferable]:\n    \"\"\"\n    If all possible combinations are either True or False, return that:\n    >>> _do_compare([1, 2], '<=', [3, 4])\n    True\n    >>> _do_compare([1, 2], '==', [3, 4])\n    False\n    If any item is uninferable, or if some combinations are True and some\n    are False, return Uninferable:\n    >>> _do_compare([1, 3], '<=', [2, 4])\n    util.Uninferable\n    \"\"\"\n    retval: bool | None = None\n    if op in UNINFERABLE_OPS:\n        return util.Uninferable\n    op_func = COMPARE_OPS[op]\n    for left, right in itertools.product(left_iter, right_iter):\n        if left is util.Uninferable or right is util.Uninferable:\n            return util.Uninferable\n        try:\n            left, right = _to_literal(left), _to_literal(right)\n        except (SyntaxError, ValueError, AttributeError):\n            return util.Uninferable\n        try:\n            expr = op_func(left, right)\n        except TypeError as exc:\n            raise AstroidTypeError from exc\n        if retval is None:\n            retval = expr\n        elif retval != expr:\n            return util.Uninferable\n            # (or both, but \"True | False\" is basically the same)\n    assert retval is not None\n    return retval  # it was all the same value\ndef _infer_compare(\n    self: nodes.Compare, context: InferenceContext | None = None\n) -> Iterator[nodes.Const | type[util.Uninferable]]:\n    \"\"\"Chained comparison inference logic.\"\"\"\n    retval: bool | type[util.Uninferable] = True\n    ops = self.ops\n    left_node = self.left\n    lhs = list(left_node.infer(context=context))\n    # should we break early if first element is uninferable?\n    for op, right_node in ops:\n        # eagerly evaluate rhs so that values can be re-used as lhs\n        rhs = list(right_node.infer(context=context))\n        try:\n            retval = _do_compare(lhs, op, rhs)\n        except AstroidTypeError:\n            retval = util.Uninferable\n            break\n        if retval is not True:\n            break  # short-circuit\n        lhs = rhs  # continue\n    if retval is util.Uninferable:\n        yield retval  # type: ignore[misc]\n    else:\n        yield nodes.Const(retval)\nnodes.Compare._infer = _infer_compare  # type: ignore[assignment]\ndef _infer_augassign(self, context=None):\n    \"\"\"Inference logic for augmented binary operations.\"\"\"\n    if context is None:\n        context = InferenceContext()\n    rhs_context = context.clone()\n    lhs_iter = self.target.infer_lhs(context=context)\n    rhs_iter = self.value.infer(context=rhs_context)\n    for lhs, rhs in itertools.product(lhs_iter, rhs_iter):\n        if any(value is util.Uninferable for value in (rhs, lhs)):\n            # Don't know how to process this.\n            yield util.Uninferable\n            return\n        try:\n            yield from _infer_binary_operation(\n                left=lhs,\n                right=rhs,\n                binary_opnode=self,\n                context=context,\n                flow_factory=_get_aug_flow,\n            )\n        except _NonDeducibleTypeHierarchy:\n            yield util.Uninferable\n@decorators.raise_if_nothing_inferred\n@decorators.path_wrapper\ndef infer_augassign(self, context=None):\n    return _filter_operation_errors(\n        self, _infer_augassign, context, util.BadBinaryOperationMessage\n    )\nnodes.AugAssign._infer_augassign = _infer_augassign\nnodes.AugAssign._infer = infer_augassign\n# End of binary operation inference.\n@decorators.raise_if_nothing_inferred\ndef infer_arguments(self, context=None):\n    name = context.lookupname\n    if name is None:\n        raise InferenceError(node=self, context=context)\n    return protocols._arguments_infer_argname(self, name, context)\nnodes.Arguments._infer = infer_arguments  # type: ignore[assignment]\n@decorators.raise_if_nothing_inferred\n@decorators.path_wrapper\ndef infer_assign(self, context=None):\n    \"\"\"infer a AssignName/AssignAttr: need to inspect the RHS part of the\n    assign node\n    \"\"\"\n    if isinstance(self.parent, nodes.AugAssign):\n        return self.parent.infer(context)\n    stmts = list(self.assigned_stmts(context=context))\n    return bases._infer_stmts(stmts, context)\nnodes.AssignName._infer = infer_assign\nnodes.AssignAttr._infer = infer_assign\n@decorators.raise_if_nothing_inferred\n@decorators.path_wrapper\ndef infer_empty_node(self, context=None):\n    if not self.has_underlying_object():\n        yield util.Uninferable\n    else:\n        try:\n            yield from AstroidManager().infer_ast_from_something(\n                self.object, context=context\n            )\n        except AstroidError:\n            yield util.Uninferable\nnodes.EmptyNode._infer = infer_empty_node  # type: ignore[assignment]\n@decorators.raise_if_nothing_inferred\ndef infer_index(self, context=None):\n    return self.value.infer(context)\nnodes.Index._infer = infer_index  # type: ignore[assignment]\ndef _populate_context_lookup(call, context):\n    # Allows context to be saved for later\n    # for inference inside a function\n    context_lookup = {}\n    if context is None:\n        return context_lookup\n    for arg in call.args:\n        if isinstance(arg, nodes.Starred):\n            context_lookup[arg.value] = context\n        else:\n            context_lookup[arg] = context\n    keywords = call.keywords if call.keywords is not None else []\n    for keyword in keywords:\n        context_lookup[keyword.value] = context\n    return context_lookup\n@decorators.raise_if_nothing_inferred\ndef infer_ifexp(self, context=None):\n    \"\"\"Support IfExp inference\n    If we can't infer the truthiness of the condition, we default\n    to inferring both branches. Otherwise, we infer either branch\n    depending on the condition.\n    \"\"\"\n    both_branches = False\n    # We use two separate contexts for evaluating lhs and rhs because\n    # evaluating lhs may leave some undesired entries in context.path\n    # which may not let us infer right value of rhs.\n    context = context or InferenceContext()\n    lhs_context = copy_context(context)\n    rhs_context = copy_context(context)\n    try:\n        test = next(self.test.infer(context=context.clone()))\n    except (InferenceError, StopIteration):\n        both_branches = True\n    else:\n        if test is not util.Uninferable:\n            if test.bool_value():\n                yield from self.body.infer(context=lhs_context)\n            else:\n                yield from self.orelse.infer(context=rhs_context)\n        else:\n            both_branches = True\n    if both_branches:\n        yield from self.body.infer(context=lhs_context)\n        yield from self.orelse.infer(context=rhs_context)\nnodes.IfExp._infer = infer_ifexp  # type: ignore[assignment]\ndef infer_functiondef(\n    self: _FunctionDefT, context: InferenceContext | None = None\n) -> Generator[Property | _FunctionDefT, None, InferenceErrorInfo]:\n    if not self.decorators or not bases._is_property(self):\n        yield self\n        return InferenceErrorInfo(node=self, context=context)\n    # When inferring a property, we instantiate a new `objects.Property` object,\n    # which in turn, because it inherits from `FunctionDef`, sets itself in the locals\n    # of the wrapping frame. This means that every time we infer a property, the locals\n    # are mutated with a new instance of the property. To avoid this, we detect this\n    # scenario and avoid passing the `parent` argument to the constructor.\n    parent_frame = self.parent.frame(future=True)\n    property_already_in_parent_locals = self.name in parent_frame.locals and any(\n        isinstance(val, objects.Property) for val in parent_frame.locals[self.name]\n    )\n    prop_func = objects.Property(\n        function=self,\n        name=self.name,\n        lineno=self.lineno,\n        parent=self.parent if not property_already_in_parent_locals else None,\n        col_offset=self.col_offset,\n    )\n    if property_already_in_parent_locals:\n        prop_func.parent = self.parent\n    prop_func.postinit(body=[], args=self.args, doc_node=self.doc_node)\n    yield prop_func\n    return InferenceErrorInfo(node=self, context=context)\nnodes.FunctionDef._infer = infer_functiondef  # type: ignore[assignment]",
        "file_path": "astroid/inference.py",
        "chunk_index": 1,
        "metadata": {}
      },
      {
        "rank": 10,
        "score": 0.5358454585075378,
        "content": "# Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n# For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n# Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n\"\"\"this module contains a set of functions to handle python protocols for nodes\nwhere it makes sense.\n\"\"\"\nfrom __future__ import annotations\nimport collections\nimport itertools\nimport operator as operator_mod\nfrom collections.abc import Generator\nfrom typing import Any\nfrom astroid import arguments, bases, decorators, helpers, nodes, util\nfrom astroid.const import Context\nfrom astroid.context import InferenceContext, copy_context\nfrom astroid.exceptions import (\n    AstroidIndexError,\n    AstroidTypeError,\n    AttributeInferenceError,\n    InferenceError,\n    NoDefault,\n)\nfrom astroid.nodes import node_classes\nraw_building = util.lazy_import(\"raw_building\")\nobjects = util.lazy_import(\"objects\")\ndef _reflected_name(name):\n    return \"__r\" + name[2:]\ndef _augmented_name(name):\n    return \"__i\" + name[2:]\n_CONTEXTLIB_MGR = \"contextlib.contextmanager\"\nBIN_OP_METHOD = {\n    \"+\": \"__add__\",\n    \"-\": \"__sub__\",\n    \"/\": \"__truediv__\",\n    \"//\": \"__floordiv__\",\n    \"*\": \"__mul__\",\n    \"**\": \"__pow__\",\n    \"%\": \"__mod__\",\n    \"&\": \"__and__\",\n    \"|\": \"__or__\",\n    \"^\": \"__xor__\",\n    \"<<\": \"__lshift__\",\n    \">>\": \"__rshift__\",\n    \"@\": \"__matmul__\",\n}\nREFLECTED_BIN_OP_METHOD = {\n    key: _reflected_name(value) for (key, value) in BIN_OP_METHOD.items()\n}\nAUGMENTED_OP_METHOD = {\n    key + \"=\": _augmented_name(value) for (key, value) in BIN_OP_METHOD.items()\n}\nUNARY_OP_METHOD = {\n    \"+\": \"__pos__\",\n    \"-\": \"__neg__\",\n    \"~\": \"__invert__\",\n    \"not\": None,  # XXX not '__nonzero__'\n}\n_UNARY_OPERATORS = {\n    \"+\": operator_mod.pos,\n    \"-\": operator_mod.neg,\n    \"~\": operator_mod.invert,\n    \"not\": operator_mod.not_,\n}\ndef _infer_unary_op(obj, op):\n    func = _UNARY_OPERATORS[op]\n    value = func(obj)\n    return nodes.const_factory(value)\nnodes.Tuple.infer_unary_op = lambda self, op: _infer_unary_op(tuple(self.elts), op)\nnodes.List.infer_unary_op = lambda self, op: _infer_unary_op(self.elts, op)\nnodes.Set.infer_unary_op = lambda self, op: _infer_unary_op(set(self.elts), op)\nnodes.Const.infer_unary_op = lambda self, op: _infer_unary_op(self.value, op)\nnodes.Dict.infer_unary_op = lambda self, op: _infer_unary_op(dict(self.items), op)\n# Binary operations\nBIN_OP_IMPL = {\n    \"+\": lambda a, b: a + b,\n    \"-\": lambda a, b: a - b,\n    \"/\": lambda a, b: a / b,\n    \"//\": lambda a, b: a // b,\n    \"*\": lambda a, b: a * b,\n    \"**\": lambda a, b: a**b,\n    \"%\": lambda a, b: a % b,\n    \"&\": lambda a, b: a & b,\n    \"|\": lambda a, b: a | b,\n    \"^\": lambda a, b: a ^ b,\n    \"<<\": lambda a, b: a << b,\n    \">>\": lambda a, b: a >> b,\n    \"@\": operator_mod.matmul,\n}\nfor _KEY, _IMPL in list(BIN_OP_IMPL.items()):\n    BIN_OP_IMPL[_KEY + \"=\"] = _IMPL\n@decorators.yes_if_nothing_inferred\ndef const_infer_binary_op(self, opnode, operator, other, context, _):\n    not_implemented = nodes.Const(NotImplemented)\n    if isinstance(other, nodes.Const):\n        try:\n            impl = BIN_OP_IMPL[operator]\n            try:\n                yield nodes.const_factory(impl(self.value, other.value))\n            except TypeError:\n                # ArithmeticError is not enough: float >> float is a TypeError\n                yield not_implemented\n            except Exception:  # pylint: disable=broad-except\n                yield util.Uninferable\n        except TypeError:\n            yield not_implemented\n    elif isinstance(self.value, str) and operator == \"%\":\n        # TODO(cpopa): implement string interpolation later on.\n        yield util.Uninferable\n    else:\n        yield not_implemented\nnodes.Const.infer_binary_op = const_infer_binary_op\ndef _multiply_seq_by_int(self, opnode, other, context):\n    node = self.__class__(parent=opnode)\n    filtered_elts = (\n        helpers.safe_infer(elt, context) or util.Uninferable\n        for elt in self.elts\n        if elt is not util.Uninferable\n    )\n    node.elts = list(filtered_elts) * other.value\n    return node\ndef _filter_uninferable_nodes(elts, context):\n    for elt in elts:\n        if elt is util.Uninferable:\n            yield nodes.Unknown()\n        else:\n            for inferred in elt.infer(context):\n                if inferred is not util.Uninferable:\n                    yield inferred\n                else:\n                    yield nodes.Unknown()\n@decorators.yes_if_nothing_inferred\ndef tl_infer_binary_op(\n    self,\n    opnode: nodes.BinOp,\n    operator: str,\n    other: nodes.NodeNG,\n    context: InferenceContext,\n    method: nodes.FunctionDef,\n) -> Generator[nodes.NodeNG, None, None]:\n    \"\"\"Infer a binary operation on a tuple or list.\n    The instance on which the binary operation is performed is a tuple\n    or list. This refers to the left-hand side of the operation, so:\n    'tuple() + 1' or '[] + A()'\n    \"\"\"\n    # For tuples and list the boundnode is no longer the tuple or list instance\n    context.boundnode = None\n    not_implemented = nodes.Const(NotImplemented)\n    if isinstance(other, self.__class__) and operator == \"+\":\n        node = self.__class__(parent=opnode)\n        node.elts = list(\n            itertools.chain(\n                _filter_uninferable_nodes(self.elts, context),\n                _filter_uninferable_nodes(other.elts, context),\n            )\n        )\n        yield node\n    elif isinstance(other, nodes.Const) and operator == \"*\":\n        if not isinstance(other.value, int):\n            yield not_implemented\n            return\n        yield _multiply_seq_by_int(self, opnode, other, context)\n    elif isinstance(other, bases.Instance) and operator == \"*\":\n        # Verify if the instance supports __index__.\n        as_index = helpers.class_instance_as_index(other)\n        if not as_index:\n            yield util.Uninferable\n        else:\n            yield _multiply_seq_by_int(self, opnode, as_index, context)\n    else:\n        yield not_implemented\nnodes.Tuple.infer_binary_op = tl_infer_binary_op\nnodes.List.infer_binary_op = tl_infer_binary_op\n@decorators.yes_if_nothing_inferred\ndef instance_class_infer_binary_op(self, opnode, operator, other, context, method):\n    return method.infer_call_result(self, context)\nbases.Instance.infer_binary_op = instance_class_infer_binary_op\nnodes.ClassDef.infer_binary_op = instance_class_infer_binary_op\n# assignment ##################################################################\n\"\"\"the assigned_stmts method is responsible to return the assigned statement\n(e.g. not inferred) according to the assignment type.\nThe `assign_path` argument is used to record the lhs path of the original node.\nFor instance if we want assigned statements for 'c' in 'a, (b,c)', assign_path\nwill be [1, 1] once arrived to the Assign node.\nThe `context` argument is the current inference context which should be given\nto any intermediary inference necessary.\n\"\"\"\ndef _resolve_looppart(parts, assign_path, context):\n    \"\"\"recursive function to resolve multiple assignments on loops\"\"\"\n    assign_path = assign_path[:]\n    index = assign_path.pop(0)\n    for part in parts:\n        if part is util.Uninferable:\n            continue\n        if not hasattr(part, \"itered\"):\n            continue\n        try:\n            itered = part.itered()\n        except TypeError:\n            continue\n        try:\n            if isinstance(itered[index], (nodes.Const, nodes.Name)):\n                itered = [part]\n        except IndexError:\n            pass\n        for stmt in itered:\n            index_node = nodes.Const(index)\n            try:\n                assigned = stmt.getitem(index_node, context)\n            except (AttributeError, AstroidTypeError, AstroidIndexError):\n                continue\n            if not assign_path:\n                # we achieved to resolved the assignment path,\n                # don't infer the last part\n                yield assigned\n            elif assigned is util.Uninferable:\n                break\n            else:\n                # we are not yet on the last part of the path\n                # search on each possibly inferred value\n                try:\n                    yield from _resolve_looppart(\n                        assigned.infer(context), assign_path, context\n                    )\n                except InferenceError:\n                    break\n@decorators.raise_if_nothing_inferred\ndef for_assigned_stmts(\n    self: nodes.For | nodes.Comprehension,\n    node: node_classes.AssignedStmtsPossibleNode = None,\n    context: InferenceContext | None = None,\n    assign_path: list[int] | None = None,\n) -> Any:\n    if isinstance(self, nodes.AsyncFor) or getattr(self, \"is_async\", False):\n        # Skip inferring of async code for now\n        return dict(node=self, unknown=node, assign_path=assign_path, context=context)\n    if assign_path is None:\n        for lst in self.iter.infer(context):\n            if isinstance(lst, (nodes.Tuple, nodes.List)):\n                yield from lst.elts\n    else:\n        yield from _resolve_looppart(self.iter.infer(context), assign_path, context)\n    return dict(node=self, unknown=node, assign_path=assign_path, context=context)\nnodes.For.assigned_stmts = for_assigned_stmts\nnodes.Comprehension.assigned_stmts = for_assigned_stmts\ndef sequence_assigned_stmts(\n    self: nodes.Tuple | nodes.List,\n    node: node_classes.AssignedStmtsPossibleNode = None,\n    context: InferenceContext | None = None,\n    assign_path: list[int] | None = None,\n) -> Any:\n    if assign_path is None:\n        assign_path = []\n    try:\n        index = self.elts.index(node)\n    except ValueError as exc:\n        raise InferenceError(\n            \"Tried to retrieve a node {node!r} which does not exist\",\n            node=self,\n            assign_path=assign_path,\n            context=context,\n        ) from exc\n    assign_path.insert(0, index)\n    return self.parent.assigned_stmts(\n        node=self, context=context, assign_path=assign_path\n    )\nnodes.Tuple.assigned_stmts = sequence_assigned_stmts\nnodes.List.assigned_stmts = sequence_assigned_stmts\ndef assend_assigned_stmts(\n    self: nodes.AssignName | nodes.AssignAttr,\n    node: node_classes.AssignedStmtsPossibleNode = None,\n    context: InferenceContext | None = None,\n    assign_path: list[int] | None = None,\n) -> Any:\n    return self.parent.assigned_stmts(node=self, context=context)\nnodes.AssignName.assigned_stmts = assend_assigned_stmts\nnodes.AssignAttr.assigned_stmts = assend_assigned_stmts\ndef _arguments_infer_argname(self, name, context):\n    # arguments information may be missing, in which case we can't do anything\n    # more\n    if not (self.arguments or self.vararg or self.kwarg):\n        yield util.Uninferable\n        return\n    functype = self.parent.type\n    # first argument of instance/class method\n    if (\n        self.arguments\n        and getattr(self.arguments[0], \"name\", None) == name\n        and functype != \"staticmethod\"\n    ):\n        cls = self.parent.parent.scope()\n        is_metaclass = isinstance(cls, nodes.ClassDef) and cls.type == \"metaclass\"\n        # If this is a metaclass, then the first argument will always\n        # be the class, not an instance.\n        if context.boundnode and isinstance(context.boundnode, bases.Instance):\n            cls = context.boundnode._proxied\n        if is_metaclass or functype == \"classmethod\":\n            yield cls\n            return\n        if functype == \"method\":\n            yield cls.instantiate_class()\n            return\n    if context and context.callcontext:\n        callee = context.callcontext.callee\n        while hasattr(callee, \"_proxied\"):\n            callee = callee._proxied\n        if getattr(callee, \"name\", None) == self.parent.name:\n            call_site = arguments.CallSite(context.callcontext, context.extra_context)\n            yield from call_site.infer_argument(self.parent, name, context)\n            return\n    if name == self.vararg:\n        vararg = nodes.const_factory(())\n        vararg.parent = self\n        if not self.arguments and self.parent.name == \"__init__\":\n            cls = self.parent.parent.scope()\n            vararg.elts = [cls.instantiate_class()]\n        yield vararg\n        return\n    if name == self.kwarg:\n        kwarg = nodes.const_factory({})\n        kwarg.parent = self\n        yield kwarg\n        return\n    # if there is a default value, yield it. And then yield Uninferable to reflect\n    # we can't guess given argument value\n    try:\n        context = copy_context(context)\n        yield from self.default_value(name).infer(context)\n        yield util.Uninferable\n    except NoDefault:\n        yield util.Uninferable\ndef arguments_assigned_stmts(\n    self: nodes.Arguments,\n    node: node_classes.AssignedStmtsPossibleNode = None,\n    context: InferenceContext | None = None,\n    assign_path: list[int] | None = None,\n) -> Any:\n    if context.callcontext:\n        callee = context.callcontext.callee\n        while hasattr(callee, \"_proxied\"):\n            callee = callee._proxied\n    else:\n        callee = None\n    if (\n        context.callcontext\n        and node\n        and getattr(callee, \"name\", None) == node.frame(future=True).name\n    ):\n        # reset call context/name\n        callcontext = context.callcontext\n        context = copy_context(context)\n        context.callcontext = None\n        args = arguments.CallSite(callcontext, context=context)\n        return args.infer_argument(self.parent, node.name, context)\n    return _arguments_infer_argname(self, node.name, context)\nnodes.Arguments.assigned_stmts = arguments_assigned_stmts\n@decorators.raise_if_nothing_inferred\ndef assign_assigned_stmts(\n    self: nodes.AugAssign | nodes.Assign | nodes.AnnAssign,\n    node: node_classes.AssignedStmtsPossibleNode = None,\n    context: InferenceContext | None = None,\n    assign_path: list[int] | None = None,\n) -> Any:\n    if not assign_path:\n        yield self.value\n        return None\n    yield from _resolve_assignment_parts(\n        self.value.infer(context), assign_path, context\n    )\n    return dict(node=self, unknown=node, assign_path=assign_path, context=context)\ndef assign_annassigned_stmts(\n    self: nodes.AnnAssign,\n    node: node_classes.AssignedStmtsPossibleNode = None,\n    context: InferenceContext | None = None,\n    assign_path: list[int] | None = None,\n) -> Any:\n    for inferred in assign_assigned_stmts(self, node, context, assign_path):\n        if inferred is None:\n            yield util.Uninferable\n        else:\n            yield inferred\nnodes.Assign.assigned_stmts = assign_assigned_stmts\nnodes.AnnAssign.assigned_stmts = assign_annassigned_stmts\nnodes.AugAssign.assigned_stmts = assign_assigned_stmts\ndef _resolve_assignment_parts(parts, assign_path, context):\n    \"\"\"recursive function to resolve multiple assignments\"\"\"\n    assign_path = assign_path[:]\n    index = assign_path.pop(0)\n    for part in parts:\n        assigned = None\n        if isinstance(part, nodes.Dict):\n            # A dictionary in an iterating context\n            try:\n                assigned, _ = part.items[index]\n            except IndexError:\n                return\n        elif hasattr(part, \"getitem\"):\n            index_node = nodes.Const(index)\n            try:\n                assigned = part.getitem(index_node, context)\n            except (AstroidTypeError, AstroidIndexError):\n                return\n        if not assigned:\n            return\n        if not assign_path:\n            # we achieved to resolved the assignment path, don't infer the\n            # last part\n            yield assigned\n        elif assigned is util.Uninferable:\n            return\n        else:\n            # we are not yet on the last part of the path search on each\n            # possibly inferred value\n            try:\n                yield from _resolve_assignment_parts(\n                    assigned.infer(context), assign_path, context\n                )\n            except InferenceError:\n                return\n@decorators.raise_if_nothing_inferred\ndef excepthandler_assigned_stmts(\n    self: nodes.ExceptHandler,\n    node: node_classes.AssignedStmtsPossibleNode = None,\n    context: InferenceContext | None = None,\n    assign_path: list[int] | None = None,\n) -> Any:\n    for assigned in node_classes.unpack_infer(self.type):\n        if isinstance(assigned, nodes.ClassDef):\n            assigned = objects.ExceptionInstance(assigned)\n        yield assigned\n    return dict(node=self, unknown=node, assign_path=assign_path, context=context)\nnodes.ExceptHandler.assigned_stmts = excepthandler_assigned_stmts\ndef _infer_context_manager(self, mgr, context):\n    try:\n        inferred = next(mgr.infer(context=context))\n    except StopIteration as e:\n        raise InferenceError(node=mgr) from e\n    if isinstance(inferred, bases.Generator):\n        # Check if it is decorated with contextlib.contextmanager.\n        func = inferred.parent\n        if not func.decorators:\n            raise InferenceError(\n                \"No decorators found on inferred generator %s\", node=func\n            )\n        for decorator_node in func.decorators.nodes:\n            decorator = next(decorator_node.infer(context=context), None)\n            if isinstance(decorator, nodes.FunctionDef):\n                if decorator.qname() == _CONTEXTLIB_MGR:\n                    break\n        else:\n            # It doesn't interest us.\n            raise InferenceError(node=func)\n        try:\n            yield next(inferred.infer_yield_types())\n        except StopIteration as e:\n            raise InferenceError(node=func) from e\n    elif isinstance(inferred, bases.Instance):\n        try:\n            enter = next(inferred.igetattr(\"__enter__\", context=context))\n        except (InferenceError, AttributeInferenceError, StopIteration) as exc:\n            raise InferenceError(node=inferred) from exc\n        if not isinstance(enter, bases.BoundMethod):\n            raise InferenceError(node=enter)\n        yield from enter.infer_call_result(self, context)\n    else:\n        raise InferenceError(node=mgr)\n@decorators.raise_if_nothing_inferred\ndef with_assigned_stmts(\n    self: nodes.With,\n    node: node_classes.AssignedStmtsPossibleNode = None,\n    context: InferenceContext | None = None,\n    assign_path: list[int] | None = None,\n) -> Any:\n    \"\"\"Infer names and other nodes from a *with* statement.\n    This enables only inference for name binding in a *with* statement.\n    For instance, in the following code, inferring `func` will return\n    the `ContextManager` class, not whatever ``__enter__`` returns.\n    We are doing this intentionally, because we consider that the context\n    manager result is whatever __enter__ returns and what it is binded\n    using the ``as`` keyword.\n        class ContextManager(object):\n            def __enter__(self):\n                return 42\n        with ContextManager() as f:\n            pass\n        # ContextManager().infer() will return ContextManager\n        # f.infer() will return 42.\n    Arguments:\n        self: nodes.With\n        node: The target of the assignment, `as (a, b)` in `with foo as (a, b)`.\n        context: Inference context used for caching already inferred objects\n        assign_path:\n            A list of indices, where each index specifies what item to fetch from\n            the inference results.\n    \"\"\"\n    try:\n        mgr = next(mgr for (mgr, vars) in self.items if vars == node)\n    except StopIteration:\n        return None\n    if assign_path is None:\n        yield from _infer_context_manager(self, mgr, context)",
        "file_path": "astroid/protocols.py",
        "chunk_index": 0,
        "metadata": {}
      }
    ]
  },
  "pylint-dev__astroid-1866": {
    "query": "\"TypeError: unsupported format string passed to NoneType.__format__\" while running type inference in version 2.12.x\n### Steps to reproduce\r\n\r\nI have no concise reproducer. Exception happens every time I run pylint on some internal code, with astroid 2.12.10 and 2.12.12 (debian bookworm). It does _not_ happen with earlier versions of astroid (not with version 2.9). The pylinted code itself is \"valid\", it runs in production here.\r\n\r\n### Current behavior\r\n\r\nWhen running pylint on some code, I get this exception:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3/dist-packages/pylint/utils/ast_walker.py\", line 90, in walk\r\n    callback(astroid)\r\n  File \"/usr/lib/python3/dist-packages/pylint/checkers/classes/special_methods_checker.py\", line 183, in visit_functiondef\r\n    inferred = _safe_infer_call_result(node, node)\r\n  File \"/usr/lib/python3/dist-packages/pylint/checkers/classes/special_methods_checker.py\", line 42, in _safe_infer_call_result\r\n    value = next(inferit)\r\n  File \"/usr/lib/python3/dist-packages/astroid/nodes/scoped_nodes/scoped_nodes.py\", line 1749, in infer_call_result\r\n    yield from returnnode.value.infer(context)\r\n  File \"/usr/lib/python3/dist-packages/astroid/nodes/node_ng.py\", line 159, in infer\r\n    results = list(self._explicit_inference(self, context, **kwargs))\r\n  File \"/usr/lib/python3/dist-packages/astroid/inference_tip.py\", line 45, in _inference_tip_cached\r\n    result = _cache[func, node] = list(func(*args, **kwargs))\r\n  File \"/usr/lib/python3/dist-packages/astroid/brain/brain_builtin_inference.py\", line 956, in _infer_str_format_call\r\n    formatted_string = format_template.format(*pos_values, **keyword_values)\r\nTypeError: unsupported format string passed to NoneType.__format__\r\n```\r\n\r\n### Expected behavior\r\n\r\nTypeError exception should not happen\r\n\r\n### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\r\n\r\n2.12.10,\r\n2.12.12\n",
    "method": "sliding",
    "top_k": 10,
    "results": [
      {
        "rank": 1,
        "score": 0.6133655905723572,
        "content": "    result will be an Uninferable object. Otherwise, the normal attribute\n    lookup will be done.\n    \"\"\"\n    obj, attr = _infer_getattr_args(node, context)\n    if (\n        obj is util.Uninferable\n        or attr is util.Uninferable\n        or not hasattr(obj, \"igetattr\")\n    ):\n        return util.Uninferable\n    try:\n        return next(obj.igetattr(attr, context=context))\n    except (StopIteration, InferenceError, AttributeInferenceError):\n        if len(node.args) == 3:\n            # Try to infer the default and return it instead.\n            try:\n                return next(node.args[2].infer(context=context))\n            except (StopIteration, InferenceError) as exc:\n                raise UseInferenceDefault from exc\n    raise UseInferenceDefault\ndef infer_hasattr(node, context=None):\n    \"\"\"Understand hasattr calls\n    This always guarantees three possible outcomes for calling\n    hasattr: Const(False) when we are sure that the object\n    doesn't have the intended attribute, Const(True) when\n    we know that the object has the attribute and Uninferable\n    when we are unsure of the outcome of the function call.\n    \"\"\"\n    try:\n        obj, attr = _infer_getattr_args(node, context)\n        if (\n            obj is util.Uninferable\n            or attr is util.Uninferable\n            or not hasattr(obj, \"getattr\")\n        ):\n            return util.Uninferable\n        obj.getattr(attr, context=context)\n    except UseInferenceDefault:\n        # Can't infer something from this function call.\n        return util.Uninferable\n    except AttributeInferenceError:\n        # Doesn't have it.\n        return nodes.Const(False)\n    return nodes.Const(True)\ndef infer_callable(node, context=None):\n    \"\"\"Understand callable calls\n    This follows Python's semantics, where an object\n    is callable if it provides an attribute __call__,\n    even though that attribute is something which can't be\n    called.\n    \"\"\"\n    if len(node.args) != 1:\n        # Invalid callable call.\n        raise UseInferenceDefault\n    argument = node.args[0]\n    try:\n        inferred = next(argument.infer(context=context))\n    except (InferenceError, StopIteration):\n        return util.Uninferable\n    if inferred is util.Uninferable:\n        return util.Uninferable\n    return nodes.Const(inferred.callable())\ndef infer_property(\n    node: nodes.Call, context: InferenceContext | None = None\n) -> objects.Property:\n    \"\"\"Understand `property` class\n    This only infers the output of `property`\n    call, not the arguments themselves.\n    \"\"\"\n    if len(node.args) < 1:\n        # Invalid property call.\n        raise UseInferenceDefault\n    getter = node.args[0]\n    try:\n        inferred = next(getter.infer(context=context))\n    except (InferenceError, StopIteration) as exc:\n        raise UseInferenceDefault from exc\n    if not isinstance(inferred, (nodes.FunctionDef, nodes.Lambda)):\n        raise UseInferenceDefault\n    prop_func = objects.Property(\n        function=inferred,\n        name=inferred.name,\n        lineno=node.lineno,\n        parent=node,\n        col_offset=node.col_offset,\n    )\n    prop_func.postinit(\n        body=[],\n        args=inferred.args,\n        doc_node=getattr(inferred, \"doc_node\", None),\n    )\n    return prop_func\ndef infer_bool(node, context=None):\n    \"\"\"Understand bool calls.\"\"\"\n    if len(node.args) > 1:\n        # Invalid bool call.\n        raise UseInferenceDefault\n    if not node.args:\n        return nodes.Const(False)\n    argument = node.args[0]\n    try:\n        inferred = next(argument.infer(context=context))\n    except (InferenceError, StopIteration):\n        return util.Uninferable\n    if inferred is util.Uninferable:\n        return util.Uninferable\n    bool_value = inferred.bool_value(context=context)\n    if bool_value is util.Uninferable:\n        return util.Uninferable\n    return nodes.Const(bool_value)\ndef infer_type(node, context=None):\n    \"\"\"Understand the one-argument form of *type*.\"\"\"\n    if len(node.args) != 1:\n        raise UseInferenceDefault\n    return helpers.object_type(node.args[0], context)\ndef infer_slice(node, context=None):\n    \"\"\"Understand `slice` calls.\"\"\"\n    args = node.args\n    if not 0 < len(args) <= 3:\n        raise UseInferenceDefault\n    infer_func = partial(helpers.safe_infer, context=context)\n    args = [infer_func(arg) for arg in args]\n    for arg in args:\n        if not arg or arg is util.Uninferable:\n            raise UseInferenceDefault\n        if not isinstance(arg, nodes.Const):\n            raise UseInferenceDefault\n        if not isinstance(arg.value, (type(None), int)):\n            raise UseInferenceDefault\n    if len(args) < 3:\n        # Make sure we have 3 arguments.\n        args.extend([None] * (3 - len(args)))\n    slice_node = nodes.Slice(\n        lineno=node.lineno, col_offset=node.col_offset, parent=node.parent\n    )\n    slice_node.postinit(*args)\n    return slice_node\ndef _infer_object__new__decorator(node, context=None):\n    # Instantiate class immediately\n    # since that's what @object.__new__ does\n    return iter((node.instantiate_class(),))\ndef _infer_object__new__decorator_check(node):\n    \"\"\"Predicate before inference_tip\n    Check if the given ClassDef has an @object.__new__ decorator\n    \"\"\"\n    if not node.decorators:\n        return False\n    for decorator in node.decorators.nodes:\n        if isinstance(decorator, nodes.Attribute):\n            if decorator.as_string() == OBJECT_DUNDER_NEW:\n                return True\n    return False\ndef infer_issubclass(callnode, context=None):\n    \"\"\"Infer issubclass() calls\n    :param nodes.Call callnode: an `issubclass` call\n    :param InferenceContext context: the context for the inference\n    :rtype nodes.Const: Boolean Const value of the `issubclass` call\n    :raises UseInferenceDefault: If the node cannot be inferred\n    \"\"\"\n    call = arguments.CallSite.from_call(callnode, context=context)\n    if call.keyword_arguments:\n        # issubclass doesn't support keyword arguments\n        raise UseInferenceDefault(\"TypeError: issubclass() takes no keyword arguments\")\n    if len(call.positional_arguments) != 2:\n        raise UseInferenceDefault(\n            f\"Expected two arguments, got {len(call.positional_arguments)}\"\n        )\n    # The left hand argument is the obj to be checked\n    obj_node, class_or_tuple_node = call.positional_arguments\n    try:\n        obj_type = next(obj_node.infer(context=context))\n    except (InferenceError, StopIteration) as exc:\n        raise UseInferenceDefault from exc\n    if not isinstance(obj_type, nodes.ClassDef):\n        raise UseInferenceDefault(\"TypeError: arg 1 must be class\")\n    # The right hand argument is the class(es) that the given\n    # object is to be checked against.\n    try:\n        class_container = _class_or_tuple_to_container(\n            class_or_tuple_node, context=context\n        )\n    except InferenceError as exc:\n        raise UseInferenceDefault from exc\n    try:\n        issubclass_bool = helpers.object_issubclass(obj_type, class_container, context)\n    except AstroidTypeError as exc:\n        raise UseInferenceDefault(\"TypeError: \" + str(exc)) from exc\n    except MroError as exc:\n        raise UseInferenceDefault from exc\n    return nodes.Const(issubclass_bool)\ndef infer_isinstance(callnode, context=None):\n    \"\"\"Infer isinstance calls\n    :param nodes.Call callnode: an isinstance call\n    :param InferenceContext context: context for call\n        (currently unused but is a common interface for inference)\n    :rtype nodes.Const: Boolean Const value of isinstance call\n    :raises UseInferenceDefault: If the node cannot be inferred\n    \"\"\"\n    call = arguments.CallSite.from_call(callnode, context=context)\n    if call.keyword_arguments:\n        # isinstance doesn't support keyword arguments\n        raise UseInferenceDefault(\"TypeError: isinstance() takes no keyword arguments\")\n    if len(call.positional_arguments) != 2:\n        raise UseInferenceDefault(\n            f\"Expected two arguments, got {len(call.positional_arguments)}\"\n        )\n    # The left hand argument is the obj to be checked\n    obj_node, class_or_tuple_node = call.positional_arguments\n    # The right hand argument is the class(es) that the given\n    # obj is to be check is an instance of\n    try:\n        class_container = _class_or_tuple_to_container(\n            class_or_tuple_node, context=context\n        )\n    except InferenceError as exc:\n        raise UseInferenceDefault from exc\n    try:\n        isinstance_bool = helpers.object_isinstance(obj_node, class_container, context)\n    except AstroidTypeError as exc:\n        raise UseInferenceDefault(\"TypeError: \" + str(exc)) from exc\n    except MroError as exc:\n        raise UseInferenceDefault from exc\n    if isinstance_bool is util.Uninferable:\n        raise UseInferenceDefault\n    return nodes.Const(isinstance_bool)\ndef _class_or_tuple_to_container(node, context=None):\n    # Move inferences results into container\n    # to simplify later logic\n    # raises InferenceError if any of the inferences fall through\n    try:\n        node_infer = next(node.infer(context=context))\n    except StopIteration as e:\n        raise InferenceError(node=node, context=context) from e\n    # arg2 MUST be a type or a TUPLE of types\n    # for isinstance\n    if isinstance(node_infer, nodes.Tuple):\n        try:\n            class_container = [\n                next(node.infer(context=context)) for node in node_infer.elts\n            ]\n        except StopIteration as e:\n            raise InferenceError(node=node, context=context) from e\n        class_container = [\n            klass_node for klass_node in class_container if klass_node is not None\n        ]\n    else:\n        class_container = [node_infer]\n    return class_container\ndef infer_len(node, context=None):\n    \"\"\"Infer length calls\n    :param nodes.Call node: len call to infer\n    :param context.InferenceContext: node context\n    :rtype nodes.Const: a Const node with the inferred length, if possible\n    \"\"\"\n    call = arguments.CallSite.from_call(node, context=context)\n    if call.keyword_arguments:\n        raise UseInferenceDefault(\"TypeError: len() must take no keyword arguments\")\n    if len(call.positional_arguments) != 1:\n        raise UseInferenceDefault(\n            \"TypeError: len() must take exactly one argument \"\n            \"({len}) given\".format(len=len(call.positional_arguments))\n        )\n    [argument_node] = call.positional_arguments\n    try:\n        return nodes.Const(helpers.object_len(argument_node, context=context))\n    except (AstroidTypeError, InferenceError) as exc:\n        raise UseInferenceDefault(str(exc)) from exc\ndef infer_str(node, context=None):\n    \"\"\"Infer str() calls\n    :param nodes.Call node: str() call to infer\n    :param context.InferenceContext: node context\n    :rtype nodes.Const: a Const containing an empty string\n    \"\"\"\n    call = arguments.CallSite.from_call(node, context=context)\n    if call.keyword_arguments:\n        raise UseInferenceDefault(\"TypeError: str() must take no keyword arguments\")\n    try:\n        return nodes.Const(\"\")\n    except (AstroidTypeError, InferenceError) as exc:\n        raise UseInferenceDefault(str(exc)) from exc\ndef infer_int(node, context=None):\n    \"\"\"Infer int() calls\n    :param nodes.Call node: int() call to infer\n    :param context.InferenceContext: node context\n    :rtype nodes.Const: a Const containing the integer value of the int() call\n    \"\"\"\n    call = arguments.CallSite.from_call(node, context=context)\n    if call.keyword_arguments:\n        raise UseInferenceDefault(\"TypeError: int() must take no keyword arguments\")\n    if call.positional_arguments:\n        try:\n            first_value = next(call.positional_arguments[0].infer(context=context))\n        except (InferenceError, StopIteration) as exc:\n            raise UseInferenceDefault(str(exc)) from exc\n        if first_value is util.Uninferable:\n            raise UseInferenceDefault\n        if isinstance(first_value, nodes.Const) and isinstance(\n            first_value.value, (int, str)\n        ):\n            try:\n                actual_value = int(first_value.value)\n            except ValueError:\n                return nodes.Const(0)\n            return nodes.Const(actual_value)\n    return nodes.Const(0)\ndef infer_dict_fromkeys(node, context=None):\n    \"\"\"Infer dict.fromkeys\n    :param nodes.Call node: dict.fromkeys() call to infer\n    :param context.InferenceContext context: node context\n    :rtype nodes.Dict:\n        a Dictionary containing the values that astroid was able to infer.\n        In case the inference failed for any reason, an empty dictionary\n        will be inferred instead.\n    \"\"\"\n    def _build_dict_with_elements(elements):\n        new_node = nodes.Dict(\n            col_offset=node.col_offset, lineno=node.lineno, parent=node.parent\n        )\n        new_node.postinit(elements)\n        return new_node\n    call = arguments.CallSite.from_call(node, context=context)\n    if call.keyword_arguments:\n        raise UseInferenceDefault(\"TypeError: int() must take no keyword arguments\")\n    if len(call.positional_arguments) not in {1, 2}:\n        raise UseInferenceDefault(\n            \"TypeError: Needs between 1 and 2 positional arguments\"\n        )\n    default = nodes.Const(None)\n    values = call.positional_arguments[0]\n    try:\n        inferred_values = next(values.infer(context=context))\n    except (InferenceError, StopIteration):\n        return _build_dict_with_elements([])\n    if inferred_values is util.Uninferable:\n        return _build_dict_with_elements([])\n    # Limit to a couple of potential values, as this can become pretty complicated\n    accepted_iterable_elements = (nodes.Const,)\n    if isinstance(inferred_values, (nodes.List, nodes.Set, nodes.Tuple)):\n        elements = inferred_values.elts\n        for element in elements:\n            if not isinstance(element, accepted_iterable_elements):\n                # Fallback to an empty dict\n                return _build_dict_with_elements([])\n        elements_with_value = [(element, default) for element in elements]\n        return _build_dict_with_elements(elements_with_value)\n    if isinstance(inferred_values, nodes.Const) and isinstance(\n        inferred_values.value, (str, bytes)\n    ):\n        elements = [\n            (nodes.Const(element), default) for element in inferred_values.value\n        ]\n        return _build_dict_with_elements(elements)\n    if isinstance(inferred_values, nodes.Dict):\n        keys = inferred_values.itered()\n        for key in keys:\n            if not isinstance(key, accepted_iterable_elements):\n                # Fallback to an empty dict\n                return _build_dict_with_elements([])\n        elements_with_value = [(element, default) for element in keys]\n        return _build_dict_with_elements(elements_with_value)\n    # Fallback to an empty dictionary\n    return _build_dict_with_elements([])\ndef _infer_copy_method(\n    node: nodes.Call, context: InferenceContext | None = None\n) -> Iterator[nodes.NodeNG]:\n    assert isinstance(node.func, nodes.Attribute)\n    inferred_orig, inferred_copy = itertools.tee(node.func.expr.infer(context=context))\n    if all(\n        isinstance(\n            inferred_node, (nodes.Dict, nodes.List, nodes.Set, objects.FrozenSet)\n        )\n        for inferred_node in inferred_orig\n    ):\n        return inferred_copy\n    raise UseInferenceDefault()\ndef _is_str_format_call(node: nodes.Call) -> bool:\n    \"\"\"Catch calls to str.format().\"\"\"\n    if not isinstance(node.func, nodes.Attribute) or not node.func.attrname == \"format\":\n        return False\n    if isinstance(node.func.expr, nodes.Name):\n        value = helpers.safe_infer(node.func.expr)\n    else:\n        value = node.func.expr\n    return isinstance(value, nodes.Const) and isinstance(value.value, str)\ndef _infer_str_format_call(\n    node: nodes.Call, context: InferenceContext | None = None\n) -> Iterator[nodes.Const | type[util.Uninferable]]:\n    \"\"\"Return a Const node based on the template and passed arguments.\"\"\"\n    call = arguments.CallSite.from_call(node, context=context)\n    if isinstance(node.func.expr, nodes.Name):\n        value: nodes.Const = helpers.safe_infer(node.func.expr)\n    else:\n        value = node.func.expr\n    format_template = value.value\n    # Get the positional arguments passed\n    inferred_positional = [\n        helpers.safe_infer(i, context) for i in call.positional_arguments\n    ]\n    if not all(isinstance(i, nodes.Const) for i in inferred_positional):\n        return iter([util.Uninferable])\n    pos_values: list[str] = [i.value for i in inferred_positional]\n    # Get the keyword arguments passed\n    inferred_keyword = {\n        k: helpers.safe_infer(v, context) for k, v in call.keyword_arguments.items()\n    }\n    if not all(isinstance(i, nodes.Const) for i in inferred_keyword.values()):\n        return iter([util.Uninferable])\n    keyword_values: dict[str, str] = {k: v.value for k, v in inferred_keyword.items()}\n    try:\n        formatted_string = format_template.format(*pos_values, **keyword_values)\n    except (IndexError, KeyError):\n        # If there is an IndexError there are too few arguments to interpolate\n        return iter([util.Uninferable])\n    return iter([nodes.const_factory(formatted_string)])\n# Builtins inference\nregister_builtin_transform(infer_bool, \"bool\")\nregister_builtin_transform(infer_super, \"super\")\nregister_builtin_transform(infer_callable, \"callable\")\nregister_builtin_transform(infer_property, \"property\")\nregister_builtin_transform(infer_getattr, \"getattr\")\nregister_builtin_transform(infer_hasattr, \"hasattr\")\nregister_builtin_transform(infer_tuple, \"tuple\")\nregister_builtin_transform(infer_set, \"set\")\nregister_builtin_transform(infer_list, \"list\")\nregister_builtin_transform(infer_dict, \"dict\")\nregister_builtin_transform(infer_frozenset, \"frozenset\")\nregister_builtin_transform(infer_type, \"type\")\nregister_builtin_transform(infer_slice, \"slice\")\nregister_builtin_transform(infer_isinstance, \"isinstance\")\nregister_builtin_transform(infer_issubclass, \"issubclass\")\nregister_builtin_transform(infer_len, \"len\")\nregister_builtin_transform(infer_str, \"str\")\nregister_builtin_transform(infer_int, \"int\")\nregister_builtin_transform(infer_dict_fromkeys, \"dict.fromkeys\")\n# Infer object.__new__ calls\nAstroidManager().register_transform(\n    nodes.ClassDef,\n    inference_tip(_infer_object__new__decorator),\n    _infer_object__new__decorator_check,\n)\nAstroidManager().register_transform(\n    nodes.Call,\n    inference_tip(_infer_copy_method),\n    lambda node: isinstance(node.func, nodes.Attribute)\n    and node.func.attrname == \"copy\",\n)\nAstroidManager().register_transform(\n    nodes.Call, inference_tip(_infer_str_format_call), _is_str_format_call\n)",
        "file_path": "astroid/brain/brain_builtin_inference.py",
        "chunk_index": 1,
        "metadata": {}
      },
      {
        "rank": 2,
        "score": 0.6090314388275146,
        "content": "# Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n# For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n# Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n\"\"\"Astroid hooks for typing.py support.\"\"\"\nfrom __future__ import annotations\nimport typing\nfrom collections.abc import Iterator\nfrom functools import partial\nfrom astroid import context, extract_node, inference_tip\nfrom astroid.builder import _extract_single_node\nfrom astroid.const import PY38_PLUS, PY39_PLUS\nfrom astroid.exceptions import (\n    AttributeInferenceError,\n    InferenceError,\n    UseInferenceDefault,\n)\nfrom astroid.manager import AstroidManager\nfrom astroid.nodes.node_classes import (\n    Assign,\n    AssignName,\n    Attribute,\n    Call,\n    Const,\n    JoinedStr,\n    Name,\n    NodeNG,\n    Subscript,\n    Tuple,\n)\nfrom astroid.nodes.scoped_nodes import ClassDef, FunctionDef\nfrom astroid.util import Uninferable\nTYPING_NAMEDTUPLE_BASENAMES = {\"NamedTuple\", \"typing.NamedTuple\"}\nTYPING_TYPEVARS = {\"TypeVar\", \"NewType\"}\nTYPING_TYPEVARS_QUALIFIED = {\"typing.TypeVar\", \"typing.NewType\"}\nTYPING_TYPE_TEMPLATE = \"\"\"\nclass Meta(type):\n    def __getitem__(self, item):\n        return self\n    @property\n    def __args__(self):\n        return ()\nclass {0}(metaclass=Meta):\n    pass\n\"\"\"\nTYPING_MEMBERS = set(getattr(typing, \"__all__\", []))\nTYPING_ALIAS = frozenset(\n    (\n        \"typing.Hashable\",\n        \"typing.Awaitable\",\n        \"typing.Coroutine\",\n        \"typing.AsyncIterable\",\n        \"typing.AsyncIterator\",\n        \"typing.Iterable\",\n        \"typing.Iterator\",\n        \"typing.Reversible\",\n        \"typing.Sized\",\n        \"typing.Container\",\n        \"typing.Collection\",\n        \"typing.Callable\",\n        \"typing.AbstractSet\",\n        \"typing.MutableSet\",\n        \"typing.Mapping\",\n        \"typing.MutableMapping\",\n        \"typing.Sequence\",\n        \"typing.MutableSequence\",\n        \"typing.ByteString\",\n        \"typing.Tuple\",\n        \"typing.List\",\n        \"typing.Deque\",\n        \"typing.Set\",\n        \"typing.FrozenSet\",\n        \"typing.MappingView\",\n        \"typing.KeysView\",\n        \"typing.ItemsView\",\n        \"typing.ValuesView\",\n        \"typing.ContextManager\",\n        \"typing.AsyncContextManager\",\n        \"typing.Dict\",\n        \"typing.DefaultDict\",\n        \"typing.OrderedDict\",\n        \"typing.Counter\",\n        \"typing.ChainMap\",\n        \"typing.Generator\",\n        \"typing.AsyncGenerator\",\n        \"typing.Type\",\n        \"typing.Pattern\",\n        \"typing.Match\",\n    )\n)\nCLASS_GETITEM_TEMPLATE = \"\"\"\n@classmethod\ndef __class_getitem__(cls, item):\n    return cls\n\"\"\"\ndef looks_like_typing_typevar_or_newtype(node):\n    func = node.func\n    if isinstance(func, Attribute):\n        return func.attrname in TYPING_TYPEVARS\n    if isinstance(func, Name):\n        return func.name in TYPING_TYPEVARS\n    return False\ndef infer_typing_typevar_or_newtype(node, context_itton=None):\n    \"\"\"Infer a typing.TypeVar(...) or typing.NewType(...) call\"\"\"\n    try:\n        func = next(node.func.infer(context=context_itton))\n    except (InferenceError, StopIteration) as exc:\n        raise UseInferenceDefault from exc\n    if func.qname() not in TYPING_TYPEVARS_QUALIFIED:\n        raise UseInferenceDefault\n    if not node.args:\n        raise UseInferenceDefault\n    # Cannot infer from a dynamic class name (f-string)\n    if isinstance(node.args[0], JoinedStr):\n        raise UseInferenceDefault\n    typename = node.args[0].as_string().strip(\"'\")\n    node = extract_node(TYPING_TYPE_TEMPLATE.format(typename))\n    return node.infer(context=context_itton)\ndef _looks_like_typing_subscript(node):\n    \"\"\"Try to figure out if a Subscript node *might* be a typing-related subscript\"\"\"\n    if isinstance(node, Name):\n        return node.name in TYPING_MEMBERS\n    if isinstance(node, Attribute):\n        return node.attrname in TYPING_MEMBERS\n    if isinstance(node, Subscript):\n        return _looks_like_typing_subscript(node.value)\n    return False\ndef infer_typing_attr(\n    node: Subscript, ctx: context.InferenceContext | None = None\n) -> Iterator[ClassDef]:\n    \"\"\"Infer a typing.X[...] subscript\"\"\"\n    try:\n        value = next(node.value.infer())  # type: ignore[union-attr] # value shouldn't be None for Subscript.\n    except (InferenceError, StopIteration) as exc:\n        raise UseInferenceDefault from exc\n    if not value.qname().startswith(\"typing.\") or value.qname() in TYPING_ALIAS:\n        # If typing subscript belongs to an alias handle it separately.\n        raise UseInferenceDefault\n    if isinstance(value, ClassDef) and value.qname() in {\n        \"typing.Generic\",\n        \"typing.Annotated\",\n        \"typing_extensions.Annotated\",\n    }:\n        # typing.Generic and typing.Annotated (PY39) are subscriptable\n        # through __class_getitem__. Since astroid can't easily\n        # infer the native methods, replace them for an easy inference tip\n        func_to_add = _extract_single_node(CLASS_GETITEM_TEMPLATE)\n        value.locals[\"__class_getitem__\"] = [func_to_add]\n        if (\n            isinstance(node.parent, ClassDef)\n            and node in node.parent.bases\n            and getattr(node.parent, \"__cache\", None)\n        ):\n            # node.parent.slots is evaluated and cached before the inference tip\n            # is first applied. Remove the last result to allow a recalculation of slots\n            cache = node.parent.__cache  # type: ignore[attr-defined] # Unrecognized getattr\n            if cache.get(node.parent.slots) is not None:\n                del cache[node.parent.slots]\n        return iter([value])\n    node = extract_node(TYPING_TYPE_TEMPLATE.format(value.qname().split(\".\")[-1]))\n    return node.infer(context=ctx)\ndef _looks_like_typedDict(  # pylint: disable=invalid-name\n    node: FunctionDef | ClassDef,\n) -> bool:\n    \"\"\"Check if node is TypedDict FunctionDef.\"\"\"\n    return node.qname() in {\"typing.TypedDict\", \"typing_extensions.TypedDict\"}\ndef infer_old_typedDict(  # pylint: disable=invalid-name\n    node: ClassDef, ctx: context.InferenceContext | None = None\n) -> Iterator[ClassDef]:\n    func_to_add = _extract_single_node(\"dict\")\n    node.locals[\"__call__\"] = [func_to_add]\n    return iter([node])\ndef infer_typedDict(  # pylint: disable=invalid-name\n    node: FunctionDef, ctx: context.InferenceContext | None = None\n) -> Iterator[ClassDef]:\n    \"\"\"Replace TypedDict FunctionDef with ClassDef.\"\"\"\n    class_def = ClassDef(\n        name=\"TypedDict\",\n        lineno=node.lineno,\n        col_offset=node.col_offset,\n        parent=node.parent,\n    )\n    class_def.postinit(bases=[extract_node(\"dict\")], body=[], decorators=None)\n    func_to_add = _extract_single_node(\"dict\")\n    class_def.locals[\"__call__\"] = [func_to_add]\n    return iter([class_def])\ndef _looks_like_typing_alias(node: Call) -> bool:\n    \"\"\"\n    Returns True if the node corresponds to a call to _alias function.\n    For example :\n    MutableSet = _alias(collections.abc.MutableSet, T)\n    :param node: call node\n    \"\"\"\n    return (\n        isinstance(node.func, Name)\n        and node.func.name == \"_alias\"\n        and (\n            # _alias function works also for builtins object such as list and dict\n            isinstance(node.args[0], (Attribute, Name))\n        )\n    )\ndef _forbid_class_getitem_access(node: ClassDef) -> None:\n    \"\"\"\n    Disable the access to __class_getitem__ method for the node in parameters\n    \"\"\"\n    def full_raiser(origin_func, attr, *args, **kwargs):\n        \"\"\"\n        Raises an AttributeInferenceError in case of access to __class_getitem__ method.\n        Otherwise, just call origin_func.\n        \"\"\"\n        if attr == \"__class_getitem__\":\n            raise AttributeInferenceError(\"__class_getitem__ access is not allowed\")\n        return origin_func(attr, *args, **kwargs)\n    try:\n        node.getattr(\"__class_getitem__\")\n        # If we are here, then we are sure to modify an object that does have\n        # __class_getitem__ method (which origin is the protocol defined in\n        # collections module) whereas the typing module considers it should not.\n        # We do not want __class_getitem__ to be found in the classdef\n        partial_raiser = partial(full_raiser, node.getattr)\n        node.getattr = partial_raiser\n    except AttributeInferenceError:\n        pass\ndef infer_typing_alias(\n    node: Call, ctx: context.InferenceContext | None = None\n) -> Iterator[ClassDef]:\n    \"\"\"\n    Infers the call to _alias function\n    Insert ClassDef, with same name as aliased class,\n    in mro to simulate _GenericAlias.\n    :param node: call node\n    :param context: inference context\n    \"\"\"\n    if (\n        not isinstance(node.parent, Assign)\n        or not len(node.parent.targets) == 1\n        or not isinstance(node.parent.targets[0], AssignName)\n    ):\n        raise UseInferenceDefault\n    try:\n        res = next(node.args[0].infer(context=ctx))\n    except StopIteration as e:\n        raise InferenceError(node=node.args[0], context=context) from e\n    assign_name = node.parent.targets[0]\n    class_def = ClassDef(\n        name=assign_name.name,\n        lineno=assign_name.lineno,\n        col_offset=assign_name.col_offset,\n        parent=node.parent,\n    )\n    if res != Uninferable and isinstance(res, ClassDef):\n        # Only add `res` as base if it's a `ClassDef`\n        # This isn't the case for `typing.Pattern` and `typing.Match`\n        class_def.postinit(bases=[res], body=[], decorators=None)\n    maybe_type_var = node.args[1]\n    if (\n        not PY39_PLUS\n        and not (isinstance(maybe_type_var, Tuple) and not maybe_type_var.elts)\n        or PY39_PLUS\n        and isinstance(maybe_type_var, Const)\n        and maybe_type_var.value > 0\n    ):\n        # If typing alias is subscriptable, add `__class_getitem__` to ClassDef\n        func_to_add = _extract_single_node(CLASS_GETITEM_TEMPLATE)\n        class_def.locals[\"__class_getitem__\"] = [func_to_add]\n    else:\n        # If not, make sure that `__class_getitem__` access is forbidden.\n        # This is an issue in cases where the aliased class implements it,\n        # but the typing alias isn't subscriptable. E.g., `typing.ByteString` for PY39+\n        _forbid_class_getitem_access(class_def)\n    return iter([class_def])\ndef _looks_like_special_alias(node: Call) -> bool:\n    \"\"\"Return True if call is for Tuple or Callable alias.\n    In PY37 and PY38 the call is to '_VariadicGenericAlias' with 'tuple' as\n    first argument. In PY39+ it is replaced by a call to '_TupleType'.\n    PY37: Tuple = _VariadicGenericAlias(tuple, (), inst=False, special=True)\n    PY39: Tuple = _TupleType(tuple, -1, inst=False, name='Tuple')\n    PY37: Callable = _VariadicGenericAlias(collections.abc.Callable, (), special=True)\n    PY39: Callable = _CallableType(collections.abc.Callable, 2)\n    \"\"\"\n    return isinstance(node.func, Name) and (\n        not PY39_PLUS\n        and node.func.name == \"_VariadicGenericAlias\"\n        and (\n            isinstance(node.args[0], Name)\n            and node.args[0].name == \"tuple\"\n            or isinstance(node.args[0], Attribute)\n            and node.args[0].as_string() == \"collections.abc.Callable\"\n        )\n        or PY39_PLUS\n        and (\n            node.func.name == \"_TupleType\"\n            and isinstance(node.args[0], Name)\n            and node.args[0].name == \"tuple\"\n            or node.func.name == \"_CallableType\"\n            and isinstance(node.args[0], Attribute)\n            and node.args[0].as_string() == \"collections.abc.Callable\"\n        )\n    )\ndef infer_special_alias(\n    node: Call, ctx: context.InferenceContext | None = None\n) -> Iterator[ClassDef]:\n    \"\"\"Infer call to tuple alias as new subscriptable class typing.Tuple.\"\"\"\n    if not (\n        isinstance(node.parent, Assign)\n        and len(node.parent.targets) == 1\n        and isinstance(node.parent.targets[0], AssignName)\n    ):\n        raise UseInferenceDefault\n    try:\n        res = next(node.args[0].infer(context=ctx))\n    except StopIteration as e:\n        raise InferenceError(node=node.args[0], context=context) from e\n    assign_name = node.parent.targets[0]\n    class_def = ClassDef(\n        name=assign_name.name,\n        parent=node.parent,\n    )\n    class_def.postinit(bases=[res], body=[], decorators=None)\n    func_to_add = _extract_single_node(CLASS_GETITEM_TEMPLATE)\n    class_def.locals[\"__class_getitem__\"] = [func_to_add]\n    return iter([class_def])\ndef _looks_like_typing_cast(node: Call) -> bool:\n    return isinstance(node, Call) and (\n        isinstance(node.func, Name)\n        and node.func.name == \"cast\"\n        or isinstance(node.func, Attribute)\n        and node.func.attrname == \"cast\"\n    )\ndef infer_typing_cast(\n    node: Call, ctx: context.InferenceContext | None = None\n) -> Iterator[NodeNG]:\n    \"\"\"Infer call to cast() returning same type as casted-from var\"\"\"\n    if not isinstance(node.func, (Name, Attribute)):\n        raise UseInferenceDefault\n    try:\n        func = next(node.func.infer(context=ctx))\n    except (InferenceError, StopIteration) as exc:\n        raise UseInferenceDefault from exc\n    if (\n        not isinstance(func, FunctionDef)\n        or func.qname() != \"typing.cast\"\n        or len(node.args) != 2\n    ):\n        raise UseInferenceDefault\n    return node.args[1].infer(context=ctx)\nAstroidManager().register_transform(\n    Call,\n    inference_tip(infer_typing_typevar_or_newtype),\n    looks_like_typing_typevar_or_newtype,\n)\nAstroidManager().register_transform(\n    Subscript, inference_tip(infer_typing_attr), _looks_like_typing_subscript\n)\nAstroidManager().register_transform(\n    Call, inference_tip(infer_typing_cast), _looks_like_typing_cast\n)\nif PY39_PLUS:\n    AstroidManager().register_transform(\n        FunctionDef, inference_tip(infer_typedDict), _looks_like_typedDict\n    )\nelif PY38_PLUS:\n    AstroidManager().register_transform(\n        ClassDef, inference_tip(infer_old_typedDict), _looks_like_typedDict\n    )\nAstroidManager().register_transform(\n    Call, inference_tip(infer_typing_alias), _looks_like_typing_alias\n)\nAstroidManager().register_transform(\n    Call, inference_tip(infer_special_alias), _looks_like_special_alias\n)",
        "file_path": "astroid/brain/brain_typing.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 3,
        "score": 0.5928948521614075,
        "content": "# Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n# For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n# Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\nimport importlib\nimport warnings\nimport lazy_object_proxy\ndef lazy_descriptor(obj):\n    class DescriptorProxy(lazy_object_proxy.Proxy):\n        def __get__(self, instance, owner=None):\n            return self.__class__.__get__(self, instance)\n    return DescriptorProxy(obj)\ndef lazy_import(module_name):\n    return lazy_object_proxy.Proxy(\n        lambda: importlib.import_module(\".\" + module_name, \"astroid\")\n    )\n@object.__new__\nclass Uninferable:\n    \"\"\"Special inference object, which is returned when inference fails.\"\"\"\n    def __repr__(self):\n        return \"Uninferable\"\n    __str__ = __repr__\n    def __getattribute__(self, name):\n        if name == \"next\":\n            raise AttributeError(\"next method should not be called\")\n        if name.startswith(\"__\") and name.endswith(\"__\"):\n            return object.__getattribute__(self, name)\n        if name == \"accept\":\n            return object.__getattribute__(self, name)\n        return self\n    def __call__(self, *args, **kwargs):\n        return self\n    def __bool__(self):\n        return False\n    __nonzero__ = __bool__\n    def accept(self, visitor):\n        return visitor.visit_uninferable(self)\nclass BadOperationMessage:\n    \"\"\"Object which describes a TypeError occurred somewhere in the inference chain\n    This is not an exception, but a container object which holds the types and\n    the error which occurred.\n    \"\"\"\nclass BadUnaryOperationMessage(BadOperationMessage):\n    \"\"\"Object which describes operational failures on UnaryOps.\"\"\"\n    def __init__(self, operand, op, error):\n        self.operand = operand\n        self.op = op\n        self.error = error\n    @property\n    def _object_type_helper(self):\n        helpers = lazy_import(\"helpers\")\n        return helpers.object_type\n    def _object_type(self, obj):\n        objtype = self._object_type_helper(obj)\n        if objtype is Uninferable:\n            return None\n        return objtype\n    def __str__(self):\n        if hasattr(self.operand, \"name\"):\n            operand_type = self.operand.name\n        else:\n            object_type = self._object_type(self.operand)\n            if hasattr(object_type, \"name\"):\n                operand_type = object_type.name\n            else:\n                # Just fallback to as_string\n                operand_type = object_type.as_string()\n        msg = \"bad operand type for unary {}: {}\"\n        return msg.format(self.op, operand_type)\nclass BadBinaryOperationMessage(BadOperationMessage):\n    \"\"\"Object which describes type errors for BinOps.\"\"\"\n    def __init__(self, left_type, op, right_type):\n        self.left_type = left_type\n        self.right_type = right_type\n        self.op = op\n    def __str__(self):\n        msg = \"unsupported operand type(s) for {}: {!r} and {!r}\"\n        return msg.format(self.op, self.left_type.name, self.right_type.name)\ndef _instancecheck(cls, other):\n    wrapped = cls.__wrapped__\n    other_cls = other.__class__\n    is_instance_of = wrapped is other_cls or issubclass(other_cls, wrapped)\n    warnings.warn(\n        \"%r is deprecated and slated for removal in astroid \"\n        \"2.0, use %r instead\" % (cls.__class__.__name__, wrapped.__name__),\n        PendingDeprecationWarning,\n        stacklevel=2,\n    )\n    return is_instance_of\ndef proxy_alias(alias_name, node_type):\n    \"\"\"Get a Proxy from the given name to the given node type.\"\"\"\n    proxy = type(\n        alias_name,\n        (lazy_object_proxy.Proxy,),\n        {\n            \"__class__\": object.__dict__[\"__class__\"],\n            \"__instancecheck__\": _instancecheck,\n        },\n    )\n    return proxy(lambda: node_type)\ndef check_warnings_filter() -> bool:\n    \"\"\"Return True if any other than the default DeprecationWarning filter is enabled.\n    https://docs.python.org/3/library/warnings.html#default-warning-filter\n    \"\"\"\n    return any(\n        issubclass(DeprecationWarning, filter[2])\n        and filter[0] != \"ignore\"\n        and filter[3] != \"__main__\"\n        for filter in warnings.filters\n    )",
        "file_path": "astroid/util.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 4,
        "score": 0.5899204015731812,
        "content": "# Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n# For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n# Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n\"\"\"Astroid hooks for various builtins.\"\"\"\nfrom __future__ import annotations\nimport itertools\nfrom collections.abc import Iterator\nfrom functools import partial\nfrom astroid import arguments, helpers, inference_tip, nodes, objects, util\nfrom astroid.builder import AstroidBuilder\nfrom astroid.context import InferenceContext\nfrom astroid.exceptions import (\n    AstroidTypeError,\n    AttributeInferenceError,\n    InferenceError,\n    MroError,\n    UseInferenceDefault,\n)\nfrom astroid.manager import AstroidManager\nfrom astroid.nodes import scoped_nodes\nOBJECT_DUNDER_NEW = \"object.__new__\"\nSTR_CLASS = \"\"\"\nclass whatever(object):\n    def join(self, iterable):\n        return {rvalue}\n    def replace(self, old, new, count=None):\n        return {rvalue}\n    def format(self, *args, **kwargs):\n        return {rvalue}\n    def encode(self, encoding='ascii', errors=None):\n        return b''\n    def decode(self, encoding='ascii', errors=None):\n        return u''\n    def capitalize(self):\n        return {rvalue}\n    def title(self):\n        return {rvalue}\n    def lower(self):\n        return {rvalue}\n    def upper(self):\n        return {rvalue}\n    def swapcase(self):\n        return {rvalue}\n    def index(self, sub, start=None, end=None):\n        return 0\n    def find(self, sub, start=None, end=None):\n        return 0\n    def count(self, sub, start=None, end=None):\n        return 0\n    def strip(self, chars=None):\n        return {rvalue}\n    def lstrip(self, chars=None):\n        return {rvalue}\n    def rstrip(self, chars=None):\n        return {rvalue}\n    def rjust(self, width, fillchar=None):\n        return {rvalue}\n    def center(self, width, fillchar=None):\n        return {rvalue}\n    def ljust(self, width, fillchar=None):\n        return {rvalue}\n\"\"\"\nBYTES_CLASS = \"\"\"\nclass whatever(object):\n    def join(self, iterable):\n        return {rvalue}\n    def replace(self, old, new, count=None):\n        return {rvalue}\n    def decode(self, encoding='ascii', errors=None):\n        return u''\n    def capitalize(self):\n        return {rvalue}\n    def title(self):\n        return {rvalue}\n    def lower(self):\n        return {rvalue}\n    def upper(self):\n        return {rvalue}\n    def swapcase(self):\n        return {rvalue}\n    def index(self, sub, start=None, end=None):\n        return 0\n    def find(self, sub, start=None, end=None):\n        return 0\n    def count(self, sub, start=None, end=None):\n        return 0\n    def strip(self, chars=None):\n        return {rvalue}\n    def lstrip(self, chars=None):\n        return {rvalue}\n    def rstrip(self, chars=None):\n        return {rvalue}\n    def rjust(self, width, fillchar=None):\n        return {rvalue}\n    def center(self, width, fillchar=None):\n        return {rvalue}\n    def ljust(self, width, fillchar=None):\n        return {rvalue}\n\"\"\"\ndef _extend_string_class(class_node, code, rvalue):\n    \"\"\"function to extend builtin str/unicode class\"\"\"\n    code = code.format(rvalue=rvalue)\n    fake = AstroidBuilder(AstroidManager()).string_build(code)[\"whatever\"]\n    for method in fake.mymethods():\n        method.parent = class_node\n        method.lineno = None\n        method.col_offset = None\n        if \"__class__\" in method.locals:\n            method.locals[\"__class__\"] = [class_node]\n        class_node.locals[method.name] = [method]\n        method.parent = class_node\ndef _extend_builtins(class_transforms):\n    builtin_ast = AstroidManager().builtins_module\n    for class_name, transform in class_transforms.items():\n        transform(builtin_ast[class_name])\n_extend_builtins(\n    {\n        \"bytes\": partial(_extend_string_class, code=BYTES_CLASS, rvalue=\"b''\"),\n        \"str\": partial(_extend_string_class, code=STR_CLASS, rvalue=\"''\"),\n    }\n)\ndef _builtin_filter_predicate(node, builtin_name):\n    if (\n        builtin_name == \"type\"\n        and node.root().name == \"re\"\n        and isinstance(node.func, nodes.Name)\n        and node.func.name == \"type\"\n        and isinstance(node.parent, nodes.Assign)\n        and len(node.parent.targets) == 1\n        and isinstance(node.parent.targets[0], nodes.AssignName)\n        and node.parent.targets[0].name in {\"Pattern\", \"Match\"}\n    ):\n        # Handle re.Pattern and re.Match in brain_re\n        # Match these patterns from stdlib/re.py\n        # ```py\n        # Pattern = type(...)\n        # Match = type(...)\n        # ```\n        return False\n    if isinstance(node.func, nodes.Name) and node.func.name == builtin_name:\n        return True\n    if isinstance(node.func, nodes.Attribute):\n        return (\n            node.func.attrname == \"fromkeys\"\n            and isinstance(node.func.expr, nodes.Name)\n            and node.func.expr.name == \"dict\"\n        )\n    return False\ndef register_builtin_transform(transform, builtin_name):\n    \"\"\"Register a new transform function for the given *builtin_name*.\n    The transform function must accept two parameters, a node and\n    an optional context.\n    \"\"\"\n    def _transform_wrapper(node, context=None):\n        result = transform(node, context=context)\n        if result:\n            if not result.parent:\n                # Let the transformation function determine\n                # the parent for its result. Otherwise,\n                # we set it to be the node we transformed from.\n                result.parent = node\n            if result.lineno is None:\n                result.lineno = node.lineno\n            # Can be a 'Module' see https://github.com/PyCQA/pylint/issues/4671\n            # We don't have a regression test on this one: tread carefully\n            if hasattr(result, \"col_offset\") and result.col_offset is None:\n                result.col_offset = node.col_offset\n        return iter([result])\n    AstroidManager().register_transform(\n        nodes.Call,\n        inference_tip(_transform_wrapper),\n        partial(_builtin_filter_predicate, builtin_name=builtin_name),\n    )\ndef _container_generic_inference(node, context, node_type, transform):\n    args = node.args\n    if not args:\n        return node_type()\n    if len(node.args) > 1:\n        raise UseInferenceDefault()\n    (arg,) = args\n    transformed = transform(arg)\n    if not transformed:\n        try:\n            inferred = next(arg.infer(context=context))\n        except (InferenceError, StopIteration) as exc:\n            raise UseInferenceDefault from exc\n        if inferred is util.Uninferable:\n            raise UseInferenceDefault\n        transformed = transform(inferred)\n    if not transformed or transformed is util.Uninferable:\n        raise UseInferenceDefault\n    return transformed\ndef _container_generic_transform(  # pylint: disable=inconsistent-return-statements\n    arg, context, klass, iterables, build_elts\n):\n    if isinstance(arg, klass):\n        return arg\n    if isinstance(arg, iterables):\n        if all(isinstance(elt, nodes.Const) for elt in arg.elts):\n            elts = [elt.value for elt in arg.elts]\n        else:\n            # TODO: Does not handle deduplication for sets.\n            elts = []\n            for element in arg.elts:\n                if not element:\n                    continue\n                inferred = helpers.safe_infer(element, context=context)\n                if inferred:\n                    evaluated_object = nodes.EvaluatedObject(\n                        original=element, value=inferred\n                    )\n                    elts.append(evaluated_object)\n    elif isinstance(arg, nodes.Dict):\n        # Dicts need to have consts as strings already.\n        if not all(isinstance(elt[0], nodes.Const) for elt in arg.items):\n            raise UseInferenceDefault()\n        elts = [item[0].value for item in arg.items]\n    elif isinstance(arg, nodes.Const) and isinstance(arg.value, (str, bytes)):\n        elts = arg.value\n    else:\n        return\n    return klass.from_elements(elts=build_elts(elts))\ndef _infer_builtin_container(\n    node, context, klass=None, iterables=None, build_elts=None\n):\n    transform_func = partial(\n        _container_generic_transform,\n        context=context,\n        klass=klass,\n        iterables=iterables,\n        build_elts=build_elts,\n    )\n    return _container_generic_inference(node, context, klass, transform_func)\n# pylint: disable=invalid-name\ninfer_tuple = partial(\n    _infer_builtin_container,\n    klass=nodes.Tuple,\n    iterables=(\n        nodes.List,\n        nodes.Set,\n        objects.FrozenSet,\n        objects.DictItems,\n        objects.DictKeys,\n        objects.DictValues,\n    ),\n    build_elts=tuple,\n)\ninfer_list = partial(\n    _infer_builtin_container,\n    klass=nodes.List,\n    iterables=(\n        nodes.Tuple,\n        nodes.Set,\n        objects.FrozenSet,\n        objects.DictItems,\n        objects.DictKeys,\n        objects.DictValues,\n    ),\n    build_elts=list,\n)\ninfer_set = partial(\n    _infer_builtin_container,\n    klass=nodes.Set,\n    iterables=(nodes.List, nodes.Tuple, objects.FrozenSet, objects.DictKeys),\n    build_elts=set,\n)\ninfer_frozenset = partial(\n    _infer_builtin_container,\n    klass=objects.FrozenSet,\n    iterables=(nodes.List, nodes.Tuple, nodes.Set, objects.FrozenSet, objects.DictKeys),\n    build_elts=frozenset,\n)\ndef _get_elts(arg, context):\n    def is_iterable(n):\n        return isinstance(n, (nodes.List, nodes.Tuple, nodes.Set))\n    try:\n        inferred = next(arg.infer(context))\n    except (InferenceError, StopIteration) as exc:\n        raise UseInferenceDefault from exc\n    if isinstance(inferred, nodes.Dict):\n        items = inferred.items\n    elif is_iterable(inferred):\n        items = []\n        for elt in inferred.elts:\n            # If an item is not a pair of two items,\n            # then fallback to the default inference.\n            # Also, take in consideration only hashable items,\n            # tuples and consts. We are choosing Names as well.\n            if not is_iterable(elt):\n                raise UseInferenceDefault()\n            if len(elt.elts) != 2:\n                raise UseInferenceDefault()\n            if not isinstance(elt.elts[0], (nodes.Tuple, nodes.Const, nodes.Name)):\n                raise UseInferenceDefault()\n            items.append(tuple(elt.elts))\n    else:\n        raise UseInferenceDefault()\n    return items\ndef infer_dict(node, context=None):\n    \"\"\"Try to infer a dict call to a Dict node.\n    The function treats the following cases:\n        * dict()\n        * dict(mapping)\n        * dict(iterable)\n        * dict(iterable, **kwargs)\n        * dict(mapping, **kwargs)\n        * dict(**kwargs)\n    If a case can't be inferred, we'll fallback to default inference.\n    \"\"\"\n    call = arguments.CallSite.from_call(node, context=context)\n    if call.has_invalid_arguments() or call.has_invalid_keywords():\n        raise UseInferenceDefault\n    args = call.positional_arguments\n    kwargs = list(call.keyword_arguments.items())\n    if not args and not kwargs:\n        # dict()\n        return nodes.Dict()\n    if kwargs and not args:\n        # dict(a=1, b=2, c=4)\n        items = [(nodes.Const(key), value) for key, value in kwargs]\n    elif len(args) == 1 and kwargs:\n        # dict(some_iterable, b=2, c=4)\n        elts = _get_elts(args[0], context)\n        keys = [(nodes.Const(key), value) for key, value in kwargs]\n        items = elts + keys\n    elif len(args) == 1:\n        items = _get_elts(args[0], context)\n    else:\n        raise UseInferenceDefault()\n    value = nodes.Dict(\n        col_offset=node.col_offset, lineno=node.lineno, parent=node.parent\n    )\n    value.postinit(items)\n    return value\ndef infer_super(node, context=None):\n    \"\"\"Understand super calls.\n    There are some restrictions for what can be understood:\n        * unbounded super (one argument form) is not understood.\n        * if the super call is not inside a function (classmethod or method),\n          then the default inference will be used.\n        * if the super arguments can't be inferred, the default inference\n          will be used.\n    \"\"\"\n    if len(node.args) == 1:\n        # Ignore unbounded super.\n        raise UseInferenceDefault\n    scope = node.scope()\n    if not isinstance(scope, nodes.FunctionDef):\n        # Ignore non-method uses of super.\n        raise UseInferenceDefault\n    if scope.type not in (\"classmethod\", \"method\"):\n        # Not interested in staticmethods.\n        raise UseInferenceDefault\n    cls = scoped_nodes.get_wrapping_class(scope)\n    if not node.args:\n        mro_pointer = cls\n        # In we are in a classmethod, the interpreter will fill\n        # automatically the class as the second argument, not an instance.\n        if scope.type == \"classmethod\":\n            mro_type = cls\n        else:\n            mro_type = cls.instantiate_class()\n    else:\n        try:\n            mro_pointer = next(node.args[0].infer(context=context))\n        except (InferenceError, StopIteration) as exc:\n            raise UseInferenceDefault from exc\n        try:\n            mro_type = next(node.args[1].infer(context=context))\n        except (InferenceError, StopIteration) as exc:\n            raise UseInferenceDefault from exc\n    if mro_pointer is util.Uninferable or mro_type is util.Uninferable:\n        # No way we could understand this.\n        raise UseInferenceDefault\n    super_obj = objects.Super(\n        mro_pointer=mro_pointer, mro_type=mro_type, self_class=cls, scope=scope\n    )\n    super_obj.parent = node\n    return super_obj\ndef _infer_getattr_args(node, context):\n    if len(node.args) not in (2, 3):\n        # Not a valid getattr call.\n        raise UseInferenceDefault\n    try:\n        obj = next(node.args[0].infer(context=context))\n        attr = next(node.args[1].infer(context=context))\n    except (InferenceError, StopIteration) as exc:\n        raise UseInferenceDefault from exc\n    if obj is util.Uninferable or attr is util.Uninferable:\n        # If one of the arguments is something we can't infer,\n        # then also make the result of the getattr call something\n        # which is unknown.\n        return util.Uninferable, util.Uninferable\n    is_string = isinstance(attr, nodes.Const) and isinstance(attr.value, str)\n    if not is_string:\n        raise UseInferenceDefault\n    return obj, attr.value\ndef infer_getattr(node, context=None):\n    \"\"\"Understand getattr calls\n    If one of the arguments is an Uninferable object, then the\n    result will be an Uninferable object. Otherwise, the normal attribute\n    lookup will be done.\n    \"\"\"\n    obj, attr = _infer_getattr_args(node, context)\n    if (\n        obj is util.Uninferable\n        or attr is util.Uninferable\n        or not hasattr(obj, \"igetattr\")\n    ):\n        return util.Uninferable\n    try:\n        return next(obj.igetattr(attr, context=context))\n    except (StopIteration, InferenceError, AttributeInferenceError):\n        if len(node.args) == 3:\n            # Try to infer the default and return it instead.\n            try:\n                return next(node.args[2].infer(context=context))\n            except (StopIteration, InferenceError) as exc:\n                raise UseInferenceDefault from exc\n    raise UseInferenceDefault\ndef infer_hasattr(node, context=None):\n    \"\"\"Understand hasattr calls\n    This always guarantees three possible outcomes for calling\n    hasattr: Const(False) when we are sure that the object\n    doesn't have the intended attribute, Const(True) when\n    we know that the object has the attribute and Uninferable\n    when we are unsure of the outcome of the function call.\n    \"\"\"\n    try:\n        obj, attr = _infer_getattr_args(node, context)\n        if (\n            obj is util.Uninferable\n            or attr is util.Uninferable\n            or not hasattr(obj, \"getattr\")\n        ):\n            return util.Uninferable\n        obj.getattr(attr, context=context)\n    except UseInferenceDefault:\n        # Can't infer something from this function call.\n        return util.Uninferable\n    except AttributeInferenceError:\n        # Doesn't have it.\n        return nodes.Const(False)\n    return nodes.Const(True)\ndef infer_callable(node, context=None):\n    \"\"\"Understand callable calls\n    This follows Python's semantics, where an object\n    is callable if it provides an attribute __call__,\n    even though that attribute is something which can't be\n    called.\n    \"\"\"\n    if len(node.args) != 1:\n        # Invalid callable call.\n        raise UseInferenceDefault\n    argument = node.args[0]\n    try:\n        inferred = next(argument.infer(context=context))\n    except (InferenceError, StopIteration):\n        return util.Uninferable\n    if inferred is util.Uninferable:\n        return util.Uninferable\n    return nodes.Const(inferred.callable())\ndef infer_property(\n    node: nodes.Call, context: InferenceContext | None = None\n) -> objects.Property:\n    \"\"\"Understand `property` class\n    This only infers the output of `property`\n    call, not the arguments themselves.\n    \"\"\"\n    if len(node.args) < 1:\n        # Invalid property call.\n        raise UseInferenceDefault\n    getter = node.args[0]\n    try:\n        inferred = next(getter.infer(context=context))\n    except (InferenceError, StopIteration) as exc:\n        raise UseInferenceDefault from exc\n    if not isinstance(inferred, (nodes.FunctionDef, nodes.Lambda)):\n        raise UseInferenceDefault\n    prop_func = objects.Property(\n        function=inferred,\n        name=inferred.name,\n        lineno=node.lineno,\n        parent=node,\n        col_offset=node.col_offset,\n    )\n    prop_func.postinit(\n        body=[],\n        args=inferred.args,\n        doc_node=getattr(inferred, \"doc_node\", None),\n    )\n    return prop_func\ndef infer_bool(node, context=None):\n    \"\"\"Understand bool calls.\"\"\"\n    if len(node.args) > 1:\n        # Invalid bool call.\n        raise UseInferenceDefault\n    if not node.args:\n        return nodes.Const(False)\n    argument = node.args[0]",
        "file_path": "astroid/brain/brain_builtin_inference.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 5,
        "score": 0.5850207805633545,
        "content": "# Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n# For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n# Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n\"\"\"Astroid hooks for the Python standard library.\"\"\"\nfrom __future__ import annotations\nimport functools\nimport keyword\nfrom collections.abc import Iterator\nfrom textwrap import dedent\nimport astroid\nfrom astroid import arguments, bases, inference_tip, nodes, util\nfrom astroid.builder import AstroidBuilder, _extract_single_node, extract_node\nfrom astroid.context import InferenceContext\nfrom astroid.exceptions import (\n    AstroidTypeError,\n    AstroidValueError,\n    InferenceError,\n    MroError,\n    UseInferenceDefault,\n)\nfrom astroid.manager import AstroidManager\nTYPING_NAMEDTUPLE_BASENAMES = {\"NamedTuple\", \"typing.NamedTuple\"}\nENUM_BASE_NAMES = {\n    \"Enum\",\n    \"IntEnum\",\n    \"enum.Enum\",\n    \"enum.IntEnum\",\n    \"IntFlag\",\n    \"enum.IntFlag\",\n}\ndef _infer_first(node, context):\n    if node is util.Uninferable:\n        raise UseInferenceDefault\n    try:\n        value = next(node.infer(context=context))\n    except StopIteration as exc:\n        raise InferenceError from exc\n    if value is util.Uninferable:\n        raise UseInferenceDefault()\n    return value\ndef _find_func_form_arguments(node, context):\n    def _extract_namedtuple_arg_or_keyword(  # pylint: disable=inconsistent-return-statements\n        position, key_name=None\n    ):\n        if len(args) > position:\n            return _infer_first(args[position], context)\n        if key_name and key_name in found_keywords:\n            return _infer_first(found_keywords[key_name], context)\n    args = node.args\n    keywords = node.keywords\n    found_keywords = (\n        {keyword.arg: keyword.value for keyword in keywords} if keywords else {}\n    )\n    name = _extract_namedtuple_arg_or_keyword(position=0, key_name=\"typename\")\n    names = _extract_namedtuple_arg_or_keyword(position=1, key_name=\"field_names\")\n    if name and names:\n        return name.value, names\n    raise UseInferenceDefault()\ndef infer_func_form(\n    node: nodes.Call,\n    base_type: list[nodes.NodeNG],\n    context: InferenceContext | None = None,\n    enum: bool = False,\n) -> tuple[nodes.ClassDef, str, list[str]]:\n    \"\"\"Specific inference function for namedtuple or Python 3 enum.\"\"\"\n    # node is a Call node, class name as first argument and generated class\n    # attributes as second argument\n    # namedtuple or enums list of attributes can be a list of strings or a\n    # whitespace-separate string\n    try:\n        name, names = _find_func_form_arguments(node, context)\n        try:\n            attributes: list[str] = names.value.replace(\",\", \" \").split()\n        except AttributeError as exc:\n            # Handle attributes of NamedTuples\n            if not enum:\n                attributes = []\n                fields = _get_namedtuple_fields(node)\n                if fields:\n                    fields_node = extract_node(fields)\n                    attributes = [\n                        _infer_first(const, context).value for const in fields_node.elts\n                    ]\n            # Handle attributes of Enums\n            else:\n                # Enums supports either iterator of (name, value) pairs\n                # or mappings.\n                if hasattr(names, \"items\") and isinstance(names.items, list):\n                    attributes = [\n                        _infer_first(const[0], context).value\n                        for const in names.items\n                        if isinstance(const[0], nodes.Const)\n                    ]\n                elif hasattr(names, \"elts\"):\n                    # Enums can support either [\"a\", \"b\", \"c\"]\n                    # or [(\"a\", 1), (\"b\", 2), ...], but they can't\n                    # be mixed.\n                    if all(isinstance(const, nodes.Tuple) for const in names.elts):\n                        attributes = [\n                            _infer_first(const.elts[0], context).value\n                            for const in names.elts\n                            if isinstance(const, nodes.Tuple)\n                        ]\n                    else:\n                        attributes = [\n                            _infer_first(const, context).value for const in names.elts\n                        ]\n                else:\n                    raise AttributeError from exc\n                if not attributes:\n                    raise AttributeError from exc\n    except (AttributeError, InferenceError) as exc:\n        raise UseInferenceDefault from exc\n    if not enum:\n        # namedtuple maps sys.intern(str()) over over field_names\n        attributes = [str(attr) for attr in attributes]\n        # XXX this should succeed *unless* __str__/__repr__ is incorrect or throws\n        # in which case we should not have inferred these values and raised earlier\n    attributes = [attr for attr in attributes if \" \" not in attr]\n    # If we can't infer the name of the class, don't crash, up to this point\n    # we know it is a namedtuple anyway.\n    name = name or \"Uninferable\"\n    # we want to return a Class node instance with proper attributes set\n    class_node = nodes.ClassDef(name)\n    # A typical ClassDef automatically adds its name to the parent scope,\n    # but doing so causes problems, so defer setting parent until after init\n    # see: https://github.com/PyCQA/pylint/issues/5982\n    class_node.parent = node.parent\n    class_node.postinit(\n        # set base class=tuple\n        bases=base_type,\n        body=[],\n        decorators=None,\n    )\n    # XXX add __init__(*attributes) method\n    for attr in attributes:\n        fake_node = nodes.EmptyNode()\n        fake_node.parent = class_node\n        fake_node.attrname = attr\n        class_node.instance_attrs[attr] = [fake_node]\n    return class_node, name, attributes\ndef _has_namedtuple_base(node):\n    \"\"\"Predicate for class inference tip\n    :type node: ClassDef\n    :rtype: bool\n    \"\"\"\n    return set(node.basenames) & TYPING_NAMEDTUPLE_BASENAMES\ndef _looks_like(node, name):\n    func = node.func\n    if isinstance(func, nodes.Attribute):\n        return func.attrname == name\n    if isinstance(func, nodes.Name):\n        return func.name == name\n    return False\n_looks_like_namedtuple = functools.partial(_looks_like, name=\"namedtuple\")\n_looks_like_enum = functools.partial(_looks_like, name=\"Enum\")\n_looks_like_typing_namedtuple = functools.partial(_looks_like, name=\"NamedTuple\")\ndef infer_named_tuple(\n    node: nodes.Call, context: InferenceContext | None = None\n) -> Iterator[nodes.ClassDef]:\n    \"\"\"Specific inference function for namedtuple Call node\"\"\"\n    tuple_base_name: list[nodes.NodeNG] = [nodes.Name(name=\"tuple\", parent=node.root())]\n    class_node, name, attributes = infer_func_form(\n        node, tuple_base_name, context=context\n    )\n    call_site = arguments.CallSite.from_call(node, context=context)\n    node = extract_node(\"import collections; collections.namedtuple\")\n    try:\n        func = next(node.infer())\n    except StopIteration as e:\n        raise InferenceError(node=node) from e\n    try:\n        rename = next(call_site.infer_argument(func, \"rename\", context)).bool_value()\n    except (InferenceError, StopIteration):\n        rename = False\n    try:\n        attributes = _check_namedtuple_attributes(name, attributes, rename)\n    except AstroidTypeError as exc:\n        raise UseInferenceDefault(\"TypeError: \" + str(exc)) from exc\n    except AstroidValueError as exc:\n        raise UseInferenceDefault(\"ValueError: \" + str(exc)) from exc\n    replace_args = \", \".join(f\"{arg}=None\" for arg in attributes)\n    field_def = (\n        \"    {name} = property(lambda self: self[{index:d}], \"\n        \"doc='Alias for field number {index:d}')\"\n    )\n    field_defs = \"\\n\".join(\n        field_def.format(name=name, index=index)\n        for index, name in enumerate(attributes)\n    )\n    fake = AstroidBuilder(AstroidManager()).string_build(\n        f\"\"\"\nclass {name}(tuple):\n    __slots__ = ()\n    _fields = {attributes!r}\n    def _asdict(self):\n        return self.__dict__\n    @classmethod\n    def _make(cls, iterable, new=tuple.__new__, len=len):\n        return new(cls, iterable)\n    def _replace(self, {replace_args}):\n        return self\n    def __getnewargs__(self):\n        return tuple(self)\n{field_defs}\n    \"\"\"\n    )\n    class_node.locals[\"_asdict\"] = fake.body[0].locals[\"_asdict\"]\n    class_node.locals[\"_make\"] = fake.body[0].locals[\"_make\"]\n    class_node.locals[\"_replace\"] = fake.body[0].locals[\"_replace\"]\n    class_node.locals[\"_fields\"] = fake.body[0].locals[\"_fields\"]\n    for attr in attributes:\n        class_node.locals[attr] = fake.body[0].locals[attr]\n    # we use UseInferenceDefault, we can't be a generator so return an iterator\n    return iter([class_node])\ndef _get_renamed_namedtuple_attributes(field_names):\n    names = list(field_names)\n    seen = set()\n    for i, name in enumerate(field_names):\n        if (\n            not all(c.isalnum() or c == \"_\" for c in name)\n            or keyword.iskeyword(name)\n            or not name\n            or name[0].isdigit()\n            or name.startswith(\"_\")\n            or name in seen\n        ):\n            names[i] = \"_%d\" % i\n        seen.add(name)\n    return tuple(names)\ndef _check_namedtuple_attributes(typename, attributes, rename=False):\n    attributes = tuple(attributes)\n    if rename:\n        attributes = _get_renamed_namedtuple_attributes(attributes)\n    # The following snippet is derived from the CPython Lib/collections/__init__.py sources\n    # <snippet>\n    for name in (typename,) + attributes:\n        if not isinstance(name, str):\n            raise AstroidTypeError(\"Type names and field names must be strings\")\n        if not name.isidentifier():\n            raise AstroidValueError(\n                \"Type names and field names must be valid\" + f\"identifiers: {name!r}\"\n            )\n        if keyword.iskeyword(name):\n            raise AstroidValueError(\n                f\"Type names and field names cannot be a keyword: {name!r}\"\n            )\n    seen = set()\n    for name in attributes:\n        if name.startswith(\"_\") and not rename:\n            raise AstroidValueError(\n                f\"Field names cannot start with an underscore: {name!r}\"\n            )\n        if name in seen:\n            raise AstroidValueError(f\"Encountered duplicate field name: {name!r}\")\n        seen.add(name)\n    # </snippet>\n    return attributes\ndef infer_enum(\n    node: nodes.Call, context: InferenceContext | None = None\n) -> Iterator[bases.Instance]:\n    \"\"\"Specific inference function for enum Call node.\"\"\"\n    enum_meta = _extract_single_node(\n        \"\"\"\n    class EnumMeta(object):\n        'docstring'\n        def __call__(self, node):\n            class EnumAttribute(object):\n                name = ''\n                value = 0\n            return EnumAttribute()\n        def __iter__(self):\n            class EnumAttribute(object):\n                name = ''\n                value = 0\n            return [EnumAttribute()]\n        def __reversed__(self):\n            class EnumAttribute(object):\n                name = ''\n                value = 0\n            return (EnumAttribute, )\n        def __next__(self):\n            return next(iter(self))\n        def __getitem__(self, attr):\n            class Value(object):\n                @property\n                def name(self):\n                    return ''\n                @property\n                def value(self):\n                    return attr\n            return Value()\n        __members__ = ['']\n    \"\"\"\n    )\n    class_node = infer_func_form(node, [enum_meta], context=context, enum=True)[0]\n    return iter([class_node.instantiate_class()])\nINT_FLAG_ADDITION_METHODS = \"\"\"\n    def __or__(self, other):\n        return {name}(self.value | other.value)\n    def __and__(self, other):\n        return {name}(self.value & other.value)\n    def __xor__(self, other):\n        return {name}(self.value ^ other.value)\n    def __add__(self, other):\n        return {name}(self.value + other.value)\n    def __div__(self, other):\n        return {name}(self.value / other.value)\n    def __invert__(self):\n        return {name}(~self.value)\n    def __mul__(self, other):\n        return {name}(self.value * other.value)\n\"\"\"\ndef infer_enum_class(node: nodes.ClassDef) -> nodes.ClassDef:\n    \"\"\"Specific inference for enums.\"\"\"\n    for basename in (b for cls in node.mro() for b in cls.basenames):\n        if node.root().name == \"enum\":\n            # Skip if the class is directly from enum module.\n            break\n        dunder_members = {}\n        target_names = set()\n        for local, values in node.locals.items():\n            if any(not isinstance(value, nodes.AssignName) for value in values):\n                continue\n            stmt = values[0].statement(future=True)\n            if isinstance(stmt, nodes.Assign):\n                if isinstance(stmt.targets[0], nodes.Tuple):\n                    targets = stmt.targets[0].itered()\n                else:\n                    targets = stmt.targets\n            elif isinstance(stmt, nodes.AnnAssign):\n                targets = [stmt.target]\n            else:\n                continue\n            inferred_return_value = None\n            if stmt.value is not None:\n                if isinstance(stmt.value, nodes.Const):\n                    if isinstance(stmt.value.value, str):\n                        inferred_return_value = repr(stmt.value.value)\n                    else:\n                        inferred_return_value = stmt.value.value\n                else:\n                    inferred_return_value = stmt.value.as_string()\n            new_targets = []\n            for target in targets:\n                if isinstance(target, nodes.Starred):\n                    continue\n                target_names.add(target.name)\n                # Replace all the assignments with our mocked class.\n                classdef = dedent(\n                    \"\"\"\n                class {name}({types}):\n                    @property\n                    def value(self):\n                        return {return_value}\n                    @property\n                    def name(self):\n                        return \"{name}\"\n                \"\"\".format(\n                        name=target.name,\n                        types=\", \".join(node.basenames),\n                        return_value=inferred_return_value,\n                    )\n                )\n                if \"IntFlag\" in basename:\n                    # Alright, we need to add some additional methods.\n                    # Unfortunately we still can't infer the resulting objects as\n                    # Enum members, but once we'll be able to do that, the following\n                    # should result in some nice symbolic execution\n                    classdef += INT_FLAG_ADDITION_METHODS.format(name=target.name)\n                fake = AstroidBuilder(\n                    AstroidManager(), apply_transforms=False\n                ).string_build(classdef)[target.name]\n                fake.parent = target.parent\n                for method in node.mymethods():\n                    fake.locals[method.name] = [method]\n                new_targets.append(fake.instantiate_class())\n                dunder_members[local] = fake\n            node.locals[local] = new_targets\n        # The undocumented `_value2member_map_` member:\n        node.locals[\"_value2member_map_\"] = [nodes.Dict(parent=node)]\n        members = nodes.Dict(parent=node)\n        members.postinit(\n            [\n                (nodes.Const(k, parent=members), nodes.Name(v.name, parent=members))\n                for k, v in dunder_members.items()\n            ]\n        )\n        node.locals[\"__members__\"] = [members]\n        # The enum.Enum class itself defines two @DynamicClassAttribute data-descriptors\n        # \"name\" and \"value\" (which we override in the mocked class for each enum member\n        # above). When dealing with inference of an arbitrary instance of the enum\n        # class, e.g. in a method defined in the class body like:\n        #     class SomeEnum(enum.Enum):\n        #         def method(self):\n        #             self.name  # <- here\n        # In the absence of an enum member called \"name\" or \"value\", these attributes\n        # should resolve to the descriptor on that particular instance, i.e. enum member.\n        # For \"value\", we have no idea what that should be, but for \"name\", we at least\n        # know that it should be a string, so infer that as a guess.\n        if \"name\" not in target_names:\n            code = dedent(\n                \"\"\"\n            @property\n            def name(self):\n                return ''\n            \"\"\"\n            )\n            name_dynamicclassattr = AstroidBuilder(AstroidManager()).string_build(code)[\n                \"name\"\n            ]\n            node.locals[\"name\"] = [name_dynamicclassattr]\n        break\n    return node\ndef infer_typing_namedtuple_class(class_node, context=None):\n    \"\"\"Infer a subclass of typing.NamedTuple\"\"\"\n    # Check if it has the corresponding bases\n    annassigns_fields = [\n        annassign.target.name\n        for annassign in class_node.body\n        if isinstance(annassign, nodes.AnnAssign)\n    ]\n    code = dedent(\n        \"\"\"\n    from collections import namedtuple\n    namedtuple({typename!r}, {fields!r})\n    \"\"\"\n    ).format(typename=class_node.name, fields=\",\".join(annassigns_fields))\n    node = extract_node(code)\n    try:\n        generated_class_node = next(infer_named_tuple(node, context))\n    except StopIteration as e:\n        raise InferenceError(node=node, context=context) from e\n    for method in class_node.mymethods():\n        generated_class_node.locals[method.name] = [method]\n    for body_node in class_node.body:\n        if isinstance(body_node, nodes.Assign):\n            for target in body_node.targets:\n                attr = target.name\n                generated_class_node.locals[attr] = class_node.locals[attr]\n        elif isinstance(body_node, nodes.ClassDef):\n            generated_class_node.locals[body_node.name] = [body_node]\n    return iter((generated_class_node,))\ndef infer_typing_namedtuple_function(node, context=None):\n    \"\"\"\n    Starting with python3.9, NamedTuple is a function of the typing module.\n    The class NamedTuple is build dynamically through a call to `type` during\n    initialization of the `_NamedTuple` variable.\n    \"\"\"\n    klass = extract_node(\n        \"\"\"\n        from typing import _NamedTuple\n        _NamedTuple\n        \"\"\"\n    )\n    return klass.infer(context)\ndef infer_typing_namedtuple(\n    node: nodes.Call, context: InferenceContext | None = None\n) -> Iterator[nodes.ClassDef]:\n    \"\"\"Infer a typing.NamedTuple(...) call.\"\"\"\n    # This is essentially a namedtuple with different arguments\n    # so we extract the args and infer a named tuple.\n    try:\n        func = next(node.func.infer())\n    except (InferenceError, StopIteration) as exc:\n        raise UseInferenceDefault from exc\n    if func.qname() != \"typing.NamedTuple\":\n        raise UseInferenceDefault\n    if len(node.args) != 2:\n        raise UseInferenceDefault\n    if not isinstance(node.args[1], (nodes.List, nodes.Tuple)):\n        raise UseInferenceDefault\n    return infer_named_tuple(node, context)\ndef _get_namedtuple_fields(node: nodes.Call) -> str:\n    \"\"\"Get and return fields of a NamedTuple in code-as-a-string.\n    Because the fields are represented in their code form we can\n    extract a node from them later on.\n    \"\"\"\n    names = []\n    container = None\n    try:\n        container = next(node.args[1].infer())\n    except (InferenceError, StopIteration) as exc:\n        raise UseInferenceDefault from exc\n    # We pass on IndexError as we'll try to infer 'field_names' from the keywords\n    except IndexError:\n        pass\n    if not container:\n        for keyword_node in node.keywords:\n            if keyword_node.arg == \"field_names\":\n                try:\n                    container = next(keyword_node.value.infer())\n                except (InferenceError, StopIteration) as exc:\n                    raise UseInferenceDefault from exc\n                break\n    if not isinstance(container, nodes.BaseContainer):\n        raise UseInferenceDefault\n    for elt in container.elts:\n        if isinstance(elt, nodes.Const):\n            names.append(elt.as_string())",
        "file_path": "astroid/brain/brain_namedtuple_enum.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 6,
        "score": 0.567794144153595,
        "content": "# Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n# For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n# Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n\"\"\"\nAstroid hooks for type support.\nStarting from python3.9, type object behaves as it had __class_getitem__ method.\nHowever it was not possible to simply add this method inside type's body, otherwise\nall types would also have this method. In this case it would have been possible\nto write str[int].\nGuido Van Rossum proposed a hack to handle this in the interpreter:\nhttps://github.com/python/cpython/blob/67e394562d67cbcd0ac8114e5439494e7645b8f5/Objects/abstract.c#L181-L184\nThis brain follows the same logic. It is no wise to add permanently the __class_getitem__ method\nto the type object. Instead we choose to add it only in the case of a subscript node\nwhich inside name node is type.\nDoing this type[int] is allowed whereas str[int] is not.\nThanks to Lukasz Langa for fruitful discussion.\n\"\"\"\nfrom astroid import extract_node, inference_tip, nodes\nfrom astroid.const import PY39_PLUS\nfrom astroid.exceptions import UseInferenceDefault\nfrom astroid.manager import AstroidManager\ndef _looks_like_type_subscript(node):\n    \"\"\"\n    Try to figure out if a Name node is used inside a type related subscript\n    :param node: node to check\n    :type node: astroid.nodes.node_classes.NodeNG\n    :return: true if the node is a Name node inside a type related subscript\n    :rtype: bool\n    \"\"\"\n    if isinstance(node, nodes.Name) and isinstance(node.parent, nodes.Subscript):\n        return node.name == \"type\"\n    return False\ndef infer_type_sub(node, context=None):\n    \"\"\"\n    Infer a type[...] subscript\n    :param node: node to infer\n    :type node: astroid.nodes.node_classes.NodeNG\n    :param context: inference context\n    :type context: astroid.context.InferenceContext\n    :return: the inferred node\n    :rtype: nodes.NodeNG\n    \"\"\"\n    node_scope, _ = node.scope().lookup(\"type\")\n    if not isinstance(node_scope, nodes.Module) or node_scope.qname() != \"builtins\":\n        raise UseInferenceDefault()\n    class_src = \"\"\"\n    class type:\n        def __class_getitem__(cls, key):\n            return cls\n     \"\"\"\n    node = extract_node(class_src)\n    return node.infer(context=context)\nif PY39_PLUS:\n    AstroidManager().register_transform(\n        nodes.Name, inference_tip(infer_type_sub), _looks_like_type_subscript\n    )",
        "file_path": "astroid/brain/brain_type.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 7,
        "score": 0.5673331618309021,
        "content": "# Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n# For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n# Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\nfrom __future__ import annotations\nimport sys\nfrom typing import TYPE_CHECKING, Any, Callable, Union\nif TYPE_CHECKING:\n    from astroid import bases, exceptions, nodes, transforms, util\n    from astroid.context import InferenceContext\n    from astroid.interpreter._import import spec\nif sys.version_info >= (3, 8):\n    from typing import TypedDict\nelse:\n    from typing_extensions import TypedDict\nclass InferenceErrorInfo(TypedDict):\n    \"\"\"Store additional Inference error information\n    raised with StopIteration exception.\n    \"\"\"\n    node: nodes.NodeNG\n    context: InferenceContext | None\nInferFn = Callable[..., Any]\nclass AstroidManagerBrain(TypedDict):\n    \"\"\"Dictionary to store relevant information for a AstroidManager class.\"\"\"\n    astroid_cache: dict[str, nodes.Module]\n    _mod_file_cache: dict[\n        tuple[str, str | None], spec.ModuleSpec | exceptions.AstroidImportError\n    ]\n    _failed_import_hooks: list[Callable[[str], nodes.Module]]\n    always_load_extensions: bool\n    optimize_ast: bool\n    extension_package_whitelist: set[str]\n    _transform: transforms.TransformVisitor\nInferenceResult = Union[\"nodes.NodeNG\", \"type[util.Uninferable]\", \"bases.Proxy\"]\nSuccessfulInferenceResult = Union[\"nodes.NodeNG\", \"bases.Proxy\"]\nConstFactoryResult = Union[\n    \"nodes.List\",\n    \"nodes.Set\",\n    \"nodes.Tuple\",\n    \"nodes.Dict\",\n    \"nodes.Const\",\n    \"nodes.EmptyNode\",\n]",
        "file_path": "astroid/typing.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 8,
        "score": 0.5622036457061768,
        "content": "# Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n# For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n# Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n\"\"\"this module contains exceptions used in the astroid library\n\"\"\"\nfrom __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom astroid import util\nif TYPE_CHECKING:\n    from astroid import arguments, bases, nodes, objects\n    from astroid.context import InferenceContext\n__all__ = (\n    \"AstroidBuildingError\",\n    \"AstroidBuildingException\",\n    \"AstroidError\",\n    \"AstroidImportError\",\n    \"AstroidIndexError\",\n    \"AstroidSyntaxError\",\n    \"AstroidTypeError\",\n    \"AstroidValueError\",\n    \"AttributeInferenceError\",\n    \"BinaryOperationError\",\n    \"DuplicateBasesError\",\n    \"InconsistentMroError\",\n    \"InferenceError\",\n    \"InferenceOverwriteError\",\n    \"MroError\",\n    \"NameInferenceError\",\n    \"NoDefault\",\n    \"NotFoundError\",\n    \"OperationError\",\n    \"ParentMissingError\",\n    \"ResolveError\",\n    \"StatementMissing\",\n    \"SuperArgumentTypeError\",\n    \"SuperError\",\n    \"TooManyLevelsError\",\n    \"UnaryOperationError\",\n    \"UnresolvableName\",\n    \"UseInferenceDefault\",\n)\nclass AstroidError(Exception):\n    \"\"\"base exception class for all astroid related exceptions\n    AstroidError and its subclasses are structured, intended to hold\n    objects representing state when the exception is thrown.  Field\n    values are passed to the constructor as keyword-only arguments.\n    Each subclass has its own set of standard fields, but use your\n    best judgment to decide whether a specific exception instance\n    needs more or fewer fields for debugging.  Field values may be\n    used to lazily generate the error message: self.message.format()\n    will be called with the field names and values supplied as keyword\n    arguments.\n    \"\"\"\n    def __init__(self, message: str = \"\", **kws: Any) -> None:\n        super().__init__(message)\n        self.message = message\n        for key, value in kws.items():\n            setattr(self, key, value)\n    def __str__(self) -> str:\n        return self.message.format(**vars(self))\nclass AstroidBuildingError(AstroidError):\n    \"\"\"exception class when we are unable to build an astroid representation\n    Standard attributes:\n        modname: Name of the module that AST construction failed for.\n        error: Exception raised during construction.\n    \"\"\"\n    def __init__(\n        self,\n        message: str = \"Failed to import module {modname}.\",\n        modname: str | None = None,\n        error: Exception | None = None,\n        source: str | None = None,\n        path: str | None = None,\n        cls: type | None = None,\n        class_repr: str | None = None,\n        **kws: Any,\n    ) -> None:\n        self.modname = modname\n        self.error = error\n        self.source = source\n        self.path = path\n        self.cls = cls\n        self.class_repr = class_repr\n        super().__init__(message, **kws)\nclass AstroidImportError(AstroidBuildingError):\n    \"\"\"Exception class used when a module can't be imported by astroid.\"\"\"\nclass TooManyLevelsError(AstroidImportError):\n    \"\"\"Exception class which is raised when a relative import was beyond the top-level.\n    Standard attributes:\n        level: The level which was attempted.\n        name: the name of the module on which the relative import was attempted.\n    \"\"\"\n    def __init__(\n        self,\n        message: str = \"Relative import with too many levels \"\n        \"({level}) for module {name!r}\",\n        level: int | None = None,\n        name: str | None = None,\n        **kws: Any,\n    ) -> None:\n        self.level = level\n        self.name = name\n        super().__init__(message, **kws)\nclass AstroidSyntaxError(AstroidBuildingError):\n    \"\"\"Exception class used when a module can't be parsed.\"\"\"\n    def __init__(\n        self,\n        message: str,\n        modname: str | None,\n        error: Exception,\n        path: str | None,\n        source: str | None = None,\n    ) -> None:\n        super().__init__(message, modname, error, source, path)\nclass NoDefault(AstroidError):\n    \"\"\"raised by function's `default_value` method when an argument has\n    no default value\n    Standard attributes:\n        func: Function node.\n        name: Name of argument without a default.\n    \"\"\"\n    def __init__(\n        self,\n        message: str = \"{func!r} has no default for {name!r}.\",\n        func: nodes.FunctionDef | None = None,\n        name: str | None = None,\n        **kws: Any,\n    ) -> None:\n        self.func = func\n        self.name = name\n        super().__init__(message, **kws)\nclass ResolveError(AstroidError):\n    \"\"\"Base class of astroid resolution/inference error.\n    ResolveError is not intended to be raised.\n    Standard attributes:\n        context: InferenceContext object.\n    \"\"\"\n    def __init__(\n        self, message: str = \"\", context: InferenceContext | None = None, **kws: Any\n    ) -> None:\n        self.context = context\n        super().__init__(message, **kws)\nclass MroError(ResolveError):\n    \"\"\"Error raised when there is a problem with method resolution of a class.\n    Standard attributes:\n        mros: A sequence of sequences containing ClassDef nodes.\n        cls: ClassDef node whose MRO resolution failed.\n        context: InferenceContext object.\n    \"\"\"\n    def __init__(\n        self,\n        message: str,\n        mros: list[nodes.ClassDef],\n        cls: nodes.ClassDef,\n        context: InferenceContext | None = None,\n        **kws: Any,\n    ) -> None:\n        self.mros = mros\n        self.cls = cls\n        self.context = context\n        super().__init__(message, **kws)\n    def __str__(self) -> str:\n        mro_names = \", \".join(f\"({', '.join(b.name for b in m)})\" for m in self.mros)\n        return self.message.format(mros=mro_names, cls=self.cls)\nclass DuplicateBasesError(MroError):\n    \"\"\"Error raised when there are duplicate bases in the same class bases.\"\"\"\nclass InconsistentMroError(MroError):\n    \"\"\"Error raised when a class's MRO is inconsistent.\"\"\"\nclass SuperError(ResolveError):\n    \"\"\"Error raised when there is a problem with a *super* call.\n    Standard attributes:\n        *super_*: The Super instance that raised the exception.\n        context: InferenceContext object.\n    \"\"\"\n    def __init__(self, message: str, super_: objects.Super, **kws: Any) -> None:\n        self.super_ = super_\n        super().__init__(message, **kws)\n    def __str__(self) -> str:\n        return self.message.format(**vars(self.super_))\nclass InferenceError(ResolveError):  # pylint: disable=too-many-instance-attributes\n    \"\"\"raised when we are unable to infer a node\n    Standard attributes:\n        node: The node inference was called on.\n        context: InferenceContext object.\n    \"\"\"\n    def __init__(  # pylint: disable=too-many-arguments\n        self,\n        message: str = \"Inference failed for {node!r}.\",\n        node: nodes.NodeNG | bases.Instance | None = None,\n        context: InferenceContext | None = None,\n        target: nodes.NodeNG | bases.Instance | None = None,\n        targets: nodes.Tuple | None = None,\n        attribute: str | None = None,\n        unknown: nodes.NodeNG | bases.Instance | None = None,\n        assign_path: list[int] | None = None,\n        caller: nodes.Call | None = None,\n        stmts: Sequence[nodes.NodeNG | bases.Instance] | None = None,\n        frame: nodes.LocalsDictNodeNG | None = None,\n        call_site: arguments.CallSite | None = None,\n        func: nodes.FunctionDef | None = None,\n        arg: str | None = None,\n        positional_arguments: list | None = None,\n        unpacked_args: list | None = None,\n        keyword_arguments: dict | None = None,\n        unpacked_kwargs: dict | None = None,\n        **kws: Any,\n    ) -> None:\n        self.node = node\n        self.context = context\n        self.target = target\n        self.targets = targets\n        self.attribute = attribute\n        self.unknown = unknown\n        self.assign_path = assign_path\n        self.caller = caller\n        self.stmts = stmts\n        self.frame = frame\n        self.call_site = call_site\n        self.func = func\n        self.arg = arg\n        self.positional_arguments = positional_arguments\n        self.unpacked_args = unpacked_args\n        self.keyword_arguments = keyword_arguments\n        self.unpacked_kwargs = unpacked_kwargs\n        super().__init__(message, **kws)\n# Why does this inherit from InferenceError rather than ResolveError?\n# Changing it causes some inference tests to fail.\nclass NameInferenceError(InferenceError):\n    \"\"\"Raised when a name lookup fails, corresponds to NameError.\n    Standard attributes:\n        name: The name for which lookup failed, as a string.\n        scope: The node representing the scope in which the lookup occurred.\n        context: InferenceContext object.\n    \"\"\"\n    def __init__(\n        self,\n        message: str = \"{name!r} not found in {scope!r}.\",\n        name: str | None = None,\n        scope: nodes.LocalsDictNodeNG | None = None,\n        context: InferenceContext | None = None,\n        **kws: Any,\n    ) -> None:\n        self.name = name\n        self.scope = scope\n        self.context = context\n        super().__init__(message, **kws)\nclass AttributeInferenceError(ResolveError):\n    \"\"\"Raised when an attribute lookup fails, corresponds to AttributeError.\n    Standard attributes:\n        target: The node for which lookup failed.\n        attribute: The attribute for which lookup failed, as a string.\n        context: InferenceContext object.\n    \"\"\"\n    def __init__(\n        self,\n        message: str = \"{attribute!r} not found on {target!r}.\",\n        attribute: str = \"\",\n        target: nodes.NodeNG | bases.Instance | None = None,\n        context: InferenceContext | None = None,\n        mros: list[nodes.ClassDef] | None = None,\n        super_: nodes.ClassDef | None = None,\n        cls: nodes.ClassDef | None = None,\n        **kws: Any,\n    ) -> None:\n        self.attribute = attribute\n        self.target = target\n        self.context = context\n        self.mros = mros\n        self.super_ = super_\n        self.cls = cls\n        super().__init__(message, **kws)\nclass UseInferenceDefault(Exception):\n    \"\"\"exception to be raised in custom inference function to indicate that it\n    should go back to the default behaviour\n    \"\"\"\nclass _NonDeducibleTypeHierarchy(Exception):\n    \"\"\"Raised when is_subtype / is_supertype can't deduce the relation between two types.\"\"\"\nclass AstroidIndexError(AstroidError):\n    \"\"\"Raised when an Indexable / Mapping does not have an index / key.\"\"\"\n    def __init__(\n        self,\n        message: str = \"\",\n        node: nodes.NodeNG | bases.Instance | None = None,\n        index: nodes.Subscript | None = None,\n        context: InferenceContext | None = None,\n        **kws: Any,\n    ) -> None:\n        self.node = node\n        self.index = index\n        self.context = context\n        super().__init__(message, **kws)\nclass AstroidTypeError(AstroidError):\n    \"\"\"Raised when a TypeError would be expected in Python code.\"\"\"\n    def __init__(\n        self,\n        message: str = \"\",\n        node: nodes.NodeNG | bases.Instance | None = None,\n        index: nodes.Subscript | None = None,\n        context: InferenceContext | None = None,\n        **kws: Any,\n    ) -> None:\n        self.node = node\n        self.index = index\n        self.context = context\n        super().__init__(message, **kws)\nclass AstroidValueError(AstroidError):\n    \"\"\"Raised when a ValueError would be expected in Python code.\"\"\"\nclass InferenceOverwriteError(AstroidError):\n    \"\"\"Raised when an inference tip is overwritten\n    Currently only used for debugging.\n    \"\"\"\nclass ParentMissingError(AstroidError):\n    \"\"\"Raised when a node which is expected to have a parent attribute is missing one\n    Standard attributes:\n        target: The node for which the parent lookup failed.\n    \"\"\"\n    def __init__(self, target: nodes.NodeNG) -> None:\n        self.target = target\n        super().__init__(message=f\"Parent not found on {target!r}.\")\nclass StatementMissing(ParentMissingError):\n    \"\"\"Raised when a call to node.statement() does not return a node. This is because\n    a node in the chain does not have a parent attribute and therefore does not\n    return a node for statement().\n    Standard attributes:\n        target: The node for which the parent lookup failed.\n    \"\"\"\n    def __init__(self, target: nodes.NodeNG) -> None:\n        super(ParentMissingError, self).__init__(\n            message=f\"Statement not found on {target!r}\"\n        )\n# Backwards-compatibility aliases\nOperationError = util.BadOperationMessage\nUnaryOperationError = util.BadUnaryOperationMessage\nBinaryOperationError = util.BadBinaryOperationMessage\nSuperArgumentTypeError = SuperError\nUnresolvableName = NameInferenceError\nNotFoundError = AttributeInferenceError\nAstroidBuildingException = AstroidBuildingError",
        "file_path": "astroid/exceptions.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 9,
        "score": 0.5595486164093018,
        "content": "        if \"name\" not in target_names:\n            code = dedent(\n                \"\"\"\n            @property\n            def name(self):\n                return ''\n            \"\"\"\n            )\n            name_dynamicclassattr = AstroidBuilder(AstroidManager()).string_build(code)[\n                \"name\"\n            ]\n            node.locals[\"name\"] = [name_dynamicclassattr]\n        break\n    return node\ndef infer_typing_namedtuple_class(class_node, context=None):\n    \"\"\"Infer a subclass of typing.NamedTuple\"\"\"\n    # Check if it has the corresponding bases\n    annassigns_fields = [\n        annassign.target.name\n        for annassign in class_node.body\n        if isinstance(annassign, nodes.AnnAssign)\n    ]\n    code = dedent(\n        \"\"\"\n    from collections import namedtuple\n    namedtuple({typename!r}, {fields!r})\n    \"\"\"\n    ).format(typename=class_node.name, fields=\",\".join(annassigns_fields))\n    node = extract_node(code)\n    try:\n        generated_class_node = next(infer_named_tuple(node, context))\n    except StopIteration as e:\n        raise InferenceError(node=node, context=context) from e\n    for method in class_node.mymethods():\n        generated_class_node.locals[method.name] = [method]\n    for body_node in class_node.body:\n        if isinstance(body_node, nodes.Assign):\n            for target in body_node.targets:\n                attr = target.name\n                generated_class_node.locals[attr] = class_node.locals[attr]\n        elif isinstance(body_node, nodes.ClassDef):\n            generated_class_node.locals[body_node.name] = [body_node]\n    return iter((generated_class_node,))\ndef infer_typing_namedtuple_function(node, context=None):\n    \"\"\"\n    Starting with python3.9, NamedTuple is a function of the typing module.\n    The class NamedTuple is build dynamically through a call to `type` during\n    initialization of the `_NamedTuple` variable.\n    \"\"\"\n    klass = extract_node(\n        \"\"\"\n        from typing import _NamedTuple\n        _NamedTuple\n        \"\"\"\n    )\n    return klass.infer(context)\ndef infer_typing_namedtuple(\n    node: nodes.Call, context: InferenceContext | None = None\n) -> Iterator[nodes.ClassDef]:\n    \"\"\"Infer a typing.NamedTuple(...) call.\"\"\"\n    # This is essentially a namedtuple with different arguments\n    # so we extract the args and infer a named tuple.\n    try:\n        func = next(node.func.infer())\n    except (InferenceError, StopIteration) as exc:\n        raise UseInferenceDefault from exc\n    if func.qname() != \"typing.NamedTuple\":\n        raise UseInferenceDefault\n    if len(node.args) != 2:\n        raise UseInferenceDefault\n    if not isinstance(node.args[1], (nodes.List, nodes.Tuple)):\n        raise UseInferenceDefault\n    return infer_named_tuple(node, context)\ndef _get_namedtuple_fields(node: nodes.Call) -> str:\n    \"\"\"Get and return fields of a NamedTuple in code-as-a-string.\n    Because the fields are represented in their code form we can\n    extract a node from them later on.\n    \"\"\"\n    names = []\n    container = None\n    try:\n        container = next(node.args[1].infer())\n    except (InferenceError, StopIteration) as exc:\n        raise UseInferenceDefault from exc\n    # We pass on IndexError as we'll try to infer 'field_names' from the keywords\n    except IndexError:\n        pass\n    if not container:\n        for keyword_node in node.keywords:\n            if keyword_node.arg == \"field_names\":\n                try:\n                    container = next(keyword_node.value.infer())\n                except (InferenceError, StopIteration) as exc:\n                    raise UseInferenceDefault from exc\n                break\n    if not isinstance(container, nodes.BaseContainer):\n        raise UseInferenceDefault\n    for elt in container.elts:\n        if isinstance(elt, nodes.Const):\n            names.append(elt.as_string())\n            continue\n        if not isinstance(elt, (nodes.List, nodes.Tuple)):\n            raise UseInferenceDefault\n        if len(elt.elts) != 2:\n            raise UseInferenceDefault\n        names.append(elt.elts[0].as_string())\n    if names:\n        field_names = f\"({','.join(names)},)\"\n    else:\n        field_names = \"\"\n    return field_names\ndef _is_enum_subclass(cls: astroid.ClassDef) -> bool:\n    \"\"\"Return whether cls is a subclass of an Enum.\"\"\"\n    try:\n        return any(\n            klass.name in ENUM_BASE_NAMES\n            and getattr(klass.root(), \"name\", None) == \"enum\"\n            for klass in cls.mro()\n        )\n    except MroError:\n        return False\nAstroidManager().register_transform(\n    nodes.Call, inference_tip(infer_named_tuple), _looks_like_namedtuple\n)\nAstroidManager().register_transform(\n    nodes.Call, inference_tip(infer_enum), _looks_like_enum\n)\nAstroidManager().register_transform(\n    nodes.ClassDef, infer_enum_class, predicate=_is_enum_subclass\n)\nAstroidManager().register_transform(\n    nodes.ClassDef, inference_tip(infer_typing_namedtuple_class), _has_namedtuple_base\n)\nAstroidManager().register_transform(\n    nodes.FunctionDef,\n    inference_tip(infer_typing_namedtuple_function),\n    lambda node: node.name == \"NamedTuple\"\n    and getattr(node.root(), \"name\", None) == \"typing\",\n)\nAstroidManager().register_transform(\n    nodes.Call, inference_tip(infer_typing_namedtuple), _looks_like_typing_namedtuple\n)",
        "file_path": "astroid/brain/brain_namedtuple_enum.py",
        "chunk_index": 1,
        "metadata": {}
      },
      {
        "rank": 10,
        "score": 0.5517198443412781,
        "content": "# Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n# For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n# Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\nimport collections.abc\nfrom astroid.manager import AstroidManager\nfrom astroid.nodes.node_classes import FormattedValue\ndef _clone_node_with_lineno(node, parent, lineno):\n    cls = node.__class__\n    other_fields = node._other_fields\n    _astroid_fields = node._astroid_fields\n    init_params = {\"lineno\": lineno, \"col_offset\": node.col_offset, \"parent\": parent}\n    postinit_params = {param: getattr(node, param) for param in _astroid_fields}\n    if other_fields:\n        init_params.update({param: getattr(node, param) for param in other_fields})\n    new_node = cls(**init_params)\n    if hasattr(node, \"postinit\") and _astroid_fields:\n        for param, child in postinit_params.items():\n            if child and not isinstance(child, collections.abc.Sequence):\n                cloned_child = _clone_node_with_lineno(\n                    node=child, lineno=new_node.lineno, parent=new_node\n                )\n                postinit_params[param] = cloned_child\n        new_node.postinit(**postinit_params)\n    return new_node\ndef _transform_formatted_value(node):  # pylint: disable=inconsistent-return-statements\n    if node.value and node.value.lineno == 1:\n        if node.lineno != node.value.lineno:\n            new_node = FormattedValue(\n                lineno=node.lineno, col_offset=node.col_offset, parent=node.parent\n            )\n            new_value = _clone_node_with_lineno(\n                node=node.value, lineno=node.lineno, parent=new_node\n            )\n            new_node.postinit(value=new_value, format_spec=node.format_spec)\n            return new_node\n# TODO: this fix tries to *patch* http://bugs.python.org/issue29051\n# The problem is that FormattedValue.value, which is a Name node,\n# has wrong line numbers, usually 1. This creates problems for pylint,\n# which expects correct line numbers for things such as message control.\nAstroidManager().register_transform(FormattedValue, _transform_formatted_value)",
        "file_path": "astroid/brain/brain_fstrings.py",
        "chunk_index": 0,
        "metadata": {}
      }
    ]
  },
  "pylint-dev__astroid-1268": {
    "query": "'AsStringVisitor' object has no attribute 'visit_unknown'\n```python\r\n>>> import astroid\r\n>>> astroid.nodes.Unknown().as_string()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/tusharsadhwani/code/marvin-python/venv/lib/python3.9/site-packages/astroid/nodes/node_ng.py\", line 609, in as_string\r\n    return AsStringVisitor()(self)\r\n  File \"/Users/tusharsadhwani/code/marvin-python/venv/lib/python3.9/site-packages/astroid/nodes/as_string.py\", line 56, in __call__\r\n    return node.accept(self).replace(DOC_NEWLINE, \"\\n\")\r\n  File \"/Users/tusharsadhwani/code/marvin-python/venv/lib/python3.9/site-packages/astroid/nodes/node_ng.py\", line 220, in accept\r\n    func = getattr(visitor, \"visit_\" + self.__class__.__name__.lower())\r\nAttributeError: 'AsStringVisitor' object has no attribute 'visit_unknown'\r\n>>> \r\n```\r\n### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\r\n\r\n2.8.6-dev0\n",
    "method": "sliding",
    "top_k": 10,
    "results": [
      {
        "rank": 1,
        "score": 0.6915596723556519,
        "content": "# Copyright (c) 2006-2014 LOGILAB S.A. (Paris, FRANCE) <contact@logilab.fr>\n# Copyright (c) 2014-2020 Claudiu Popa <pcmanticore@gmail.com>\n# Copyright (c) 2014 Google, Inc.\n# Copyright (c) 2015-2017 Ceridwen <ceridwenv@gmail.com>\n# Copyright (c) 2015 Florian Bruhin <me@the-compiler.org>\n# Copyright (c) 2015 Radosław Ganczarek <radoslaw@ganczarek.in>\n# Copyright (c) 2016 Moises Lopez <moylop260@vauxoo.com>\n# Copyright (c) 2017 Hugo <hugovk@users.noreply.github.com>\n# Copyright (c) 2017 Łukasz Rogalski <rogalski.91@gmail.com>\n# Copyright (c) 2017 Calen Pennington <cale@edx.org>\n# Copyright (c) 2018 Ville Skyttä <ville.skytta@iki.fi>\n# Copyright (c) 2018 Ashley Whetter <ashley@awhetter.co.uk>\n# Copyright (c) 2018 Bryce Guinta <bryce.paul.guinta@gmail.com>\n# Copyright (c) 2019 Uilian Ries <uilianries@gmail.com>\n# Copyright (c) 2019 Thomas Hisch <t.hisch@gmail.com>\n# Copyright (c) 2020-2021 hippo91 <guillaume.peillex@gmail.com>\n# Copyright (c) 2020 David Gilman <davidgilman1@gmail.com>\n# Copyright (c) 2020 Konrad Weihmann <kweihmann@outlook.com>\n# Copyright (c) 2020 Felix Mölder <felix.moelder@uni-due.de>\n# Copyright (c) 2020 Michael <michael-k@users.noreply.github.com>\n# Copyright (c) 2021 Pierre Sassoulas <pierre.sassoulas@gmail.com>\n# Copyright (c) 2021 Marc Mueller <30130371+cdce8p@users.noreply.github.com>\n# Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n# For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n__version__ = \"2.9.0-dev0\"\nversion = __version__",
        "file_path": "astroid/__pkginfo__.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 2,
        "score": 0.6565232276916504,
        "content": "# Copyright (c) 2009-2011, 2013-2014 LOGILAB S.A. (Paris, FRANCE) <contact@logilab.fr>\n# Copyright (c) 2010 Daniel Harding <dharding@gmail.com>\n# Copyright (c) 2013-2016, 2018-2020 Claudiu Popa <pcmanticore@gmail.com>\n# Copyright (c) 2013-2014 Google, Inc.\n# Copyright (c) 2015-2016 Ceridwen <ceridwenv@gmail.com>\n# Copyright (c) 2016 Jared Garst <jgarst@users.noreply.github.com>\n# Copyright (c) 2016 Jakub Wilk <jwilk@jwilk.net>\n# Copyright (c) 2017, 2019 Łukasz Rogalski <rogalski.91@gmail.com>\n# Copyright (c) 2017 rr- <rr-@sakuya.pl>\n# Copyright (c) 2018 Serhiy Storchaka <storchaka@gmail.com>\n# Copyright (c) 2018 Ville Skyttä <ville.skytta@iki.fi>\n# Copyright (c) 2018 brendanator <brendan.maginnis@gmail.com>\n# Copyright (c) 2018 Nick Drozd <nicholasdrozd@gmail.com>\n# Copyright (c) 2019 Alex Hall <alex.mojaki@gmail.com>\n# Copyright (c) 2019 Hugo van Kemenade <hugovk@users.noreply.github.com>\n# Copyright (c) 2021 Daniël van Noord <13665637+DanielNoord@users.noreply.github.com>\n# Copyright (c) 2021 Pierre Sassoulas <pierre.sassoulas@gmail.com>\n# Copyright (c) 2021 Marc Mueller <30130371+cdce8p@users.noreply.github.com>\n# Copyright (c) 2021 pre-commit-ci[bot] <bot@noreply.github.com>\n# Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n# For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n\"\"\"This module renders Astroid nodes as string\"\"\"\nfrom typing import TYPE_CHECKING, List\nif TYPE_CHECKING:\n    from astroid.nodes.node_classes import (\n        Match,\n        MatchAs,\n        MatchCase,\n        MatchClass,\n        MatchMapping,\n        MatchOr,\n        MatchSequence,\n        MatchSingleton,\n        MatchStar,\n        MatchValue,\n    )\n# pylint: disable=unused-argument\nDOC_NEWLINE = \"\\0\"\n# Visitor pattern require argument all the time and is not better with staticmethod\n# noinspection PyUnusedLocal,PyMethodMayBeStatic\nclass AsStringVisitor:\n    \"\"\"Visitor to render an Astroid node as a valid python code string\"\"\"\n    def __init__(self, indent=\"    \"):\n        self.indent = indent\n    def __call__(self, node):\n        \"\"\"Makes this visitor behave as a simple function\"\"\"\n        return node.accept(self).replace(DOC_NEWLINE, \"\\n\")\n    def _docs_dedent(self, doc):\n        \"\"\"Stop newlines in docs being indented by self._stmt_list\"\"\"\n        return '\\n{}\"\"\"{}\"\"\"'.format(self.indent, doc.replace(\"\\n\", DOC_NEWLINE))\n    def _stmt_list(self, stmts, indent=True):\n        \"\"\"return a list of nodes to string\"\"\"\n        stmts = \"\\n\".join(nstr for nstr in [n.accept(self) for n in stmts] if nstr)\n        if indent:\n            return self.indent + stmts.replace(\"\\n\", \"\\n\" + self.indent)\n        return stmts\n    def _precedence_parens(self, node, child, is_left=True):\n        \"\"\"Wrap child in parens only if required to keep same semantics\"\"\"\n        if self._should_wrap(node, child, is_left):\n            return f\"({child.accept(self)})\"\n        return child.accept(self)\n    def _should_wrap(self, node, child, is_left):\n        \"\"\"Wrap child if:\n        - it has lower precedence\n        - same precedence with position opposite to associativity direction\n        \"\"\"\n        node_precedence = node.op_precedence()\n        child_precedence = child.op_precedence()\n        if node_precedence > child_precedence:\n            # 3 * (4 + 5)\n            return True\n        if (\n            node_precedence == child_precedence\n            and is_left != node.op_left_associative()\n        ):\n            # 3 - (4 - 5)\n            # (2**3)**4\n            return True\n        return False\n    # visit_<node> methods ###########################################\n    def visit_await(self, node):\n        return f\"await {node.value.accept(self)}\"\n    def visit_asyncwith(self, node):\n        return f\"async {self.visit_with(node)}\"\n    def visit_asyncfor(self, node):\n        return f\"async {self.visit_for(node)}\"\n    def visit_arguments(self, node):\n        \"\"\"return an astroid.Function node as string\"\"\"\n        return node.format_args()\n    def visit_assignattr(self, node):\n        \"\"\"return an astroid.AssAttr node as string\"\"\"\n        return self.visit_attribute(node)\n    def visit_assert(self, node):\n        \"\"\"return an astroid.Assert node as string\"\"\"\n        if node.fail:\n            return f\"assert {node.test.accept(self)}, {node.fail.accept(self)}\"\n        return f\"assert {node.test.accept(self)}\"\n    def visit_assignname(self, node):\n        \"\"\"return an astroid.AssName node as string\"\"\"\n        return node.name\n    def visit_assign(self, node):\n        \"\"\"return an astroid.Assign node as string\"\"\"\n        lhs = \" = \".join(n.accept(self) for n in node.targets)\n        return f\"{lhs} = {node.value.accept(self)}\"\n    def visit_augassign(self, node):\n        \"\"\"return an astroid.AugAssign node as string\"\"\"\n        return f\"{node.target.accept(self)} {node.op} {node.value.accept(self)}\"\n    def visit_annassign(self, node):\n        \"\"\"Return an astroid.AugAssign node as string\"\"\"\n        target = node.target.accept(self)\n        annotation = node.annotation.accept(self)\n        if node.value is None:\n            return f\"{target}: {annotation}\"\n        return f\"{target}: {annotation} = {node.value.accept(self)}\"\n    def visit_binop(self, node):\n        \"\"\"return an astroid.BinOp node as string\"\"\"\n        left = self._precedence_parens(node, node.left)\n        right = self._precedence_parens(node, node.right, is_left=False)\n        if node.op == \"**\":\n            return f\"{left}{node.op}{right}\"\n        return f\"{left} {node.op} {right}\"\n    def visit_boolop(self, node):\n        \"\"\"return an astroid.BoolOp node as string\"\"\"\n        values = [f\"{self._precedence_parens(node, n)}\" for n in node.values]\n        return (f\" {node.op} \").join(values)\n    def visit_break(self, node):\n        \"\"\"return an astroid.Break node as string\"\"\"\n        return \"break\"\n    def visit_call(self, node):\n        \"\"\"return an astroid.Call node as string\"\"\"\n        expr_str = self._precedence_parens(node, node.func)\n        args = [arg.accept(self) for arg in node.args]\n        if node.keywords:\n            keywords = [kwarg.accept(self) for kwarg in node.keywords]\n        else:\n            keywords = []\n        args.extend(keywords)\n        return f\"{expr_str}({', '.join(args)})\"\n    def visit_classdef(self, node):\n        \"\"\"return an astroid.ClassDef node as string\"\"\"\n        decorate = node.decorators.accept(self) if node.decorators else \"\"\n        args = [n.accept(self) for n in node.bases]\n        if node._metaclass and not node.has_metaclass_hack():\n            args.append(\"metaclass=\" + node._metaclass.accept(self))\n        args += [n.accept(self) for n in node.keywords]\n        args = f\"({', '.join(args)})\" if args else \"\"\n        docs = self._docs_dedent(node.doc) if node.doc else \"\"\n        return \"\\n\\n{}class {}{}:{}\\n{}\\n\".format(\n            decorate, node.name, args, docs, self._stmt_list(node.body)\n        )\n    def visit_compare(self, node):\n        \"\"\"return an astroid.Compare node as string\"\"\"\n        rhs_str = \" \".join(\n            f\"{op} {self._precedence_parens(node, expr, is_left=False)}\"\n            for op, expr in node.ops\n        )\n        return f\"{self._precedence_parens(node, node.left)} {rhs_str}\"\n    def visit_comprehension(self, node):\n        \"\"\"return an astroid.Comprehension node as string\"\"\"\n        ifs = \"\".join(f\" if {n.accept(self)}\" for n in node.ifs)\n        generated = f\"for {node.target.accept(self)} in {node.iter.accept(self)}{ifs}\"\n        return f\"{'async ' if node.is_async else ''}{generated}\"\n    def visit_const(self, node):\n        \"\"\"return an astroid.Const node as string\"\"\"\n        if node.value is Ellipsis:\n            return \"...\"\n        return repr(node.value)\n    def visit_continue(self, node):\n        \"\"\"return an astroid.Continue node as string\"\"\"\n        return \"continue\"\n    def visit_delete(self, node):  # XXX check if correct\n        \"\"\"return an astroid.Delete node as string\"\"\"\n        return f\"del {', '.join(child.accept(self) for child in node.targets)}\"\n    def visit_delattr(self, node):\n        \"\"\"return an astroid.DelAttr node as string\"\"\"\n        return self.visit_attribute(node)\n    def visit_delname(self, node):\n        \"\"\"return an astroid.DelName node as string\"\"\"\n        return node.name\n    def visit_decorators(self, node):\n        \"\"\"return an astroid.Decorators node as string\"\"\"\n        return \"@%s\\n\" % \"\\n@\".join(item.accept(self) for item in node.nodes)\n    def visit_dict(self, node):\n        \"\"\"return an astroid.Dict node as string\"\"\"\n        return \"{%s}\" % \", \".join(self._visit_dict(node))\n    def _visit_dict(self, node):\n        for key, value in node.items:\n            key = key.accept(self)\n            value = value.accept(self)\n            if key == \"**\":\n                # It can only be a DictUnpack node.\n                yield key + value\n            else:\n                yield f\"{key}: {value}\"\n    def visit_dictunpack(self, node):\n        return \"**\"\n    def visit_dictcomp(self, node):\n        \"\"\"return an astroid.DictComp node as string\"\"\"\n        return \"{{{}: {} {}}}\".format(\n            node.key.accept(self),\n            node.value.accept(self),\n            \" \".join(n.accept(self) for n in node.generators),\n        )\n    def visit_expr(self, node):\n        \"\"\"return an astroid.Discard node as string\"\"\"\n        return node.value.accept(self)\n    def visit_emptynode(self, node):\n        \"\"\"dummy method for visiting an Empty node\"\"\"\n        return \"\"\n    def visit_excepthandler(self, node):\n        if node.type:\n            if node.name:\n                excs = f\"except {node.type.accept(self)} as {node.name.accept(self)}\"\n            else:\n                excs = f\"except {node.type.accept(self)}\"\n        else:\n            excs = \"except\"\n        return f\"{excs}:\\n{self._stmt_list(node.body)}\"\n    def visit_empty(self, node):\n        \"\"\"return an Empty node as string\"\"\"\n        return \"\"\n    def visit_for(self, node):\n        \"\"\"return an astroid.For node as string\"\"\"\n        fors = \"for {} in {}:\\n{}\".format(\n            node.target.accept(self), node.iter.accept(self), self._stmt_list(node.body)\n        )\n        if node.orelse:\n            fors = f\"{fors}\\nelse:\\n{self._stmt_list(node.orelse)}\"\n        return fors\n    def visit_importfrom(self, node):\n        \"\"\"return an astroid.ImportFrom node as string\"\"\"\n        return \"from {} import {}\".format(\n            \".\" * (node.level or 0) + node.modname, _import_string(node.names)\n        )\n    def visit_joinedstr(self, node):\n        string = \"\".join(\n            # Use repr on the string literal parts\n            # to get proper escapes, e.g. \\n, \\\\, \\\"\n            # But strip the quotes off the ends\n            # (they will always be one character: ' or \")\n            repr(value.value)[1:-1]\n            # Literal braces must be doubled to escape them\n            .replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n            # Each value in values is either a string literal (Const)\n            # or a FormattedValue\n            if type(value).__name__ == \"Const\" else value.accept(self)\n            for value in node.values\n        )\n        # Try to find surrounding quotes that don't appear at all in the string.\n        # Because the formatted values inside {} can't contain backslash (\\)\n        # using a triple quote is sometimes necessary\n        for quote in (\"'\", '\"', '\"\"\"', \"'''\"):\n            if quote not in string:\n                break\n        return \"f\" + quote + string + quote\n    def visit_formattedvalue(self, node):\n        result = node.value.accept(self)\n        if node.conversion and node.conversion >= 0:\n            # e.g. if node.conversion == 114: result += \"!r\"\n            result += \"!\" + chr(node.conversion)\n        if node.format_spec:\n            # The format spec is itself a JoinedString, i.e. an f-string\n            # We strip the f and quotes of the ends\n            result += \":\" + node.format_spec.accept(self)[2:-1]\n        return \"{%s}\" % result\n    def handle_functiondef(self, node, keyword):\n        \"\"\"return a (possibly async) function definition node as string\"\"\"\n        decorate = node.decorators.accept(self) if node.decorators else \"\"\n        docs = self._docs_dedent(node.doc) if node.doc else \"\"\n        trailer = \":\"\n        if node.returns:\n            return_annotation = \" -> \" + node.returns.as_string()\n            trailer = return_annotation + \":\"\n        def_format = \"\\n%s%s %s(%s)%s%s\\n%s\"\n        return def_format % (\n            decorate,\n            keyword,\n            node.name,\n            node.args.accept(self),\n            trailer,\n            docs,\n            self._stmt_list(node.body),\n        )\n    def visit_functiondef(self, node):\n        \"\"\"return an astroid.FunctionDef node as string\"\"\"\n        return self.handle_functiondef(node, \"def\")\n    def visit_asyncfunctiondef(self, node):\n        \"\"\"return an astroid.AsyncFunction node as string\"\"\"\n        return self.handle_functiondef(node, \"async def\")\n    def visit_generatorexp(self, node):\n        \"\"\"return an astroid.GeneratorExp node as string\"\"\"\n        return \"({} {})\".format(\n            node.elt.accept(self), \" \".join(n.accept(self) for n in node.generators)\n        )\n    def visit_attribute(self, node):\n        \"\"\"return an astroid.Getattr node as string\"\"\"\n        left = self._precedence_parens(node, node.expr)\n        if left.isdigit():\n            left = f\"({left})\"\n        return f\"{left}.{node.attrname}\"\n    def visit_global(self, node):\n        \"\"\"return an astroid.Global node as string\"\"\"\n        return f\"global {', '.join(node.names)}\"\n    def visit_if(self, node):\n        \"\"\"return an astroid.If node as string\"\"\"\n        ifs = [f\"if {node.test.accept(self)}:\\n{self._stmt_list(node.body)}\"]\n        if node.has_elif_block():\n            ifs.append(f\"el{self._stmt_list(node.orelse, indent=False)}\")\n        elif node.orelse:\n            ifs.append(f\"else:\\n{self._stmt_list(node.orelse)}\")\n        return \"\\n\".join(ifs)\n    def visit_ifexp(self, node):\n        \"\"\"return an astroid.IfExp node as string\"\"\"\n        return \"{} if {} else {}\".format(\n            self._precedence_parens(node, node.body, is_left=True),\n            self._precedence_parens(node, node.test, is_left=True),\n            self._precedence_parens(node, node.orelse, is_left=False),\n        )\n    def visit_import(self, node):\n        \"\"\"return an astroid.Import node as string\"\"\"\n        return f\"import {_import_string(node.names)}\"\n    def visit_keyword(self, node):\n        \"\"\"return an astroid.Keyword node as string\"\"\"\n        if node.arg is None:\n            return f\"**{node.value.accept(self)}\"\n        return f\"{node.arg}={node.value.accept(self)}\"\n    def visit_lambda(self, node):\n        \"\"\"return an astroid.Lambda node as string\"\"\"\n        args = node.args.accept(self)\n        body = node.body.accept(self)\n        if args:\n            return f\"lambda {args}: {body}\"\n        return f\"lambda: {body}\"\n    def visit_list(self, node):\n        \"\"\"return an astroid.List node as string\"\"\"\n        return f\"[{', '.join(child.accept(self) for child in node.elts)}]\"\n    def visit_listcomp(self, node):\n        \"\"\"return an astroid.ListComp node as string\"\"\"\n        return \"[{} {}]\".format(\n            node.elt.accept(self), \" \".join(n.accept(self) for n in node.generators)\n        )\n    def visit_module(self, node):\n        \"\"\"return an astroid.Module node as string\"\"\"\n        docs = f'\"\"\"{node.doc}\"\"\"\\n\\n' if node.doc else \"\"\n        return docs + \"\\n\".join(n.accept(self) for n in node.body) + \"\\n\\n\"\n    def visit_name(self, node):\n        \"\"\"return an astroid.Name node as string\"\"\"\n        return node.name\n    def visit_namedexpr(self, node):\n        \"\"\"Return an assignment expression node as string\"\"\"\n        target = node.target.accept(self)\n        value = node.value.accept(self)\n        return f\"{target} := {value}\"\n    def visit_nonlocal(self, node):\n        \"\"\"return an astroid.Nonlocal node as string\"\"\"\n        return f\"nonlocal {', '.join(node.names)}\"\n    def visit_pass(self, node):\n        \"\"\"return an astroid.Pass node as string\"\"\"\n        return \"pass\"\n    def visit_raise(self, node):\n        \"\"\"return an astroid.Raise node as string\"\"\"\n        if node.exc:\n            if node.cause:\n                return f\"raise {node.exc.accept(self)} from {node.cause.accept(self)}\"\n            return f\"raise {node.exc.accept(self)}\"\n        return \"raise\"\n    def visit_return(self, node):\n        \"\"\"return an astroid.Return node as string\"\"\"\n        if node.is_tuple_return() and len(node.value.elts) > 1:\n            elts = [child.accept(self) for child in node.value.elts]\n            return f\"return {', '.join(elts)}\"\n        if node.value:\n            return f\"return {node.value.accept(self)}\"\n        return \"return\"\n    def visit_set(self, node):\n        \"\"\"return an astroid.Set node as string\"\"\"\n        return \"{%s}\" % \", \".join(child.accept(self) for child in node.elts)\n    def visit_setcomp(self, node):\n        \"\"\"return an astroid.SetComp node as string\"\"\"\n        return \"{{{} {}}}\".format(\n            node.elt.accept(self), \" \".join(n.accept(self) for n in node.generators)\n        )\n    def visit_slice(self, node):\n        \"\"\"return an astroid.Slice node as string\"\"\"\n        lower = node.lower.accept(self) if node.lower else \"\"\n        upper = node.upper.accept(self) if node.upper else \"\"\n        step = node.step.accept(self) if node.step else \"\"\n        if step:\n            return f\"{lower}:{upper}:{step}\"\n        return f\"{lower}:{upper}\"\n    def visit_subscript(self, node):\n        \"\"\"return an astroid.Subscript node as string\"\"\"\n        idx = node.slice\n        if idx.__class__.__name__.lower() == \"index\":\n            idx = idx.value\n        idxstr = idx.accept(self)\n        if idx.__class__.__name__.lower() == \"tuple\" and idx.elts:\n            # Remove parenthesis in tuple and extended slice.\n            # a[(::1, 1:)] is not valid syntax.\n            idxstr = idxstr[1:-1]\n        return f\"{self._precedence_parens(node, node.value)}[{idxstr}]\"\n    def visit_tryexcept(self, node):\n        \"\"\"return an astroid.TryExcept node as string\"\"\"\n        trys = [f\"try:\\n{self._stmt_list(node.body)}\"]\n        for handler in node.handlers:\n            trys.append(handler.accept(self))\n        if node.orelse:\n            trys.append(f\"else:\\n{self._stmt_list(node.orelse)}\")\n        return \"\\n\".join(trys)\n    def visit_tryfinally(self, node):\n        \"\"\"return an astroid.TryFinally node as string\"\"\"\n        return \"try:\\n{}\\nfinally:\\n{}\".format(\n            self._stmt_list(node.body), self._stmt_list(node.finalbody)\n        )\n    def visit_tuple(self, node):\n        \"\"\"return an astroid.Tuple node as string\"\"\"\n        if len(node.elts) == 1:\n            return f\"({node.elts[0].accept(self)}, )\"\n        return f\"({', '.join(child.accept(self) for child in node.elts)})\"\n    def visit_unaryop(self, node):\n        \"\"\"return an astroid.UnaryOp node as string\"\"\"\n        if node.op == \"not\":\n            operator = \"not \"\n        else:\n            operator = node.op\n        return f\"{operator}{self._precedence_parens(node, node.operand)}\"\n    def visit_while(self, node):\n        \"\"\"return an astroid.While node as string\"\"\"\n        whiles = f\"while {node.test.accept(self)}:\\n{self._stmt_list(node.body)}\"\n        if node.orelse:\n            whiles = f\"{whiles}\\nelse:\\n{self._stmt_list(node.orelse)}\"\n        return whiles\n    def visit_with(self, node):  # 'with' without 'as' is possible\n        \"\"\"return an astroid.With node as string\"\"\"\n        items = \", \".join(\n            f\"{expr.accept(self)}\" + (v and f\" as {v.accept(self)}\" or \"\")\n            for expr, v in node.items\n        )\n        return f\"with {items}:\\n{self._stmt_list(node.body)}\"\n    def visit_yield(self, node):\n        \"\"\"yield an ast.Yield node as string\"\"\"\n        yi_val = (\" \" + node.value.accept(self)) if node.value else \"\"\n        expr = \"yield\" + yi_val\n        if node.parent.is_statement:\n            return expr\n        return f\"({expr})\"\n    def visit_yieldfrom(self, node):\n        \"\"\"Return an astroid.YieldFrom node as string.\"\"\"\n        yi_val = (\" \" + node.value.accept(self)) if node.value else \"\"\n        expr = \"yield from\" + yi_val\n        if node.parent.is_statement:\n            return expr\n        return f\"({expr})\"\n    def visit_starred(self, node):\n        \"\"\"return Starred node as string\"\"\"\n        return \"*\" + node.value.accept(self)\n    def visit_match(self, node: \"Match\") -> str:\n        \"\"\"Return an astroid.Match node as string.\"\"\"\n        return f\"match {node.subject.accept(self)}:\\n{self._stmt_list(node.cases)}\"\n    def visit_matchcase(self, node: \"MatchCase\") -> str:\n        \"\"\"Return an astroid.MatchCase node as string.\"\"\"\n        guard_str = f\" if {node.guard.accept(self)}\" if node.guard else \"\"\n        return (\n            f\"case {node.pattern.accept(self)}{guard_str}:\\n\"\n            f\"{self._stmt_list(node.body)}\"\n        )\n    def visit_matchvalue(self, node: \"MatchValue\") -> str:\n        \"\"\"Return an astroid.MatchValue node as string.\"\"\"\n        return node.value.accept(self)\n    @staticmethod\n    def visit_matchsingleton(node: \"MatchSingleton\") -> str:\n        \"\"\"Return an astroid.MatchSingleton node as string.\"\"\"\n        return str(node.value)\n    def visit_matchsequence(self, node: \"MatchSequence\") -> str:\n        \"\"\"Return an astroid.MatchSequence node as string.\"\"\"\n        if node.patterns is None:\n            return \"[]\"\n        return f\"[{', '.join(p.accept(self) for p in node.patterns)}]\"\n    def visit_matchmapping(self, node: \"MatchMapping\") -> str:\n        \"\"\"Return an astroid.MatchMapping node as string.\"\"\"\n        mapping_strings: List[str] = []\n        if node.keys and node.patterns:\n            mapping_strings.extend(\n                f\"{key.accept(self)}: {p.accept(self)}\"\n                for key, p in zip(node.keys, node.patterns)\n            )\n        if node.rest:\n            mapping_strings.append(f\"**{node.rest.accept(self)}\")\n        return f\"{'{'}{', '.join(mapping_strings)}{'}'}\"\n    def visit_matchclass(self, node: \"MatchClass\") -> str:\n        \"\"\"Return an astroid.MatchClass node as string.\"\"\"\n        if node.cls is None:\n            raise Exception(f\"{node} does not have a 'cls' node\")\n        class_strings: List[str] = []\n        if node.patterns:\n            class_strings.extend(p.accept(self) for p in node.patterns)\n        if node.kwd_attrs and node.kwd_patterns:\n            for attr, pattern in zip(node.kwd_attrs, node.kwd_patterns):\n                class_strings.append(f\"{attr}={pattern.accept(self)}\")\n        return f\"{node.cls.accept(self)}({', '.join(class_strings)})\"",
        "file_path": "astroid/nodes/as_string.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 3,
        "score": 0.6255497932434082,
        "content": "        return f\"{self._precedence_parens(node, node.value)}[{idxstr}]\"\n    def visit_tryexcept(self, node):\n        \"\"\"return an astroid.TryExcept node as string\"\"\"\n        trys = [f\"try:\\n{self._stmt_list(node.body)}\"]\n        for handler in node.handlers:\n            trys.append(handler.accept(self))\n        if node.orelse:\n            trys.append(f\"else:\\n{self._stmt_list(node.orelse)}\")\n        return \"\\n\".join(trys)\n    def visit_tryfinally(self, node):\n        \"\"\"return an astroid.TryFinally node as string\"\"\"\n        return \"try:\\n{}\\nfinally:\\n{}\".format(\n            self._stmt_list(node.body), self._stmt_list(node.finalbody)\n        )\n    def visit_tuple(self, node):\n        \"\"\"return an astroid.Tuple node as string\"\"\"\n        if len(node.elts) == 1:\n            return f\"({node.elts[0].accept(self)}, )\"\n        return f\"({', '.join(child.accept(self) for child in node.elts)})\"\n    def visit_unaryop(self, node):\n        \"\"\"return an astroid.UnaryOp node as string\"\"\"\n        if node.op == \"not\":\n            operator = \"not \"\n        else:\n            operator = node.op\n        return f\"{operator}{self._precedence_parens(node, node.operand)}\"\n    def visit_while(self, node):\n        \"\"\"return an astroid.While node as string\"\"\"\n        whiles = f\"while {node.test.accept(self)}:\\n{self._stmt_list(node.body)}\"\n        if node.orelse:\n            whiles = f\"{whiles}\\nelse:\\n{self._stmt_list(node.orelse)}\"\n        return whiles\n    def visit_with(self, node):  # 'with' without 'as' is possible\n        \"\"\"return an astroid.With node as string\"\"\"\n        items = \", \".join(\n            f\"{expr.accept(self)}\" + (v and f\" as {v.accept(self)}\" or \"\")\n            for expr, v in node.items\n        )\n        return f\"with {items}:\\n{self._stmt_list(node.body)}\"\n    def visit_yield(self, node):\n        \"\"\"yield an ast.Yield node as string\"\"\"\n        yi_val = (\" \" + node.value.accept(self)) if node.value else \"\"\n        expr = \"yield\" + yi_val\n        if node.parent.is_statement:\n            return expr\n        return f\"({expr})\"\n    def visit_yieldfrom(self, node):\n        \"\"\"Return an astroid.YieldFrom node as string.\"\"\"\n        yi_val = (\" \" + node.value.accept(self)) if node.value else \"\"\n        expr = \"yield from\" + yi_val\n        if node.parent.is_statement:\n            return expr\n        return f\"({expr})\"\n    def visit_starred(self, node):\n        \"\"\"return Starred node as string\"\"\"\n        return \"*\" + node.value.accept(self)\n    def visit_match(self, node: \"Match\") -> str:\n        \"\"\"Return an astroid.Match node as string.\"\"\"\n        return f\"match {node.subject.accept(self)}:\\n{self._stmt_list(node.cases)}\"\n    def visit_matchcase(self, node: \"MatchCase\") -> str:\n        \"\"\"Return an astroid.MatchCase node as string.\"\"\"\n        guard_str = f\" if {node.guard.accept(self)}\" if node.guard else \"\"\n        return (\n            f\"case {node.pattern.accept(self)}{guard_str}:\\n\"\n            f\"{self._stmt_list(node.body)}\"\n        )\n    def visit_matchvalue(self, node: \"MatchValue\") -> str:\n        \"\"\"Return an astroid.MatchValue node as string.\"\"\"\n        return node.value.accept(self)\n    @staticmethod\n    def visit_matchsingleton(node: \"MatchSingleton\") -> str:\n        \"\"\"Return an astroid.MatchSingleton node as string.\"\"\"\n        return str(node.value)\n    def visit_matchsequence(self, node: \"MatchSequence\") -> str:\n        \"\"\"Return an astroid.MatchSequence node as string.\"\"\"\n        if node.patterns is None:\n            return \"[]\"\n        return f\"[{', '.join(p.accept(self) for p in node.patterns)}]\"\n    def visit_matchmapping(self, node: \"MatchMapping\") -> str:\n        \"\"\"Return an astroid.MatchMapping node as string.\"\"\"\n        mapping_strings: List[str] = []\n        if node.keys and node.patterns:\n            mapping_strings.extend(\n                f\"{key.accept(self)}: {p.accept(self)}\"\n                for key, p in zip(node.keys, node.patterns)\n            )\n        if node.rest:\n            mapping_strings.append(f\"**{node.rest.accept(self)}\")\n        return f\"{'{'}{', '.join(mapping_strings)}{'}'}\"\n    def visit_matchclass(self, node: \"MatchClass\") -> str:\n        \"\"\"Return an astroid.MatchClass node as string.\"\"\"\n        if node.cls is None:\n            raise Exception(f\"{node} does not have a 'cls' node\")\n        class_strings: List[str] = []\n        if node.patterns:\n            class_strings.extend(p.accept(self) for p in node.patterns)\n        if node.kwd_attrs and node.kwd_patterns:\n            for attr, pattern in zip(node.kwd_attrs, node.kwd_patterns):\n                class_strings.append(f\"{attr}={pattern.accept(self)}\")\n        return f\"{node.cls.accept(self)}({', '.join(class_strings)})\"\n    def visit_matchstar(self, node: \"MatchStar\") -> str:\n        \"\"\"Return an astroid.MatchStar node as string.\"\"\"\n        return f\"*{node.name.accept(self) if node.name else '_'}\"\n    def visit_matchas(self, node: \"MatchAs\") -> str:\n        \"\"\"Return an astroid.MatchAs node as string.\"\"\"\n        # pylint: disable=import-outside-toplevel\n        # Prevent circular dependency\n        from astroid.nodes.node_classes import MatchClass, MatchMapping, MatchSequence\n        if isinstance(node.parent, (MatchSequence, MatchMapping, MatchClass)):\n            return node.name.accept(self) if node.name else \"_\"\n        return (\n            f\"{node.pattern.accept(self) if node.pattern else '_'}\"\n            f\"{f' as {node.name.accept(self)}' if node.name else ''}\"\n        )\n    def visit_matchor(self, node: \"MatchOr\") -> str:\n        \"\"\"Return an astroid.MatchOr node as string.\"\"\"\n        if node.patterns is None:\n            raise Exception(f\"{node} does not have pattern nodes\")\n        return \" | \".join(p.accept(self) for p in node.patterns)\n    # These aren't for real AST nodes, but for inference objects.\n    def visit_frozenset(self, node):\n        return node.parent.accept(self)\n    def visit_super(self, node):\n        return node.parent.accept(self)\n    def visit_uninferable(self, node):\n        return str(node)\n    def visit_property(self, node):\n        return node.function.accept(self)\n    def visit_evaluatedobject(self, node):\n        return node.original.accept(self)\ndef _import_string(names):\n    \"\"\"return a list of (name, asname) formatted as a string\"\"\"\n    _names = []\n    for name, asname in names:\n        if asname is not None:\n            _names.append(f\"{name} as {asname}\")\n        else:\n            _names.append(name)\n    return \", \".join(_names)\n# This sets the default indent to 4 spaces.\nto_code = AsStringVisitor(\"    \")",
        "file_path": "astroid/nodes/as_string.py",
        "chunk_index": 1,
        "metadata": {}
      },
      {
        "rank": 4,
        "score": 0.619521975517273,
        "content": "# Copyright (c) 2006-2013, 2015 LOGILAB S.A. (Paris, FRANCE) <contact@logilab.fr>\n# Copyright (c) 2014 Google, Inc.\n# Copyright (c) 2014 Eevee (Alex Munroe) <amunroe@yelp.com>\n# Copyright (c) 2015-2016, 2018, 2020 Claudiu Popa <pcmanticore@gmail.com>\n# Copyright (c) 2015-2016 Ceridwen <ceridwenv@gmail.com>\n# Copyright (c) 2016 Derek Gustafson <degustaf@gmail.com>\n# Copyright (c) 2016 Moises Lopez <moylop260@vauxoo.com>\n# Copyright (c) 2018 Bryce Guinta <bryce.paul.guinta@gmail.com>\n# Copyright (c) 2019 Nick Drozd <nicholasdrozd@gmail.com>\n# Copyright (c) 2020-2021 hippo91 <guillaume.peillex@gmail.com>\n# Copyright (c) 2021 Pierre Sassoulas <pierre.sassoulas@gmail.com>\n# Copyright (c) 2021 Daniël van Noord <13665637+DanielNoord@users.noreply.github.com>\n# Copyright (c) 2021 Marc Mueller <30130371+cdce8p@users.noreply.github.com>\n# Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n# For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n\"\"\"Python Abstract Syntax Tree New Generation\nThe aim of this module is to provide a common base representation of\npython source code for projects such as pychecker, pyreverse,\npylint... Well, actually the development of this library is essentially\ngoverned by pylint's needs.\nIt extends class defined in the python's _ast module with some\nadditional methods and attributes. Instance attributes are added by a\nbuilder object, which can either generate extended ast (let's call\nthem astroid ;) by visiting an existent ast tree or by inspecting living\nobject. Methods are added by monkey patching ast classes.\nMain modules are:\n* nodes and scoped_nodes for more information about methods and\n  attributes added to different node classes\n* the manager contains a high level object to get astroid trees from\n  source files and living objects. It maintains a cache of previously\n  constructed tree for quick access\n* builder contains the class responsible to build astroid trees\n\"\"\"\nfrom importlib import import_module\nfrom pathlib import Path\n# isort: off\n# We have an isort: off on '__version__' because the packaging need to access\n# the version before the dependencies are installed (in particular 'wrapt'\n# that is imported in astroid.inference)\nfrom astroid.__pkginfo__ import __version__, version\nfrom astroid.nodes import node_classes, scoped_nodes\n# isort: on\nfrom astroid import inference, raw_building\nfrom astroid.astroid_manager import MANAGER\nfrom astroid.bases import BaseInstance, BoundMethod, Instance, UnboundMethod\nfrom astroid.brain.helpers import register_module_extender\nfrom astroid.builder import extract_node, parse\nfrom astroid.const import Context, Del, Load, Store\nfrom astroid.exceptions import *\nfrom astroid.inference_tip import _inference_tip_cached, inference_tip\nfrom astroid.objects import ExceptionInstance\n# isort: off\n# It's impossible to import from astroid.nodes with a wildcard, because\n# there is a cyclic import that prevent creating an __all__ in astroid/nodes\n# and we need astroid/scoped_nodes and astroid/node_classes to work. So\n# importing with a wildcard would clash with astroid/nodes/scoped_nodes\n# and astroid/nodes/node_classes.\nfrom astroid.nodes import (  # pylint: disable=redefined-builtin (Ellipsis)\n    CONST_CLS,\n    AnnAssign,\n    Arguments,\n    Assert,\n    Assign,\n    AssignAttr,\n    AssignName,\n    AsyncFor,\n    AsyncFunctionDef,\n    AsyncWith,\n    Attribute,\n    AugAssign,\n    Await,\n    BinOp,\n    BoolOp,\n    Break,\n    Call,\n    ClassDef,\n    Compare,\n    Comprehension,\n    ComprehensionScope,\n    Const,\n    Continue,\n    Decorators,\n    DelAttr,\n    Delete,\n    DelName,\n    Dict,\n    DictComp,\n    DictUnpack,\n    Ellipsis,\n    EmptyNode,\n    EvaluatedObject,\n    ExceptHandler,\n    Expr,\n    ExtSlice,\n    For,\n    FormattedValue,\n    FunctionDef,\n    GeneratorExp,\n    Global,\n    If,\n    IfExp,\n    Import,\n    ImportFrom,\n    Index,\n    JoinedStr,\n    Keyword,\n    Lambda,\n    List,\n    ListComp,\n    Match,\n    MatchAs,\n    MatchCase,\n    MatchClass,\n    MatchMapping,\n    MatchOr,\n    MatchSequence,\n    MatchSingleton,\n    MatchStar,\n    MatchValue,\n    Module,\n    Name,\n    NamedExpr,\n    NodeNG,\n    Nonlocal,\n    Pass,\n    Raise,\n    Return,\n    Set,\n    SetComp,\n    Slice,\n    Starred,\n    Subscript,\n    TryExcept,\n    TryFinally,\n    Tuple,\n    UnaryOp,\n    Unknown,\n    While,\n    With,\n    Yield,\n    YieldFrom,\n    are_exclusive,\n    builtin_lookup,\n    unpack_infer,\n    function_to_method,\n)\n# isort: on\nfrom astroid.util import Uninferable\n# load brain plugins\nASTROID_INSTALL_DIRECTORY = Path(__file__).parent\nBRAIN_MODULES_DIRECTORY = ASTROID_INSTALL_DIRECTORY / \"brain\"\nfor module in BRAIN_MODULES_DIRECTORY.iterdir():\n    if module.suffix == \".py\":\n        import_module(f\"astroid.brain.{module.stem}\")",
        "file_path": "astroid/__init__.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 5,
        "score": 0.6121736764907837,
        "content": "# Copyright (c) 2007, 2009-2010, 2013 LOGILAB S.A. (Paris, FRANCE) <contact@logilab.fr>\n# Copyright (c) 2014 Google, Inc.\n# Copyright (c) 2015-2018, 2020 Claudiu Popa <pcmanticore@gmail.com>\n# Copyright (c) 2015-2016 Ceridwen <ceridwenv@gmail.com>\n# Copyright (c) 2016 Derek Gustafson <degustaf@gmail.com>\n# Copyright (c) 2018 Bryce Guinta <bryce.paul.guinta@gmail.com>\n# Copyright (c) 2020-2021 hippo91 <guillaume.peillex@gmail.com>\n# Copyright (c) 2021 Pierre Sassoulas <pierre.sassoulas@gmail.com>\n# Copyright (c) 2021 Daniël van Noord <13665637+DanielNoord@users.noreply.github.com>\n# Copyright (c) 2021 Marc Mueller <30130371+cdce8p@users.noreply.github.com>\n# Copyright (c) 2021 Andrew Haigh <hello@nelf.in>\n# Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n# For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n\"\"\"this module contains exceptions used in the astroid library\n\"\"\"\nfrom typing import TYPE_CHECKING\nfrom astroid import util\nif TYPE_CHECKING:\n    from astroid import nodes\n__all__ = (\n    \"AstroidBuildingError\",\n    \"AstroidBuildingException\",\n    \"AstroidError\",\n    \"AstroidImportError\",\n    \"AstroidIndexError\",\n    \"AstroidSyntaxError\",\n    \"AstroidTypeError\",\n    \"AstroidValueError\",\n    \"AttributeInferenceError\",\n    \"BinaryOperationError\",\n    \"DuplicateBasesError\",\n    \"InconsistentMroError\",\n    \"InferenceError\",\n    \"InferenceOverwriteError\",\n    \"MroError\",\n    \"NameInferenceError\",\n    \"NoDefault\",\n    \"NotFoundError\",\n    \"OperationError\",\n    \"ResolveError\",\n    \"SuperArgumentTypeError\",\n    \"SuperError\",\n    \"TooManyLevelsError\",\n    \"UnaryOperationError\",\n    \"UnresolvableName\",\n    \"UseInferenceDefault\",\n)\nclass AstroidError(Exception):\n    \"\"\"base exception class for all astroid related exceptions\n    AstroidError and its subclasses are structured, intended to hold\n    objects representing state when the exception is thrown.  Field\n    values are passed to the constructor as keyword-only arguments.\n    Each subclass has its own set of standard fields, but use your\n    best judgment to decide whether a specific exception instance\n    needs more or fewer fields for debugging.  Field values may be\n    used to lazily generate the error message: self.message.format()\n    will be called with the field names and values supplied as keyword\n    arguments.\n    \"\"\"\n    def __init__(self, message=\"\", **kws):\n        super().__init__(message)\n        self.message = message\n        for key, value in kws.items():\n            setattr(self, key, value)\n    def __str__(self):\n        return self.message.format(**vars(self))\nclass AstroidBuildingError(AstroidError):\n    \"\"\"exception class when we are unable to build an astroid representation\n    Standard attributes:\n        modname: Name of the module that AST construction failed for.\n        error: Exception raised during construction.\n    \"\"\"\n    def __init__(self, message=\"Failed to import module {modname}.\", **kws):\n        super().__init__(message, **kws)\nclass AstroidImportError(AstroidBuildingError):\n    \"\"\"Exception class used when a module can't be imported by astroid.\"\"\"\nclass TooManyLevelsError(AstroidImportError):\n    \"\"\"Exception class which is raised when a relative import was beyond the top-level.\n    Standard attributes:\n        level: The level which was attempted.\n        name: the name of the module on which the relative import was attempted.\n    \"\"\"\n    level = None\n    name = None\n    def __init__(\n        self,\n        message=\"Relative import with too many levels \" \"({level}) for module {name!r}\",\n        **kws,\n    ):\n        super().__init__(message, **kws)\nclass AstroidSyntaxError(AstroidBuildingError):\n    \"\"\"Exception class used when a module can't be parsed.\"\"\"\nclass NoDefault(AstroidError):\n    \"\"\"raised by function's `default_value` method when an argument has\n    no default value\n    Standard attributes:\n        func: Function node.\n        name: Name of argument without a default.\n    \"\"\"\n    func = None\n    name = None\n    def __init__(self, message=\"{func!r} has no default for {name!r}.\", **kws):\n        super().__init__(message, **kws)\nclass ResolveError(AstroidError):\n    \"\"\"Base class of astroid resolution/inference error.\n    ResolveError is not intended to be raised.\n    Standard attributes:\n        context: InferenceContext object.\n    \"\"\"\n    context = None\nclass MroError(ResolveError):\n    \"\"\"Error raised when there is a problem with method resolution of a class.\n    Standard attributes:\n        mros: A sequence of sequences containing ClassDef nodes.\n        cls: ClassDef node whose MRO resolution failed.\n        context: InferenceContext object.\n    \"\"\"\n    mros = ()\n    cls = None\n    def __str__(self):\n        mro_names = \", \".join(f\"({', '.join(b.name for b in m)})\" for m in self.mros)\n        return self.message.format(mros=mro_names, cls=self.cls)\nclass DuplicateBasesError(MroError):\n    \"\"\"Error raised when there are duplicate bases in the same class bases.\"\"\"\nclass InconsistentMroError(MroError):\n    \"\"\"Error raised when a class's MRO is inconsistent.\"\"\"\nclass SuperError(ResolveError):\n    \"\"\"Error raised when there is a problem with a *super* call.\n    Standard attributes:\n        *super_*: The Super instance that raised the exception.\n        context: InferenceContext object.\n    \"\"\"\n    super_ = None\n    def __str__(self):\n        return self.message.format(**vars(self.super_))\nclass InferenceError(ResolveError):\n    \"\"\"raised when we are unable to infer a node\n    Standard attributes:\n        node: The node inference was called on.\n        context: InferenceContext object.\n    \"\"\"\n    node = None\n    context = None\n    def __init__(self, message=\"Inference failed for {node!r}.\", **kws):\n        super().__init__(message, **kws)\n# Why does this inherit from InferenceError rather than ResolveError?\n# Changing it causes some inference tests to fail.\nclass NameInferenceError(InferenceError):\n    \"\"\"Raised when a name lookup fails, corresponds to NameError.\n    Standard attributes:\n        name: The name for which lookup failed, as a string.\n        scope: The node representing the scope in which the lookup occurred.\n        context: InferenceContext object.\n    \"\"\"\n    name = None\n    scope = None\n    def __init__(self, message=\"{name!r} not found in {scope!r}.\", **kws):\n        super().__init__(message, **kws)\nclass AttributeInferenceError(ResolveError):\n    \"\"\"Raised when an attribute lookup fails, corresponds to AttributeError.\n    Standard attributes:\n        target: The node for which lookup failed.\n        attribute: The attribute for which lookup failed, as a string.\n        context: InferenceContext object.\n    \"\"\"\n    target = None\n    attribute = None\n    def __init__(self, message=\"{attribute!r} not found on {target!r}.\", **kws):\n        super().__init__(message, **kws)\nclass UseInferenceDefault(Exception):\n    \"\"\"exception to be raised in custom inference function to indicate that it\n    should go back to the default behaviour\n    \"\"\"\nclass _NonDeducibleTypeHierarchy(Exception):\n    \"\"\"Raised when is_subtype / is_supertype can't deduce the relation between two types.\"\"\"\nclass AstroidIndexError(AstroidError):\n    \"\"\"Raised when an Indexable / Mapping does not have an index / key.\"\"\"\nclass AstroidTypeError(AstroidError):\n    \"\"\"Raised when a TypeError would be expected in Python code.\"\"\"\nclass AstroidValueError(AstroidError):\n    \"\"\"Raised when a ValueError would be expected in Python code.\"\"\"\nclass InferenceOverwriteError(AstroidError):\n    \"\"\"Raised when an inference tip is overwritten\n    Currently only used for debugging.\n    \"\"\"\nclass ParentMissingError(AstroidError):\n    \"\"\"Raised when a node which is expected to have a parent attribute is missing one\n    Standard attributes:\n        target: The node for which the parent lookup failed.\n    \"\"\"\n    def __init__(self, target: \"nodes.NodeNG\") -> None:\n        self.target = target\n        super().__init__(message=f\"Parent not found on {target!r}.\")\nclass StatementMissing(ParentMissingError):\n    \"\"\"Raised when a call to node.statement() does not return a node. This is because\n    a node in the chain does not have a parent attribute and therefore does not\n    return a node for statement().\n    Standard attributes:\n        target: The node for which the parent lookup failed.\n    \"\"\"\n    def __init__(self, target: \"nodes.NodeNG\") -> None:\n        # pylint: disable-next=bad-super-call\n        # https://github.com/PyCQA/pylint/issues/2903\n        # https://github.com/PyCQA/astroid/pull/1217#discussion_r744149027\n        super(ParentMissingError, self).__init__(\n            message=f\"Statement not found on {target!r}\"\n        )\n# Backwards-compatibility aliases\nOperationError = util.BadOperationMessage\nUnaryOperationError = util.BadUnaryOperationMessage\nBinaryOperationError = util.BadBinaryOperationMessage\nSuperArgumentTypeError = SuperError\nUnresolvableName = NameInferenceError\nNotFoundError = AttributeInferenceError\nAstroidBuildingException = AstroidBuildingError",
        "file_path": "astroid/exceptions.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 6,
        "score": 0.6036071181297302,
        "content": "# Copyright (c) 2009-2014 LOGILAB S.A. (Paris, FRANCE) <contact@logilab.fr>\n# Copyright (c) 2013-2020 Claudiu Popa <pcmanticore@gmail.com>\n# Copyright (c) 2013-2014 Google, Inc.\n# Copyright (c) 2014 Alexander Presnyakov <flagist0@gmail.com>\n# Copyright (c) 2014 Eevee (Alex Munroe) <amunroe@yelp.com>\n# Copyright (c) 2015-2016 Ceridwen <ceridwenv@gmail.com>\n# Copyright (c) 2016-2017 Derek Gustafson <degustaf@gmail.com>\n# Copyright (c) 2016 Jared Garst <jgarst@users.noreply.github.com>\n# Copyright (c) 2017 Hugo <hugovk@users.noreply.github.com>\n# Copyright (c) 2017 Łukasz Rogalski <rogalski.91@gmail.com>\n# Copyright (c) 2017 rr- <rr-@sakuya.pl>\n# Copyright (c) 2018-2019 Ville Skyttä <ville.skytta@iki.fi>\n# Copyright (c) 2018 Tomas Gavenciak <gavento@ucw.cz>\n# Copyright (c) 2018 Serhiy Storchaka <storchaka@gmail.com>\n# Copyright (c) 2018 Nick Drozd <nicholasdrozd@gmail.com>\n# Copyright (c) 2018 Bryce Guinta <bryce.paul.guinta@gmail.com>\n# Copyright (c) 2019-2021 Ashley Whetter <ashley@awhetter.co.uk>\n# Copyright (c) 2019 Hugo van Kemenade <hugovk@users.noreply.github.com>\n# Copyright (c) 2019 Zbigniew Jędrzejewski-Szmek <zbyszek@in.waw.pl>\n# Copyright (c) 2021 Marc Mueller <30130371+cdce8p@users.noreply.github.com>\n# Copyright (c) 2021 Daniël van Noord <13665637+DanielNoord@users.noreply.github.com>\n# Copyright (c) 2021 Pierre Sassoulas <pierre.sassoulas@gmail.com>\n# Copyright (c) 2021 Federico Bond <federicobond@gmail.com>\n# Copyright (c) 2021 hippo91 <guillaume.peillex@gmail.com>\n# Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n# For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n\"\"\"this module contains utilities for rebuilding an _ast tree in\norder to get a single Astroid representation\n\"\"\"\nimport sys\nfrom typing import (\n    TYPE_CHECKING,\n    Callable,\n    Dict,\n    Generator,\n    List,\n    Optional,\n    Tuple,\n    Type,\n    TypeVar,\n    Union,\n    cast,\n    overload,\n)\nfrom astroid import nodes\nfrom astroid._ast import ParserModule, get_parser_module, parse_function_type_comment\nfrom astroid.const import PY37_PLUS, PY38_PLUS, PY39_PLUS, Context\nfrom astroid.manager import AstroidManager\nfrom astroid.nodes import NodeNG\nif sys.version_info >= (3, 8):\n    from typing import Final\nelse:\n    from typing_extensions import Final\nif TYPE_CHECKING:\n    import ast\nREDIRECT: Final[Dict[str, str]] = {\n    \"arguments\": \"Arguments\",\n    \"comprehension\": \"Comprehension\",\n    \"ListCompFor\": \"Comprehension\",\n    \"GenExprFor\": \"Comprehension\",\n    \"excepthandler\": \"ExceptHandler\",\n    \"keyword\": \"Keyword\",\n    \"match_case\": \"MatchCase\",\n}\nT_Doc = TypeVar(\n    \"T_Doc\",\n    \"ast.Module\",\n    \"ast.ClassDef\",\n    Union[\"ast.FunctionDef\", \"ast.AsyncFunctionDef\"],\n)\nT_Function = TypeVar(\"T_Function\", nodes.FunctionDef, nodes.AsyncFunctionDef)\nT_For = TypeVar(\"T_For\", nodes.For, nodes.AsyncFor)\nT_With = TypeVar(\"T_With\", nodes.With, nodes.AsyncWith)\n# noinspection PyMethodMayBeStatic\nclass TreeRebuilder:\n    \"\"\"Rebuilds the _ast tree to become an Astroid tree\"\"\"\n    def __init__(\n        self, manager: AstroidManager, parser_module: Optional[ParserModule] = None\n    ):\n        self._manager = manager\n        self._global_names: List[Dict[str, List[nodes.Global]]] = []\n        self._import_from_nodes: List[nodes.ImportFrom] = []\n        self._delayed_assattr: List[nodes.AssignAttr] = []\n        self._visit_meths: Dict[\n            Type[\"ast.AST\"], Callable[[\"ast.AST\", NodeNG], NodeNG]\n        ] = {}\n        if parser_module is None:\n            self._parser_module = get_parser_module()\n        else:\n            self._parser_module = parser_module\n        self._module = self._parser_module.module\n    def _get_doc(self, node: T_Doc) -> Tuple[T_Doc, Optional[str]]:\n        try:\n            if PY37_PLUS and hasattr(node, \"docstring\"):\n                doc = node.docstring\n                return node, doc\n            if node.body and isinstance(node.body[0], self._module.Expr):\n                first_value = node.body[0].value\n                if isinstance(first_value, self._module.Str) or (\n                    PY38_PLUS\n                    and isinstance(first_value, self._module.Constant)\n                    and isinstance(first_value.value, str)\n                ):\n                    doc = first_value.value if PY38_PLUS else first_value.s\n                    node.body = node.body[1:]\n                    return node, doc\n        except IndexError:\n            pass  # ast built from scratch\n        return node, None\n    def _get_context(\n        self,\n        node: Union[\n            \"ast.Attribute\",\n            \"ast.List\",\n            \"ast.Name\",\n            \"ast.Subscript\",\n            \"ast.Starred\",\n            \"ast.Tuple\",\n        ],\n    ) -> Context:\n        return self._parser_module.context_classes.get(type(node.ctx), Context.Load)\n    def visit_module(\n        self, node: \"ast.Module\", modname: str, modpath: str, package: bool\n    ) -> nodes.Module:\n        \"\"\"visit a Module node by returning a fresh instance of it\n        Note: Method not called by 'visit'\n        \"\"\"\n        node, doc = self._get_doc(node)\n        newnode = nodes.Module(\n            name=modname,\n            doc=doc,\n            file=modpath,\n            path=[modpath],\n            package=package,\n            parent=None,\n        )\n        newnode.postinit([self.visit(child, newnode) for child in node.body])\n        return newnode\n    if sys.version_info >= (3, 10):\n        @overload\n        def visit(self, node: \"ast.arg\", parent: NodeNG) -> nodes.AssignName:\n            ...\n        @overload\n        def visit(self, node: \"ast.arguments\", parent: NodeNG) -> nodes.Arguments:\n            ...\n        @overload\n        def visit(self, node: \"ast.Assert\", parent: NodeNG) -> nodes.Assert:\n            ...\n        @overload\n        def visit(\n            self, node: \"ast.AsyncFunctionDef\", parent: NodeNG\n        ) -> nodes.AsyncFunctionDef:\n            ...\n        @overload\n        def visit(self, node: \"ast.AsyncFor\", parent: NodeNG) -> nodes.AsyncFor:\n            ...\n        @overload\n        def visit(self, node: \"ast.Await\", parent: NodeNG) -> nodes.Await:\n            ...\n        @overload\n        def visit(self, node: \"ast.AsyncWith\", parent: NodeNG) -> nodes.AsyncWith:\n            ...\n        @overload\n        def visit(self, node: \"ast.Assign\", parent: NodeNG) -> nodes.Assign:\n            ...\n        @overload\n        def visit(self, node: \"ast.AnnAssign\", parent: NodeNG) -> nodes.AnnAssign:\n            ...\n        @overload\n        def visit(self, node: \"ast.AugAssign\", parent: NodeNG) -> nodes.AugAssign:\n            ...\n        @overload\n        def visit(self, node: \"ast.BinOp\", parent: NodeNG) -> nodes.BinOp:\n            ...\n        @overload\n        def visit(self, node: \"ast.BoolOp\", parent: NodeNG) -> nodes.BoolOp:\n            ...\n        @overload\n        def visit(self, node: \"ast.Break\", parent: NodeNG) -> nodes.Break:\n            ...\n        @overload\n        def visit(self, node: \"ast.Call\", parent: NodeNG) -> nodes.Call:\n            ...\n        @overload\n        def visit(self, node: \"ast.ClassDef\", parent: NodeNG) -> nodes.ClassDef:\n            ...\n        @overload\n        def visit(self, node: \"ast.Continue\", parent: NodeNG) -> nodes.Continue:\n            ...\n        @overload\n        def visit(self, node: \"ast.Compare\", parent: NodeNG) -> nodes.Compare:\n            ...\n        @overload\n        def visit(\n            self, node: \"ast.comprehension\", parent: NodeNG\n        ) -> nodes.Comprehension:\n            ...\n        @overload\n        def visit(self, node: \"ast.Delete\", parent: NodeNG) -> nodes.Delete:\n            ...\n        @overload\n        def visit(self, node: \"ast.Dict\", parent: NodeNG) -> nodes.Dict:\n            ...\n        @overload\n        def visit(self, node: \"ast.DictComp\", parent: NodeNG) -> nodes.DictComp:\n            ...\n        @overload\n        def visit(self, node: \"ast.Expr\", parent: NodeNG) -> nodes.Expr:\n            ...\n        # Not used in Python 3.8+\n        @overload\n        def visit(self, node: \"ast.Ellipsis\", parent: NodeNG) -> nodes.Const:\n            ...\n        @overload\n        def visit(\n            self, node: \"ast.ExceptHandler\", parent: NodeNG\n        ) -> nodes.ExceptHandler:\n            ...\n        # Not used in Python 3.9+\n        @overload\n        def visit(self, node: \"ast.ExtSlice\", parent: nodes.Subscript) -> nodes.Tuple:\n            ...\n        @overload\n        def visit(self, node: \"ast.For\", parent: NodeNG) -> nodes.For:\n            ...\n        @overload\n        def visit(self, node: \"ast.ImportFrom\", parent: NodeNG) -> nodes.ImportFrom:\n            ...\n        @overload\n        def visit(self, node: \"ast.FunctionDef\", parent: NodeNG) -> nodes.FunctionDef:\n            ...\n        @overload\n        def visit(self, node: \"ast.GeneratorExp\", parent: NodeNG) -> nodes.GeneratorExp:\n            ...\n        @overload\n        def visit(self, node: \"ast.Attribute\", parent: NodeNG) -> nodes.Attribute:\n            ...\n        @overload\n        def visit(self, node: \"ast.Global\", parent: NodeNG) -> nodes.Global:\n            ...\n        @overload\n        def visit(self, node: \"ast.If\", parent: NodeNG) -> nodes.If:\n            ...\n        @overload\n        def visit(self, node: \"ast.IfExp\", parent: NodeNG) -> nodes.IfExp:\n            ...\n        @overload\n        def visit(self, node: \"ast.Import\", parent: NodeNG) -> nodes.Import:\n            ...\n        @overload\n        def visit(self, node: \"ast.JoinedStr\", parent: NodeNG) -> nodes.JoinedStr:\n            ...\n        @overload\n        def visit(\n            self, node: \"ast.FormattedValue\", parent: NodeNG\n        ) -> nodes.FormattedValue:\n            ...\n        @overload\n        def visit(self, node: \"ast.NamedExpr\", parent: NodeNG) -> nodes.NamedExpr:\n            ...\n        # Not used in Python 3.9+\n        @overload\n        def visit(self, node: \"ast.Index\", parent: nodes.Subscript) -> NodeNG:\n            ...\n        @overload\n        def visit(self, node: \"ast.keyword\", parent: NodeNG) -> nodes.Keyword:\n            ...\n        @overload\n        def visit(self, node: \"ast.Lambda\", parent: NodeNG) -> nodes.Lambda:\n            ...\n        @overload\n        def visit(self, node: \"ast.List\", parent: NodeNG) -> nodes.List:\n            ...\n        @overload\n        def visit(self, node: \"ast.ListComp\", parent: NodeNG) -> nodes.ListComp:\n            ...\n        @overload\n        def visit(\n            self, node: \"ast.Name\", parent: NodeNG\n        ) -> Union[nodes.Name, nodes.Const, nodes.AssignName, nodes.DelName]:\n            ...\n        # Not used in Python 3.8+\n        @overload\n        def visit(self, node: \"ast.NameConstant\", parent: NodeNG) -> nodes.Const:\n            ...\n        @overload\n        def visit(self, node: \"ast.Nonlocal\", parent: NodeNG) -> nodes.Nonlocal:\n            ...\n        # Not used in Python 3.8+\n        @overload\n        def visit(self, node: \"ast.Str\", parent: NodeNG) -> nodes.Const:\n            ...\n        # Not used in Python 3.8+\n        @overload\n        def visit(self, node: \"ast.Bytes\", parent: NodeNG) -> nodes.Const:\n            ...\n        # Not used in Python 3.8+\n        @overload\n        def visit(self, node: \"ast.Num\", parent: NodeNG) -> nodes.Const:\n            ...\n        @overload\n        def visit(self, node: \"ast.Constant\", parent: NodeNG) -> nodes.Const:\n            ...\n        @overload\n        def visit(self, node: \"ast.Pass\", parent: NodeNG) -> nodes.Pass:\n            ...\n        @overload\n        def visit(self, node: \"ast.Raise\", parent: NodeNG) -> nodes.Raise:\n            ...\n        @overload\n        def visit(self, node: \"ast.Return\", parent: NodeNG) -> nodes.Return:\n            ...\n        @overload\n        def visit(self, node: \"ast.Set\", parent: NodeNG) -> nodes.Set:\n            ...\n        @overload\n        def visit(self, node: \"ast.SetComp\", parent: NodeNG) -> nodes.SetComp:\n            ...\n        @overload\n        def visit(self, node: \"ast.Slice\", parent: nodes.Subscript) -> nodes.Slice:\n            ...\n        @overload\n        def visit(self, node: \"ast.Subscript\", parent: NodeNG) -> nodes.Subscript:\n            ...\n        @overload\n        def visit(self, node: \"ast.Starred\", parent: NodeNG) -> nodes.Starred:\n            ...\n        @overload\n        def visit(\n            self, node: \"ast.Try\", parent: NodeNG\n        ) -> Union[nodes.TryExcept, nodes.TryFinally]:\n            ...\n        @overload\n        def visit(self, node: \"ast.Tuple\", parent: NodeNG) -> nodes.Tuple:\n            ...\n        @overload\n        def visit(self, node: \"ast.UnaryOp\", parent: NodeNG) -> nodes.UnaryOp:\n            ...\n        @overload\n        def visit(self, node: \"ast.While\", parent: NodeNG) -> nodes.While:\n            ...\n        @overload\n        def visit(self, node: \"ast.With\", parent: NodeNG) -> nodes.With:\n            ...\n        @overload\n        def visit(self, node: \"ast.Yield\", parent: NodeNG) -> nodes.Yield:\n            ...\n        @overload\n        def visit(self, node: \"ast.YieldFrom\", parent: NodeNG) -> nodes.YieldFrom:\n            ...\n        @overload\n        def visit(self, node: \"ast.Match\", parent: NodeNG) -> nodes.Match:\n            ...\n        @overload\n        def visit(self, node: \"ast.match_case\", parent: NodeNG) -> nodes.MatchCase:\n            ...\n        @overload\n        def visit(self, node: \"ast.MatchValue\", parent: NodeNG) -> nodes.MatchValue:\n            ...\n        @overload\n        def visit(\n            self, node: \"ast.MatchSingleton\", parent: NodeNG\n        ) -> nodes.MatchSingleton:\n            ...\n        @overload\n        def visit(\n            self, node: \"ast.MatchSequence\", parent: NodeNG\n        ) -> nodes.MatchSequence:\n            ...\n        @overload\n        def visit(self, node: \"ast.MatchMapping\", parent: NodeNG) -> nodes.MatchMapping:\n            ...\n        @overload\n        def visit(self, node: \"ast.MatchClass\", parent: NodeNG) -> nodes.MatchClass:\n            ...\n        @overload\n        def visit(self, node: \"ast.MatchStar\", parent: NodeNG) -> nodes.MatchStar:\n            ...\n        @overload\n        def visit(self, node: \"ast.MatchAs\", parent: NodeNG) -> nodes.MatchAs:\n            ...\n        @overload\n        def visit(self, node: \"ast.MatchOr\", parent: NodeNG) -> nodes.MatchOr:\n            ...\n        @overload\n        def visit(self, node: \"ast.pattern\", parent: NodeNG) -> nodes.Pattern:\n            ...\n        @overload\n        def visit(self, node: \"ast.AST\", parent: NodeNG) -> NodeNG:\n            ...\n        @overload\n        def visit(self, node: None, parent: NodeNG) -> None:\n            ...\n        def visit(self, node: Optional[\"ast.AST\"], parent: NodeNG) -> Optional[NodeNG]:\n            if node is None:\n                return None\n            cls = node.__class__\n            if cls in self._visit_meths:\n                visit_method = self._visit_meths[cls]\n            else:\n                cls_name = cls.__name__\n                visit_name = \"visit_\" + REDIRECT.get(cls_name, cls_name).lower()\n                visit_method = getattr(self, visit_name)\n                self._visit_meths[cls] = visit_method\n            return visit_method(node, parent)\n    else:\n        @overload\n        def visit(self, node: \"ast.arg\", parent: NodeNG) -> nodes.AssignName:\n            ...\n        @overload\n        def visit(self, node: \"ast.arguments\", parent: NodeNG) -> nodes.Arguments:\n            ...\n        @overload\n        def visit(self, node: \"ast.Assert\", parent: NodeNG) -> nodes.Assert:\n            ...\n        @overload\n        def visit(\n            self, node: \"ast.AsyncFunctionDef\", parent: NodeNG\n        ) -> nodes.AsyncFunctionDef:\n            ...\n        @overload\n        def visit(self, node: \"ast.AsyncFor\", parent: NodeNG) -> nodes.AsyncFor:\n            ...\n        @overload\n        def visit(self, node: \"ast.Await\", parent: NodeNG) -> nodes.Await:\n            ...\n        @overload\n        def visit(self, node: \"ast.AsyncWith\", parent: NodeNG) -> nodes.AsyncWith:\n            ...\n        @overload\n        def visit(self, node: \"ast.Assign\", parent: NodeNG) -> nodes.Assign:\n            ...\n        @overload\n        def visit(self, node: \"ast.AnnAssign\", parent: NodeNG) -> nodes.AnnAssign:\n            ...\n        @overload\n        def visit(self, node: \"ast.AugAssign\", parent: NodeNG) -> nodes.AugAssign:\n            ...\n        @overload\n        def visit(self, node: \"ast.BinOp\", parent: NodeNG) -> nodes.BinOp:\n            ...\n        @overload\n        def visit(self, node: \"ast.BoolOp\", parent: NodeNG) -> nodes.BoolOp:\n            ...\n        @overload\n        def visit(self, node: \"ast.Break\", parent: NodeNG) -> nodes.Break:\n            ...\n        @overload\n        def visit(self, node: \"ast.Call\", parent: NodeNG) -> nodes.Call:\n            ...\n        @overload\n        def visit(self, node: \"ast.ClassDef\", parent: NodeNG) -> nodes.ClassDef:\n            ...\n        @overload\n        def visit(self, node: \"ast.Continue\", parent: NodeNG) -> nodes.Continue:\n            ...\n        @overload\n        def visit(self, node: \"ast.Compare\", parent: NodeNG) -> nodes.Compare:\n            ...\n        @overload\n        def visit(\n            self, node: \"ast.comprehension\", parent: NodeNG\n        ) -> nodes.Comprehension:\n            ...\n        @overload\n        def visit(self, node: \"ast.Delete\", parent: NodeNG) -> nodes.Delete:\n            ...\n        @overload\n        def visit(self, node: \"ast.Dict\", parent: NodeNG) -> nodes.Dict:\n            ...\n        @overload\n        def visit(self, node: \"ast.DictComp\", parent: NodeNG) -> nodes.DictComp:\n            ...\n        @overload\n        def visit(self, node: \"ast.Expr\", parent: NodeNG) -> nodes.Expr:\n            ...\n        # Not used in Python 3.8+\n        @overload\n        def visit(self, node: \"ast.Ellipsis\", parent: NodeNG) -> nodes.Const:\n            ...\n        @overload\n        def visit(\n            self, node: \"ast.ExceptHandler\", parent: NodeNG\n        ) -> nodes.ExceptHandler:\n            ...\n        # Not used in Python 3.9+\n        @overload\n        def visit(self, node: \"ast.ExtSlice\", parent: nodes.Subscript) -> nodes.Tuple:\n            ...\n        @overload\n        def visit(self, node: \"ast.For\", parent: NodeNG) -> nodes.For:\n            ...\n        @overload\n        def visit(self, node: \"ast.ImportFrom\", parent: NodeNG) -> nodes.ImportFrom:\n            ...\n        @overload\n        def visit(self, node: \"ast.FunctionDef\", parent: NodeNG) -> nodes.FunctionDef:\n            ...\n        @overload\n        def visit(self, node: \"ast.GeneratorExp\", parent: NodeNG) -> nodes.GeneratorExp:",
        "file_path": "astroid/rebuilder.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 7,
        "score": 0.5918296575546265,
        "content": "        self, node: \"ast.AST\", parent: NodeNG, node_name: str\n    ) -> nodes.AssignName:\n        ...\n    @overload\n    def visit_assignname(\n        self, node: \"ast.AST\", parent: NodeNG, node_name: None\n    ) -> None:\n        ...\n    def visit_assignname(\n        self, node: \"ast.AST\", parent: NodeNG, node_name: Optional[str]\n    ) -> Optional[nodes.AssignName]:\n        \"\"\"visit a node and return a AssignName node\n        Note: Method not called by 'visit'\n        \"\"\"\n        if node_name is None:\n            return None\n        newnode = nodes.AssignName(\n            node_name,\n            node.lineno,\n            node.col_offset,\n            parent,\n        )\n        self._save_assignment(newnode)\n        return newnode\n    def visit_augassign(self, node: \"ast.AugAssign\", parent: NodeNG) -> nodes.AugAssign:\n        \"\"\"visit a AugAssign node by returning a fresh instance of it\"\"\"\n        newnode = nodes.AugAssign(\n            self._parser_module.bin_op_classes[type(node.op)] + \"=\",\n            node.lineno,\n            node.col_offset,\n            parent,\n        )\n        newnode.postinit(\n            self.visit(node.target, newnode), self.visit(node.value, newnode)\n        )\n        return newnode\n    def visit_binop(self, node: \"ast.BinOp\", parent: NodeNG) -> nodes.BinOp:\n        \"\"\"visit a BinOp node by returning a fresh instance of it\"\"\"\n        newnode = nodes.BinOp(\n            self._parser_module.bin_op_classes[type(node.op)],\n            node.lineno,\n            node.col_offset,\n            parent,\n        )\n        newnode.postinit(\n            self.visit(node.left, newnode), self.visit(node.right, newnode)\n        )\n        return newnode\n    def visit_boolop(self, node: \"ast.BoolOp\", parent: NodeNG) -> nodes.BoolOp:\n        \"\"\"visit a BoolOp node by returning a fresh instance of it\"\"\"\n        newnode = nodes.BoolOp(\n            self._parser_module.bool_op_classes[type(node.op)],\n            node.lineno,\n            node.col_offset,\n            parent,\n        )\n        newnode.postinit([self.visit(child, newnode) for child in node.values])\n        return newnode\n    def visit_break(self, node: \"ast.Break\", parent: NodeNG) -> nodes.Break:\n        \"\"\"visit a Break node by returning a fresh instance of it\"\"\"\n        return nodes.Break(node.lineno, node.col_offset, parent)\n    def visit_call(self, node: \"ast.Call\", parent: NodeNG) -> nodes.Call:\n        \"\"\"visit a CallFunc node by returning a fresh instance of it\"\"\"\n        newnode = nodes.Call(node.lineno, node.col_offset, parent)\n        newnode.postinit(\n            func=self.visit(node.func, newnode),\n            args=[self.visit(child, newnode) for child in node.args],\n            keywords=[self.visit(child, newnode) for child in node.keywords],\n        )\n        return newnode\n    def visit_classdef(\n        self, node: \"ast.ClassDef\", parent: NodeNG, newstyle: bool = True\n    ) -> nodes.ClassDef:\n        \"\"\"visit a ClassDef node to become astroid\"\"\"\n        node, doc = self._get_doc(node)\n        newnode = nodes.ClassDef(node.name, doc, node.lineno, node.col_offset, parent)\n        metaclass = None\n        for keyword in node.keywords:\n            if keyword.arg == \"metaclass\":\n                metaclass = self.visit(keyword, newnode).value\n                break\n        decorators = self.visit_decorators(node, newnode)\n        newnode.postinit(\n            [self.visit(child, newnode) for child in node.bases],\n            [self.visit(child, newnode) for child in node.body],\n            decorators,\n            newstyle,\n            metaclass,\n            [\n                self.visit(kwd, newnode)\n                for kwd in node.keywords\n                if kwd.arg != \"metaclass\"\n            ],\n        )\n        return newnode\n    def visit_continue(self, node: \"ast.Continue\", parent: NodeNG) -> nodes.Continue:\n        \"\"\"visit a Continue node by returning a fresh instance of it\"\"\"\n        return nodes.Continue(node.lineno, node.col_offset, parent)\n    def visit_compare(self, node: \"ast.Compare\", parent: NodeNG) -> nodes.Compare:\n        \"\"\"visit a Compare node by returning a fresh instance of it\"\"\"\n        newnode = nodes.Compare(node.lineno, node.col_offset, parent)\n        newnode.postinit(\n            self.visit(node.left, newnode),\n            [\n                (\n                    self._parser_module.cmp_op_classes[op.__class__],\n                    self.visit(expr, newnode),\n                )\n                for (op, expr) in zip(node.ops, node.comparators)\n            ],\n        )\n        return newnode\n    def visit_comprehension(\n        self, node: \"ast.comprehension\", parent: NodeNG\n    ) -> nodes.Comprehension:\n        \"\"\"visit a Comprehension node by returning a fresh instance of it\"\"\"\n        newnode = nodes.Comprehension(parent)\n        newnode.postinit(\n            self.visit(node.target, newnode),\n            self.visit(node.iter, newnode),\n            [self.visit(child, newnode) for child in node.ifs],\n            bool(node.is_async),\n        )\n        return newnode\n    def visit_decorators(\n        self,\n        node: Union[\"ast.ClassDef\", \"ast.FunctionDef\", \"ast.AsyncFunctionDef\"],\n        parent: NodeNG,\n    ) -> Optional[nodes.Decorators]:\n        \"\"\"visit a Decorators node by returning a fresh instance of it\n        Note: Method not called by 'visit'\n        \"\"\"\n        if not node.decorator_list:\n            return None\n        # /!\\ node is actually an _ast.FunctionDef node while\n        # parent is an astroid.nodes.FunctionDef node\n        if PY38_PLUS:\n            # Set the line number of the first decorator for Python 3.8+.\n            lineno = node.decorator_list[0].lineno\n        else:\n            lineno = node.lineno\n        newnode = nodes.Decorators(lineno, node.col_offset, parent)\n        newnode.postinit([self.visit(child, newnode) for child in node.decorator_list])\n        return newnode\n    def visit_delete(self, node: \"ast.Delete\", parent: NodeNG) -> nodes.Delete:\n        \"\"\"visit a Delete node by returning a fresh instance of it\"\"\"\n        newnode = nodes.Delete(node.lineno, node.col_offset, parent)\n        newnode.postinit([self.visit(child, newnode) for child in node.targets])\n        return newnode\n    def _visit_dict_items(\n        self, node: \"ast.Dict\", parent: NodeNG, newnode: nodes.Dict\n    ) -> Generator[Tuple[NodeNG, NodeNG], None, None]:\n        for key, value in zip(node.keys, node.values):\n            rebuilt_key: NodeNG\n            rebuilt_value = self.visit(value, newnode)\n            if not key:\n                # Extended unpacking\n                rebuilt_key = nodes.DictUnpack(\n                    rebuilt_value.lineno, rebuilt_value.col_offset, parent\n                )\n            else:\n                rebuilt_key = self.visit(key, newnode)\n            yield rebuilt_key, rebuilt_value\n    def visit_dict(self, node: \"ast.Dict\", parent: NodeNG) -> nodes.Dict:\n        \"\"\"visit a Dict node by returning a fresh instance of it\"\"\"\n        newnode = nodes.Dict(node.lineno, node.col_offset, parent)\n        items = list(self._visit_dict_items(node, parent, newnode))\n        newnode.postinit(items)\n        return newnode\n    def visit_dictcomp(self, node: \"ast.DictComp\", parent: NodeNG) -> nodes.DictComp:\n        \"\"\"visit a DictComp node by returning a fresh instance of it\"\"\"\n        newnode = nodes.DictComp(node.lineno, node.col_offset, parent)\n        newnode.postinit(\n            self.visit(node.key, newnode),\n            self.visit(node.value, newnode),\n            [self.visit(child, newnode) for child in node.generators],\n        )\n        return newnode\n    def visit_expr(self, node: \"ast.Expr\", parent: NodeNG) -> nodes.Expr:\n        \"\"\"visit a Expr node by returning a fresh instance of it\"\"\"\n        newnode = nodes.Expr(node.lineno, node.col_offset, parent)\n        newnode.postinit(self.visit(node.value, newnode))\n        return newnode\n    # Not used in Python 3.8+.\n    def visit_ellipsis(self, node: \"ast.Ellipsis\", parent: NodeNG) -> nodes.Const:\n        \"\"\"visit an Ellipsis node by returning a fresh instance of Const\"\"\"\n        return nodes.Const(\n            value=Ellipsis,\n            lineno=node.lineno,\n            col_offset=node.col_offset,\n            parent=parent,\n        )\n    def visit_excepthandler(\n        self, node: \"ast.ExceptHandler\", parent: NodeNG\n    ) -> nodes.ExceptHandler:\n        \"\"\"visit an ExceptHandler node by returning a fresh instance of it\"\"\"\n        newnode = nodes.ExceptHandler(node.lineno, node.col_offset, parent)\n        newnode.postinit(\n            self.visit(node.type, newnode),\n            self.visit_assignname(node, newnode, node.name),\n            [self.visit(child, newnode) for child in node.body],\n        )\n        return newnode\n    # Not used in Python 3.9+.\n    def visit_extslice(\n        self, node: \"ast.ExtSlice\", parent: nodes.Subscript\n    ) -> nodes.Tuple:\n        \"\"\"visit an ExtSlice node by returning a fresh instance of Tuple\"\"\"\n        newnode = nodes.Tuple(ctx=Context.Load, parent=parent)\n        newnode.postinit([self.visit(dim, newnode) for dim in node.dims])  # type: ignore[attr-defined]\n        return newnode\n    @overload\n    def _visit_for(\n        self, cls: Type[nodes.For], node: \"ast.For\", parent: NodeNG\n    ) -> nodes.For:\n        ...\n    @overload\n    def _visit_for(\n        self, cls: Type[nodes.AsyncFor], node: \"ast.AsyncFor\", parent: NodeNG\n    ) -> nodes.AsyncFor:\n        ...\n    def _visit_for(\n        self, cls: Type[T_For], node: Union[\"ast.For\", \"ast.AsyncFor\"], parent: NodeNG\n    ) -> T_For:\n        \"\"\"visit a For node by returning a fresh instance of it\"\"\"\n        newnode = cls(node.lineno, node.col_offset, parent)\n        type_annotation = self.check_type_comment(node, parent=newnode)\n        newnode.postinit(\n            target=self.visit(node.target, newnode),\n            iter=self.visit(node.iter, newnode),\n            body=[self.visit(child, newnode) for child in node.body],\n            orelse=[self.visit(child, newnode) for child in node.orelse],\n            type_annotation=type_annotation,\n        )\n        return newnode\n    def visit_for(self, node: \"ast.For\", parent: NodeNG) -> nodes.For:\n        return self._visit_for(nodes.For, node, parent)\n    def visit_importfrom(\n        self, node: \"ast.ImportFrom\", parent: NodeNG\n    ) -> nodes.ImportFrom:\n        \"\"\"visit an ImportFrom node by returning a fresh instance of it\"\"\"\n        names = [(alias.name, alias.asname) for alias in node.names]\n        newnode = nodes.ImportFrom(\n            node.module or \"\",\n            names,\n            node.level or None,\n            node.lineno,\n            node.col_offset,\n            parent,\n        )\n        # store From names to add them to locals after building\n        self._import_from_nodes.append(newnode)\n        return newnode\n    @overload\n    def _visit_functiondef(\n        self, cls: Type[nodes.FunctionDef], node: \"ast.FunctionDef\", parent: NodeNG\n    ) -> nodes.FunctionDef:\n        ...\n    @overload\n    def _visit_functiondef(\n        self,\n        cls: Type[nodes.AsyncFunctionDef],\n        node: \"ast.AsyncFunctionDef\",\n        parent: NodeNG,\n    ) -> nodes.AsyncFunctionDef:\n        ...\n    def _visit_functiondef(\n        self,\n        cls: Type[T_Function],\n        node: Union[\"ast.FunctionDef\", \"ast.AsyncFunctionDef\"],\n        parent: NodeNG,\n    ) -> T_Function:\n        \"\"\"visit an FunctionDef node to become astroid\"\"\"\n        self._global_names.append({})\n        node, doc = self._get_doc(node)\n        lineno = node.lineno\n        if PY38_PLUS and node.decorator_list:\n            # Python 3.8 sets the line number of a decorated function\n            # to be the actual line number of the function, but the\n            # previous versions expected the decorator's line number instead.\n            # We reset the function's line number to that of the\n            # first decorator to maintain backward compatibility.\n            # It's not ideal but this discrepancy was baked into\n            # the framework for *years*.\n            lineno = node.decorator_list[0].lineno\n        newnode = cls(node.name, doc, lineno, node.col_offset, parent)\n        decorators = self.visit_decorators(node, newnode)\n        returns: Optional[NodeNG]\n        if node.returns:\n            returns = self.visit(node.returns, newnode)\n        else:\n            returns = None\n        type_comment_args = type_comment_returns = None\n        type_comment_annotation = self.check_function_type_comment(node, newnode)\n        if type_comment_annotation:\n            type_comment_returns, type_comment_args = type_comment_annotation\n        newnode.postinit(\n            args=self.visit(node.args, newnode),\n            body=[self.visit(child, newnode) for child in node.body],\n            decorators=decorators,\n            returns=returns,\n            type_comment_returns=type_comment_returns,\n            type_comment_args=type_comment_args,\n        )\n        self._global_names.pop()\n        return newnode\n    def visit_functiondef(\n        self, node: \"ast.FunctionDef\", parent: NodeNG\n    ) -> nodes.FunctionDef:\n        return self._visit_functiondef(nodes.FunctionDef, node, parent)\n    def visit_generatorexp(\n        self, node: \"ast.GeneratorExp\", parent: NodeNG\n    ) -> nodes.GeneratorExp:\n        \"\"\"visit a GeneratorExp node by returning a fresh instance of it\"\"\"\n        newnode = nodes.GeneratorExp(node.lineno, node.col_offset, parent)\n        newnode.postinit(\n            self.visit(node.elt, newnode),\n            [self.visit(child, newnode) for child in node.generators],\n        )\n        return newnode\n    def visit_attribute(\n        self, node: \"ast.Attribute\", parent: NodeNG\n    ) -> Union[nodes.Attribute, nodes.AssignAttr, nodes.DelAttr]:\n        \"\"\"visit an Attribute node by returning a fresh instance of it\"\"\"\n        context = self._get_context(node)\n        newnode: Union[nodes.Attribute, nodes.AssignAttr, nodes.DelAttr]\n        if context == Context.Del:\n            # FIXME : maybe we should reintroduce and visit_delattr ?\n            # for instance, deactivating assign_ctx\n            newnode = nodes.DelAttr(node.attr, node.lineno, node.col_offset, parent)\n        elif context == Context.Store:\n            newnode = nodes.AssignAttr(node.attr, node.lineno, node.col_offset, parent)\n            # Prohibit a local save if we are in an ExceptHandler.\n            if not isinstance(parent, nodes.ExceptHandler):\n                self._delayed_assattr.append(newnode)\n        else:\n            newnode = nodes.Attribute(node.attr, node.lineno, node.col_offset, parent)\n        newnode.postinit(self.visit(node.value, newnode))\n        return newnode\n    def visit_global(self, node: \"ast.Global\", parent: NodeNG) -> nodes.Global:\n        \"\"\"visit a Global node to become astroid\"\"\"\n        newnode = nodes.Global(\n            node.names,\n            node.lineno,\n            node.col_offset,\n            parent,\n        )\n        if self._global_names:  # global at the module level, no effect\n            for name in node.names:\n                self._global_names[-1].setdefault(name, []).append(newnode)\n        return newnode\n    def visit_if(self, node: \"ast.If\", parent: NodeNG) -> nodes.If:\n        \"\"\"visit an If node by returning a fresh instance of it\"\"\"\n        newnode = nodes.If(node.lineno, node.col_offset, parent)\n        newnode.postinit(\n            self.visit(node.test, newnode),\n            [self.visit(child, newnode) for child in node.body],\n            [self.visit(child, newnode) for child in node.orelse],\n        )\n        return newnode\n    def visit_ifexp(self, node: \"ast.IfExp\", parent: NodeNG) -> nodes.IfExp:\n        \"\"\"visit a IfExp node by returning a fresh instance of it\"\"\"\n        newnode = nodes.IfExp(node.lineno, node.col_offset, parent)\n        newnode.postinit(\n            self.visit(node.test, newnode),\n            self.visit(node.body, newnode),\n            self.visit(node.orelse, newnode),\n        )\n        return newnode\n    def visit_import(self, node: \"ast.Import\", parent: NodeNG) -> nodes.Import:\n        \"\"\"visit a Import node by returning a fresh instance of it\"\"\"\n        names = [(alias.name, alias.asname) for alias in node.names]\n        newnode = nodes.Import(\n            names,\n            node.lineno,\n            node.col_offset,\n            parent,\n        )\n        # save import names in parent's locals:\n        for (name, asname) in newnode.names:\n            name = asname or name\n            parent.set_local(name.split(\".\")[0], newnode)\n        return newnode\n    def visit_joinedstr(self, node: \"ast.JoinedStr\", parent: NodeNG) -> nodes.JoinedStr:\n        newnode = nodes.JoinedStr(node.lineno, node.col_offset, parent)\n        newnode.postinit([self.visit(child, newnode) for child in node.values])\n        return newnode\n    def visit_formattedvalue(\n        self, node: \"ast.FormattedValue\", parent: NodeNG\n    ) -> nodes.FormattedValue:\n        newnode = nodes.FormattedValue(node.lineno, node.col_offset, parent)\n        newnode.postinit(\n            self.visit(node.value, newnode),\n            node.conversion,\n            self.visit(node.format_spec, newnode),\n        )\n        return newnode\n    def visit_namedexpr(self, node: \"ast.NamedExpr\", parent: NodeNG) -> nodes.NamedExpr:\n        newnode = nodes.NamedExpr(node.lineno, node.col_offset, parent)\n        newnode.postinit(\n            self.visit(node.target, newnode), self.visit(node.value, newnode)\n        )\n        return newnode\n    # Not used in Python 3.9+.\n    def visit_index(self, node: \"ast.Index\", parent: nodes.Subscript) -> NodeNG:\n        \"\"\"visit a Index node by returning a fresh instance of NodeNG\"\"\"\n        return self.visit(node.value, parent)  # type: ignore[attr-defined]\n    def visit_keyword(self, node: \"ast.keyword\", parent: NodeNG) -> nodes.Keyword:\n        \"\"\"visit a Keyword node by returning a fresh instance of it\"\"\"\n        if PY39_PLUS:\n            newnode = nodes.Keyword(node.arg, node.lineno, node.col_offset, parent)\n        else:\n            newnode = nodes.Keyword(node.arg, parent=parent)\n        newnode.postinit(self.visit(node.value, newnode))\n        return newnode\n    def visit_lambda(self, node: \"ast.Lambda\", parent: NodeNG) -> nodes.Lambda:\n        \"\"\"visit a Lambda node by returning a fresh instance of it\"\"\"\n        newnode = nodes.Lambda(node.lineno, node.col_offset, parent)\n        newnode.postinit(self.visit(node.args, newnode), self.visit(node.body, newnode))\n        return newnode\n    def visit_list(self, node: \"ast.List\", parent: NodeNG) -> nodes.List:\n        \"\"\"visit a List node by returning a fresh instance of it\"\"\"\n        context = self._get_context(node)\n        newnode = nodes.List(\n            ctx=context, lineno=node.lineno, col_offset=node.col_offset, parent=parent\n        )\n        newnode.postinit([self.visit(child, newnode) for child in node.elts])\n        return newnode\n    def visit_listcomp(self, node: \"ast.ListComp\", parent: NodeNG) -> nodes.ListComp:\n        \"\"\"visit a ListComp node by returning a fresh instance of it\"\"\"\n        newnode = nodes.ListComp(node.lineno, node.col_offset, parent)\n        newnode.postinit(\n            self.visit(node.elt, newnode),\n            [self.visit(child, newnode) for child in node.generators],\n        )\n        return newnode\n    def visit_name(\n        self, node: \"ast.Name\", parent: NodeNG\n    ) -> Union[nodes.Name, nodes.AssignName, nodes.DelName]:\n        \"\"\"visit a Name node by returning a fresh instance of it\"\"\"\n        context = self._get_context(node)\n        newnode: Union[nodes.Name, nodes.AssignName, nodes.DelName]\n        if context == Context.Del:\n            newnode = nodes.DelName(node.id, node.lineno, node.col_offset, parent)\n        elif context == Context.Store:\n            newnode = nodes.AssignName(node.id, node.lineno, node.col_offset, parent)\n        else:\n            newnode = nodes.Name(node.id, node.lineno, node.col_offset, parent)\n        # XXX REMOVE me :\n        if context in (Context.Del, Context.Store):  # 'Aug' ??\n            newnode = cast(Union[nodes.AssignName, nodes.DelName], newnode)\n            self._save_assignment(newnode)\n        return newnode\n    # Not used in Python 3.8+.\n    def visit_nameconstant(\n        self, node: \"ast.NameConstant\", parent: NodeNG\n    ) -> nodes.Const:\n        # For singleton values True / False / None\n        return nodes.Const(\n            node.value,\n            node.lineno,\n            node.col_offset,\n            parent,\n        )\n    def visit_nonlocal(self, node: \"ast.Nonlocal\", parent: NodeNG) -> nodes.Nonlocal:\n        \"\"\"visit a Nonlocal node and return a new instance of it\"\"\"\n        return nodes.Nonlocal(\n            node.names,\n            node.lineno,\n            node.col_offset,\n            parent,\n        )\n    def visit_constant(self, node: \"ast.Constant\", parent: NodeNG) -> nodes.Const:\n        \"\"\"visit a Constant node by returning a fresh instance of Const\"\"\"\n        return nodes.Const(\n            node.value,\n            node.lineno,\n            node.col_offset,\n            parent,\n            node.kind,\n        )\n    # Not used in Python 3.8+.\n    def visit_str(\n        self, node: Union[\"ast.Str\", \"ast.Bytes\"], parent: NodeNG\n    ) -> nodes.Const:\n        \"\"\"visit a String/Bytes node by returning a fresh instance of Const\"\"\"\n        return nodes.Const(\n            node.s,\n            node.lineno,\n            node.col_offset,\n            parent,\n        )\n    # Not used in Python 3.8+\n    visit_bytes = visit_str\n    # Not used in Python 3.8+.\n    def visit_num(self, node: \"ast.Num\", parent: NodeNG) -> nodes.Const:\n        \"\"\"visit a Num node by returning a fresh instance of Const\"\"\"\n        return nodes.Const(\n            node.n,\n            node.lineno,",
        "file_path": "astroid/rebuilder.py",
        "chunk_index": 2,
        "metadata": {}
      },
      {
        "rank": 8,
        "score": 0.5906869173049927,
        "content": "# Copyright (c) 2006-2011, 2013-2014 LOGILAB S.A. (Paris, FRANCE) <contact@logilab.fr>\n# Copyright (c) 2014-2020 Claudiu Popa <pcmanticore@gmail.com>\n# Copyright (c) 2014 BioGeek <jeroen.vangoey@gmail.com>\n# Copyright (c) 2014 Google, Inc.\n# Copyright (c) 2014 Eevee (Alex Munroe) <amunroe@yelp.com>\n# Copyright (c) 2015-2016 Ceridwen <ceridwenv@gmail.com>\n# Copyright (c) 2016 Derek Gustafson <degustaf@gmail.com>\n# Copyright (c) 2017 Iva Miholic <ivamiho@gmail.com>\n# Copyright (c) 2018 Bryce Guinta <bryce.paul.guinta@gmail.com>\n# Copyright (c) 2018 Nick Drozd <nicholasdrozd@gmail.com>\n# Copyright (c) 2019 Raphael Gaschignard <raphael@makeleaps.com>\n# Copyright (c) 2020-2021 hippo91 <guillaume.peillex@gmail.com>\n# Copyright (c) 2020 Raphael Gaschignard <raphael@rtpg.co>\n# Copyright (c) 2020 Anubhav <35621759+anubh-v@users.noreply.github.com>\n# Copyright (c) 2020 Ashley Whetter <ashley@awhetter.co.uk>\n# Copyright (c) 2021 Daniël van Noord <13665637+DanielNoord@users.noreply.github.com>\n# Copyright (c) 2021 Pierre Sassoulas <pierre.sassoulas@gmail.com>\n# Copyright (c) 2021 grayjk <grayjk@gmail.com>\n# Copyright (c) 2021 Marc Mueller <30130371+cdce8p@users.noreply.github.com>\n# Copyright (c) 2021 Andrew Haigh <hello@nelf.in>\n# Copyright (c) 2021 DudeNr33 <3929834+DudeNr33@users.noreply.github.com>\n# Copyright (c) 2021 pre-commit-ci[bot] <bot@noreply.github.com>\n# Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n# For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n\"\"\"astroid manager: avoid multiple astroid build of a same module when\npossible by providing a class responsible to get astroid representation\nfrom various source and using a cache of built modules)\n\"\"\"\nimport os\nimport types\nimport zipimport\nfrom typing import TYPE_CHECKING, ClassVar, List\nfrom astroid.exceptions import AstroidBuildingError, AstroidImportError\nfrom astroid.interpreter._import import spec\nfrom astroid.modutils import (\n    NoSourceFile,\n    file_info_from_modpath,\n    get_source_file,\n    is_module_name_part_of_extension_package_whitelist,\n    is_python_source,\n    is_standard_module,\n    load_module_from_name,\n    modpath_from_file,\n)\nfrom astroid.transforms import TransformVisitor\nif TYPE_CHECKING:\n    from astroid import nodes\nZIP_IMPORT_EXTS = (\".zip\", \".egg\", \".whl\", \".pyz\", \".pyzw\")\ndef safe_repr(obj):\n    try:\n        return repr(obj)\n    except Exception:  # pylint: disable=broad-except\n        return \"???\"\nclass AstroidManager:\n    \"\"\"Responsible to build astroid from files or modules.\n    Use the Borg (singleton) pattern.\n    \"\"\"\n    name = \"astroid loader\"\n    brain = {}\n    max_inferable_values: ClassVar[int] = 100\n    def __init__(self):\n        self.__dict__ = AstroidManager.brain\n        if not self.__dict__:\n            # NOTE: cache entries are added by the [re]builder\n            self.astroid_cache = {}\n            self._mod_file_cache = {}\n            self._failed_import_hooks = []\n            self.always_load_extensions = False\n            self.optimize_ast = False\n            self.extension_package_whitelist = set()\n            self._transform = TransformVisitor()\n    @property\n    def register_transform(self):\n        # This and unregister_transform below are exported for convenience\n        return self._transform.register_transform\n    @property\n    def unregister_transform(self):\n        return self._transform.unregister_transform\n    @property\n    def builtins_module(self):\n        return self.astroid_cache[\"builtins\"]\n    def visit_transforms(self, node):\n        \"\"\"Visit the transforms and apply them to the given *node*.\"\"\"\n        return self._transform.visit(node)\n    def ast_from_file(self, filepath, modname=None, fallback=True, source=False):\n        \"\"\"given a module name, return the astroid object\"\"\"\n        try:\n            filepath = get_source_file(filepath, include_no_ext=True)\n            source = True\n        except NoSourceFile:\n            pass\n        if modname is None:\n            try:\n                modname = \".\".join(modpath_from_file(filepath))\n            except ImportError:\n                modname = filepath\n        if (\n            modname in self.astroid_cache\n            and self.astroid_cache[modname].file == filepath\n        ):\n            return self.astroid_cache[modname]\n        if source:\n            # pylint: disable=import-outside-toplevel; circular import\n            from astroid.builder import AstroidBuilder\n            return AstroidBuilder(self).file_build(filepath, modname)\n        if fallback and modname:\n            return self.ast_from_module_name(modname)\n        raise AstroidBuildingError(\"Unable to build an AST for {path}.\", path=filepath)\n    def ast_from_string(self, data, modname=\"\", filepath=None):\n        \"\"\"Given some source code as a string, return its corresponding astroid object\"\"\"\n        # pylint: disable=import-outside-toplevel; circular import\n        from astroid.builder import AstroidBuilder\n        return AstroidBuilder(self).string_build(data, modname, filepath)\n    def _build_stub_module(self, modname):\n        # pylint: disable=import-outside-toplevel; circular import\n        from astroid.builder import AstroidBuilder\n        return AstroidBuilder(self).string_build(\"\", modname)\n    def _build_namespace_module(self, modname: str, path: List[str]) -> \"nodes.Module\":\n        # pylint: disable=import-outside-toplevel; circular import\n        from astroid.builder import build_namespace_package_module\n        return build_namespace_package_module(modname, path)\n    def _can_load_extension(self, modname: str) -> bool:\n        if self.always_load_extensions:\n            return True\n        if is_standard_module(modname):\n            return True\n        return is_module_name_part_of_extension_package_whitelist(\n            modname, self.extension_package_whitelist\n        )\n    def ast_from_module_name(self, modname, context_file=None):\n        \"\"\"given a module name, return the astroid object\"\"\"\n        if modname in self.astroid_cache:\n            return self.astroid_cache[modname]\n        if modname == \"__main__\":\n            return self._build_stub_module(modname)\n        if context_file:\n            old_cwd = os.getcwd()\n            os.chdir(os.path.dirname(context_file))\n        try:\n            found_spec = self.file_from_module_name(modname, context_file)\n            if found_spec.type == spec.ModuleType.PY_ZIPMODULE:\n                module = self.zip_import_data(found_spec.location)\n                if module is not None:\n                    return module\n            elif found_spec.type in (\n                spec.ModuleType.C_BUILTIN,\n                spec.ModuleType.C_EXTENSION,\n            ):\n                if (\n                    found_spec.type == spec.ModuleType.C_EXTENSION\n                    and not self._can_load_extension(modname)\n                ):\n                    return self._build_stub_module(modname)\n                try:\n                    module = load_module_from_name(modname)\n                except Exception as e:\n                    raise AstroidImportError(\n                        \"Loading {modname} failed with:\\n{error}\",\n                        modname=modname,\n                        path=found_spec.location,\n                    ) from e\n                return self.ast_from_module(module, modname)\n            elif found_spec.type == spec.ModuleType.PY_COMPILED:\n                raise AstroidImportError(\n                    \"Unable to load compiled module {modname}.\",\n                    modname=modname,\n                    path=found_spec.location,\n                )\n            elif found_spec.type == spec.ModuleType.PY_NAMESPACE:\n                return self._build_namespace_module(\n                    modname, found_spec.submodule_search_locations\n                )\n            elif found_spec.type == spec.ModuleType.PY_FROZEN:\n                return self._build_stub_module(modname)\n            if found_spec.location is None:\n                raise AstroidImportError(\n                    \"Can't find a file for module {modname}.\", modname=modname\n                )\n            return self.ast_from_file(found_spec.location, modname, fallback=False)\n        except AstroidBuildingError as e:\n            for hook in self._failed_import_hooks:\n                try:\n                    return hook(modname)\n                except AstroidBuildingError:\n                    pass\n            raise e\n        finally:\n            if context_file:\n                os.chdir(old_cwd)\n    def zip_import_data(self, filepath):\n        if zipimport is None:\n            return None\n        # pylint: disable=import-outside-toplevel; circular import\n        from astroid.builder import AstroidBuilder\n        builder = AstroidBuilder(self)\n        for ext in ZIP_IMPORT_EXTS:\n            try:\n                eggpath, resource = filepath.rsplit(ext + os.path.sep, 1)\n            except ValueError:\n                continue\n            try:\n                importer = zipimport.zipimporter(eggpath + ext)\n                # pylint: enable=no-member\n                zmodname = resource.replace(os.path.sep, \".\")\n                if importer.is_package(resource):\n                    zmodname = zmodname + \".__init__\"\n                module = builder.string_build(\n                    importer.get_source(resource), zmodname, filepath\n                )\n                return module\n            except Exception:  # pylint: disable=broad-except\n                continue\n        return None\n    def file_from_module_name(self, modname, contextfile):\n        try:\n            value = self._mod_file_cache[(modname, contextfile)]\n        except KeyError:\n            try:\n                value = file_info_from_modpath(\n                    modname.split(\".\"), context_file=contextfile\n                )\n            except ImportError as e:\n                value = AstroidImportError(\n                    \"Failed to import module {modname} with error:\\n{error}.\",\n                    modname=modname,\n                    # we remove the traceback here to save on memory usage (since these exceptions are cached)\n                    error=e.with_traceback(None),\n                )\n            self._mod_file_cache[(modname, contextfile)] = value\n        if isinstance(value, AstroidBuildingError):\n            # we remove the traceback here to save on memory usage (since these exceptions are cached)\n            raise value.with_traceback(None)\n        return value\n    def ast_from_module(self, module: types.ModuleType, modname: str = None):\n        \"\"\"given an imported module, return the astroid object\"\"\"\n        modname = modname or module.__name__\n        if modname in self.astroid_cache:\n            return self.astroid_cache[modname]\n        try:\n            # some builtin modules don't have __file__ attribute\n            filepath = module.__file__\n            if is_python_source(filepath):\n                return self.ast_from_file(filepath, modname)\n        except AttributeError:\n            pass\n        # pylint: disable=import-outside-toplevel; circular import\n        from astroid.builder import AstroidBuilder\n        return AstroidBuilder(self).module_build(module, modname)\n    def ast_from_class(self, klass, modname=None):\n        \"\"\"get astroid for the given class\"\"\"\n        if modname is None:\n            try:\n                modname = klass.__module__\n            except AttributeError as exc:\n                raise AstroidBuildingError(\n                    \"Unable to get module for class {class_name}.\",\n                    cls=klass,\n                    class_repr=safe_repr(klass),\n                    modname=modname,\n                ) from exc\n        modastroid = self.ast_from_module_name(modname)\n        return modastroid.getattr(klass.__name__)[0]  # XXX\n    def infer_ast_from_something(self, obj, context=None):\n        \"\"\"infer astroid for the given class\"\"\"\n        if hasattr(obj, \"__class__\") and not isinstance(obj, type):\n            klass = obj.__class__\n        else:\n            klass = obj\n        try:\n            modname = klass.__module__\n        except AttributeError as exc:\n            raise AstroidBuildingError(\n                \"Unable to get module for {class_repr}.\",\n                cls=klass,\n                class_repr=safe_repr(klass),\n            ) from exc\n        except Exception as exc:\n            raise AstroidImportError(\n                \"Unexpected error while retrieving module for {class_repr}:\\n\"\n                \"{error}\",\n                cls=klass,\n                class_repr=safe_repr(klass),\n            ) from exc\n        try:\n            name = klass.__name__\n        except AttributeError as exc:\n            raise AstroidBuildingError(\n                \"Unable to get name for {class_repr}:\\n\",\n                cls=klass,\n                class_repr=safe_repr(klass),\n            ) from exc\n        except Exception as exc:\n            raise AstroidImportError(\n                \"Unexpected error while retrieving name for {class_repr}:\\n\" \"{error}\",\n                cls=klass,\n                class_repr=safe_repr(klass),\n            ) from exc\n        # take care, on living object __module__ is regularly wrong :(\n        modastroid = self.ast_from_module_name(modname)\n        if klass is obj:\n            for inferred in modastroid.igetattr(name, context):\n                yield inferred\n        else:\n            for inferred in modastroid.igetattr(name, context):\n                yield inferred.instantiate_class()\n    def register_failed_import_hook(self, hook):\n        \"\"\"Registers a hook to resolve imports that cannot be found otherwise.\n        `hook` must be a function that accepts a single argument `modname` which\n        contains the name of the module or package that could not be imported.\n        If `hook` can resolve the import, must return a node of type `astroid.Module`,\n        otherwise, it must raise `AstroidBuildingError`.\n        \"\"\"\n        self._failed_import_hooks.append(hook)\n    def cache_module(self, module):\n        \"\"\"Cache a module if no module with the same name is known yet.\"\"\"\n        self.astroid_cache.setdefault(module.name, module)\n    def bootstrap(self):\n        \"\"\"Bootstrap the required AST modules needed for the manager to work\n        The bootstrap usually involves building the AST for the builtins\n        module, which is required by the rest of astroid to work correctly.\n        \"\"\"\n        from astroid import raw_building  # pylint: disable=import-outside-toplevel\n        raw_building._astroid_bootstrapping()\n    def clear_cache(self):\n        \"\"\"Clear the underlying cache. Also bootstraps the builtins module.\"\"\"\n        self.astroid_cache.clear()\n        self.bootstrap()",
        "file_path": "astroid/manager.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 9,
        "score": 0.5858863592147827,
        "content": "                cls_name = cls.__name__\n                visit_name = \"visit_\" + REDIRECT.get(cls_name, cls_name).lower()\n                visit_method = getattr(self, visit_name)\n                self._visit_meths[cls] = visit_method\n            return visit_method(node, parent)\n    else:\n        @overload\n        def visit(self, node: \"ast.arg\", parent: NodeNG) -> nodes.AssignName:\n            ...\n        @overload\n        def visit(self, node: \"ast.arguments\", parent: NodeNG) -> nodes.Arguments:\n            ...\n        @overload\n        def visit(self, node: \"ast.Assert\", parent: NodeNG) -> nodes.Assert:\n            ...\n        @overload\n        def visit(\n            self, node: \"ast.AsyncFunctionDef\", parent: NodeNG\n        ) -> nodes.AsyncFunctionDef:\n            ...\n        @overload\n        def visit(self, node: \"ast.AsyncFor\", parent: NodeNG) -> nodes.AsyncFor:\n            ...\n        @overload\n        def visit(self, node: \"ast.Await\", parent: NodeNG) -> nodes.Await:\n            ...\n        @overload\n        def visit(self, node: \"ast.AsyncWith\", parent: NodeNG) -> nodes.AsyncWith:\n            ...\n        @overload\n        def visit(self, node: \"ast.Assign\", parent: NodeNG) -> nodes.Assign:\n            ...\n        @overload\n        def visit(self, node: \"ast.AnnAssign\", parent: NodeNG) -> nodes.AnnAssign:\n            ...\n        @overload\n        def visit(self, node: \"ast.AugAssign\", parent: NodeNG) -> nodes.AugAssign:\n            ...\n        @overload\n        def visit(self, node: \"ast.BinOp\", parent: NodeNG) -> nodes.BinOp:\n            ...\n        @overload\n        def visit(self, node: \"ast.BoolOp\", parent: NodeNG) -> nodes.BoolOp:\n            ...\n        @overload\n        def visit(self, node: \"ast.Break\", parent: NodeNG) -> nodes.Break:\n            ...\n        @overload\n        def visit(self, node: \"ast.Call\", parent: NodeNG) -> nodes.Call:\n            ...\n        @overload\n        def visit(self, node: \"ast.ClassDef\", parent: NodeNG) -> nodes.ClassDef:\n            ...\n        @overload\n        def visit(self, node: \"ast.Continue\", parent: NodeNG) -> nodes.Continue:\n            ...\n        @overload\n        def visit(self, node: \"ast.Compare\", parent: NodeNG) -> nodes.Compare:\n            ...\n        @overload\n        def visit(\n            self, node: \"ast.comprehension\", parent: NodeNG\n        ) -> nodes.Comprehension:\n            ...\n        @overload\n        def visit(self, node: \"ast.Delete\", parent: NodeNG) -> nodes.Delete:\n            ...\n        @overload\n        def visit(self, node: \"ast.Dict\", parent: NodeNG) -> nodes.Dict:\n            ...\n        @overload\n        def visit(self, node: \"ast.DictComp\", parent: NodeNG) -> nodes.DictComp:\n            ...\n        @overload\n        def visit(self, node: \"ast.Expr\", parent: NodeNG) -> nodes.Expr:\n            ...\n        # Not used in Python 3.8+\n        @overload\n        def visit(self, node: \"ast.Ellipsis\", parent: NodeNG) -> nodes.Const:\n            ...\n        @overload\n        def visit(\n            self, node: \"ast.ExceptHandler\", parent: NodeNG\n        ) -> nodes.ExceptHandler:\n            ...\n        # Not used in Python 3.9+\n        @overload\n        def visit(self, node: \"ast.ExtSlice\", parent: nodes.Subscript) -> nodes.Tuple:\n            ...\n        @overload\n        def visit(self, node: \"ast.For\", parent: NodeNG) -> nodes.For:\n            ...\n        @overload\n        def visit(self, node: \"ast.ImportFrom\", parent: NodeNG) -> nodes.ImportFrom:\n            ...\n        @overload\n        def visit(self, node: \"ast.FunctionDef\", parent: NodeNG) -> nodes.FunctionDef:\n            ...\n        @overload\n        def visit(self, node: \"ast.GeneratorExp\", parent: NodeNG) -> nodes.GeneratorExp:\n            ...\n        @overload\n        def visit(self, node: \"ast.Attribute\", parent: NodeNG) -> nodes.Attribute:\n            ...\n        @overload\n        def visit(self, node: \"ast.Global\", parent: NodeNG) -> nodes.Global:\n            ...\n        @overload\n        def visit(self, node: \"ast.If\", parent: NodeNG) -> nodes.If:\n            ...\n        @overload\n        def visit(self, node: \"ast.IfExp\", parent: NodeNG) -> nodes.IfExp:\n            ...\n        @overload\n        def visit(self, node: \"ast.Import\", parent: NodeNG) -> nodes.Import:\n            ...\n        @overload\n        def visit(self, node: \"ast.JoinedStr\", parent: NodeNG) -> nodes.JoinedStr:\n            ...\n        @overload\n        def visit(\n            self, node: \"ast.FormattedValue\", parent: NodeNG\n        ) -> nodes.FormattedValue:\n            ...\n        @overload\n        def visit(self, node: \"ast.NamedExpr\", parent: NodeNG) -> nodes.NamedExpr:\n            ...\n        # Not used in Python 3.9+\n        @overload\n        def visit(self, node: \"ast.Index\", parent: nodes.Subscript) -> NodeNG:\n            ...\n        @overload\n        def visit(self, node: \"ast.keyword\", parent: NodeNG) -> nodes.Keyword:\n            ...\n        @overload\n        def visit(self, node: \"ast.Lambda\", parent: NodeNG) -> nodes.Lambda:\n            ...\n        @overload\n        def visit(self, node: \"ast.List\", parent: NodeNG) -> nodes.List:\n            ...\n        @overload\n        def visit(self, node: \"ast.ListComp\", parent: NodeNG) -> nodes.ListComp:\n            ...\n        @overload\n        def visit(\n            self, node: \"ast.Name\", parent: NodeNG\n        ) -> Union[nodes.Name, nodes.Const, nodes.AssignName, nodes.DelName]:\n            ...\n        # Not used in Python 3.8+\n        @overload\n        def visit(self, node: \"ast.NameConstant\", parent: NodeNG) -> nodes.Const:\n            ...\n        @overload\n        def visit(self, node: \"ast.Nonlocal\", parent: NodeNG) -> nodes.Nonlocal:\n            ...\n        # Not used in Python 3.8+\n        @overload\n        def visit(self, node: \"ast.Str\", parent: NodeNG) -> nodes.Const:\n            ...\n        # Not used in Python 3.8+\n        @overload\n        def visit(self, node: \"ast.Bytes\", parent: NodeNG) -> nodes.Const:\n            ...\n        # Not used in Python 3.8+\n        @overload\n        def visit(self, node: \"ast.Num\", parent: NodeNG) -> nodes.Const:\n            ...\n        @overload\n        def visit(self, node: \"ast.Constant\", parent: NodeNG) -> nodes.Const:\n            ...\n        @overload\n        def visit(self, node: \"ast.Pass\", parent: NodeNG) -> nodes.Pass:\n            ...\n        @overload\n        def visit(self, node: \"ast.Raise\", parent: NodeNG) -> nodes.Raise:\n            ...\n        @overload\n        def visit(self, node: \"ast.Return\", parent: NodeNG) -> nodes.Return:\n            ...\n        @overload\n        def visit(self, node: \"ast.Set\", parent: NodeNG) -> nodes.Set:\n            ...\n        @overload\n        def visit(self, node: \"ast.SetComp\", parent: NodeNG) -> nodes.SetComp:\n            ...\n        @overload\n        def visit(self, node: \"ast.Slice\", parent: nodes.Subscript) -> nodes.Slice:\n            ...\n        @overload\n        def visit(self, node: \"ast.Subscript\", parent: NodeNG) -> nodes.Subscript:\n            ...\n        @overload\n        def visit(self, node: \"ast.Starred\", parent: NodeNG) -> nodes.Starred:\n            ...\n        @overload\n        def visit(\n            self, node: \"ast.Try\", parent: NodeNG\n        ) -> Union[nodes.TryExcept, nodes.TryFinally]:\n            ...\n        @overload\n        def visit(self, node: \"ast.Tuple\", parent: NodeNG) -> nodes.Tuple:\n            ...\n        @overload\n        def visit(self, node: \"ast.UnaryOp\", parent: NodeNG) -> nodes.UnaryOp:\n            ...\n        @overload\n        def visit(self, node: \"ast.While\", parent: NodeNG) -> nodes.While:\n            ...\n        @overload\n        def visit(self, node: \"ast.With\", parent: NodeNG) -> nodes.With:\n            ...\n        @overload\n        def visit(self, node: \"ast.Yield\", parent: NodeNG) -> nodes.Yield:\n            ...\n        @overload\n        def visit(self, node: \"ast.YieldFrom\", parent: NodeNG) -> nodes.YieldFrom:\n            ...\n        @overload\n        def visit(self, node: \"ast.AST\", parent: NodeNG) -> NodeNG:\n            ...\n        @overload\n        def visit(self, node: None, parent: NodeNG) -> None:\n            ...\n        def visit(self, node: Optional[\"ast.AST\"], parent: NodeNG) -> Optional[NodeNG]:\n            if node is None:\n                return None\n            cls = node.__class__\n            if cls in self._visit_meths:\n                visit_method = self._visit_meths[cls]\n            else:\n                cls_name = cls.__name__\n                visit_name = \"visit_\" + REDIRECT.get(cls_name, cls_name).lower()\n                visit_method = getattr(self, visit_name)\n                self._visit_meths[cls] = visit_method\n            return visit_method(node, parent)\n    def _save_assignment(self, node: Union[nodes.AssignName, nodes.DelName]) -> None:\n        \"\"\"save assignment situation since node.parent is not available yet\"\"\"\n        if self._global_names and node.name in self._global_names[-1]:\n            node.root().set_local(node.name, node)\n        else:\n            node.parent.set_local(node.name, node)\n    def visit_arg(self, node: \"ast.arg\", parent: NodeNG) -> nodes.AssignName:\n        \"\"\"visit an arg node by returning a fresh AssName instance\"\"\"\n        return self.visit_assignname(node, parent, node.arg)\n    def visit_arguments(self, node: \"ast.arguments\", parent: NodeNG) -> nodes.Arguments:\n        \"\"\"visit an Arguments node by returning a fresh instance of it\"\"\"\n        vararg: Optional[str] = None\n        kwarg: Optional[str] = None\n        newnode = nodes.Arguments(\n            node.vararg.arg if node.vararg else None,\n            node.kwarg.arg if node.kwarg else None,\n            parent,\n        )\n        args = [self.visit(child, newnode) for child in node.args]\n        defaults = [self.visit(child, newnode) for child in node.defaults]\n        varargannotation: Optional[NodeNG] = None\n        kwargannotation: Optional[NodeNG] = None\n        posonlyargs: List[nodes.AssignName] = []\n        if node.vararg:\n            vararg = node.vararg.arg\n            varargannotation = self.visit(node.vararg.annotation, newnode)\n        if node.kwarg:\n            kwarg = node.kwarg.arg\n            kwargannotation = self.visit(node.kwarg.annotation, newnode)\n        kwonlyargs = [self.visit(child, newnode) for child in node.kwonlyargs]\n        kw_defaults = [self.visit(child, newnode) for child in node.kw_defaults]\n        annotations = [self.visit(arg.annotation, newnode) for arg in node.args]\n        kwonlyargs_annotations = [\n            self.visit(arg.annotation, newnode) for arg in node.kwonlyargs\n        ]\n        posonlyargs_annotations: List[Optional[NodeNG]] = []\n        if PY38_PLUS:\n            posonlyargs = [self.visit(child, newnode) for child in node.posonlyargs]\n            posonlyargs_annotations = [\n                self.visit(arg.annotation, newnode) for arg in node.posonlyargs\n            ]\n        type_comment_args = [\n            self.check_type_comment(child, parent=newnode) for child in node.args\n        ]\n        type_comment_kwonlyargs = [\n            self.check_type_comment(child, parent=newnode) for child in node.kwonlyargs\n        ]\n        type_comment_posonlyargs: List[Optional[NodeNG]] = []\n        if PY38_PLUS:\n            type_comment_posonlyargs = [\n                self.check_type_comment(child, parent=newnode)\n                for child in node.posonlyargs\n            ]\n        newnode.postinit(\n            args=args,\n            defaults=defaults,\n            kwonlyargs=kwonlyargs,\n            posonlyargs=posonlyargs,\n            kw_defaults=kw_defaults,\n            annotations=annotations,\n            kwonlyargs_annotations=kwonlyargs_annotations,\n            posonlyargs_annotations=posonlyargs_annotations,\n            varargannotation=varargannotation,\n            kwargannotation=kwargannotation,\n            type_comment_args=type_comment_args,\n            type_comment_kwonlyargs=type_comment_kwonlyargs,\n            type_comment_posonlyargs=type_comment_posonlyargs,\n        )\n        # save argument names in locals:\n        if vararg:\n            newnode.parent.set_local(vararg, newnode)\n        if kwarg:\n            newnode.parent.set_local(kwarg, newnode)\n        return newnode\n    def visit_assert(self, node: \"ast.Assert\", parent: NodeNG) -> nodes.Assert:\n        \"\"\"visit a Assert node by returning a fresh instance of it\"\"\"\n        newnode = nodes.Assert(node.lineno, node.col_offset, parent)\n        msg: Optional[NodeNG] = None\n        if node.msg:\n            msg = self.visit(node.msg, newnode)\n        newnode.postinit(self.visit(node.test, newnode), msg)\n        return newnode\n    def check_type_comment(\n        self,\n        node: Union[\n            \"ast.Assign\",\n            \"ast.arg\",\n            \"ast.For\",\n            \"ast.AsyncFor\",\n            \"ast.With\",\n            \"ast.AsyncWith\",\n        ],\n        parent: Union[\n            nodes.Assign,\n            nodes.Arguments,\n            nodes.For,\n            nodes.AsyncFor,\n            nodes.With,\n            nodes.AsyncWith,\n        ],\n    ) -> Optional[NodeNG]:\n        type_comment = getattr(node, \"type_comment\", None)  # Added in Python 3.8\n        if not type_comment:\n            return None\n        try:\n            type_comment_ast = self._parser_module.parse(type_comment)\n        except SyntaxError:\n            # Invalid type comment, just skip it.\n            return None\n        type_object = self.visit(type_comment_ast.body[0], parent=parent)\n        if not isinstance(type_object, nodes.Expr):\n            return None\n        return type_object.value\n    def check_function_type_comment(\n        self, node: Union[\"ast.FunctionDef\", \"ast.AsyncFunctionDef\"], parent: NodeNG\n    ) -> Optional[Tuple[Optional[NodeNG], List[NodeNG]]]:\n        type_comment = getattr(node, \"type_comment\", None)  # Added in Python 3.8\n        if not type_comment:\n            return None\n        try:\n            type_comment_ast = parse_function_type_comment(type_comment)\n        except SyntaxError:\n            # Invalid type comment, just skip it.\n            return None\n        returns: Optional[NodeNG] = None\n        argtypes: List[NodeNG] = [\n            self.visit(elem, parent) for elem in (type_comment_ast.argtypes or [])\n        ]\n        if type_comment_ast.returns:\n            returns = self.visit(type_comment_ast.returns, parent)\n        return returns, argtypes\n    def visit_asyncfunctiondef(\n        self, node: \"ast.AsyncFunctionDef\", parent: NodeNG\n    ) -> nodes.AsyncFunctionDef:\n        return self._visit_functiondef(nodes.AsyncFunctionDef, node, parent)\n    def visit_asyncfor(self, node: \"ast.AsyncFor\", parent: NodeNG) -> nodes.AsyncFor:\n        return self._visit_for(nodes.AsyncFor, node, parent)\n    def visit_await(self, node: \"ast.Await\", parent: NodeNG) -> nodes.Await:\n        newnode = nodes.Await(node.lineno, node.col_offset, parent)\n        newnode.postinit(value=self.visit(node.value, newnode))\n        return newnode\n    def visit_asyncwith(self, node: \"ast.AsyncWith\", parent: NodeNG) -> nodes.AsyncWith:\n        return self._visit_with(nodes.AsyncWith, node, parent)\n    def visit_assign(self, node: \"ast.Assign\", parent: NodeNG) -> nodes.Assign:\n        \"\"\"visit a Assign node by returning a fresh instance of it\"\"\"\n        newnode = nodes.Assign(node.lineno, node.col_offset, parent)\n        type_annotation = self.check_type_comment(node, parent=newnode)\n        newnode.postinit(\n            targets=[self.visit(child, newnode) for child in node.targets],\n            value=self.visit(node.value, newnode),\n            type_annotation=type_annotation,\n        )\n        return newnode\n    def visit_annassign(self, node: \"ast.AnnAssign\", parent: NodeNG) -> nodes.AnnAssign:\n        \"\"\"visit an AnnAssign node by returning a fresh instance of it\"\"\"\n        newnode = nodes.AnnAssign(node.lineno, node.col_offset, parent)\n        newnode.postinit(\n            target=self.visit(node.target, newnode),\n            annotation=self.visit(node.annotation, newnode),\n            simple=node.simple,\n            value=self.visit(node.value, newnode),\n        )\n        return newnode\n    @overload\n    def visit_assignname(\n        self, node: \"ast.AST\", parent: NodeNG, node_name: str\n    ) -> nodes.AssignName:\n        ...\n    @overload\n    def visit_assignname(\n        self, node: \"ast.AST\", parent: NodeNG, node_name: None\n    ) -> None:\n        ...\n    def visit_assignname(\n        self, node: \"ast.AST\", parent: NodeNG, node_name: Optional[str]\n    ) -> Optional[nodes.AssignName]:\n        \"\"\"visit a node and return a AssignName node\n        Note: Method not called by 'visit'\n        \"\"\"\n        if node_name is None:\n            return None\n        newnode = nodes.AssignName(\n            node_name,\n            node.lineno,\n            node.col_offset,\n            parent,\n        )\n        self._save_assignment(newnode)\n        return newnode\n    def visit_augassign(self, node: \"ast.AugAssign\", parent: NodeNG) -> nodes.AugAssign:\n        \"\"\"visit a AugAssign node by returning a fresh instance of it\"\"\"\n        newnode = nodes.AugAssign(\n            self._parser_module.bin_op_classes[type(node.op)] + \"=\",\n            node.lineno,\n            node.col_offset,\n            parent,\n        )\n        newnode.postinit(\n            self.visit(node.target, newnode), self.visit(node.value, newnode)\n        )\n        return newnode\n    def visit_binop(self, node: \"ast.BinOp\", parent: NodeNG) -> nodes.BinOp:\n        \"\"\"visit a BinOp node by returning a fresh instance of it\"\"\"\n        newnode = nodes.BinOp(\n            self._parser_module.bin_op_classes[type(node.op)],\n            node.lineno,\n            node.col_offset,\n            parent,\n        )\n        newnode.postinit(\n            self.visit(node.left, newnode), self.visit(node.right, newnode)\n        )\n        return newnode\n    def visit_boolop(self, node: \"ast.BoolOp\", parent: NodeNG) -> nodes.BoolOp:\n        \"\"\"visit a BoolOp node by returning a fresh instance of it\"\"\"\n        newnode = nodes.BoolOp(\n            self._parser_module.bool_op_classes[type(node.op)],\n            node.lineno,\n            node.col_offset,\n            parent,\n        )\n        newnode.postinit([self.visit(child, newnode) for child in node.values])\n        return newnode\n    def visit_break(self, node: \"ast.Break\", parent: NodeNG) -> nodes.Break:\n        \"\"\"visit a Break node by returning a fresh instance of it\"\"\"\n        return nodes.Break(node.lineno, node.col_offset, parent)\n    def visit_call(self, node: \"ast.Call\", parent: NodeNG) -> nodes.Call:\n        \"\"\"visit a CallFunc node by returning a fresh instance of it\"\"\"\n        newnode = nodes.Call(node.lineno, node.col_offset, parent)\n        newnode.postinit(\n            func=self.visit(node.func, newnode),\n            args=[self.visit(child, newnode) for child in node.args],\n            keywords=[self.visit(child, newnode) for child in node.keywords],\n        )\n        return newnode\n    def visit_classdef(\n        self, node: \"ast.ClassDef\", parent: NodeNG, newstyle: bool = True\n    ) -> nodes.ClassDef:\n        \"\"\"visit a ClassDef node to become astroid\"\"\"\n        node, doc = self._get_doc(node)\n        newnode = nodes.ClassDef(node.name, doc, node.lineno, node.col_offset, parent)\n        metaclass = None\n        for keyword in node.keywords:\n            if keyword.arg == \"metaclass\":\n                metaclass = self.visit(keyword, newnode).value\n                break\n        decorators = self.visit_decorators(node, newnode)\n        newnode.postinit(\n            [self.visit(child, newnode) for child in node.bases],\n            [self.visit(child, newnode) for child in node.body],\n            decorators,\n            newstyle,\n            metaclass,\n            [\n                self.visit(kwd, newnode)\n                for kwd in node.keywords\n                if kwd.arg != \"metaclass\"\n            ],\n        )\n        return newnode\n    def visit_continue(self, node: \"ast.Continue\", parent: NodeNG) -> nodes.Continue:\n        \"\"\"visit a Continue node by returning a fresh instance of it\"\"\"\n        return nodes.Continue(node.lineno, node.col_offset, parent)\n    def visit_compare(self, node: \"ast.Compare\", parent: NodeNG) -> nodes.Compare:\n        \"\"\"visit a Compare node by returning a fresh instance of it\"\"\"",
        "file_path": "astroid/rebuilder.py",
        "chunk_index": 1,
        "metadata": {}
      },
      {
        "rank": 10,
        "score": 0.5819274187088013,
        "content": "# Copyright (c) 2006-2011, 2013-2014 LOGILAB S.A. (Paris, FRANCE) <contact@logilab.fr>\n# Copyright (c) 2013 Phil Schaf <flying-sheep@web.de>\n# Copyright (c) 2014-2020 Claudiu Popa <pcmanticore@gmail.com>\n# Copyright (c) 2014-2015 Google, Inc.\n# Copyright (c) 2014 Alexander Presnyakov <flagist0@gmail.com>\n# Copyright (c) 2015-2016 Ceridwen <ceridwenv@gmail.com>\n# Copyright (c) 2016 Derek Gustafson <degustaf@gmail.com>\n# Copyright (c) 2017 Łukasz Rogalski <rogalski.91@gmail.com>\n# Copyright (c) 2018 Anthony Sottile <asottile@umich.edu>\n# Copyright (c) 2020-2021 hippo91 <guillaume.peillex@gmail.com>\n# Copyright (c) 2021 Daniël van Noord <13665637+DanielNoord@users.noreply.github.com>\n# Copyright (c) 2021 Pierre Sassoulas <pierre.sassoulas@gmail.com>\n# Copyright (c) 2021 Marc Mueller <30130371+cdce8p@users.noreply.github.com>\n# Copyright (c) 2021 Andrew Haigh <hello@nelf.in>\n# Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n# For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n\"\"\"The AstroidBuilder makes astroid from living object and / or from _ast\nThe builder is not thread safe and can't be used to parse different sources\nat the same time.\n\"\"\"\nimport os\nimport textwrap\nimport types\nfrom tokenize import detect_encoding\nfrom typing import List, Union\nfrom astroid import bases, modutils, nodes, raw_building, rebuilder, util\nfrom astroid._ast import get_parser_module\nfrom astroid.exceptions import AstroidBuildingError, AstroidSyntaxError, InferenceError\nfrom astroid.manager import AstroidManager\nfrom astroid.nodes.node_classes import NodeNG\nobjects = util.lazy_import(\"objects\")\n# The name of the transient function that is used to\n# wrap expressions to be extracted when calling\n# extract_node.\n_TRANSIENT_FUNCTION = \"__\"\n# The comment used to select a statement to be extracted\n# when calling extract_node.\n_STATEMENT_SELECTOR = \"#@\"\nMISPLACED_TYPE_ANNOTATION_ERROR = \"misplaced type annotation\"\ndef open_source_file(filename):\n    # pylint: disable=consider-using-with\n    with open(filename, \"rb\") as byte_stream:\n        encoding = detect_encoding(byte_stream.readline)[0]\n    stream = open(filename, newline=None, encoding=encoding)\n    data = stream.read()\n    return stream, encoding, data\ndef _can_assign_attr(node, attrname):\n    try:\n        slots = node.slots()\n    except NotImplementedError:\n        pass\n    else:\n        if slots and attrname not in {slot.value for slot in slots}:\n            return False\n    return node.qname() != \"builtins.object\"\nclass AstroidBuilder(raw_building.InspectBuilder):\n    \"\"\"Class for building an astroid tree from source code or from a live module.\n    The param *manager* specifies the manager class which should be used.\n    If no manager is given, then the default one will be used. The\n    param *apply_transforms* determines if the transforms should be\n    applied after the tree was built from source or from a live object,\n    by default being True.\n    \"\"\"\n    # pylint: disable=redefined-outer-name\n    def __init__(self, manager=None, apply_transforms=True):\n        super().__init__(manager)\n        self._apply_transforms = apply_transforms\n    def module_build(\n        self, module: types.ModuleType, modname: str = None\n    ) -> nodes.Module:\n        \"\"\"Build an astroid from a living module instance.\"\"\"\n        node = None\n        path = getattr(module, \"__file__\", None)\n        if path is not None:\n            path_, ext = os.path.splitext(modutils._path_from_filename(path))\n            if ext in {\".py\", \".pyc\", \".pyo\"} and os.path.exists(path_ + \".py\"):\n                node = self.file_build(path_ + \".py\", modname)\n        if node is None:\n            # this is a built-in module\n            # get a partial representation by introspection\n            node = self.inspect_build(module, modname=modname, path=path)\n            if self._apply_transforms:\n                # We have to handle transformation by ourselves since the\n                # rebuilder isn't called for builtin nodes\n                node = self._manager.visit_transforms(node)\n        return node\n    def file_build(self, path, modname=None):\n        \"\"\"Build astroid from a source code file (i.e. from an ast)\n        *path* is expected to be a python source file\n        \"\"\"\n        try:\n            stream, encoding, data = open_source_file(path)\n        except OSError as exc:\n            raise AstroidBuildingError(\n                \"Unable to load file {path}:\\n{error}\",\n                modname=modname,\n                path=path,\n                error=exc,\n            ) from exc\n        except (SyntaxError, LookupError) as exc:\n            raise AstroidSyntaxError(\n                \"Python 3 encoding specification error or unknown encoding:\\n\"\n                \"{error}\",\n                modname=modname,\n                path=path,\n                error=exc,\n            ) from exc\n        except UnicodeError as exc:  # wrong encoding\n            # detect_encoding returns utf-8 if no encoding specified\n            raise AstroidBuildingError(\n                \"Wrong or no encoding specified for {filename}.\", filename=path\n            ) from exc\n        with stream:\n            # get module name if necessary\n            if modname is None:\n                try:\n                    modname = \".\".join(modutils.modpath_from_file(path))\n                except ImportError:\n                    modname = os.path.splitext(os.path.basename(path))[0]\n            # build astroid representation\n            module = self._data_build(data, modname, path)\n            return self._post_build(module, encoding)\n    def string_build(self, data, modname=\"\", path=None):\n        \"\"\"Build astroid from source code string.\"\"\"\n        module = self._data_build(data, modname, path)\n        module.file_bytes = data.encode(\"utf-8\")\n        return self._post_build(module, \"utf-8\")\n    def _post_build(self, module, encoding):\n        \"\"\"Handles encoding and delayed nodes after a module has been built\"\"\"\n        module.file_encoding = encoding\n        self._manager.cache_module(module)\n        # post tree building steps after we stored the module in the cache:\n        for from_node in module._import_from_nodes:\n            if from_node.modname == \"__future__\":\n                for symbol, _ in from_node.names:\n                    module.future_imports.add(symbol)\n            self.add_from_names_to_locals(from_node)\n        # handle delayed assattr nodes\n        for delayed in module._delayed_assattr:\n            self.delayed_assattr(delayed)\n        # Visit the transforms\n        if self._apply_transforms:\n            module = self._manager.visit_transforms(module)\n        return module\n    def _data_build(self, data, modname, path):\n        \"\"\"Build tree node from data and add some informations\"\"\"\n        try:\n            node, parser_module = _parse_string(data, type_comments=True)\n        except (TypeError, ValueError, SyntaxError) as exc:\n            raise AstroidSyntaxError(\n                \"Parsing Python code failed:\\n{error}\",\n                source=data,\n                modname=modname,\n                path=path,\n                error=exc,\n            ) from exc\n        if path is not None:\n            node_file = os.path.abspath(path)\n        else:\n            node_file = \"<?>\"\n        if modname.endswith(\".__init__\"):\n            modname = modname[:-9]\n            package = True\n        else:\n            package = (\n                path is not None\n                and os.path.splitext(os.path.basename(path))[0] == \"__init__\"\n            )\n        builder = rebuilder.TreeRebuilder(self._manager, parser_module)\n        module = builder.visit_module(node, modname, node_file, package)\n        module._import_from_nodes = builder._import_from_nodes\n        module._delayed_assattr = builder._delayed_assattr\n        return module\n    def add_from_names_to_locals(self, node):\n        \"\"\"Store imported names to the locals\n        Resort the locals if coming from a delayed node\n        \"\"\"\n        def _key_func(node):\n            return node.fromlineno\n        def sort_locals(my_list):\n            my_list.sort(key=_key_func)\n        for (name, asname) in node.names:\n            if name == \"*\":\n                try:\n                    imported = node.do_import_module()\n                except AstroidBuildingError:\n                    continue\n                for name in imported.public_names():\n                    node.parent.set_local(name, node)\n                    sort_locals(node.parent.scope().locals[name])\n            else:\n                node.parent.set_local(asname or name, node)\n                sort_locals(node.parent.scope().locals[asname or name])\n    def delayed_assattr(self, node):\n        \"\"\"Visit a AssAttr node\n        This adds name to locals and handle members definition.\n        \"\"\"\n        try:\n            frame = node.frame()\n            for inferred in node.expr.infer():\n                if inferred is util.Uninferable:\n                    continue\n                try:\n                    cls = inferred.__class__\n                    if cls is bases.Instance or cls is objects.ExceptionInstance:\n                        inferred = inferred._proxied\n                        iattrs = inferred.instance_attrs\n                        if not _can_assign_attr(inferred, node.attrname):\n                            continue\n                    elif isinstance(inferred, bases.Instance):\n                        # Const, Tuple or other containers that inherit from\n                        # `Instance`\n                        continue\n                    elif inferred.is_function:\n                        iattrs = inferred.instance_attrs\n                    else:\n                        iattrs = inferred.locals\n                except AttributeError:\n                    # XXX log error\n                    continue\n                values = iattrs.setdefault(node.attrname, [])\n                if node in values:\n                    continue\n                # get assign in __init__ first XXX useful ?\n                if (\n                    frame.name == \"__init__\"\n                    and values\n                    and values[0].frame().name != \"__init__\"\n                ):\n                    values.insert(0, node)\n                else:\n                    values.append(node)\n        except InferenceError:\n            pass\ndef build_namespace_package_module(name: str, path: List[str]) -> nodes.Module:\n    return nodes.Module(name, doc=\"\", path=path, package=True)\ndef parse(code, module_name=\"\", path=None, apply_transforms=True):\n    \"\"\"Parses a source string in order to obtain an astroid AST from it\n    :param str code: The code for the module.\n    :param str module_name: The name for the module, if any\n    :param str path: The path for the module\n    :param bool apply_transforms:\n        Apply the transforms for the give code. Use it if you\n        don't want the default transforms to be applied.\n    \"\"\"\n    code = textwrap.dedent(code)\n    builder = AstroidBuilder(\n        manager=AstroidManager(), apply_transforms=apply_transforms\n    )\n    return builder.string_build(code, modname=module_name, path=path)\ndef _extract_expressions(node):\n    \"\"\"Find expressions in a call to _TRANSIENT_FUNCTION and extract them.\n    The function walks the AST recursively to search for expressions that\n    are wrapped into a call to _TRANSIENT_FUNCTION. If it finds such an\n    expression, it completely removes the function call node from the tree,\n    replacing it by the wrapped expression inside the parent.\n    :param node: An astroid node.\n    :type node:  astroid.bases.NodeNG\n    :yields: The sequence of wrapped expressions on the modified tree\n    expression can be found.\n    \"\"\"\n    if (\n        isinstance(node, nodes.Call)\n        and isinstance(node.func, nodes.Name)\n        and node.func.name == _TRANSIENT_FUNCTION\n    ):\n        real_expr = node.args[0]\n        real_expr.parent = node.parent\n        # Search for node in all _astng_fields (the fields checked when\n        # get_children is called) of its parent. Some of those fields may\n        # be lists or tuples, in which case the elements need to be checked.\n        # When we find it, replace it by real_expr, so that the AST looks\n        # like no call to _TRANSIENT_FUNCTION ever took place.\n        for name in node.parent._astroid_fields:\n            child = getattr(node.parent, name)\n            if isinstance(child, (list, tuple)):\n                for idx, compound_child in enumerate(child):\n                    if compound_child is node:\n                        child[idx] = real_expr\n            elif child is node:\n                setattr(node.parent, name, real_expr)\n        yield real_expr\n    else:\n        for child in node.get_children():\n            yield from _extract_expressions(child)\ndef _find_statement_by_line(node, line):\n    \"\"\"Extracts the statement on a specific line from an AST.\n    If the line number of node matches line, it will be returned;\n    otherwise its children are iterated and the function is called\n    recursively.\n    :param node: An astroid node.\n    :type node: astroid.bases.NodeNG\n    :param line: The line number of the statement to extract.\n    :type line: int\n    :returns: The statement on the line, or None if no statement for the line\n      can be found.\n    :rtype:  astroid.bases.NodeNG or None\n    \"\"\"\n    if isinstance(node, (nodes.ClassDef, nodes.FunctionDef, nodes.MatchCase)):\n        # This is an inaccuracy in the AST: the nodes that can be\n        # decorated do not carry explicit information on which line\n        # the actual definition (class/def), but .fromline seems to\n        # be close enough.\n        node_line = node.fromlineno\n    else:\n        node_line = node.lineno\n    if node_line == line:\n        return node\n    for child in node.get_children():\n        result = _find_statement_by_line(child, line)\n        if result:\n            return result\n    return None\ndef extract_node(code: str, module_name: str = \"\") -> Union[NodeNG, List[NodeNG]]:\n    \"\"\"Parses some Python code as a module and extracts a designated AST node.\n    Statements:\n     To extract one or more statement nodes, append #@ to the end of the line\n     Examples:\n       >>> def x():\n       >>>   def y():\n       >>>     return 1 #@\n       The return statement will be extracted.\n       >>> class X(object):\n       >>>   def meth(self): #@\n       >>>     pass\n      The function object 'meth' will be extracted.\n    Expressions:\n     To extract arbitrary expressions, surround them with the fake\n     function call __(...). After parsing, the surrounded expression\n     will be returned and the whole AST (accessible via the returned\n     node's parent attribute) will look like the function call was\n     never there in the first place.\n     Examples:\n       >>> a = __(1)\n       The const node will be extracted.\n       >>> def x(d=__(foo.bar)): pass\n       The node containing the default argument will be extracted.\n       >>> def foo(a, b):\n       >>>   return 0 < __(len(a)) < b\n       The node containing the function call 'len' will be extracted.\n    If no statements or expressions are selected, the last toplevel\n    statement will be returned.\n    If the selected statement is a discard statement, (i.e. an expression\n    turned into a statement), the wrapped expression is returned instead.\n    For convenience, singleton lists are unpacked.\n    :param str code: A piece of Python code that is parsed as\n    a module. Will be passed through textwrap.dedent first.\n    :param str module_name: The name of the module.\n    :returns: The designated node from the parse tree, or a list of nodes.\n    \"\"\"\n    def _extract(node):\n        if isinstance(node, nodes.Expr):\n            return node.value\n        return node\n    requested_lines = []\n    for idx, line in enumerate(code.splitlines()):\n        if line.strip().endswith(_STATEMENT_SELECTOR):\n            requested_lines.append(idx + 1)\n    tree = parse(code, module_name=module_name)\n    if not tree.body:\n        raise ValueError(\"Empty tree, cannot extract from it\")\n    extracted = []\n    if requested_lines:\n        extracted = [_find_statement_by_line(tree, line) for line in requested_lines]\n    # Modifies the tree.\n    extracted.extend(_extract_expressions(tree))\n    if not extracted:\n        extracted.append(tree.body[-1])\n    extracted = [_extract(node) for node in extracted]\n    if len(extracted) == 1:\n        return extracted[0]\n    return extracted\ndef _parse_string(data, type_comments=True):\n    parser_module = get_parser_module(type_comments=type_comments)\n    try:\n        parsed = parser_module.parse(data + \"\\n\", type_comments=type_comments)\n    except SyntaxError as exc:\n        # If the type annotations are misplaced for some reason, we do not want\n        # to fail the entire parsing of the file, so we need to retry the parsing without\n        # type comment support.\n        if exc.args[0] != MISPLACED_TYPE_ANNOTATION_ERROR or not type_comments:\n            raise\n        parser_module = get_parser_module(type_comments=False)\n        parsed = parser_module.parse(data + \"\\n\", type_comments=False)\n    return parsed, parser_module",
        "file_path": "astroid/builder.py",
        "chunk_index": 0,
        "metadata": {}
      }
    ]
  },
  "pyvista__pyvista-4315": {
    "query": "Rectilinear grid does not allow Sequences as inputs\n### Describe the bug, what's wrong, and what you expected.\r\n\r\nRectilinear grid gives an error when `Sequence`s are passed in, but `ndarray` are ok.\r\n\r\n### Steps to reproduce the bug.\r\n\r\nThis doesn't work\r\n```python\r\nimport pyvista as pv\r\npv.RectilinearGrid([0, 1], [0, 1], [0, 1])\r\n```\r\n\r\nThis works\r\n```py\r\nimport pyvista as pv\r\nimport numpy as np\r\npv.RectilinearGrid(np.ndarray([0, 1]), np.ndarray([0, 1]), np.ndarray([0, 1]))\r\n```\r\n### System Information\r\n\r\n```shell\r\n--------------------------------------------------------------------------------\r\n  Date: Wed Apr 19 20:15:10 2023 UTC\r\n\r\n                OS : Linux\r\n            CPU(s) : 2\r\n           Machine : x86_64\r\n      Architecture : 64bit\r\n       Environment : IPython\r\n        GPU Vendor : Mesa/X.org\r\n      GPU Renderer : llvmpipe (LLVM 11.0.1, 256 bits)\r\n       GPU Version : 4.5 (Core Profile) Mesa 20.3.5\r\n\r\n  Python 3.11.2 (main, Mar 23 2023, 17:12:29) [GCC 10.2.1 20210110]\r\n\r\n           pyvista : 0.38.5\r\n               vtk : 9.2.6\r\n             numpy : 1.24.2\r\n           imageio : 2.27.0\r\n            scooby : 0.7.1\r\n             pooch : v1.7.0\r\n        matplotlib : 3.7.1\r\n           IPython : 8.12.0\r\n--------------------------------------------------------------------------------\r\n```\r\n\r\n\r\n### Screenshots\r\n\r\n_No response_\n",
    "method": "sliding",
    "top_k": 10,
    "results": [
      {
        "rank": 1,
        "score": 0.7195675373077393,
        "content": "\"\"\"Sub-classes for vtk.vtkRectilinearGrid and vtk.vtkImageData.\"\"\"\nfrom functools import wraps\nimport pathlib\nfrom typing import Sequence, Tuple, Union\nimport warnings\nimport numpy as np\nimport pyvista\nfrom pyvista import _vtk\nfrom pyvista.core.dataset import DataSet\nfrom pyvista.core.filters import RectilinearGridFilters, UniformGridFilters, _get_output\nfrom pyvista.utilities import abstract_class, assert_empty_kwargs\nimport pyvista.utilities.helpers as helpers\nfrom pyvista.utilities.misc import PyVistaDeprecationWarning, raise_has_duplicates\n@abstract_class\nclass Grid(DataSet):\n    \"\"\"A class full of common methods for non-pointset grids.\"\"\"\n    def __init__(self, *args, **kwargs):\n        \"\"\"Initialize the grid.\"\"\"\n        super().__init__()\n    @property\n    def dimensions(self) -> Tuple[int, int, int]:\n        \"\"\"Return the grid's dimensions.\n        These are effectively the number of points along each of the\n        three dataset axes.\n        Examples\n        --------\n        Create a uniform grid with dimensions ``(1, 2, 3)``.\n        >>> import pyvista\n        >>> grid = pyvista.UniformGrid(dimensions=(2, 3, 4))\n        >>> grid.dimensions\n        (2, 3, 4)\n        >>> grid.plot(show_edges=True)\n        Set the dimensions to ``(3, 4, 5)``\n        >>> grid.dimensions = (3, 4, 5)\n        >>> grid.plot(show_edges=True)\n        \"\"\"\n        return self.GetDimensions()\n    @dimensions.setter\n    def dimensions(self, dims: Sequence[int]):\n        \"\"\"Set the dataset dimensions.\"\"\"\n        self.SetDimensions(*dims)\n        self.Modified()\n    def _get_attrs(self):\n        \"\"\"Return the representation methods (internal helper).\"\"\"\n        attrs = DataSet._get_attrs(self)\n        attrs.append((\"Dimensions\", self.dimensions, \"{:d}, {:d}, {:d}\"))\n        return attrs\nclass RectilinearGrid(_vtk.vtkRectilinearGrid, Grid, RectilinearGridFilters):\n    \"\"\"Dataset with variable spacing in the three coordinate directions.\n    Can be initialized in several ways:\n    * Create empty grid\n    * Initialize from a ``vtk.vtkRectilinearGrid`` object\n    * Initialize directly from the point arrays\n    Parameters\n    ----------\n    uinput : str, pathlib.Path, vtk.vtkRectilinearGrid, numpy.ndarray, optional\n        Filename, dataset, or array to initialize the rectilinear grid from. If a\n        filename is passed, pyvista will attempt to load it as a\n        :class:`RectilinearGrid`. If passed a ``vtk.vtkRectilinearGrid``, it\n        will be wrapped. If a :class:`numpy.ndarray` is passed, this will be\n        loaded as the x range.\n    y : numpy.ndarray, optional\n        Coordinates of the points in y direction. If this is passed, ``uinput``\n        must be a :class:`numpy.ndarray`.\n    z : numpy.ndarray, optional\n        Coordinates of the points in z direction. If this is passed, ``uinput``\n        and ``y`` must be a :class:`numpy.ndarray`.\n    check_duplicates : bool, optional\n        Check for duplications in any arrays that are passed. Defaults to\n        ``False``. If ``True``, an error is raised if there are any duplicate\n        values in any of the array-valued input arguments.\n    deep : bool, optional\n        Whether to deep copy a ``vtk.vtkRectilinearGrid`` object.\n        Default is ``False``.  Keyword only.\n    Examples\n    --------\n    >>> import pyvista\n    >>> import vtk\n    >>> import numpy as np\n    Create an empty grid.\n    >>> grid = pyvista.RectilinearGrid()\n    Initialize from a vtk.vtkRectilinearGrid object\n    >>> vtkgrid = vtk.vtkRectilinearGrid()\n    >>> grid = pyvista.RectilinearGrid(vtkgrid)\n    Create from NumPy arrays.\n    >>> xrng = np.arange(-10, 10, 2)\n    >>> yrng = np.arange(-10, 10, 5)\n    >>> zrng = np.arange(-10, 10, 1)\n    >>> grid = pyvista.RectilinearGrid(xrng, yrng, zrng)\n    >>> grid.plot(show_edges=True)\n    \"\"\"\n    _WRITERS = {'.vtk': _vtk.vtkRectilinearGridWriter, '.vtr': _vtk.vtkXMLRectilinearGridWriter}\n    def __init__(self, *args, check_duplicates=False, deep=False, **kwargs):\n        \"\"\"Initialize the rectilinear grid.\"\"\"\n        super().__init__()\n        if len(args) == 1:\n            if isinstance(args[0], _vtk.vtkRectilinearGrid):\n                if deep:\n                    self.deep_copy(args[0])\n                else:\n                    self.shallow_copy(args[0])\n            elif isinstance(args[0], (str, pathlib.Path)):\n                self._from_file(args[0], **kwargs)\n            elif isinstance(args[0], np.ndarray):\n                self._from_arrays(args[0], None, None, check_duplicates)\n            else:\n                raise TypeError(f'Type ({type(args[0])}) not understood by `RectilinearGrid`')\n        elif len(args) == 3 or len(args) == 2:\n            arg0_is_arr = isinstance(args[0], np.ndarray)\n            arg1_is_arr = isinstance(args[1], np.ndarray)\n            if len(args) == 3:\n                arg2_is_arr = isinstance(args[2], np.ndarray)\n            else:\n                arg2_is_arr = False\n            if all([arg0_is_arr, arg1_is_arr, arg2_is_arr]):\n                self._from_arrays(args[0], args[1], args[2], check_duplicates)\n            elif all([arg0_is_arr, arg1_is_arr]):\n                self._from_arrays(args[0], args[1], None, check_duplicates)\n            else:\n                raise TypeError(\"Arguments not understood by `RectilinearGrid`.\")\n    def __repr__(self):\n        \"\"\"Return the default representation.\"\"\"\n        return DataSet.__repr__(self)\n    def __str__(self):\n        \"\"\"Return the str representation.\"\"\"\n        return DataSet.__str__(self)\n    def _update_dimensions(self):\n        \"\"\"Update the dimensions if coordinates have changed.\"\"\"\n        return self.SetDimensions(len(self.x), len(self.y), len(self.z))\n    def _from_arrays(\n        self, x: np.ndarray, y: np.ndarray, z: np.ndarray, check_duplicates: bool = False\n    ):\n        \"\"\"Create VTK rectilinear grid directly from numpy arrays.\n        Each array gives the uniques coordinates of the mesh along each axial\n        direction. To help ensure you are using this correctly, we take the unique\n        values of each argument.\n        Parameters\n        ----------\n        x : numpy.ndarray\n            Coordinates of the points in x direction.\n        y : numpy.ndarray\n            Coordinates of the points in y direction.\n        z : numpy.ndarray\n            Coordinates of the points in z direction.\n        check_duplicates : bool, optional\n            Check for duplications in any arrays that are passed.\n        \"\"\"\n        # Set the coordinates along each axial direction\n        # Must at least be an x array\n        if check_duplicates:\n            raise_has_duplicates(x)\n        # edges are shown as triangles if x is not floating point\n        if not np.issubdtype(x.dtype, np.floating):\n            x = x.astype(float)\n        self.SetXCoordinates(helpers.convert_array(x.ravel()))\n        if y is not None:\n            if check_duplicates:\n                raise_has_duplicates(y)\n            if not np.issubdtype(y.dtype, np.floating):\n                y = y.astype(float)\n            self.SetYCoordinates(helpers.convert_array(y.ravel()))\n        if z is not None:\n            if check_duplicates:\n                raise_has_duplicates(z)\n            if not np.issubdtype(z.dtype, np.floating):\n                z = z.astype(float)\n            self.SetZCoordinates(helpers.convert_array(z.ravel()))\n        # Ensure dimensions are properly set\n        self._update_dimensions()\n    @property\n    def meshgrid(self) -> list:\n        \"\"\"Return a meshgrid of numpy arrays for this mesh.\n        This simply returns a :func:`numpy.meshgrid` of the\n        coordinates for this mesh in ``ij`` indexing. These are a copy\n        of the points of this mesh.\n        \"\"\"\n        return np.meshgrid(self.x, self.y, self.z, indexing='ij')\n    @property  # type: ignore\n    def points(self) -> np.ndarray:  # type: ignore\n        \"\"\"Return a copy of the points as an n by 3 numpy array.\n        Notes\n        -----\n        Points of a :class:`pyvista.RectilinearGrid` cannot be\n        set. Set point coordinates with :attr:`RectilinearGrid.x`,\n        :attr:`RectilinearGrid.y`, or :attr:`RectilinearGrid.z`.\n        Examples\n        --------\n        >>> import numpy as np\n        >>> import pyvista\n        >>> xrng = np.arange(-10, 10, 10, dtype=float)\n        >>> yrng = np.arange(-10, 10, 10, dtype=float)\n        >>> zrng = np.arange(-10, 10, 10, dtype=float)\n        >>> grid = pyvista.RectilinearGrid(xrng, yrng, zrng)\n        >>> grid.points\n        array([[-10., -10., -10.],\n               [  0., -10., -10.],\n               [-10.,   0., -10.],\n               [  0.,   0., -10.],\n               [-10., -10.,   0.],\n               [  0., -10.,   0.],\n               [-10.,   0.,   0.],\n               [  0.,   0.,   0.]])\n        \"\"\"\n        xx, yy, zz = self.meshgrid\n        return np.c_[xx.ravel(order='F'), yy.ravel(order='F'), zz.ravel(order='F')]\n    @points.setter\n    def points(self, points):\n        \"\"\"Raise an AttributeError.\n        This setter overrides the base class's setter to ensure a user\n        does not attempt to set them.\n        \"\"\"\n        raise AttributeError(\n            \"The points cannot be set. The points of \"\n            \"`RectilinearGrid` are defined in each axial direction. Please \"\n            \"use the `x`, `y`, and `z` setters individually.\"\n        )\n    @property\n    def x(self) -> np.ndarray:\n        \"\"\"Return or set the coordinates along the X-direction.\n        Examples\n        --------\n        Return the x coordinates of a RectilinearGrid.\n        >>> import numpy as np\n        >>> import pyvista\n        >>> xrng = np.arange(-10, 10, 10, dtype=float)\n        >>> yrng = np.arange(-10, 10, 10, dtype=float)\n        >>> zrng = np.arange(-10, 10, 10, dtype=float)\n        >>> grid = pyvista.RectilinearGrid(xrng, yrng, zrng)\n        >>> grid.x\n        array([-10.,   0.])\n        Set the x coordinates of a RectilinearGrid.\n        >>> grid.x = [-10.0, 0.0, 10.0]\n        >>> grid.x\n        array([-10.,   0.,  10.])\n        \"\"\"\n        return helpers.convert_array(self.GetXCoordinates())\n    @x.setter\n    def x(self, coords: Sequence):\n        \"\"\"Set the coordinates along the X-direction.\"\"\"\n        self.SetXCoordinates(helpers.convert_array(coords))\n        self._update_dimensions()\n        self.Modified()\n    @property\n    def y(self) -> np.ndarray:\n        \"\"\"Return or set the coordinates along the Y-direction.\n        Examples\n        --------\n        Return the y coordinates of a RectilinearGrid.\n        >>> import numpy as np\n        >>> import pyvista\n        >>> xrng = np.arange(-10, 10, 10, dtype=float)\n        >>> yrng = np.arange(-10, 10, 10, dtype=float)\n        >>> zrng = np.arange(-10, 10, 10, dtype=float)\n        >>> grid = pyvista.RectilinearGrid(xrng, yrng, zrng)\n        >>> grid.y\n        array([-10.,   0.])\n        Set the y coordinates of a RectilinearGrid.\n        >>> grid.y = [-10.0, 0.0, 10.0]\n        >>> grid.y\n        array([-10.,   0.,  10.])\n        \"\"\"\n        return helpers.convert_array(self.GetYCoordinates())\n    @y.setter\n    def y(self, coords: Sequence):\n        \"\"\"Set the coordinates along the Y-direction.\"\"\"\n        self.SetYCoordinates(helpers.convert_array(coords))\n        self._update_dimensions()\n        self.Modified()\n    @property\n    def z(self) -> np.ndarray:\n        \"\"\"Return or set the coordinates along the Z-direction.\n        Examples\n        --------\n        Return the z coordinates of a RectilinearGrid.\n        >>> import numpy as np\n        >>> import pyvista\n        >>> xrng = np.arange(-10, 10, 10, dtype=float)\n        >>> yrng = np.arange(-10, 10, 10, dtype=float)\n        >>> zrng = np.arange(-10, 10, 10, dtype=float)\n        >>> grid = pyvista.RectilinearGrid(xrng, yrng, zrng)\n        >>> grid.z\n        array([-10.,   0.])\n        Set the z coordinates of a RectilinearGrid.\n        >>> grid.z = [-10.0, 0.0, 10.0]\n        >>> grid.z\n        array([-10.,   0.,  10.])\n        \"\"\"\n        return helpers.convert_array(self.GetZCoordinates())\n    @z.setter\n    def z(self, coords: Sequence):\n        \"\"\"Set the coordinates along the Z-direction.\"\"\"\n        self.SetZCoordinates(helpers.convert_array(coords))\n        self._update_dimensions()\n        self.Modified()\n    @Grid.dimensions.setter  # type: ignore\n    def dimensions(self, dims):\n        \"\"\"Do not let the dimensions of the RectilinearGrid be set.\"\"\"\n        raise AttributeError(\n            \"The dimensions of a `RectilinearGrid` are implicitly \"\n            \"defined and thus cannot be set.\"\n        )\n    def cast_to_structured_grid(self) -> 'pyvista.StructuredGrid':\n        \"\"\"Cast this rectilinear grid to a structured grid.\n        Returns\n        -------\n        pyvista.StructuredGrid\n            This grid as a structured grid.\n        \"\"\"\n        alg = _vtk.vtkRectilinearGridToPointSet()\n        alg.SetInputData(self)\n        alg.Update()\n        return _get_output(alg)\nclass UniformGrid(_vtk.vtkImageData, Grid, UniformGridFilters):\n    \"\"\"Models datasets with uniform spacing in the three coordinate directions.\n    Can be initialized in one of several ways:\n    - Create empty grid\n    - Initialize from a vtk.vtkImageData object\n    - Initialize based on dimensions, cell spacing, and origin.\n    .. versionchanged:: 0.33.0\n        First argument must now be either a path or\n        ``vtk.vtkImageData``. Use keyword arguments to specify the\n        dimensions, spacing, and origin of the uniform grid.\n    .. versionchanged:: 0.37.0\n        The ``dims`` parameter has been renamed to ``dimensions``.\n    Parameters\n    ----------\n    uinput : str, vtk.vtkImageData, pyvista.UniformGrid, optional\n        Filename or dataset to initialize the uniform grid from.  If\n        set, remainder of arguments are ignored.\n    dimensions : sequence[int], optional\n        Dimensions of the uniform grid.\n    spacing : sequence[float], default: (1.0, 1.0, 1.0)\n        Spacing of the uniform grid in each dimension. Must be positive.\n    origin : sequence[float], default: (0.0, 0.0, 0.0)\n        Origin of the uniform grid.\n    deep : bool, default: False\n        Whether to deep copy a ``vtk.vtkImageData`` object.  Keyword only.\n    Examples\n    --------\n    Create an empty UniformGrid.\n    >>> import pyvista\n    >>> grid = pyvista.UniformGrid()\n    Initialize from a ``vtk.vtkImageData`` object.\n    >>> import vtk\n    >>> vtkgrid = vtk.vtkImageData()\n    >>> grid = pyvista.UniformGrid(vtkgrid)\n    Initialize using just the grid dimensions and default\n    spacing and origin. These must be keyword arguments.\n    >>> grid = pyvista.UniformGrid(dimensions=(10, 10, 10))\n    Initialize using dimensions and spacing.\n    >>> grid = pyvista.UniformGrid(\n    ...     dimensions=(10, 10, 10),\n    ...     spacing=(2, 1, 5),\n    ... )\n    Initialize using dimensions, spacing, and an origin.\n    >>> grid = pyvista.UniformGrid(\n    ...     dimensions=(10, 10, 10),\n    ...     spacing=(2, 1, 5),\n    ...     origin=(10, 35, 50),\n    ... )\n    Initialize from another UniformGrid.\n    >>> grid = pyvista.UniformGrid(\n    ...     dimensions=(10, 10, 10),\n    ...     spacing=(2, 1, 5),\n    ...     origin=(10, 35, 50),\n    ... )\n    >>> grid_from_grid = pyvista.UniformGrid(grid)\n    >>> grid_from_grid == grid\n    True\n    \"\"\"\n    _WRITERS = {'.vtk': _vtk.vtkDataSetWriter, '.vti': _vtk.vtkXMLImageDataWriter}\n    def __init__(\n        self,\n        uinput=None,\n        *args,\n        dimensions=None,\n        spacing=(1.0, 1.0, 1.0),\n        origin=(0.0, 0.0, 0.0),\n        deep=False,\n        **kwargs,\n    ):\n        \"\"\"Initialize the uniform grid.\"\"\"\n        super().__init__()\n        # permit old behavior\n        if isinstance(uinput, Sequence) and not isinstance(uinput, str):\n            # Deprecated on v0.37.0, estimated removal on v0.40.0\n            warnings.warn(\n                \"Behavior of pyvista.UniformGrid has changed. First argument must be \"\n                \"either a ``vtk.vtkImageData`` or path.\",\n                PyVistaDeprecationWarning,\n            )\n            dimensions = uinput\n            uinput = None\n        if dimensions is None and 'dims' in kwargs:\n            dimensions = kwargs.pop('dims')\n            # Deprecated on v0.37.0, estimated removal on v0.40.0\n            warnings.warn(\n                '`dims` argument is deprecated. Please use `dimensions`.', PyVistaDeprecationWarning\n            )\n        assert_empty_kwargs(**kwargs)\n        if args:\n            # Deprecated on v0.37.0, estimated removal on v0.40.0\n            warnings.warn(\n                \"Behavior of pyvista.UniformGrid has changed. Use keyword arguments \"\n                \"to specify dimensions, spacing, and origin. For example:\\n\\n\"\n                \"    >>> grid = pyvista.UniformGrid(\\n\"\n                \"    ...     dimensions=(10, 10, 10),\\n\"\n                \"    ...     spacing=(2, 1, 5),\\n\"\n                \"    ...     origin=(10, 35, 50),\\n\"\n                \"    ... )\\n\",\n                PyVistaDeprecationWarning,\n            )\n            origin = args[0]\n            if len(args) > 1:\n                spacing = args[1]\n            if len(args) > 2:\n                raise ValueError(\n                    \"Too many additional arguments specified for UniformGrid. \"\n                    f\"Accepts at most 2, and {len(args)} have been input.\"\n                )\n        # first argument must be either vtkImageData or a path\n        if uinput is not None:\n            if isinstance(uinput, _vtk.vtkImageData):\n                if deep:\n                    self.deep_copy(uinput)\n                else:\n                    self.shallow_copy(uinput)\n            elif isinstance(uinput, (str, pathlib.Path)):\n                self._from_file(uinput)\n            else:\n                raise TypeError(\n                    \"First argument, ``uinput`` must be either ``vtk.vtkImageData`` \"\n                    f\"or a path, not {type(uinput)}.  Use keyword arguments to \"\n                    \"specify dimensions, spacing, and origin. For example:\\n\\n\"\n                    \"    >>> grid = pyvista.UniformGrid(\\n\"\n                    \"    ...     dimensions=(10, 10, 10),\\n\"\n                    \"    ...     spacing=(2, 1, 5),\\n\"\n                    \"    ...     origin=(10, 35, 50),\\n\"\n                    \"    ... )\\n\"\n                )\n        elif dimensions is not None:\n            self._from_specs(dimensions, spacing, origin)\n    def __repr__(self):\n        \"\"\"Return the default representation.\"\"\"\n        return DataSet.__repr__(self)\n    def __str__(self):\n        \"\"\"Return the default str representation.\"\"\"\n        return DataSet.__str__(self)\n    def _from_specs(self, dims: Sequence[int], spacing=(1.0, 1.0, 1.0), origin=(0.0, 0.0, 0.0)):\n        \"\"\"Create VTK image data directly from numpy arrays.\n        A uniform grid is defined by the point spacings for each axis\n        (uniform along each individual axis) and the number of points on each axis.\n        These are relative to a specified origin (default is ``(0.0, 0.0, 0.0)``).\n        Parameters\n        ----------\n        dims : tuple(int)\n            Length 3 tuple of ints specifying how many points along each axis.\n        spacing : sequence[float], default: (1.0, 1.0, 1.0)\n            Length 3 tuple of floats/ints specifying the point spacings\n            for each axis. Must be positive.\n        origin : sequence[float], default: (0.0, 0.0, 0.0)\n            Length 3 tuple of floats/ints specifying minimum value for each axis.\n        \"\"\"\n        xn, yn, zn = dims[0], dims[1], dims[2]\n        xo, yo, zo = origin[0], origin[1], origin[2]\n        self.SetDimensions(xn, yn, zn)\n        self.SetOrigin(xo, yo, zo)\n        self.spacing = (spacing[0], spacing[1], spacing[2])\n    @property  # type: ignore\n    def points(self) -> np.ndarray:  # type: ignore\n        \"\"\"Build a copy of the implicitly defined points as a numpy array.\n        Notes\n        -----\n        The ``points`` for a :class:`pyvista.UniformGrid` cannot be set.\n        Examples\n        --------\n        >>> import pyvista\n        >>> grid = pyvista.UniformGrid(dimensions=(2, 2, 2))\n        >>> grid.points\n        array([[0., 0., 0.],\n               [1., 0., 0.],\n               [0., 1., 0.],\n               [1., 1., 0.],\n               [0., 0., 1.],\n               [1., 0., 1.],\n               [0., 1., 1.],\n               [1., 1., 1.]])\n        \"\"\"\n        # Get grid dimensions\n        nx, ny, nz = self.dimensions\n        nx -= 1\n        ny -= 1\n        nz -= 1\n        # get the points and convert to spacings\n        dx, dy, dz = self.spacing\n        # Now make the cell arrays\n        ox, oy, oz = np.array(self.origin) + np.array(self.extent[::2])  # type: ignore\n        x = np.insert(np.cumsum(np.full(nx, dx)), 0, 0.0) + ox\n        y = np.insert(np.cumsum(np.full(ny, dy)), 0, 0.0) + oy\n        z = np.insert(np.cumsum(np.full(nz, dz)), 0, 0.0) + oz",
        "file_path": "pyvista/core/grid.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 2,
        "score": 0.6705425381660461,
        "content": "    def __repr__(self):\n        \"\"\"Return the standard representation.\"\"\"\n        return DataSet.__repr__(self)\n    def __str__(self):\n        \"\"\"Return the standard str representation.\"\"\"\n        return DataSet.__str__(self)\n    def _from_arrays(self, x, y, z, force_float=True):\n        \"\"\"Create VTK structured grid directly from numpy arrays.\n        Parameters\n        ----------\n        x : numpy.ndarray\n            Position of the points in x direction.\n        y : numpy.ndarray\n            Position of the points in y direction.\n        z : numpy.ndarray\n            Position of the points in z direction.\n        force_float : bool, optional\n            Casts the datatype to ``float32`` if points datatype is\n            non-float.  Default ``True``. Set this to ``False`` to allow\n            non-float types, though this may lead to truncation of\n            intermediate floats when transforming datasets.\n        \"\"\"\n        if not (x.shape == y.shape == z.shape):\n            raise ValueError('Input point array shapes must match exactly')\n        # make the output points the same precision as the input arrays\n        points = np.empty((x.size, 3), x.dtype)\n        points[:, 0] = x.ravel('F')\n        points[:, 1] = y.ravel('F')\n        points[:, 2] = z.ravel('F')\n        # ensure that the inputs are 3D\n        dim = list(x.shape)\n        while len(dim) < 3:\n            dim.append(1)\n        # Create structured grid\n        self.SetDimensions(dim)\n        self.SetPoints(pyvista.vtk_points(points, force_float=force_float))\n    @property\n    def dimensions(self):\n        \"\"\"Return a length 3 tuple of the grid's dimensions.\n        Returns\n        -------\n        tuple\n            Grid dimensions.\n        Examples\n        --------\n        >>> import pyvista\n        >>> import numpy as np\n        >>> xrng = np.arange(-10, 10, 1, dtype=np.float32)\n        >>> yrng = np.arange(-10, 10, 2, dtype=np.float32)\n        >>> zrng = np.arange(-10, 10, 5, dtype=np.float32)\n        >>> x, y, z = np.meshgrid(xrng, yrng, zrng)\n        >>> grid = pyvista.StructuredGrid(x, y, z)\n        >>> grid.dimensions\n        (10, 20, 4)\n        \"\"\"\n        return tuple(self.GetDimensions())\n    @dimensions.setter\n    def dimensions(self, dims):\n        nx, ny, nz = dims[0], dims[1], dims[2]\n        self.SetDimensions(nx, ny, nz)\n        self.Modified()\n    @property\n    def x(self):\n        \"\"\"Return the X coordinates of all points.\n        Returns\n        -------\n        numpy.ndarray\n            Numpy array of all X coordinates.\n        Examples\n        --------\n        >>> import pyvista\n        >>> import numpy as np\n        >>> xrng = np.arange(-10, 10, 1, dtype=np.float32)\n        >>> yrng = np.arange(-10, 10, 2, dtype=np.float32)\n        >>> zrng = np.arange(-10, 10, 5, dtype=np.float32)\n        >>> x, y, z = np.meshgrid(xrng, yrng, zrng)\n        >>> grid = pyvista.StructuredGrid(x, y, z)\n        >>> grid.x.shape\n        (10, 20, 4)\n        \"\"\"\n        return self._reshape_point_array(self.points[:, 0])\n    @property\n    def y(self):\n        \"\"\"Return the Y coordinates of all points.\"\"\"\n        return self._reshape_point_array(self.points[:, 1])\n    @property\n    def z(self):\n        \"\"\"Return the Z coordinates of all points.\"\"\"\n        return self._reshape_point_array(self.points[:, 2])\n    @property\n    def points_matrix(self):\n        \"\"\"Points as a 4-D matrix, with x/y/z along the last dimension.\"\"\"\n        return self.points.reshape((*self.dimensions, 3), order='F')\n    def _get_attrs(self):\n        \"\"\"Return the representation methods (internal helper).\"\"\"\n        attrs = PointGrid._get_attrs(self)\n        attrs.append((\"Dimensions\", self.dimensions, \"{:d}, {:d}, {:d}\"))\n        return attrs\n    def __getitem__(self, key):\n        \"\"\"Slice subsets of the StructuredGrid, or extract an array field.\"\"\"\n        # legacy behavior which looks for a point or cell array\n        if not isinstance(key, tuple):\n            return super().__getitem__(key)\n        # convert slice to VOI specification - only \"basic indexing\" is supported\n        voi = []\n        rate = []\n        if len(key) != 3:\n            raise RuntimeError('Slices must have exactly 3 dimensions.')\n        for i, k in enumerate(key):\n            if isinstance(k, collections.abc.Iterable):\n                raise RuntimeError('Fancy indexing is not supported.')\n            if isinstance(k, numbers.Integral):\n                start = stop = k\n                step = 1\n            elif isinstance(k, slice):\n                start = k.start if k.start is not None else 0\n                stop = k.stop - 1 if k.stop is not None else self.dimensions[i]\n                step = k.step if k.step is not None else 1\n            voi.extend((start, stop))\n            rate.append(step)\n        return self.extract_subset(voi, rate, boundary=False)\n    def hide_cells(self, ind, inplace=False):\n        \"\"\"Hide cells without deleting them.\n        Hides cells by setting the ghost_cells array to ``HIDDEN_CELL``.\n        Parameters\n        ----------\n        ind : sequence[int]\n            List or array of cell indices to be hidden.  The array can\n            also be a boolean array of the same size as the number of\n            cells.\n        inplace : bool, default: False\n            Updates mesh in-place.\n        Returns\n        -------\n        pyvista.StructuredGrid\n            Structured grid with hidden cells.\n        Examples\n        --------\n        Hide part of the middle of a structured surface.\n        >>> import pyvista as pv\n        >>> import numpy as np\n        >>> x = np.arange(-10, 10, 0.25)\n        >>> y = np.arange(-10, 10, 0.25)\n        >>> z = 0\n        >>> x, y, z = np.meshgrid(x, y, z)\n        >>> grid = pv.StructuredGrid(x, y, z)\n        >>> grid = grid.hide_cells(range(79 * 30, 79 * 50))\n        >>> grid.plot(color=True, show_edges=True)\n        \"\"\"\n        if not inplace:\n            return self.copy().hide_cells(ind, inplace=True)\n        if isinstance(ind, np.ndarray):\n            if ind.dtype == np.bool_ and ind.size != self.n_cells:\n                raise ValueError(\n                    f'Boolean array size must match the number of cells ({self.n_cells})'\n                )\n        ghost_cells = np.zeros(self.n_cells, np.uint8)\n        ghost_cells[ind] = _vtk.vtkDataSetAttributes.HIDDENCELL\n        # NOTE: cells cannot be removed from a structured grid, only\n        # hidden setting ghost_cells to a value besides\n        # vtk.vtkDataSetAttributes.HIDDENCELL will not hide them\n        # properly, additionally, calling self.RemoveGhostCells will\n        # have no effect\n        # add but do not make active\n        self.cell_data.set_array(ghost_cells, _vtk.vtkDataSetAttributes.GhostArrayName())\n        return self\n    def hide_points(self, ind):\n        \"\"\"Hide points without deleting them.\n        Hides points by setting the ghost_points array to ``HIDDEN_CELL``.\n        Parameters\n        ----------\n        ind : sequence[int]\n            Sequence of point indices to be hidden.  The array\n            can also be a boolean array of the same size as the number\n            of points.\n        Returns\n        -------\n        pyvista.PointSet\n            Point set with hidden points.\n        Examples\n        --------\n        Hide part of the middle of a structured surface.\n        >>> import pyvista as pv\n        >>> import numpy as np\n        >>> x = np.arange(-10, 10, 0.25)\n        >>> y = np.arange(-10, 10, 0.25)\n        >>> z = 0\n        >>> x, y, z = np.meshgrid(x, y, z)\n        >>> grid = pv.StructuredGrid(x, y, z)\n        >>> grid.hide_points(range(80 * 30, 80 * 50))\n        >>> grid.plot(color=True, show_edges=True)\n        \"\"\"\n        if isinstance(ind, np.ndarray):\n            if ind.dtype == np.bool_ and ind.size != self.n_points:\n                raise ValueError(\n                    f'Boolean array size must match the number of points ({self.n_points})'\n                )\n        ghost_points = np.zeros(self.n_points, np.uint8)\n        ghost_points[ind] = _vtk.vtkDataSetAttributes.HIDDENPOINT\n        # add but do not make active\n        self.point_data.set_array(ghost_points, _vtk.vtkDataSetAttributes.GhostArrayName())\n    def _reshape_point_array(self, array):\n        \"\"\"Reshape point data to a 3-D matrix.\"\"\"\n        return array.reshape(self.dimensions, order='F')\n    def _reshape_cell_array(self, array):\n        \"\"\"Reshape cell data to a 3-D matrix.\"\"\"\n        cell_dims = np.array(self.dimensions) - 1\n        cell_dims[cell_dims == 0] = 1\n        return array.reshape(cell_dims, order='F')\nclass ExplicitStructuredGrid(_vtk.vtkExplicitStructuredGrid, PointGrid):\n    \"\"\"Extend the functionality of the ``vtk.vtkExplicitStructuredGrid`` class.\n    Can be initialized by the following:\n    - Creating an empty grid\n    - From a ``vtk.vtkExplicitStructuredGrid`` or ``vtk.vtkUnstructuredGrid`` object\n    - From a VTU or VTK file\n    - From ``dims`` and ``corners`` arrays\n    Parameters\n    ----------\n    args : vtk.vtkExplicitStructuredGrid, vtk.vtkUnstructuredGrid, str, Sequence\n        See examples below.\n    deep : bool, default: False\n        Whether to deep copy a ``vtk.vtkUnstructuredGrid`` object.\n    Examples\n    --------\n    >>> import numpy as np\n    >>> import pyvista as pv\n    >>>\n    >>> # grid size: ni*nj*nk cells; si, sj, sk steps\n    >>> ni, nj, nk = 4, 5, 6\n    >>> si, sj, sk = 20, 10, 1\n    >>>\n    >>> # create raw coordinate grid\n    >>> grid_ijk = np.mgrid[\n    ...     : (ni + 1) * si : si,\n    ...     : (nj + 1) * sj : sj,\n    ...     : (nk + 1) * sk : sk,\n    ... ]\n    >>>\n    >>> # repeat array along each Cartesian axis for connectivity\n    >>> for axis in range(1, 4):\n    ...     grid_ijk = grid_ijk.repeat(2, axis=axis)\n    ...\n    >>>\n    >>> # slice off unnecessarily doubled edge coordinates\n    >>> grid_ijk = grid_ijk[:, 1:-1, 1:-1, 1:-1]\n    >>>\n    >>> # reorder and reshape to VTK order\n    >>> corners = grid_ijk.transpose().reshape(-1, 3)\n    >>>\n    >>> dims = np.array([ni, nj, nk]) + 1\n    >>> grid = pv.ExplicitStructuredGrid(dims, corners)\n    >>> grid = grid.compute_connectivity()\n    >>> grid.plot(show_edges=True)  # doctest:+SKIP\n    \"\"\"\n    _WRITERS = {'.vtu': _vtk.vtkXMLUnstructuredGridWriter, '.vtk': _vtk.vtkUnstructuredGridWriter}\n    def __init__(self, *args, deep=False, **kwargs):\n        \"\"\"Initialize the explicit structured grid.\"\"\"\n        super().__init__()\n        n = len(args)\n        if n > 2:\n            raise ValueError(\"Too many args to create ExplicitStructuredGrid.\")\n        if n == 1:\n            arg0 = args[0]\n            if isinstance(arg0, _vtk.vtkExplicitStructuredGrid):\n                if deep:\n                    self.deep_copy(arg0)\n                else:\n                    self.shallow_copy(arg0)\n            elif isinstance(arg0, _vtk.vtkUnstructuredGrid):\n                grid = arg0.cast_to_explicit_structured_grid()\n                self.shallow_copy(grid)\n            elif isinstance(arg0, (str, pathlib.Path)):\n                grid = UnstructuredGrid(arg0)\n                grid = grid.cast_to_explicit_structured_grid()\n                self.shallow_copy(grid)\n        elif n == 2:\n            arg0, arg1 = args\n            if isinstance(arg0, tuple):\n                arg0 = np.asarray(arg0)\n            if isinstance(arg1, list):\n                arg1 = np.asarray(arg1)\n            arg0_is_arr = isinstance(arg0, np.ndarray)\n            arg1_is_arr = isinstance(arg1, np.ndarray)\n            if all([arg0_is_arr, arg1_is_arr]):\n                self._from_arrays(arg0, arg1)\n    def __repr__(self):\n        \"\"\"Return the standard representation.\"\"\"\n        return DataSet.__repr__(self)\n    def __str__(self):\n        \"\"\"Return the standard ``str`` representation.\"\"\"\n        return DataSet.__str__(self)\n    def _from_arrays(self, dims: Sequence, corners: Sequence) -> None:\n        \"\"\"Create a VTK explicit structured grid from NumPy arrays.\n        Parameters\n        ----------\n        dims : sequence[int]\n            A sequence of integers with shape (3,) containing the\n            topological dimensions of the grid.\n        corners : array_like[floats]\n            A sequence of floats with shape (number of corners, 3)\n            containing the coordinates of the corner points.\n        \"\"\"\n        shape0 = np.asanyarray(dims) - 1\n        shape1 = 2 * shape0\n        ncells = np.prod(shape0)\n        cells = 8 * np.ones((ncells, 9), dtype=int)\n        points, indices = np.unique(corners, axis=0, return_inverse=True)\n        connectivity = np.asarray(\n            [[0, 1, 1, 0, 0, 1, 1, 0], [0, 0, 1, 1, 0, 0, 1, 1], [0, 0, 0, 0, 1, 1, 1, 1]]\n        )\n        for c in range(ncells):\n            i, j, k = np.unravel_index(c, shape0, order='F')\n            coord = (2 * i + connectivity[0], 2 * j + connectivity[1], 2 * k + connectivity[2])\n            cinds = np.ravel_multi_index(coord, shape1, order='F')  # type: ignore\n            cells[c, 1:] = indices[cinds]\n        cells = cells.flatten()\n        points = pyvista.vtk_points(points)\n        cells = CellArray(cells, ncells)\n        self.SetDimensions(dims)\n        self.SetPoints(points)\n        self.SetCells(cells)\n    def cast_to_unstructured_grid(self) -> 'UnstructuredGrid':\n        \"\"\"Cast to an unstructured grid.\n        Returns\n        -------\n        UnstructuredGrid\n            An unstructured grid. VTK adds the ``'BLOCK_I'``,\n            ``'BLOCK_J'`` and ``'BLOCK_K'`` cell arrays. These arrays\n            are required to restore the explicit structured grid.\n        See Also\n        --------\n        pyvista.DataSetFilters.extract_cells : Extract a subset of a dataset.\n        pyvista.UnstructuredGrid.cast_to_explicit_structured_grid : Cast an unstructured grid to an explicit structured grid.\n        Notes\n        -----\n        The ghost cell array is disabled before casting the\n        unstructured grid in order to allow the original structure\n        and attributes data of the explicit structured grid to be\n        restored. If you don't need to restore the explicit\n        structured grid later or want to extract an unstructured\n        grid from the visible subgrid, use the ``extract_cells``\n        filter and the cell indices where the ghost cell array is\n        ``0``.\n        Examples\n        --------\n        >>> from pyvista import examples\n        >>> grid = examples.load_explicit_structured()  # doctest:+SKIP\n        >>> grid.plot(\n        ...     color='w', show_edges=True, show_bounds=True\n        ... )  # doctest:+SKIP\n        >>> grid = grid.hide_cells(range(80, 120))  # doctest:+SKIP\n        >>> grid.plot(\n        ...     color='w', show_edges=True, show_bounds=True\n        ... )  # doctest:+SKIP\n        >>> grid = grid.cast_to_unstructured_grid()  # doctest:+SKIP\n        >>> grid.plot(\n        ...     color='w', show_edges=True, show_bounds=True\n        ... )  # doctest:+SKIP\n        >>> grid = grid.cast_to_explicit_structured_grid()  # doctest:+SKIP\n        >>> grid.plot(\n        ...     color='w', show_edges=True, show_bounds=True\n        ... )  # doctest:+SKIP\n        \"\"\"\n        grid = ExplicitStructuredGrid()\n        grid.copy_structure(self)\n        alg = _vtk.vtkExplicitStructuredGridToUnstructuredGrid()\n        alg.SetInputDataObject(grid)\n        alg.Update()\n        grid = _get_output(alg)\n        grid.cell_data.remove('vtkOriginalCellIds')  # unrequired\n        grid.copy_attributes(self)  # copy ghost cell array and other arrays\n        return grid\n    def save(self, filename, binary=True):\n        \"\"\"Save this VTK object to file.\n        Parameters\n        ----------\n        filename : str\n            Output file name. VTU and VTK extensions are supported.\n        binary : bool, default: True\n            If ``True``, write as binary, else ASCII.\n        Notes\n        -----\n        VTK adds the ``'BLOCK_I'``, ``'BLOCK_J'`` and ``'BLOCK_K'``\n        cell arrays. These arrays are required to restore the explicit\n        structured grid.\n        Examples\n        --------\n        >>> import pyvista as pv\n        >>> from pyvista import examples\n        >>> grid = examples.load_explicit_structured()  # doctest:+SKIP\n        >>> grid = grid.hide_cells(range(80, 120))  # doctest:+SKIP\n        >>> grid.save('grid.vtu')  # doctest:+SKIP\n        >>> grid = pv.ExplicitStructuredGrid('grid.vtu')  # doctest:+SKIP\n        >>> grid.plot(\n        ...     color='w', show_edges=True, show_bounds=True\n        ... )  # doctest:+SKIP\n        >>> grid.show_cells()  # doctest:+SKIP\n        >>> grid.plot(\n        ...     color='w', show_edges=True, show_bounds=True\n        ... )  # doctest:+SKIP\n        \"\"\"\n        grid = self.cast_to_unstructured_grid()\n        grid.save(filename, binary)\n    def hide_cells(self, ind: Sequence[int], inplace=False) -> 'ExplicitStructuredGrid':\n        \"\"\"Hide specific cells.\n        Hides cells by setting the ghost cell array to ``HIDDENCELL``.\n        Parameters\n        ----------\n        ind : sequence[int]\n            Cell indices to be hidden. A boolean array of the same\n            size as the number of cells also is acceptable.\n        inplace : bool, default: False\n            This method is applied to this grid if ``True``\n            or to a copy otherwise.\n        Returns\n        -------\n        ExplicitStructuredGrid or None\n            A deep copy of this grid if ``inplace=False`` with the\n            hidden cells, or this grid with the hidden cells if\n            otherwise.\n        Examples\n        --------\n        >>> from pyvista import examples\n        >>> grid = examples.load_explicit_structured()\n        >>> grid = grid.hide_cells(range(80, 120))\n        >>> grid.plot(color='w', show_edges=True, show_bounds=True)\n        \"\"\"\n        ind_arr = np.asanyarray(ind)\n        if inplace:\n            array = np.zeros(self.n_cells, dtype=np.uint8)\n            array[ind_arr] = _vtk.vtkDataSetAttributes.HIDDENCELL\n            name = _vtk.vtkDataSetAttributes.GhostArrayName()\n            self.cell_data[name] = array\n            return self\n        grid = self.copy()\n        grid.hide_cells(ind, inplace=True)\n        return grid\n    def show_cells(self, inplace=False) -> 'ExplicitStructuredGrid':\n        \"\"\"Show hidden cells.\n        Shows hidden cells by setting the ghost cell array to ``0``\n        where ``HIDDENCELL``.\n        Parameters\n        ----------\n        inplace : bool, default: False\n            This method is applied to this grid if ``True``\n            or to a copy otherwise.\n        Returns\n        -------\n        ExplicitStructuredGrid\n            A deep copy of this grid if ``inplace=False`` with the\n            hidden cells shown.  Otherwise, this dataset with the\n            shown cells.\n        Examples\n        --------\n        >>> from pyvista import examples\n        >>> grid = examples.load_explicit_structured()\n        >>> grid = grid.hide_cells(range(80, 120))\n        >>> grid.plot(color='w', show_edges=True, show_bounds=True)\n        >>> grid = grid.show_cells()\n        >>> grid.plot(color='w', show_edges=True, show_bounds=True)\n        \"\"\"\n        if inplace:\n            name = _vtk.vtkDataSetAttributes.GhostArrayName()\n            if name in self.cell_data.keys():\n                array = self.cell_data[name]\n                ind = np.argwhere(array == _vtk.vtkDataSetAttributes.HIDDENCELL)\n                array[ind] = 0\n            return self\n        else:\n            grid = self.copy()\n            grid.show_cells(inplace=True)\n            return grid\n    def _dimensions(self):\n        # This method is required to avoid conflict if a developer extends `ExplicitStructuredGrid`\n        # and reimplements `dimensions` to return, for example, the number of cells in the I, J and\n        # K directions.\n        dims = self.GetExtent()\n        dims = np.reshape(dims, (3, 2))\n        dims = np.diff(dims, axis=1)\n        dims = dims.flatten() + 1\n        return int(dims[0]), int(dims[1]), int(dims[2])\n    @property\n    def dimensions(self) -> Tuple[int, int, int]:\n        \"\"\"Return the topological dimensions of the grid.\n        Returns\n        -------\n        tuple[int, int, int]\n            Number of sampling points in the I, J and Z directions respectively.\n        Examples\n        --------\n        >>> from pyvista import examples\n        >>> grid = examples.load_explicit_structured()  # doctest:+SKIP\n        >>> grid.dimensions  # doctest:+SKIP\n        (5, 6, 7)\n        \"\"\"\n        return self._dimensions()\n    @property\n    def visible_bounds(self) -> BoundsLike:\n        \"\"\"Return the bounding box of the visible cells.\n        Different from `bounds`, which returns the bounding box of the",
        "file_path": "pyvista/core/pointset.py",
        "chunk_index": 4,
        "metadata": {}
      },
      {
        "rank": 3,
        "score": 0.6685999631881714,
        "content": "        ...         [1, 1, 0],\n        ...         [0, 1, 0],\n        ...         [0, 0, 1],\n        ...         [1, 0, 1],\n        ...         [1, 1, 1],\n        ...         [0, 1, 1],\n        ...     ],\n        ...     dtype=np.float32,\n        ... )\n        >>> cell2 = np.array(\n        ...     [\n        ...         [0, 0, 2],\n        ...         [1, 0, 2],\n        ...         [1, 1, 2],\n        ...         [0, 1, 2],\n        ...         [0, 0, 3],\n        ...         [1, 0, 3],\n        ...         [1, 1, 3],\n        ...         [0, 1, 3],\n        ...     ],\n        ...     dtype=np.float32,\n        ... )\n        >>> points = np.vstack((cell1, cell2))\n        >>> grid = pyvista.UnstructuredGrid(cells, cell_type, points)\n        \"\"\"\n        if offset is not None:\n            warnings.warn('VTK 9 no longer accepts an offset array', stacklevel=3)\n        # convert to arrays upfront\n        cells = np.asarray(cells)\n        cell_type = np.asarray(cell_type)\n        points = np.asarray(points)\n        # Convert to vtk arrays\n        vtkcells = CellArray(cells, cell_type.size, deep)\n        if cell_type.dtype != np.uint8:\n            cell_type = cell_type.astype(np.uint8)\n        cell_type = _vtk.numpy_to_vtk(cell_type, deep=deep)\n        points = pyvista.vtk_points(points, deep, force_float)\n        self.SetPoints(points)\n        self.SetCells(cell_type, vtkcells)\n    def _check_for_consistency(self):\n        \"\"\"Check if size of offsets and celltypes match the number of cells.\n        Checks if the number of offsets and celltypes correspond to\n        the number of cells.  Called after initialization of the self\n        from arrays.\n        \"\"\"\n        if self.n_cells != self.celltypes.size:\n            raise ValueError(\n                f'Number of cell types ({self.celltypes.size}) '\n                f'must match the number of cells {self.n_cells})'\n            )\n        if self.n_cells != self.offset.size - 1:  # pragma: no cover\n            raise ValueError(\n                f'Size of the offset ({self.offset.size}) '\n                f'must be one greater than the number of cells ({self.n_cells})'\n            )\n    @property\n    def cells(self) -> np.ndarray:\n        \"\"\"Return a pointer to the cells as a numpy object.\n        See Also\n        --------\n        pyvista.DataSet.get_cell\n        Examples\n        --------\n        Return the indices of the first two cells from the example hex\n        beam.  Note how the cells have \"padding\" indicating the number\n        of points per cell.\n        >>> import pyvista\n        >>> from pyvista import examples\n        >>> hex_beam = pyvista.read(examples.hexbeamfile)\n        >>> hex_beam.cells[:18]  # doctest:+SKIP\n        array([ 8,  0,  2,  8,  7, 27, 36, 90, 81,  8,  2,  1,  4,\n                8, 36, 18, 54, 90])\n        \"\"\"\n        return _vtk.vtk_to_numpy(self.GetCells().GetData())\n    @property\n    def cells_dict(self) -> dict:\n        \"\"\"Return a dictionary that contains all cells mapped from cell types.\n        This function returns a :class:`numpy.ndarray` for each cell\n        type in an ordered fashion.  Note that this function only\n        works with element types of fixed sizes.\n        Returns\n        -------\n        dict\n            A dictionary mapping containing all cells of this unstructured grid.\n            Structure: vtk_enum_type (int) -> cells (:class:`numpy.ndarray`).\n        See Also\n        --------\n        pyvista.DataSet.get_cell\n        Examples\n        --------\n        Return the cells dictionary of the sample hex beam.  Note how\n        there is only one key/value pair as the hex beam example is\n        composed of only all hexahedral cells, which is\n        ``CellType.HEXAHEDRON``, which evaluates to 12.\n        Also note how there is no padding for the cell array.  This\n        approach may be more helpful than the ``cells`` property when\n        extracting cells.\n        >>> import pyvista\n        >>> from pyvista import examples\n        >>> hex_beam = pyvista.read(examples.hexbeamfile)\n        >>> hex_beam.cells_dict  # doctest:+SKIP\n        {12: array([[ 0,  2,  8,  7, 27, 36, 90, 81],\n                [ 2,  1,  4,  8, 36, 18, 54, 90],\n                [ 7,  8,  6,  5, 81, 90, 72, 63],\n                ...\n                [44, 26, 62, 98, 11, 10, 13, 17],\n                [89, 98, 80, 71, 16, 17, 15, 14],\n                [98, 62, 53, 80, 17, 13, 12, 15]])}\n        \"\"\"\n        return get_mixed_cells(self)\n    @property\n    def cell_connectivity(self) -> np.ndarray:\n        \"\"\"Return a the vtk cell connectivity as a numpy array.\n        This is effectively :attr:`UnstructuredGrid.cells` without the\n        padding.\n        Returns\n        -------\n        numpy.ndarray\n            Connectivity array.\n        See Also\n        --------\n        pyvista.DataSet.get_cell\n        Examples\n        --------\n        Return the cell connectivity for the first two cells.\n        >>> import pyvista\n        >>> from pyvista import examples\n        >>> hex_beam = pyvista.read(examples.hexbeamfile)\n        >>> hex_beam.cell_connectivity[:16]\n        array([ 0,  2,  8,  7, 27, 36, 90, 81,  2,  1,  4,  8, 36, 18, 54, 90])\n        \"\"\"\n        carr = self.GetCells()\n        return _vtk.vtk_to_numpy(carr.GetConnectivityArray())\n    def linear_copy(self, deep=False):\n        \"\"\"Return a copy of the unstructured grid containing only linear cells.\n        Converts the following cell types to their linear equivalents.\n        - ``QUADRATIC_TETRA      --> TETRA``\n        - ``QUADRATIC_PYRAMID    --> PYRAMID``\n        - ``QUADRATIC_WEDGE      --> WEDGE``\n        - ``QUADRATIC_HEXAHEDRON --> HEXAHEDRON``\n        Parameters\n        ----------\n        deep : bool, default: False\n            When ``True``, makes a copy of the points array.\n            Cells and cell types are always copied.\n        Returns\n        -------\n        pyvista.UnstructuredGrid\n            UnstructuredGrid containing only linear cells when\n            ``deep=False``.\n        \"\"\"\n        lgrid = self.copy(deep)\n        # grab the vtk object\n        vtk_cell_type = _vtk.numpy_to_vtk(self.GetCellTypesArray(), deep=True)\n        celltype = _vtk.vtk_to_numpy(vtk_cell_type)\n        celltype[celltype == CellType.QUADRATIC_TETRA] = CellType.TETRA\n        celltype[celltype == CellType.QUADRATIC_PYRAMID] = CellType.PYRAMID\n        celltype[celltype == CellType.QUADRATIC_WEDGE] = CellType.WEDGE\n        celltype[celltype == CellType.QUADRATIC_HEXAHEDRON] = CellType.HEXAHEDRON\n        # track quad mask for later\n        quad_quad_mask = celltype == CellType.QUADRATIC_QUAD\n        celltype[quad_quad_mask] = CellType.QUAD\n        quad_tri_mask = celltype == CellType.QUADRATIC_TRIANGLE\n        celltype[quad_tri_mask] = CellType.TRIANGLE\n        vtk_offset = self.GetCellLocationsArray()\n        cells = _vtk.vtkCellArray()\n        cells.DeepCopy(self.GetCells())\n        lgrid.SetCells(vtk_cell_type, vtk_offset, cells)\n        # fixing bug with display of quad cells\n        if np.any(quad_quad_mask):\n            quad_offset = lgrid.offset[:-1][quad_quad_mask]\n            base_point = lgrid.cell_connectivity[quad_offset]\n            lgrid.cell_connectivity[quad_offset + 4] = base_point\n            lgrid.cell_connectivity[quad_offset + 5] = base_point\n            lgrid.cell_connectivity[quad_offset + 6] = base_point\n            lgrid.cell_connectivity[quad_offset + 7] = base_point\n        if np.any(quad_tri_mask):\n            tri_offset = lgrid.offset[:-1][quad_tri_mask]\n            base_point = lgrid.cell_connectivity[tri_offset]\n            lgrid.cell_connectivity[tri_offset + 3] = base_point\n            lgrid.cell_connectivity[tri_offset + 4] = base_point\n            lgrid.cell_connectivity[tri_offset + 5] = base_point\n        return lgrid\n    @property\n    def celltypes(self) -> np.ndarray:\n        \"\"\"Return the cell types array.\n        Returns\n        -------\n        numpy.ndarray\n            Array of cell types.\n        Notes\n        -----\n        Here are some of the most popular cell types:\n        * ``EMPTY_CELL = 0``\n        * ``VERTEX = 1``\n        * ``POLY_VERTEX = 2``\n        * ``LINE = 3``\n        * ``POLY_LINE = 4``\n        * ``TRIANGLE = 5``\n        * ``TRIANGLE_STRIP = 6``\n        * ``POLYGON = 7``\n        * ``PIXEL = 8``\n        * ``QUAD = 9``\n        * ``TETRA = 10``\n        * ``VOXEL = 11``\n        * ``HEXAHEDRON = 12``\n        * ``WEDGE = 13``\n        * ``PYRAMID = 14``\n        * ``PENTAGONAL_PRISM = 15``\n        * ``HEXAGONAL_PRISM = 16``\n        * ``QUADRATIC_EDGE = 21``\n        * ``QUADRATIC_TRIANGLE = 22``\n        * ``QUADRATIC_QUAD = 23``\n        * ``QUADRATIC_POLYGON = 36``\n        * ``QUADRATIC_TETRA = 24``\n        * ``QUADRATIC_HEXAHEDRON = 25``\n        * ``QUADRATIC_WEDGE = 26``\n        * ``QUADRATIC_PYRAMID = 27``\n        * ``BIQUADRATIC_QUAD = 28``\n        * ``TRIQUADRATIC_HEXAHEDRON = 29``\n        * ``QUADRATIC_LINEAR_QUAD = 30``\n        * ``QUADRATIC_LINEAR_WEDGE = 31``\n        * ``BIQUADRATIC_QUADRATIC_WEDGE = 32``\n        * ``BIQUADRATIC_QUADRATIC_HEXAHEDRON = 33``\n        * ``BIQUADRATIC_TRIANGLE = 34``\n        See `vtkCellType.h\n        <https://vtk.org/doc/nightly/html/vtkCellType_8h_source.html>`_ for all\n        cell types.\n        Examples\n        --------\n        This mesh contains only linear hexahedral cells, type\n        ``CellType.HEXAHEDRON``, which evaluates to 12.\n        >>> import pyvista\n        >>> from pyvista import examples\n        >>> hex_beam = pyvista.read(examples.hexbeamfile)\n        >>> hex_beam.celltypes  # doctest:+SKIP\n        array([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n               12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n               12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12],\n               dtype=uint8)\n        \"\"\"\n        return _vtk.vtk_to_numpy(self.GetCellTypesArray())\n    @property\n    def offset(self) -> np.ndarray:\n        \"\"\"Return the cell locations array.\n        This is the location of the start of each cell in\n        :attr:`cell_connectivity`.\n        Returns\n        -------\n        numpy.ndarray\n            Array of cell offsets indicating the start of each cell.\n        Examples\n        --------\n        Return the cell offset array within ``vtk==9``.  Since this\n        mesh is composed of all hexahedral cells, note how each cell\n        starts at 8 greater than the prior cell.\n        >>> import pyvista\n        >>> from pyvista import examples\n        >>> hex_beam = pyvista.read(examples.hexbeamfile)\n        >>> hex_beam.offset\n        array([  0,   8,  16,  24,  32,  40,  48,  56,  64,  72,  80,  88,  96,\n               104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200,\n               208, 216, 224, 232, 240, 248, 256, 264, 272, 280, 288, 296, 304,\n               312, 320])\n        \"\"\"\n        carr = self.GetCells()\n        # This will be the number of cells + 1.\n        return _vtk.vtk_to_numpy(carr.GetOffsetsArray())\n    def cast_to_explicit_structured_grid(self):\n        \"\"\"Cast to an explicit structured grid.\n        Returns\n        -------\n        pyvista.ExplicitStructuredGrid\n            An explicit structured grid.\n        Raises\n        ------\n        TypeError\n            If the unstructured grid doesn't have the ``'BLOCK_I'``,\n            ``'BLOCK_J'`` and ``'BLOCK_K'`` cells arrays.\n        See Also\n        --------\n        pyvista.ExplicitStructuredGrid.cast_to_unstructured_grid\n        Examples\n        --------\n        >>> from pyvista import examples\n        >>> grid = examples.load_explicit_structured()\n        >>> grid.plot(color='w', show_edges=True, show_bounds=True)\n        >>> grid = grid.hide_cells(range(80, 120))\n        >>> grid.plot(color='w', show_edges=True, show_bounds=True)\n        >>> grid = grid.cast_to_unstructured_grid()\n        >>> grid.plot(color='w', show_edges=True, show_bounds=True)\n        >>> grid = grid.cast_to_explicit_structured_grid()\n        >>> grid.plot(color='w', show_edges=True, show_bounds=True)\n        \"\"\"\n        s1 = {'BLOCK_I', 'BLOCK_J', 'BLOCK_K'}\n        s2 = self.cell_data.keys()\n        if not s1.issubset(s2):\n            raise TypeError(\"'BLOCK_I', 'BLOCK_J' and 'BLOCK_K' cell arrays are required\")\n        alg = _vtk.vtkUnstructuredGridToExplicitStructuredGrid()\n        alg.SetInputData(self)\n        alg.SetInputArrayToProcess(0, 0, 0, 1, 'BLOCK_I')\n        alg.SetInputArrayToProcess(1, 0, 0, 1, 'BLOCK_J')\n        alg.SetInputArrayToProcess(2, 0, 0, 1, 'BLOCK_K')\n        alg.Update()\n        grid = _get_output(alg)\n        grid.cell_data.remove('ConnectivityFlags')  # unrequired\n        return grid\nclass StructuredGrid(_vtk.vtkStructuredGrid, PointGrid, StructuredGridFilters):\n    \"\"\"Dataset used for topologically regular arrays of data.\n    Can be initialized in one of the following several ways:\n    * Create empty grid.\n    * Initialize from a filename.\n    * Initialize from a ``vtk.vtkStructuredGrid`` object.\n    * Initialize directly from one or more :class:`numpy.ndarray`. See the\n      example or the documentation of ``uinput``.\n    Parameters\n    ----------\n    uinput : str, pathlib.Path, vtk.vtkStructuredGrid, numpy.ndarray, optional\n        Filename, dataset, or array to initialize the structured grid from. If\n        a filename is passed, pyvista will attempt to load it as a\n        :class:`StructuredGrid`. If passed a ``vtk.vtkStructuredGrid``, it will\n        be wrapped as a deep copy.\n        If a :class:`numpy.ndarray` is provided and ``y`` and ``z`` are empty,\n        this array will define the points of this :class:`StructuredGrid`.\n        Set the dimensions with :attr:`StructuredGrid.dimensions`.\n        Otherwise, this parameter will be loaded as the ``x`` points, and ``y``\n        and ``z`` points must be set. The shape of this array defines the shape\n        of the structured data and the shape should be ``(dimx, dimy,\n        dimz)``. Missing trailing dimensions are assumed to be ``1``.\n    y : numpy.ndarray, optional\n        Coordinates of the points in y direction. If this is passed, ``uinput``\n        must be a :class:`numpy.ndarray` and match the shape of ``y``.\n    z : numpy.ndarray, optional\n        Coordinates of the points in z direction. If this is passed, ``uinput``\n        and ``y`` must be a :class:`numpy.ndarray` and match the shape of ``z``.\n    deep : optional\n        Whether to deep copy a StructuredGrid object.\n        Default is ``False``.  Keyword only.\n    **kwargs : dict, optional\n        Additional keyword arguments passed when reading from a file or loading\n        from arrays.\n    Examples\n    --------\n    >>> import pyvista\n    >>> import vtk\n    >>> import numpy as np\n    Create an empty structured grid.\n    >>> grid = pyvista.StructuredGrid()\n    Initialize from a ``vtk.vtkStructuredGrid`` object\n    >>> vtkgrid = vtk.vtkStructuredGrid()\n    >>> grid = pyvista.StructuredGrid(vtkgrid)\n    Create from NumPy arrays.\n    >>> xrng = np.arange(-10, 10, 2, dtype=np.float32)\n    >>> yrng = np.arange(-10, 10, 2, dtype=np.float32)\n    >>> zrng = np.arange(-10, 10, 2, dtype=np.float32)\n    >>> x, y, z = np.meshgrid(xrng, yrng, zrng)\n    >>> grid = pyvista.StructuredGrid(x, y, z)\n    >>> grid  # doctest:+SKIP\n    StructuredGrid (0x7fb18f2a8580)\n    N Cells:    729\n    N Points:   1000\n    X Bounds:   -1.000e+01, 8.000e+00\n    Y Bounds:   -1.000e+01, 8.000e+00\n    Z Bounds:   -1.000e+01, 8.000e+00\n    Dimensions: 10, 10, 10\n    N Arrays:   0\n    \"\"\"\n    _WRITERS = {'.vtk': _vtk.vtkStructuredGridWriter, '.vts': _vtk.vtkXMLStructuredGridWriter}\n    def __init__(self, uinput=None, y=None, z=None, *args, deep=False, **kwargs) -> None:\n        \"\"\"Initialize the structured grid.\"\"\"\n        super().__init__()\n        if args:\n            raise ValueError(\"Too many args to create StructuredGrid.\")\n        if isinstance(uinput, _vtk.vtkStructuredGrid):\n            if deep:\n                self.deep_copy(uinput)\n            else:\n                self.shallow_copy(uinput)\n        elif isinstance(uinput, (str, pathlib.Path)):\n            self._from_file(uinput, **kwargs)\n        elif (\n            isinstance(uinput, np.ndarray)\n            and isinstance(y, np.ndarray)\n            and isinstance(z, np.ndarray)\n        ):\n            self._from_arrays(uinput, y, z, **kwargs)\n        elif isinstance(uinput, np.ndarray) and y is None and z is None:\n            self.points = uinput  # type: ignore\n        elif uinput is None:\n            # do nothing, initialize as empty structured grid\n            pass\n        else:\n            raise TypeError(\n                \"Invalid parameters. Expecting one of the following:\\n\"\n                \" - No arguments\\n\"\n                \" - Filename as the only argument\\n\"\n                \" - StructuredGrid as the only argument\\n\"\n                \" - Single `numpy.ndarray` as the only argument\"\n                \" - Three `numpy.ndarray` as the first three arguments\"\n            )\n    def __repr__(self):\n        \"\"\"Return the standard representation.\"\"\"\n        return DataSet.__repr__(self)\n    def __str__(self):\n        \"\"\"Return the standard str representation.\"\"\"\n        return DataSet.__str__(self)\n    def _from_arrays(self, x, y, z, force_float=True):\n        \"\"\"Create VTK structured grid directly from numpy arrays.\n        Parameters\n        ----------\n        x : numpy.ndarray\n            Position of the points in x direction.\n        y : numpy.ndarray\n            Position of the points in y direction.\n        z : numpy.ndarray\n            Position of the points in z direction.\n        force_float : bool, optional\n            Casts the datatype to ``float32`` if points datatype is\n            non-float.  Default ``True``. Set this to ``False`` to allow\n            non-float types, though this may lead to truncation of\n            intermediate floats when transforming datasets.\n        \"\"\"\n        if not (x.shape == y.shape == z.shape):\n            raise ValueError('Input point array shapes must match exactly')\n        # make the output points the same precision as the input arrays\n        points = np.empty((x.size, 3), x.dtype)\n        points[:, 0] = x.ravel('F')\n        points[:, 1] = y.ravel('F')\n        points[:, 2] = z.ravel('F')\n        # ensure that the inputs are 3D\n        dim = list(x.shape)\n        while len(dim) < 3:\n            dim.append(1)\n        # Create structured grid\n        self.SetDimensions(dim)\n        self.SetPoints(pyvista.vtk_points(points, force_float=force_float))\n    @property\n    def dimensions(self):\n        \"\"\"Return a length 3 tuple of the grid's dimensions.\n        Returns\n        -------\n        tuple\n            Grid dimensions.\n        Examples\n        --------\n        >>> import pyvista\n        >>> import numpy as np\n        >>> xrng = np.arange(-10, 10, 1, dtype=np.float32)\n        >>> yrng = np.arange(-10, 10, 2, dtype=np.float32)\n        >>> zrng = np.arange(-10, 10, 5, dtype=np.float32)\n        >>> x, y, z = np.meshgrid(xrng, yrng, zrng)\n        >>> grid = pyvista.StructuredGrid(x, y, z)\n        >>> grid.dimensions\n        (10, 20, 4)\n        \"\"\"\n        return tuple(self.GetDimensions())\n    @dimensions.setter\n    def dimensions(self, dims):\n        nx, ny, nz = dims[0], dims[1], dims[2]\n        self.SetDimensions(nx, ny, nz)\n        self.Modified()\n    @property\n    def x(self):\n        \"\"\"Return the X coordinates of all points.\n        Returns\n        -------\n        numpy.ndarray\n            Numpy array of all X coordinates.\n        Examples\n        --------\n        >>> import pyvista\n        >>> import numpy as np\n        >>> xrng = np.arange(-10, 10, 1, dtype=np.float32)\n        >>> yrng = np.arange(-10, 10, 2, dtype=np.float32)\n        >>> zrng = np.arange(-10, 10, 5, dtype=np.float32)\n        >>> x, y, z = np.meshgrid(xrng, yrng, zrng)\n        >>> grid = pyvista.StructuredGrid(x, y, z)\n        >>> grid.x.shape\n        (10, 20, 4)\n        \"\"\"\n        return self._reshape_point_array(self.points[:, 0])\n    @property\n    def y(self):\n        \"\"\"Return the Y coordinates of all points.\"\"\"\n        return self._reshape_point_array(self.points[:, 1])\n    @property\n    def z(self):\n        \"\"\"Return the Z coordinates of all points.\"\"\"\n        return self._reshape_point_array(self.points[:, 2])\n    @property\n    def points_matrix(self):\n        \"\"\"Points as a 4-D matrix, with x/y/z along the last dimension.\"\"\"\n        return self.points.reshape((*self.dimensions, 3), order='F')\n    def _get_attrs(self):\n        \"\"\"Return the representation methods (internal helper).\"\"\"\n        attrs = PointGrid._get_attrs(self)\n        attrs.append((\"Dimensions\", self.dimensions, \"{:d}, {:d}, {:d}\"))\n        return attrs\n    def __getitem__(self, key):\n        \"\"\"Slice subsets of the StructuredGrid, or extract an array field.\"\"\"",
        "file_path": "pyvista/core/pointset.py",
        "chunk_index": 3,
        "metadata": {}
      },
      {
        "rank": 4,
        "score": 0.6555612087249756,
        "content": "\"\"\"Supporting functions for polydata and grid objects.\"\"\"\nimport collections.abc\nimport enum\nimport logging\nimport os\nimport signal\nimport sys\nimport threading\nfrom threading import Thread\nimport traceback\nfrom typing import Optional\nimport warnings\nimport numpy as np\nimport pyvista\nfrom pyvista import _vtk\nfrom pyvista.errors import AmbiguousDataError, MissingDataError\nfrom . import transformations\nfrom .fileio import from_meshio\nclass FieldAssociation(enum.Enum):\n    \"\"\"Represents which type of vtk field a scalar or vector array is associated with.\"\"\"\n    POINT = _vtk.vtkDataObject.FIELD_ASSOCIATION_POINTS\n    CELL = _vtk.vtkDataObject.FIELD_ASSOCIATION_CELLS\n    NONE = _vtk.vtkDataObject.FIELD_ASSOCIATION_NONE\n    ROW = _vtk.vtkDataObject.FIELD_ASSOCIATION_ROWS\ndef get_vtk_type(typ):\n    \"\"\"Look up the VTK type for a given numpy data type.\n    Corrects for string type mapping issues.\n    Parameters\n    ----------\n    typ : numpy.dtype\n        Numpy data type.\n    Returns\n    -------\n    int\n        Integer type id specified in ``vtkType.h``\n    \"\"\"\n    typ = _vtk.get_vtk_array_type(typ)\n    # This handles a silly string type bug\n    if typ == 3:\n        return 13\n    return typ\ndef vtk_bit_array_to_char(vtkarr_bint):\n    \"\"\"Cast vtk bit array to a char array.\n    Parameters\n    ----------\n    vtkarr_bint : vtk.vtkBitArray\n        VTK binary array.\n    Returns\n    -------\n    vtk.vtkCharArray\n        VTK char array.\n    Notes\n    -----\n    This performs a copy.\n    \"\"\"\n    vtkarr = _vtk.vtkCharArray()\n    vtkarr.DeepCopy(vtkarr_bint)\n    return vtkarr\ndef vtk_id_list_to_array(vtk_id_list):\n    \"\"\"Convert a vtkIdList to a NumPy array.\n    Parameters\n    ----------\n    vtk_id_list : vtk.vtkIdList\n        VTK ID list.\n    Returns\n    -------\n    numpy.ndarray\n        Array of IDs.\n    \"\"\"\n    return np.array([vtk_id_list.GetId(i) for i in range(vtk_id_list.GetNumberOfIds())])\ndef convert_string_array(arr, name=None):\n    \"\"\"Convert a numpy array of strings to a vtkStringArray or vice versa.\n    Parameters\n    ----------\n    arr : numpy.ndarray\n        Numpy string array to convert.\n    name : str, optional\n        Name to set the vtkStringArray to.\n    Returns\n    -------\n    vtkStringArray\n        VTK string array.\n    Notes\n    -----\n    Note that this is terribly inefficient. If you have ideas on how\n    to make this faster, please consider opening a pull request.\n    \"\"\"\n    if isinstance(arr, np.ndarray):\n        # VTK default fonts only support ASCII. See https://gitlab.kitware.com/vtk/vtk/-/issues/16904\n        if np.issubdtype(arr.dtype, np.str_) and not ''.join(arr).isascii():  # avoids segfault\n            raise ValueError(\n                'String array contains non-ASCII characters that are not supported by VTK.'\n            )\n        vtkarr = _vtk.vtkStringArray()\n        ########### OPTIMIZE ###########\n        for val in arr:\n            vtkarr.InsertNextValue(val)\n        ################################\n        if isinstance(name, str):\n            vtkarr.SetName(name)\n        return vtkarr\n    # Otherwise it is a vtk array and needs to be converted back to numpy\n    ############### OPTIMIZE ###############\n    nvalues = arr.GetNumberOfValues()\n    return np.array([arr.GetValue(i) for i in range(nvalues)], dtype='|U')\n    ########################################\ndef convert_array(arr, name=None, deep=False, array_type=None):\n    \"\"\"Convert a NumPy array to a vtkDataArray or vice versa.\n    Parameters\n    ----------\n    arr : np.ndarray | vtkDataArray\n        A numpy array or vtkDataArry to convert.\n    name : str, optional\n        The name of the data array for VTK.\n    deep : bool, default: False\n        If input is numpy array then deep copy values.\n    array_type : int, optional\n        VTK array type ID as specified in specified in ``vtkType.h``.\n    Returns\n    -------\n    vtkDataArray or numpy.ndarray\n        The converted array.  If input is a :class:`numpy.ndarray` then\n        returns ``vtkDataArray`` or is input is ``vtkDataArray`` then\n        returns NumPy ``ndarray``.\n    \"\"\"\n    if arr is None:\n        return\n    if isinstance(arr, (list, tuple)):\n        arr = np.array(arr)\n    if isinstance(arr, np.ndarray):\n        if arr.dtype == np.dtype('O'):\n            arr = arr.astype('|S')\n        arr = np.ascontiguousarray(arr)\n        if arr.dtype.type in (np.str_, np.bytes_):\n            # This handles strings\n            vtk_data = convert_string_array(arr)\n        else:\n            # This will handle numerical data\n            arr = np.ascontiguousarray(arr)\n            vtk_data = _vtk.numpy_to_vtk(num_array=arr, deep=deep, array_type=array_type)\n        if isinstance(name, str):\n            vtk_data.SetName(name)\n        return vtk_data\n    # Otherwise input must be a vtkDataArray\n    if not isinstance(arr, (_vtk.vtkDataArray, _vtk.vtkBitArray, _vtk.vtkStringArray)):\n        raise TypeError(f'Invalid input array type ({type(arr)}).')\n    # Handle booleans\n    if isinstance(arr, _vtk.vtkBitArray):\n        arr = vtk_bit_array_to_char(arr)\n    # Handle string arrays\n    if isinstance(arr, _vtk.vtkStringArray):\n        return convert_string_array(arr)\n    # Convert from vtkDataArry to NumPy\n    return _vtk.vtk_to_numpy(arr)\ndef is_pyvista_dataset(obj):\n    \"\"\"Return ``True`` if the object is a PyVista wrapped dataset.\n    Parameters\n    ----------\n    obj : Any\n        Any object to test.\n    Returns\n    -------\n    bool\n        ``True`` when the object is a :class:`pyvista.DataSet`.\n    \"\"\"\n    return isinstance(obj, (pyvista.DataSet, pyvista.MultiBlock))\ndef _assoc_array(obj, name, association='point'):\n    \"\"\"Return a point, cell, or field array from a pyvista.DataSet or VTK object.\n    If the array or index doesn't exist, return nothing. This matches VTK's\n    behavior when using ``GetAbstractArray`` with an invalid key or index.\n    \"\"\"\n    vtk_attr = f'Get{association.title()}Data'\n    python_attr = f'{association.lower()}_data'\n    if isinstance(obj, pyvista.DataSet):\n        try:\n            return getattr(obj, python_attr).get_array(name)\n        except KeyError:  # pragma: no cover\n            return None\n    abstract_array = getattr(obj, vtk_attr)().GetAbstractArray(name)\n    if abstract_array is not None:\n        return pyvista.pyvista_ndarray(abstract_array)\n    return None\ndef point_array(obj, name):\n    \"\"\"Return point array of a pyvista or vtk object.\n    Parameters\n    ----------\n    obj : pyvista.DataSet | vtk.vtkDataSet\n        PyVista or VTK dataset.\n    name : str | int\n        Name or index of the array.\n    Returns\n    -------\n    pyvista.pyvista_ndarray or None\n        Wrapped array if the index or name is valid. Otherwise, ``None``.\n    \"\"\"\n    return _assoc_array(obj, name, 'point')\ndef field_array(obj, name):\n    \"\"\"Return field data of a pyvista or vtk object.\n    Parameters\n    ----------\n    obj : pyvista.DataSet or vtk.vtkDataSet\n        PyVista or VTK dataset.\n    name : str | int\n        Name or index of the array.\n    Returns\n    -------\n    pyvista.pyvista_ndarray or None\n        Wrapped array if the index or name is valid. Otherwise, ``None``.\n    \"\"\"\n    return _assoc_array(obj, name, 'field')\ndef cell_array(obj, name):\n    \"\"\"Return cell array of a pyvista or vtk object.\n    Parameters\n    ----------\n    obj : pyvista.DataSet or vtk.vtkDataSet\n        PyVista or VTK dataset.\n    name : str | int\n        Name or index of the array.\n    Returns\n    -------\n    pyvista.pyvista_ndarray or None\n        Wrapped array if the index or name is valid. Otherwise, ``None``.\n    \"\"\"\n    return _assoc_array(obj, name, 'cell')\ndef row_array(obj, name):\n    \"\"\"Return row array of a vtk object.\n    Parameters\n    ----------\n    obj : vtk.vtkDataSet\n        PyVista or VTK dataset.\n    name : str\n        Name of the array.\n    Returns\n    -------\n    numpy.ndarray\n        Wrapped array.\n    \"\"\"\n    vtkarr = obj.GetRowData().GetAbstractArray(name)\n    return convert_array(vtkarr)\ndef parse_field_choice(field):\n    \"\"\"Return a field association object for a given field type string.\n    Parameters\n    ----------\n    field : str, FieldAssociation\n        Name of the field (e.g, ``'cell'``, ``'field'``, ``'point'``,\n        ``'row'``).\n    Returns\n    -------\n    pyvista.FieldAssociation\n        Field association.\n    \"\"\"\n    if isinstance(field, str):\n        field = field.strip().lower()\n        if field in ['cell', 'c', 'cells']:\n            field = FieldAssociation.CELL\n        elif field in ['point', 'p', 'points']:\n            field = FieldAssociation.POINT\n        elif field in ['field', 'f', 'fields']:\n            field = FieldAssociation.NONE\n        elif field in ['row', 'r']:\n            field = FieldAssociation.ROW\n        else:\n            raise ValueError(f'Data field ({field}) not supported.')\n    elif isinstance(field, FieldAssociation):\n        pass\n    else:\n        raise TypeError(f'Data field ({field}) not supported.')\n    return field\ndef get_array(mesh, name, preference='cell', err=False) -> Optional[np.ndarray]:\n    \"\"\"Search point, cell and field data for an array.\n    Parameters\n    ----------\n    mesh : pyvista.DataSet\n        Dataset to get the array from.\n    name : str\n        The name of the array to get the range.\n    preference : str, default: \"cell\"\n        When scalars is specified, this is the preferred array type to\n        search for in the dataset.  Must be either ``'point'``,\n        ``'cell'``, or ``'field'``.\n    err : bool, default: False\n        Whether to throw an error if array is not present.\n    Returns\n    -------\n    pyvista.pyvista_ndarray or ``None``\n        Requested array.  Return ``None`` if there is no array\n        matching the ``name`` and ``err=False``.\n    \"\"\"\n    if isinstance(mesh, _vtk.vtkTable):\n        arr = row_array(mesh, name)\n        if arr is None and err:\n            raise KeyError(f'Data array ({name}) not present in this dataset.')\n        return arr\n    if not isinstance(preference, str):\n        raise TypeError('`preference` must be a string')\n    if preference not in ['cell', 'point', 'field']:\n        raise ValueError(\n            f'`preference` must be either \"cell\", \"point\", \"field\" for a '\n            f'{type(mesh)}, not \"{preference}\".'\n        )\n    parr = point_array(mesh, name)\n    carr = cell_array(mesh, name)\n    farr = field_array(mesh, name)\n    preference = parse_field_choice(preference)\n    if sum([array is not None for array in (parr, carr, farr)]) > 1:\n        if preference == FieldAssociation.CELL:\n            return carr\n        elif preference == FieldAssociation.POINT:\n            return parr\n        else:  # must be field\n            return farr\n    if parr is not None:\n        return parr\n    elif carr is not None:\n        return carr\n    elif farr is not None:\n        return farr\n    elif err:\n        raise KeyError(f'Data array ({name}) not present in this dataset.')\n    return None\ndef get_array_association(mesh, name, preference='cell', err=False) -> FieldAssociation:\n    \"\"\"Return the array association.\n    Parameters\n    ----------\n    mesh : Dataset\n        Dataset to get the array association from.\n    name : str\n        The name of the array.\n    preference : str, default: \"cell\"\n        When scalars is specified, this is the preferred array type to\n        search for in the dataset.  Must be either ``'point'``,\n        ``'cell'``, or ``'field'``.\n    err : bool, default: False\n        Boolean to control whether to throw an error if array is not\n        present.\n    Returns\n    -------\n    pyvista.utilities.helpers.FieldAssociation\n        Association of the array. If array is not present and ``err`` is\n        ``False``, ``FieldAssociation.NONE`` is returned.\n    \"\"\"\n    if isinstance(mesh, _vtk.vtkTable):\n        arr = row_array(mesh, name)\n        if arr is None and err:\n            raise KeyError(f'Data array ({name}) not present in this dataset.')\n        return FieldAssociation.ROW\n    # with multiple arrays, return the array preference if possible\n    parr = point_array(mesh, name)\n    carr = cell_array(mesh, name)\n    farr = field_array(mesh, name)\n    arrays = [parr, carr, farr]\n    preferences = [FieldAssociation.POINT, FieldAssociation.CELL, FieldAssociation.NONE]\n    preference = parse_field_choice(preference)\n    if preference not in preferences:\n        raise ValueError(f'Data field ({preference}) not supported.')\n    matches = [pref for pref, array in zip(preferences, arrays) if array is not None]\n    # optionally raise if no match\n    if not matches:\n        if err:\n            raise KeyError(f'Data array ({name}) not present in this dataset.')\n        return FieldAssociation.NONE\n    # use preference if it applies\n    if preference in matches:\n        return preference\n    # otherwise return first in order of point -> cell -> field\n    return matches[0]\ndef vtk_points(points, deep=True, force_float=False):\n    \"\"\"Convert numpy array or array-like to a ``vtkPoints`` object.\n    Parameters\n    ----------\n    points : numpy.ndarray or sequence\n        Points to convert.  Should be 1 or 2 dimensional.  Accepts a\n        single point or several points.\n    deep : bool, default: True\n        Perform a deep copy of the array.  Only applicable if\n        ``points`` is a :class:`numpy.ndarray`.\n    force_float : bool, default: False\n        Casts the datatype to ``float32`` if points datatype is\n        non-float.  Set this to ``False`` to allow non-float types,\n        though this may lead to truncation of intermediate floats\n        when transforming datasets.\n    Returns\n    -------\n    vtk.vtkPoints\n        The vtkPoints object.\n    Examples\n    --------\n    >>> import pyvista\n    >>> import numpy as np\n    >>> points = np.random.random((10, 3))\n    >>> vpoints = pyvista.vtk_points(points)\n    >>> vpoints  # doctest:+SKIP\n    (vtkmodules.vtkCommonCore.vtkPoints)0x7f0c2e26af40\n    \"\"\"\n    points = np.asanyarray(points)\n    # verify is numeric\n    if not np.issubdtype(points.dtype, np.number):\n        raise TypeError('Points must be a numeric type')\n    if force_float:\n        if not np.issubdtype(points.dtype, np.floating):\n            warnings.warn(\n                'Points is not a float type. This can cause issues when '\n                'transforming or applying filters. Casting to '\n                '``np.float32``. Disable this by passing '\n                '``force_float=False``.'\n            )\n            points = points.astype(np.float32)\n    # check dimensionality\n    if points.ndim == 1:\n        points = points.reshape(-1, 3)\n    elif points.ndim > 2:\n        raise ValueError(f'Dimension of ``points`` should be 1 or 2, not {points.ndim}')\n    # verify shape\n    if points.shape[1] != 3:\n        raise ValueError(\n            'Points array must contain three values per point. '\n            f'Shape is {points.shape} and should be (X, 3)'\n        )\n    # use the underlying vtk data if present to avoid memory leaks\n    if not deep and isinstance(points, pyvista.pyvista_ndarray):\n        if points.VTKObject is not None:\n            vtk_object = points.VTKObject\n            # we can only use the underlying data if `points` is not a slice of\n            # the VTK data object\n            if vtk_object.GetSize() == points.size:\n                vtkpts = _vtk.vtkPoints()\n                vtkpts.SetData(points.VTKObject)\n                return vtkpts\n            else:\n                deep = True\n    # points must be contiguous\n    points = np.require(points, requirements=['C'])\n    vtkpts = _vtk.vtkPoints()\n    vtk_arr = _vtk.numpy_to_vtk(points, deep=deep)\n    vtkpts.SetData(vtk_arr)\n    return vtkpts\ndef line_segments_from_points(points):\n    \"\"\"Generate non-connected line segments from points.\n    Assumes points are ordered as line segments and an even number of\n    points.\n    Parameters\n    ----------\n    points : array_like[float]\n        Points representing line segments. An even number must be\n        given as every two vertices represent a single line\n        segment. For example, two line segments would be represented\n        as ``np.array([[0, 0, 0], [1, 0, 0], [1, 0, 0], [1, 1, 0]])``.\n    Returns\n    -------\n    pyvista.PolyData\n        PolyData with lines and cells.\n    Examples\n    --------\n    This example plots two line segments at right angles to each other.\n    >>> import pyvista\n    >>> import numpy as np\n    >>> points = np.array([[0, 0, 0], [1, 0, 0], [1, 0, 0], [1, 1, 0]])\n    >>> lines = pyvista.line_segments_from_points(points)\n    >>> lines.plot()\n    \"\"\"\n    if len(points) % 2 != 0:\n        raise ValueError(\"An even number of points must be given to define each segment.\")\n    # Assuming ordered points, create array defining line order\n    n_points = len(points)\n    n_lines = n_points // 2\n    lines = np.c_[\n        (\n            2 * np.ones(n_lines, np.int_),\n            np.arange(0, n_points - 1, step=2),\n            np.arange(1, n_points + 1, step=2),\n        )\n    ]\n    poly = pyvista.PolyData()\n    poly.points = points\n    poly.lines = lines\n    return poly\ndef lines_from_points(points, close=False):\n    \"\"\"Make a connected line set given an array of points.\n    Parameters\n    ----------\n    points : array_like[float]\n        Points representing the vertices of the connected\n        segments. For example, two line segments would be represented\n        as ``np.array([[0, 0, 0], [1, 0, 0], [1, 1, 0]])``.\n    close : bool, default: False\n        If ``True``, close the line segments into a loop.\n    Returns\n    -------\n    pyvista.PolyData\n        PolyData with lines and cells.\n    Examples\n    --------\n    >>> import numpy as np\n    >>> import pyvista\n    >>> points = np.array([[0, 0, 0], [1, 0, 0], [1, 1, 0]])\n    >>> poly = pyvista.lines_from_points(points)\n    >>> poly.plot(line_width=5)\n    \"\"\"\n    poly = pyvista.PolyData()\n    poly.points = points",
        "file_path": "pyvista/utilities/helpers.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 5,
        "score": 0.6439340114593506,
        "content": "\"\"\"\n.. _linear_cells_example:\nLinear Cells\n~~~~~~~~~~~~\nThis example extends the :ref:`create_unstructured_example` example by\nincluding an explanation of linear VTK cell types and how you can create them in\nPyVista.\nLinear cells are cells where points only occur at the edges of each\ncell. Non-linear cells contain additional points along the edges of the cell.\nFor more details regarding what a :class:`pyvista.UnstructuredGrid` is, please\nsee :ref:`point_sets_api`.\n\"\"\"\nimport numpy as np\nimport pyvista as pv\nfrom pyvista.examples import cells as example_cells, plot_cell\n# random generator for examples\nrng = np.random.default_rng(2)\n###############################################################################\n# Plot an example cell\n# ~~~~~~~~~~~~~~~~~~~~\n# PyVista contains a simple utility to plot a single cell, which is the\n# fundamental unit of each :class:`pyvista.UnstructuredGrid`. For example,\n# let's plot a simple :func:`Wedge <pyvista.examples.cells.Wedge>`.\n#\ngrid = example_cells.Wedge()\nexample_cells.plot_cell(grid)\n###############################################################################\n# This linear cell is composed of 6 points.\ngrid.points\n###############################################################################\n# The UnstructuredGrid is also composed of a single cell and the point indices\n# of that cell are defined in :attr:`cells <pyvista.UnstructuredGrid.cells>`.\n#\n# .. note::\n#    The leading ``6`` is the number of points in the cell.\ngrid.cells\n###############################################################################\n# Combine two UnstructuredGrids\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# We can combine two unstructured grids to create a single unstructured grid\n# using the ``+`` operator.\n#\n# .. note::\n#    This is an inefficient way of creating :class:`pyvista.UnstructuredGrid`\n#    objects. To see a more efficient implementation see\n#    :ref:`create_unstructured_example`.\ngrid_a = example_cells.Hexahedron()\ngrid_a.points += [0, 2.5, 0]\ngrid_b = example_cells.HexagonalPrism()\ncombined = grid_b + grid_a\nplot_cell(combined, cpos='iso')\n###############################################################################\n# This example helps to illustrate meaning behind the :attr:`cells\n# <pyvista.UnstructuredGrid.cells>` attribute. The first cell, a hexahedron\n# contains 8 points and the hexagonal prism contains 12 points. The ``cells``\n# attribute shows this along with indices composing each cell.\ncombined.cells\n###############################################################################\n# Cell Types\n# ~~~~~~~~~~\n# PyVista contains the :class:`pyvista.CellType` enumerator, which contains all the\n# available VTK cell types mapped to a Python enumerator. These cell types are\n# used when creating cells and also can be used when checking the\n# :attr:`celltypes <pyvista.UnstructuredGrid.celltypes>` attribute. For example\n# ``combined.celltypes`` contains both the ``pv.CellType.HEXAHEDRON`` and\n# ``pv.CellType.HEXAGONAL_PRISM`` cell types.\nprint(pv.CellType.HEXAHEDRON, pv.CellType.HEXAGONAL_PRISM)\n(pv.CellType.HEXAHEDRON, pv.CellType.HEXAGONAL_PRISM) == combined.celltypes\n###############################################################################\n# Create an UnstructuredGrid with a single linear cell\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Now that you know the three main inputs of an\n# :class:`pyvista.UnstructuredGrid`, it's quite straightforward to create an\n# unstructured grid with a one or more cells. If you need to reference point\n# ordering or additional, you can either read the source of `cells.py\n# <https://github.com/pyvista/pyvista/blob/main/pyvista/examples/cells.py>`_ or\n# simply create a cell from the ``pyvista.cells`` module and inspect its attributes.\npoints = [\n    [1.0, 1.0, 0.0],\n    [-1.0, 1.0, 0.0],\n    [-1.0, -1.0, 0.0],\n    [1.0, -1.0, 0.0],\n    [0.0, 0.0, 1.60803807],\n]\ncells = [len(points)] + list(range(len(points)))\npyrmaid = pv.UnstructuredGrid(cells, [pv.CellType.PYRAMID], points)\nexample_cells.plot_cell(pyrmaid)\n###############################################################################\n# Plot all linear cell Types\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Let's create a ``(4, 4)`` :class:`pyvista.Plotter` and plot all 16 linear\n# cells in a single plot.\ndef add_cell_helper(pl, text, grid, subplot, cpos=None):\n    \"\"\"Add a single cell to a plotter with fancy plotting.\"\"\"\n    pl.subplot(*subplot)\n    pl.add_text(text, 'lower_edge', color='k', font_size=8)\n    pl.add_mesh(grid, opacity=0.5, color='tan', line_width=5)\n    edges = grid.extract_all_edges()\n    if edges.n_cells:\n        pl.add_mesh(grid.extract_all_edges(), line_width=5, color='k')\n    pl.add_points(grid, render_points_as_spheres=True, point_size=20, color='r')\n    pl.add_point_labels(\n        grid.points,\n        range(grid.n_points),\n        always_visible=True,\n        fill_shape=False,\n        margin=0,\n        shape_opacity=0.0,\n        font_size=20,\n        text_color='k',\n    )\n    if cpos is None:\n        pl.camera.azimuth = 20\n        pl.camera.elevation = -20\n    else:\n        pl.camera_position = cpos\n    pl.camera.zoom(0.8)\npl = pv.Plotter(shape=(4, 4))\nadd_cell_helper(pl, f'VERTEX ({pv.CellType.VERTEX})', example_cells.Vertex(), (0, 0))\nadd_cell_helper(pl, f'POLY_VERTEX ({pv.CellType.POLY_VERTEX})', example_cells.PolyVertex(), (0, 1))\nadd_cell_helper(pl, f'LINE ({pv.CellType.LINE})', example_cells.Line(), (0, 2))\nadd_cell_helper(pl, f'POLY_LINE ({pv.CellType.POLY_LINE})', example_cells.PolyLine(), (0, 3))\nadd_cell_helper(\n    pl, f'TRIANGLE ({pv.CellType.TRIANGLE})', example_cells.Triangle(), (1, 0), cpos='xy'\n)\nadd_cell_helper(\n    pl,\n    f'TRIANGLE_STRIP ({pv.CellType.TRIANGLE_STRIP})',\n    example_cells.TriangleStrip().rotate_z(90, inplace=False),\n    (1, 1),\n    cpos='xy',\n)\nadd_cell_helper(pl, f'POLYGON ({pv.CellType.POLYGON})', example_cells.Polygon(), (1, 2), cpos='xy')\nadd_cell_helper(pl, f'PIXEL ({pv.CellType.PIXEL})', example_cells.Pixel(), (1, 3), cpos='xy')\n# make irregular\nquad_grid = example_cells.Quadrilateral()\nquad_grid.points += rng.random((4, 3)) * 0.5\nadd_cell_helper(pl, f'QUAD ({pv.CellType.QUAD})', quad_grid, (2, 0))\nadd_cell_helper(pl, f'TETRA ({pv.CellType.TETRA})', example_cells.Tetrahedron(), (2, 1))\nadd_cell_helper(pl, f'VOXEL ({pv.CellType.VOXEL})', example_cells.Voxel(), (2, 2))\n# make irregular\nhex_grid = example_cells.Hexahedron()\nhex_grid.points += rng.random((8, 3)) * 0.4\nadd_cell_helper(pl, f'HEXAHEDRON ({pv.CellType.HEXAHEDRON})', hex_grid, (2, 3))\nadd_cell_helper(pl, f'WEDGE ({pv.CellType.WEDGE})', example_cells.Wedge(), (3, 0))\nadd_cell_helper(pl, f'PYRAMID ({pv.CellType.PYRAMID})', example_cells.Pyramid(), (3, 1))\nadd_cell_helper(\n    pl,\n    f'PENTAGONAL_PRISM ({pv.CellType.PENTAGONAL_PRISM})',\n    example_cells.PentagonalPrism(),\n    (3, 2),\n)\nadd_cell_helper(\n    pl, f'HEXAGONAL_PRISM ({pv.CellType.HEXAGONAL_PRISM})', example_cells.HexagonalPrism(), (3, 3)\n)\npl.background_color = 'w'\npl.show()",
        "file_path": "examples/00-load/linear-cells.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 6,
        "score": 0.6437546014785767,
        "content": "\"\"\"pyvista wrapping of vtkCellArray.\"\"\"\nfrom collections import deque\nfrom itertools import count, islice\nimport numpy as np\nimport pyvista\nfrom pyvista import _vtk\ndef ncells_from_cells(cells):\n    \"\"\"Get the number of cells from a VTK cell connectivity array.\"\"\"\n    consumer = deque(maxlen=0)\n    it = cells.flat\n    for n_cells in count():  # noqa: B007\n        skip = next(it, None)\n        if skip is None:\n            break\n        consumer.extend(islice(it, skip))\n    return n_cells\ndef numpy_to_idarr(ind, deep=False, return_ind=False):\n    \"\"\"Safely convert a numpy array to a vtkIdTypeArray.\"\"\"\n    ind = np.asarray(ind)\n    # np.asarray will eat anything, so we have to weed out bogus inputs\n    if not issubclass(ind.dtype.type, (np.bool_, np.integer)):\n        raise TypeError('Indices must be either a mask or an integer array-like')\n    if ind.dtype == np.bool_:\n        ind = ind.nonzero()[0].astype(pyvista.ID_TYPE)\n    elif ind.dtype != pyvista.ID_TYPE:\n        ind = ind.astype(pyvista.ID_TYPE)\n    elif not ind.flags['C_CONTIGUOUS']:\n        ind = np.ascontiguousarray(ind, dtype=pyvista.ID_TYPE)\n    # must ravel or segfault when saving MultiBlock\n    vtk_idarr = _vtk.numpy_to_vtkIdTypeArray(ind.ravel(), deep=deep)\n    if return_ind:\n        return vtk_idarr, ind\n    return vtk_idarr\nclass CellArray(_vtk.vtkCellArray):\n    \"\"\"pyvista wrapping of vtkCellArray.\n    Provides convenience functions to simplify creating a CellArray from\n    a numpy array or list.\n    Import an array of data with the legacy vtkCellArray layout, e.g.\n    ``{ n0, p0_0, p0_1, ..., p0_n, n1, p1_0, p1_1, ..., p1_n, ... }``\n    Where n0 is the number of points in cell 0, and pX_Y is the Y'th\n    point in cell X.\n    Examples\n    --------\n    Create a cell array containing two triangles.\n    >>> from pyvista.utilities.cells import CellArray\n    >>> cellarr = CellArray([3, 0, 1, 2, 3, 3, 4, 5])\n    \"\"\"\n    def __init__(self, cells=None, n_cells=None, deep=False):\n        \"\"\"Initialize a vtkCellArray.\"\"\"\n        if cells is not None:\n            self._set_cells(cells, n_cells, deep)\n    def _set_cells(self, cells, n_cells, deep):\n        vtk_idarr, cells = numpy_to_idarr(cells, deep=deep, return_ind=True)\n        # Get number of cells if None.  This is quite a performance\n        # bottleneck and we can consider adding a warning.  Good\n        # candidate for Cython or JIT compilation\n        if n_cells is None:\n            if cells.ndim == 1:\n                n_cells = ncells_from_cells(cells)\n            else:\n                n_cells = cells.shape[0]\n        self.SetCells(n_cells, vtk_idarr)\n    @property\n    def cells(self):\n        \"\"\"Return a numpy array of the cells.\"\"\"\n        return _vtk.vtk_to_numpy(self.GetData()).ravel()\n    @property\n    def n_cells(self):\n        \"\"\"Return the number of cells.\"\"\"\n        return self.GetNumberOfCells()\ndef create_mixed_cells(mixed_cell_dict, nr_points=None):\n    \"\"\"Generate the required cell arrays for the creation of a pyvista.UnstructuredGrid from a cell dictionary.\n    This function generates all required cell arrays according to a given cell\n    dictionary. The given cell-dictionary should contain a proper\n    mapping of vtk_type -> np.ndarray (int), where the given ndarray\n    for each cell-type has to be an array of dimensions [N, D] or\n    [N*D], where N is the number of cells and D is the size of the\n    cells for the given type (e.g. 3 for triangles).  Multiple\n    vtk_type keys with associated arrays can be present in one\n    dictionary.  This function only accepts cell types of fixed size\n    and not dynamic sized cells like ``vtk.VTK_POLYGON``\n    Parameters\n    ----------\n    mixed_cell_dict : dict\n        A dictionary that maps VTK-Enum-types (e.g. VTK_TRIANGLE) to\n        np.ndarrays of type int.  The ``np.ndarrays`` describe the cell connectivity\n    nr_points : int, optional\n        Number of points of the grid. Used only to allow additional runtime checks for\n        invalid indices, by default None\n    Returns\n    -------\n    cell_types : numpy.ndarray (uint8)\n        Types of each cell\n    cell_arr : numpy.ndarray (int)\n        VTK-cell array\n    Raises\n    ------\n    ValueError\n        If any of the cell types are not supported, have dynamic sized\n        cells, map to values with wrong size, or cell indices point\n        outside the given number of points.\n    Examples\n    --------\n    Create the cell arrays containing two triangles.\n    This will generate cell arrays to generate a mesh with two\n    disconnected triangles from 6 points.\n    >>> import numpy as np\n    >>> import vtk\n    >>> from pyvista.utilities.cells import create_mixed_cells\n    >>> cell_types, cell_arr = create_mixed_cells(\n    ...     {vtk.VTK_TRIANGLE: np.array([[0, 1, 2], [3, 4, 5]])}\n    ... )\n    \"\"\"\n    from .cell_type_helper import enum_cell_type_nr_points_map\n    if not np.all([k in enum_cell_type_nr_points_map for k in mixed_cell_dict.keys()]):\n        raise ValueError(\"Found unknown or unsupported VTK cell type in your requested cells\")\n    if not np.all([enum_cell_type_nr_points_map[k] > 0 for k in mixed_cell_dict.keys()]):\n        raise ValueError(\n            \"You requested a cell type with variable length, which can't be used in this method\"\n        )\n    final_cell_types = []\n    final_cell_arr = []\n    for elem_t, cells_arr in mixed_cell_dict.items():\n        nr_points_per_elem = enum_cell_type_nr_points_map[elem_t]\n        if (\n            not isinstance(cells_arr, np.ndarray)\n            or not np.issubdtype(cells_arr.dtype, np.integer)\n            or cells_arr.ndim not in [1, 2]\n            or (cells_arr.ndim == 1 and cells_arr.size % nr_points_per_elem != 0)\n            or (cells_arr.ndim == 2 and cells_arr.shape[-1] != nr_points_per_elem)\n        ):\n            raise ValueError(\n                f\"Expected an np.ndarray of size [N, {nr_points_per_elem}] or [N*{nr_points_per_elem}] with an integral type\"\n            )\n        if np.any(cells_arr < 0):\n            raise ValueError(f\"Non-valid index (<0) given for cells of type {elem_t}\")\n        if nr_points is not None and np.any(cells_arr >= nr_points):\n            raise ValueError(f\"Non-valid index (>={nr_points}) given for cells of type {elem_t}\")\n        if cells_arr.ndim == 1:  # Flattened array present\n            cells_arr = cells_arr.reshape([-1, nr_points_per_elem])\n        nr_elems = cells_arr.shape[0]\n        final_cell_types.append(np.array([elem_t] * nr_elems, dtype=np.uint8))\n        final_cell_arr.append(\n            np.concatenate(\n                [np.ones_like(cells_arr[..., :1]) * nr_points_per_elem, cells_arr], axis=-1\n            ).reshape([-1])\n        )\n    final_cell_types = np.concatenate(final_cell_types)\n    final_cell_arr = np.concatenate(final_cell_arr)\n    return final_cell_types, final_cell_arr\ndef get_mixed_cells(vtkobj):\n    \"\"\"Create the cells dictionary from the given pyvista.UnstructuredGrid.\n    This functions creates a cells dictionary (see\n    create_mixed_cells), with a mapping vtk_type -> np.ndarray (int)\n    for fixed size cell types. The returned dictionary will have\n    arrays of size [N, D], where N is the number of cells and D is the\n    size of the cells for the given type (e.g. 3 for triangles).\n    Parameters\n    ----------\n    vtkobj : pyvista.UnstructuredGrid\n        The unstructured grid for which the cells dictionary should be computed\n    Returns\n    -------\n    dict\n        Dictionary of cells.\n    Raises\n    ------\n    ValueError\n        If vtkobj is not a pyvista.UnstructuredGrid, any of the\n        present cells are unsupported, or have dynamic cell sizes,\n        like VTK_POLYGON.\n    \"\"\"\n    from .cell_type_helper import enum_cell_type_nr_points_map\n    return_dict = {}\n    if not isinstance(vtkobj, pyvista.UnstructuredGrid):\n        raise ValueError(\"Expected a pyvista object\")\n    nr_cells = vtkobj.n_cells\n    if nr_cells == 0:\n        return None\n    cell_types = vtkobj.celltypes\n    cells = vtkobj.cells\n    unique_cell_types = np.unique(cell_types)\n    if not np.all([k in enum_cell_type_nr_points_map for k in unique_cell_types]):\n        raise ValueError(\"Found unknown or unsupported VTK cell type in the present cells\")\n    if not np.all([enum_cell_type_nr_points_map[k] > 0 for k in unique_cell_types]):\n        raise ValueError(\n            \"You requested a cell-dictionary with a variable length cell, which is not supported \"\n            \"currently\"\n        )\n    cell_sizes = np.zeros_like(cell_types)\n    for cell_type in unique_cell_types:\n        mask = cell_types == cell_type\n        cell_sizes[mask] = enum_cell_type_nr_points_map[cell_type]\n    cell_ends = np.cumsum(cell_sizes + 1)\n    cell_starts = np.concatenate([np.array([0], dtype=cell_ends.dtype), cell_ends[:-1]]) + 1\n    for cell_type in unique_cell_types:\n        cell_size = enum_cell_type_nr_points_map[cell_type]\n        mask = cell_types == cell_type\n        current_cell_starts = cell_starts[mask]\n        cells_inds = current_cell_starts[..., np.newaxis] + np.arange(cell_size)[np.newaxis].astype(\n            cell_starts.dtype\n        )\n        return_dict[cell_type] = cells[cells_inds]\n    return return_dict",
        "file_path": "pyvista/utilities/cells.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 7,
        "score": 0.6415521502494812,
        "content": "\"\"\"\n.. _ref_create_structured:\nCreating a Structured Surface\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nCreate a StructuredGrid surface from NumPy arrays\n\"\"\"\nimport numpy as np\n# sphinx_gallery_thumbnail_number = 2\nimport pyvista as pv\nfrom pyvista import examples\n###############################################################################\n# From NumPy Meshgrid\n# +++++++++++++++++++\n#\n# Create a simple meshgrid using NumPy\n# Make data\nx = np.arange(-10, 10, 0.25)\ny = np.arange(-10, 10, 0.25)\nx, y = np.meshgrid(x, y)\nr = np.sqrt(x**2 + y**2)\nz = np.sin(r)\n###############################################################################\n# Now pass the NumPy meshgrid to PyVista\n# Create and plot structured grid\ngrid = pv.StructuredGrid(x, y, z)\ngrid.plot()\n###############################################################################\n# Plot mean curvature as well\ngrid.plot_curvature(clim=[-1, 1])\n###############################################################################\n# Generating a structured grid is a one-liner in this module, and the points\n# from the resulting surface can be accessed as a NumPy array:\ngrid.points\n###############################################################################\n# From XYZ Points\n# +++++++++++++++\n#\n# Quite often, you might be given a set of coordinates (XYZ points) in a simple\n# tabular format where there exists some structure such that grid could be\n# built between the nodes you have. A great example is found in\n# `pyvista-support#16`_ where a structured grid that is rotated from the\n# cartesian reference frame is given as just XYZ points. In these cases, all\n# that is needed to recover the grid is the dimensions of the grid\n# (`nx` by `ny` by `nz`) and that the coordinates are ordered appropriately.\n#\n# .. _pyvista-support#16: https://github.com/pyvista/pyvista-support/issues/16\n#\n# For this example, we will create a small dataset and rotate the\n# coordinates such that they are not on orthogonal to cartesian reference\n# frame.\ndef make_point_set():\n    \"\"\"Ignore the contents of this function. Just know that it returns an\n    n by 3 numpy array of structured coordinates.\"\"\"\n    n, m = 29, 32\n    x = np.linspace(-200, 200, num=n) + np.random.uniform(-5, 5, size=n)\n    y = np.linspace(-200, 200, num=m) + np.random.uniform(-5, 5, size=m)\n    xx, yy = np.meshgrid(x, y)\n    A, b = 100, 100\n    zz = A * np.exp(-0.5 * ((xx / b) ** 2.0 + (yy / b) ** 2.0))\n    points = np.c_[xx.reshape(-1), yy.reshape(-1), zz.reshape(-1)]\n    foo = pv.PolyData(points)\n    foo.rotate_z(36.6, inplace=True)\n    return foo.points\n# Get the points as a 2D NumPy array (N by 3)\npoints = make_point_set()\npoints[0:5, :]\n###############################################################################\n# Now pretend that the (n by 3) NumPy array above are coordinates that you\n# have, possibly from a file with three columns of XYZ points.\n#\n# We simply need to recover the dimensions of the grid that these points make\n# and then we can generate a :class:`pyvista.StructuredGrid` mesh.\n#\n# Let's preview the points to see what we are dealing with:\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10, 10))\nplt.scatter(points[:, 0], points[:, 1], c=points[:, 2])\nplt.axis(\"image\")\nplt.xlabel(\"X Coordinate\")\nplt.ylabel(\"Y Coordinate\")\nplt.show()\n###############################################################################\n# In the figure above, we can see some inherit structure to the points and thus\n# we could connect the points as a structured grid. All we need to know are the\n# dimensions of the grid present. In this case, we know (because we made this\n# dataset) the dimensions are ``[29, 32, 1]``, but you might not know the\n# dimensions of your pointset. There are a few ways to figure out the\n# dimensionality of structured grid including:\n#\n# * manually counting the nodes along the edges of the pointset\n# * using a technique like principle component analysis to strip the rotation from the dataset and count the unique values along each axis for the new;y projected dataset.\n# Once you've figured out your grid's dimensions, simple create the\n# :class:`pyvista.StructuredGrid` as follows:\nmesh = pv.StructuredGrid()\n# Set the coordinates from the numpy array\nmesh.points = points\n# set the dimensions\nmesh.dimensions = [29, 32, 1]\n# and then inspect it\nmesh.plot(show_edges=True, show_grid=True, cpos=\"xy\")\n###############################################################################\n# Extending a 2D StructuredGrid to 3D\n# +++++++++++++++++++++++++++++++++++\n#\n# A 2D :class:`pyvista.StructuredGrid` mesh can be extended into a 3D mesh.\n# This is highly applicable when wanting to create a terrain following mesh\n# in earth science research applications.\n#\n# For example, we could have a :class:`pyvista.StructuredGrid` of a topography\n# surface and extend that surface to a few different levels and connect each\n# \"level\" to create the 3D terrain following mesh.\n#\n# Let's start with a simple example by extending the wave mesh to 3D\nstruct = examples.load_structured()\nstruct.plot(show_edges=True)\n###############################################################################\ntop = struct.points.copy()\nbottom = struct.points.copy()\nbottom[:, -1] = -10.0  # Wherever you want the plane\nvol = pv.StructuredGrid()\nvol.points = np.vstack((top, bottom))\nvol.dimensions = [*struct.dimensions[0:2], 2]\nvol.plot(show_edges=True)",
        "file_path": "examples/00-load/create-structured-surface.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 8,
        "score": 0.6414417028427124,
        "content": "\"\"\"\n.. _ref_create_explicit_structured_grid:\nCreating an Explicit Structured Grid\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nCreate an explicit structured grid from NumPy arrays.\n\"\"\"\nimport numpy as np\nimport pyvista as pv\nni, nj, nk = 4, 5, 6\nsi, sj, sk = 20, 10, 1\nxcorn = np.arange(0, (ni + 1) * si, si)\nxcorn = np.repeat(xcorn, 2)\nxcorn = xcorn[1:-1]\nxcorn = np.tile(xcorn, 4 * nj * nk)\nycorn = np.arange(0, (nj + 1) * sj, sj)\nycorn = np.repeat(ycorn, 2)\nycorn = ycorn[1:-1]\nycorn = np.tile(ycorn, (2 * ni, 2 * nk))\nycorn = np.transpose(ycorn)\nycorn = ycorn.flatten()\nzcorn = np.arange(0, (nk + 1) * sk, sk)\nzcorn = np.repeat(zcorn, 2)\nzcorn = zcorn[1:-1]\nzcorn = np.repeat(zcorn, (4 * ni * nj))\ncorners = np.stack((xcorn, ycorn, zcorn))\ncorners = corners.transpose()\ndims = np.asarray((ni, nj, nk)) + 1\ngrid = pv.ExplicitStructuredGrid(dims, corners)\ngrid = grid.compute_connectivity()\ngrid.plot(show_edges=True)",
        "file_path": "examples/00-load/create-explicit-structured-grid.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 9,
        "score": 0.6374374628067017,
        "content": "\"\"\"\nCreating a Uniform Grid\n~~~~~~~~~~~~~~~~~~~~~~~\nCreate a simple uniform grid from a 3D NumPy array of values.\n\"\"\"\nimport numpy as np\nimport pyvista as pv\n###############################################################################\n# Take a 3D NumPy array of data values that holds some spatial data where each\n# axis corresponds to the XYZ cartesian axes. This example will create a\n# :class:`pyvista.UniformGrid` object that will hold the spatial reference for\n# a 3D grid which a 3D NumPy array of values can be plotted against.\n###############################################################################\n# Create the 3D NumPy array of spatially referenced data.\n# This is spatially referenced such that the grid is 20 by 5 by 10\n# (nx by ny by nz)\nvalues = np.linspace(0, 10, 1000).reshape((20, 5, 10))\nvalues.shape\n# Create the spatial reference\ngrid = pv.UniformGrid()\n# Set the grid dimensions: shape + 1 because we want to inject our values on\n#   the CELL data\ngrid.dimensions = np.array(values.shape) + 1\n# Edit the spatial reference\ngrid.origin = (100, 33, 55.6)  # The bottom left corner of the data set\ngrid.spacing = (1, 5, 2)  # These are the cell sizes along each axis\n# Add the data values to the cell data\ngrid.cell_data[\"values\"] = values.flatten(order=\"F\")  # Flatten the array\n# Now plot the grid\ngrid.plot(show_edges=True)\n###############################################################################\n# Don't like cell data? You could also add the NumPy array to the point data of\n# a :class:`pyvista.UniformGrid`. Take note of the subtle difference when\n# setting the grid dimensions upon initialization.\n# Create the 3D NumPy array of spatially referenced data\n# This is spatially referenced such that the grid is 20 by 5 by 10\n#   (nx by ny by nz)\nvalues = np.linspace(0, 10, 1000).reshape((20, 5, 10))\nvalues.shape\n# Create the spatial reference\ngrid = pv.UniformGrid()\n# Set the grid dimensions: shape because we want to inject our values on the\n#   POINT data\ngrid.dimensions = values.shape\n# Edit the spatial reference\ngrid.origin = (100, 33, 55.6)  # The bottom left corner of the data set\ngrid.spacing = (1, 5, 2)  # These are the cell sizes along each axis\n# Add the data values to the cell data\ngrid.point_data[\"values\"] = values.flatten(order=\"F\")  # Flatten the array\n# Now plot the grid\ngrid.plot(show_edges=True)",
        "file_path": "examples/00-load/create-uniform-grid.py",
        "chunk_index": 0,
        "metadata": {}
      },
      {
        "rank": 10,
        "score": 0.6369936466217041,
        "content": "            warnings.warn(\n                'Points is not a float type. This can cause issues when '\n                'transforming or applying filters. Casting to '\n                '``np.float32``. Disable this by passing '\n                '``force_float=False``.'\n            )\n            points = points.astype(np.float32)\n    # check dimensionality\n    if points.ndim == 1:\n        points = points.reshape(-1, 3)\n    elif points.ndim > 2:\n        raise ValueError(f'Dimension of ``points`` should be 1 or 2, not {points.ndim}')\n    # verify shape\n    if points.shape[1] != 3:\n        raise ValueError(\n            'Points array must contain three values per point. '\n            f'Shape is {points.shape} and should be (X, 3)'\n        )\n    # use the underlying vtk data if present to avoid memory leaks\n    if not deep and isinstance(points, pyvista.pyvista_ndarray):\n        if points.VTKObject is not None:\n            vtk_object = points.VTKObject\n            # we can only use the underlying data if `points` is not a slice of\n            # the VTK data object\n            if vtk_object.GetSize() == points.size:\n                vtkpts = _vtk.vtkPoints()\n                vtkpts.SetData(points.VTKObject)\n                return vtkpts\n            else:\n                deep = True\n    # points must be contiguous\n    points = np.require(points, requirements=['C'])\n    vtkpts = _vtk.vtkPoints()\n    vtk_arr = _vtk.numpy_to_vtk(points, deep=deep)\n    vtkpts.SetData(vtk_arr)\n    return vtkpts\ndef line_segments_from_points(points):\n    \"\"\"Generate non-connected line segments from points.\n    Assumes points are ordered as line segments and an even number of\n    points.\n    Parameters\n    ----------\n    points : array_like[float]\n        Points representing line segments. An even number must be\n        given as every two vertices represent a single line\n        segment. For example, two line segments would be represented\n        as ``np.array([[0, 0, 0], [1, 0, 0], [1, 0, 0], [1, 1, 0]])``.\n    Returns\n    -------\n    pyvista.PolyData\n        PolyData with lines and cells.\n    Examples\n    --------\n    This example plots two line segments at right angles to each other.\n    >>> import pyvista\n    >>> import numpy as np\n    >>> points = np.array([[0, 0, 0], [1, 0, 0], [1, 0, 0], [1, 1, 0]])\n    >>> lines = pyvista.line_segments_from_points(points)\n    >>> lines.plot()\n    \"\"\"\n    if len(points) % 2 != 0:\n        raise ValueError(\"An even number of points must be given to define each segment.\")\n    # Assuming ordered points, create array defining line order\n    n_points = len(points)\n    n_lines = n_points // 2\n    lines = np.c_[\n        (\n            2 * np.ones(n_lines, np.int_),\n            np.arange(0, n_points - 1, step=2),\n            np.arange(1, n_points + 1, step=2),\n        )\n    ]\n    poly = pyvista.PolyData()\n    poly.points = points\n    poly.lines = lines\n    return poly\ndef lines_from_points(points, close=False):\n    \"\"\"Make a connected line set given an array of points.\n    Parameters\n    ----------\n    points : array_like[float]\n        Points representing the vertices of the connected\n        segments. For example, two line segments would be represented\n        as ``np.array([[0, 0, 0], [1, 0, 0], [1, 1, 0]])``.\n    close : bool, default: False\n        If ``True``, close the line segments into a loop.\n    Returns\n    -------\n    pyvista.PolyData\n        PolyData with lines and cells.\n    Examples\n    --------\n    >>> import numpy as np\n    >>> import pyvista\n    >>> points = np.array([[0, 0, 0], [1, 0, 0], [1, 1, 0]])\n    >>> poly = pyvista.lines_from_points(points)\n    >>> poly.plot(line_width=5)\n    \"\"\"\n    poly = pyvista.PolyData()\n    poly.points = points\n    cells = np.full((len(points) - 1, 3), 2, dtype=np.int_)\n    cells[:, 1] = np.arange(0, len(points) - 1, dtype=np.int_)\n    cells[:, 2] = np.arange(1, len(points), dtype=np.int_)\n    if close:\n        cells = np.append(cells, [[2, len(points) - 1, 0]], axis=0)\n    poly.lines = cells\n    return poly\ndef make_tri_mesh(points, faces):\n    \"\"\"Construct a ``pyvista.PolyData`` mesh using points and faces arrays.\n    Construct a mesh from an Nx3 array of points and an Mx3 array of\n    triangle indices, resulting in a mesh with N vertices and M\n    triangles.  This function does not require the standard VTK\n    \"padding\" column and simplifies mesh creation.\n    Parameters\n    ----------\n    points : np.ndarray\n        Array of points with shape ``(N, 3)`` storing the vertices of the\n        triangle mesh.\n    faces : np.ndarray\n        Array of indices with shape ``(M, 3)`` containing the triangle\n        indices.\n    Returns\n    -------\n    pyvista.PolyData\n        PolyData instance containing the triangle mesh.\n    Examples\n    --------\n    This example discretizes the unit square into a triangle mesh with\n    nine vertices and eight faces.\n    >>> import numpy as np\n    >>> import pyvista\n    >>> points = np.array(\n    ...     [\n    ...         [0, 0, 0],\n    ...         [0.5, 0, 0],\n    ...         [1, 0, 0],\n    ...         [0, 0.5, 0],\n    ...         [0.5, 0.5, 0],\n    ...         [1, 0.5, 0],\n    ...         [0, 1, 0],\n    ...         [0.5, 1, 0],\n    ...         [1, 1, 0],\n    ...     ]\n    ... )\n    >>> faces = np.array(\n    ...     [\n    ...         [0, 1, 4],\n    ...         [4, 7, 6],\n    ...         [2, 5, 4],\n    ...         [4, 5, 8],\n    ...         [0, 4, 3],\n    ...         [3, 4, 6],\n    ...         [1, 2, 4],\n    ...         [4, 8, 7],\n    ...     ]\n    ... )\n    >>> tri_mesh = pyvista.make_tri_mesh(points, faces)\n    >>> tri_mesh.plot(show_edges=True, line_width=5)\n    \"\"\"\n    if points.shape[1] != 3:\n        raise ValueError(\"Points array should have shape (N, 3).\")\n    if faces.ndim != 2 or faces.shape[1] != 3:\n        raise ValueError(\"Face array should have shape (M, 3).\")\n    cells = np.empty((faces.shape[0], 4), dtype=faces.dtype)\n    cells[:, 0] = 3\n    cells[:, 1:] = faces\n    return pyvista.PolyData(points, cells)\ndef vector_poly_data(orig, vec):\n    \"\"\"Create a pyvista.PolyData object composed of vectors.\n    Parameters\n    ----------\n    orig : array_like[float]\n        Array of vector origins.\n    vec : array_like[float]\n        Array of vectors.\n    Returns\n    -------\n    pyvista.PolyData\n        Mesh containing the ``orig`` points along with the\n        ``'vectors'`` and ``'mag'`` point arrays representing the\n        vectors and magnitude of the vectors at each point.\n    Examples\n    --------\n    Create basic vector field.  This is a point cloud where each point\n    has a vector and magnitude attached to it.\n    >>> import pyvista\n    >>> import numpy as np\n    >>> x, y = np.meshgrid(np.linspace(-5, 5, 10), np.linspace(-5, 5, 10))\n    >>> points = np.vstack((x.ravel(), y.ravel(), np.zeros(x.size))).T\n    >>> u = x / np.sqrt(x**2 + y**2)\n    >>> v = y / np.sqrt(x**2 + y**2)\n    >>> vectors = np.vstack(\n    ...     (u.ravel() ** 3, v.ravel() ** 3, np.zeros(u.size))\n    ... ).T\n    >>> pdata = pyvista.vector_poly_data(points, vectors)\n    >>> pdata.point_data.keys()\n    ['vectors', 'mag']\n    Convert these to arrows and plot it.\n    >>> pdata.glyph(orient='vectors', scale='mag').plot()\n    \"\"\"\n    # shape, dimension checking\n    if not isinstance(orig, np.ndarray):\n        orig = np.asarray(orig)\n    if not isinstance(vec, np.ndarray):\n        vec = np.asarray(vec)\n    if orig.ndim != 2:\n        orig = orig.reshape((-1, 3))\n    elif orig.shape[1] != 3:\n        raise ValueError('orig array must be 3D')\n    if vec.ndim != 2:\n        vec = vec.reshape((-1, 3))\n    elif vec.shape[1] != 3:\n        raise ValueError('vec array must be 3D')\n    # Create vtk points and cells objects\n    vpts = _vtk.vtkPoints()\n    vpts.SetData(_vtk.numpy_to_vtk(np.ascontiguousarray(orig), deep=True))\n    npts = orig.shape[0]\n    cells = np.empty((npts, 2), dtype=pyvista.ID_TYPE)\n    cells[:, 0] = 1\n    cells[:, 1] = np.arange(npts, dtype=pyvista.ID_TYPE)\n    vcells = pyvista.utilities.cells.CellArray(cells, npts)\n    # Create vtkPolyData object\n    pdata = _vtk.vtkPolyData()\n    pdata.SetPoints(vpts)\n    pdata.SetVerts(vcells)\n    # Add vectors to polydata\n    name = 'vectors'\n    vtkfloat = _vtk.numpy_to_vtk(np.ascontiguousarray(vec), deep=True)\n    vtkfloat.SetName(name)\n    pdata.GetPointData().AddArray(vtkfloat)\n    pdata.GetPointData().SetActiveVectors(name)\n    # Add magnitude of vectors to polydata\n    name = 'mag'\n    scalars = (vec * vec).sum(1) ** 0.5\n    vtkfloat = _vtk.numpy_to_vtk(np.ascontiguousarray(scalars), deep=True)\n    vtkfloat.SetName(name)\n    pdata.GetPointData().AddArray(vtkfloat)\n    pdata.GetPointData().SetActiveScalars(name)\n    return pyvista.PolyData(pdata)\ndef trans_from_matrix(matrix):  # pragma: no cover\n    \"\"\"Convert a vtk matrix to a numpy.ndarray.\n    DEPRECATED: Please use ``array_from_vtkmatrix``.\n    \"\"\"\n    # import needs to happen here to prevent a circular import\n    from pyvista.core.errors import DeprecationError\n    raise DeprecationError('DEPRECATED: Please use ``array_from_vtkmatrix``.')\ndef array_from_vtkmatrix(matrix):\n    \"\"\"Convert a vtk matrix to an array.\n    Parameters\n    ----------\n    matrix : vtk.vtkMatrix3x3 | vtk.vtkMatrix4x4\n        The vtk matrix to be converted to a ``numpy.ndarray``.\n        Returned ndarray has shape (3, 3) or (4, 4) as appropriate.\n    Returns\n    -------\n    numpy.ndarray\n        Numpy array containing the data from ``matrix``.\n    \"\"\"\n    if isinstance(matrix, _vtk.vtkMatrix3x3):\n        shape = (3, 3)\n    elif isinstance(matrix, _vtk.vtkMatrix4x4):\n        shape = (4, 4)\n    else:\n        raise TypeError(\n            'Expected vtk.vtkMatrix3x3 or vtk.vtkMatrix4x4 input,'\n            f' got {type(matrix).__name__} instead.'\n        )\n    array = np.zeros(shape)\n    for i in range(shape[0]):\n        for j in range(shape[1]):\n            array[i, j] = matrix.GetElement(i, j)\n    return array\ndef vtkmatrix_from_array(array):\n    \"\"\"Convert a ``numpy.ndarray`` or array-like to a vtk matrix.\n    Parameters\n    ----------\n    array : array_like[float]\n        The array or array-like to be converted to a vtk matrix.\n        Shape (3, 3) gets converted to a ``vtk.vtkMatrix3x3``, shape (4, 4)\n        gets converted to a ``vtk.vtkMatrix4x4``. No other shapes are valid.\n    Returns\n    -------\n    vtk.vtkMatrix3x3 or vtk.vtkMatrix4x4\n        VTK matrix.\n    \"\"\"\n    array = np.asarray(array)\n    if array.shape == (3, 3):\n        matrix = _vtk.vtkMatrix3x3()\n    elif array.shape == (4, 4):\n        matrix = _vtk.vtkMatrix4x4()\n    else:\n        raise ValueError(f'Invalid shape {array.shape}, must be (3, 3) or (4, 4).')\n    m, n = array.shape\n    for i in range(m):\n        for j in range(n):\n            matrix.SetElement(i, j, array[i, j])\n    return matrix\ndef is_meshio_mesh(obj):\n    \"\"\"Test if passed object is instance of ``meshio.Mesh``.\n    Parameters\n    ----------\n    obj\n        Any object.\n    Returns\n    -------\n    bool\n        ``True`` if ``obj`` is an ``meshio.Mesh``.\n    \"\"\"\n    try:\n        import meshio\n        return isinstance(obj, meshio.Mesh)\n    except ImportError:\n        return False\ndef wrap(dataset):\n    \"\"\"Wrap any given VTK data object to its appropriate PyVista data object.\n    Other formats that are supported include:\n    * 2D :class:`numpy.ndarray` of XYZ vertices\n    * 3D :class:`numpy.ndarray` representing a volume. Values will be scalars.\n    * 3D :class:`trimesh.Trimesh` mesh.\n    * 3D :class:`meshio.Mesh` mesh.\n    .. versionchanged:: 0.38.0\n        If the passed object is already a wrapped PyVista object, then\n        this is no-op and will return that object directly. In previous\n        versions of PyVista, this would perform a shallow copy.\n    Parameters\n    ----------\n    dataset : :class:`numpy.ndarray` | :class:`trimesh.Trimesh` | vtk.DataSet\n        Dataset to wrap.\n    Returns\n    -------\n    pyvista.DataSet\n        The PyVista wrapped dataset.\n    Examples\n    --------\n    Wrap a numpy array representing a random point cloud.\n    >>> import numpy as np\n    >>> import pyvista\n    >>> points = np.random.random((10, 3))\n    >>> cloud = pyvista.wrap(points)\n    >>> cloud  # doctest:+SKIP\n    PolyData (0x7fc52db83d70)\n      N Cells:  10\n      N Points: 10\n      X Bounds: 1.123e-01, 7.457e-01\n      Y Bounds: 1.009e-01, 9.877e-01\n      Z Bounds: 2.346e-03, 9.640e-01\n      N Arrays: 0\n    Wrap a Trimesh object.\n    >>> import trimesh\n    >>> import pyvista\n    >>> points = [[0, 0, 0], [0, 0, 1], [0, 1, 0]]\n    >>> faces = [[0, 1, 2]]\n    >>> tmesh = trimesh.Trimesh(points, faces=faces, process=False)\n    >>> mesh = pyvista.wrap(tmesh)\n    >>> mesh  # doctest:+SKIP\n    PolyData (0x7fc55ff27ad0)\n      N Cells:  1\n      N Points: 3\n      X Bounds: 0.000e+00, 0.000e+00\n      Y Bounds: 0.000e+00, 1.000e+00\n      Z Bounds: 0.000e+00, 1.000e+00\n      N Arrays: 0\n    Wrap a VTK object.\n    >>> import pyvista\n    >>> import vtk\n    >>> points = vtk.vtkPoints()\n    >>> p = [1.0, 2.0, 3.0]\n    >>> vertices = vtk.vtkCellArray()\n    >>> pid = points.InsertNextPoint(p)\n    >>> _ = vertices.InsertNextCell(1)\n    >>> _ = vertices.InsertCellPoint(pid)\n    >>> point = vtk.vtkPolyData()\n    >>> _ = point.SetPoints(points)\n    >>> _ = point.SetVerts(vertices)\n    >>> mesh = pyvista.wrap(point)\n    >>> mesh  # doctest:+SKIP\n    PolyData (0x7fc55ff27ad0)\n      N Cells:  1\n      N Points: 3\n      X Bounds: 0.000e+00, 0.000e+00\n      Y Bounds: 0.000e+00, 1.000e+00\n      Z Bounds: 0.000e+00, 1.000e+00\n      N Arrays: 0\n    \"\"\"\n    # Return if None\n    if dataset is None:\n        return\n    if isinstance(dataset, tuple(pyvista._wrappers.values())):\n        # Return object if it is already wrapped\n        return dataset\n    # Check if dataset is a numpy array.  We do this first since\n    # pyvista_ndarray contains a VTK type that we don't want to\n    # directly wrap.\n    if isinstance(dataset, (np.ndarray, pyvista.pyvista_ndarray)):\n        if dataset.ndim == 1 and dataset.shape[0] == 3:\n            return pyvista.PolyData(dataset)\n        if dataset.ndim > 1 and dataset.ndim < 3 and dataset.shape[1] == 3:\n            return pyvista.PolyData(dataset)\n        elif dataset.ndim == 3:\n            mesh = pyvista.UniformGrid(dimensions=dataset.shape)\n            if isinstance(dataset, pyvista.pyvista_ndarray):\n                # this gets rid of pesky VTK reference since we're raveling this\n                dataset = np.array(dataset, copy=False)\n            mesh['values'] = dataset.ravel(order='F')\n            mesh.active_scalars_name = 'values'\n            return mesh\n        else:\n            raise NotImplementedError('NumPy array could not be wrapped pyvista.')\n    # wrap VTK arrays as pyvista_ndarray\n    if isinstance(dataset, _vtk.vtkDataArray):\n        return pyvista.pyvista_ndarray(dataset)\n    # Check if a dataset is a VTK type\n    if hasattr(dataset, 'GetClassName'):\n        key = dataset.GetClassName()\n        try:\n            return pyvista._wrappers[key](dataset)\n        except KeyError:\n            raise TypeError(f'VTK data type ({key}) is not currently supported by pyvista.')\n        return\n    # wrap meshio\n    if is_meshio_mesh(dataset):\n        return from_meshio(dataset)\n    # wrap trimesh\n    if dataset.__class__.__name__ == 'Trimesh':\n        # trimesh doesn't pad faces\n        n_face = dataset.faces.shape[0]\n        faces = np.empty((n_face, 4), dataset.faces.dtype)\n        faces[:, 1:] = dataset.faces\n        faces[:, 0] = 3\n        polydata = pyvista.PolyData(np.asarray(dataset.vertices), faces)\n        # If the Trimesh object has uv, pass them to the PolyData\n        if hasattr(dataset.visual, 'uv'):\n            polydata.active_t_coords = np.asarray(dataset.visual.uv)\n        return polydata\n    # otherwise, flag tell the user we can't wrap this object\n    raise NotImplementedError(f'Unable to wrap ({type(dataset)}) into a pyvista type.')\ndef image_to_texture(image):\n    \"\"\"Convert ``vtkImageData`` (:class:`pyvista.UniformGrid`) to a ``vtkTexture``.\n    Parameters\n    ----------\n    image : pyvista.UniformGrid | vtkImageData\n        Image to convert.\n    Returns\n    -------\n    vtkTexture\n        VTK texture.\n    \"\"\"\n    return pyvista.Texture(image)\ndef numpy_to_texture(image):\n    \"\"\"Convert a NumPy image array to a vtk.vtkTexture.\n    Parameters\n    ----------\n    image : numpy.ndarray\n        Numpy image array. Texture datatype expected to be ``np.uint8``.\n    Returns\n    -------\n    pyvista.Texture\n        PyVista texture.\n    Examples\n    --------\n    Create an all white texture.\n    >>> import pyvista as pv\n    >>> import numpy as np\n    >>> tex_arr = np.ones((1024, 1024, 3), dtype=np.uint8) * 255\n    >>> tex = pv.numpy_to_texture(tex_arr)\n    \"\"\"\n    if image.dtype != np.uint8:\n        image = image.astype(np.uint8)\n        warnings.warn(\n            'Expected `image` dtype to be ``np.uint8``. `image` has been copied '\n            'and converted to np.uint8.',\n            UserWarning,\n        )\n    return pyvista.Texture(image)\ndef is_inside_bounds(point, bounds):\n    \"\"\"Check if a point is inside a set of bounds.\n    This is implemented through recursion so that this is N-dimensional.\n    Parameters\n    ----------\n    point : sequence[float]\n        Three item cartesian point (i.e. ``[x, y, z]``).\n    bounds : sequence[float]\n        Six item bounds in the form of ``(xMin, xMax, yMin, yMax, zMin, zMax)``.\n    Returns\n    -------\n    bool\n        ``True`` when ``point`` is inside ``bounds``.\n    \"\"\"\n    if isinstance(point, (int, float)):\n        point = [point]\n    if isinstance(point, (np.ndarray, collections.abc.Sequence)) and not isinstance(\n        point, collections.deque\n    ):\n        if len(bounds) < 2 * len(point) or len(bounds) % 2 != 0:\n            raise ValueError('Bounds mismatch point dimensionality')\n        point = collections.deque(point)\n        bounds = collections.deque(bounds)\n        return is_inside_bounds(point, bounds)\n    if not isinstance(point, collections.deque):\n        raise TypeError(f'Unknown input data type ({type(point)}).')",
        "file_path": "pyvista/utilities/helpers.py",
        "chunk_index": 1,
        "metadata": {}
      }
    ]
  }
}